
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.0.4">
    
    
      
        <title>Model Selection and Validation - Data Science Foundations</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.2a4617e2.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.9204c3b2.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../extra.css">
    
    <script>function __md_scope(e,t,_){return new URL(_||(t===localStorage?"..":".."),location).pathname+"."+e}function __md_get(e,t=localStorage,_){return JSON.parse(t.getItem(__md_scope(e,t,_)))}function __md_set(e,t,_=localStorage,o){try{_.setItem(__md_scope(e,_,o),JSON.stringify(t))}catch(e){}}</script>
    
      
  


  
  


  <script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-114664473-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){var e;this.value&&(e=document.location.pathname,ga("send","pageview",e+"?q="+this.value))}),"undefined"!=typeof location$&&location$.subscribe(function(e){ga("send","pageview",e.pathname)})})</script>
  <script async src="https://www.google-analytics.com/analytics.js"></script>


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#data-science-foundations-session-3-model-selection-and-validation" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Data Science Foundations" class="md-header__button md-logo" aria-label="Data Science Foundations" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Data Science Foundations
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Model Selection and Validation
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Data Science Foundations" class="md-nav__button md-logo" aria-label="Data Science Foundations" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Data Science Foundations
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../about/" class="md-nav__link">
        About
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Sessions
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Sessions" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Sessions
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S1_Regression_and_Analysis/" class="md-nav__link">
        Regression and Analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S2_Inferential_Statistics/" class="md-nav__link">
        Inferential Statistics
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Model Selection and Validation
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Model Selection and Validation
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#10-preparing-environment-and-importing-data" class="md-nav__link">
    1.0 Preparing Environment and Importing Data
  </a>
  
    <nav class="md-nav" aria-label="1.0 Preparing Environment and Importing Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-import-packages" class="md-nav__link">
    1.0.1 Import Packages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#102-load-dataset" class="md-nav__link">
    1.0.2 Load Dataset
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-model-validation" class="md-nav__link">
    1.1 Model Validation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#110-k-nearest-neighbors" class="md-nav__link">
    1.1.0 K-Nearest Neighbors
  </a>
  
    <nav class="md-nav" aria-label="1.1.0 K-Nearest Neighbors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111-holdout-sets" class="md-nav__link">
    1.1.1 Holdout Sets
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#112-data-leakage-and-cross-validation" class="md-nav__link">
    1.1.2 Data Leakage and Cross-Validation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#113-bias-variance-tradeoff" class="md-nav__link">
    1.1.3 Bias-Variance Tradeoff
  </a>
  
    <nav class="md-nav" aria-label="1.1.3 Bias-Variance Tradeoff">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exercise-1-quantitatively-define-performance" class="md-nav__link">
    üèãÔ∏è Exercise 1: Quantitatively Define Performance
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#114-learning-curves" class="md-nav__link">
    1.1.4 Learning Curves
  </a>
  
    <nav class="md-nav" aria-label="1.1.4 Learning Curves">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1141-considering-model-complexity" class="md-nav__link">
    1.1.4.1 Considering Model Complexity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1142-considering-training-set-size" class="md-nav__link">
    1.1.4.2 Considering Training Set Size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-2-visualization" class="md-nav__link">
    üèãÔ∏è Exercise 2: Visualization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-model-validation-in-practice" class="md-nav__link">
    1.2 Model Validation in Practice
  </a>
  
    <nav class="md-nav" aria-label="1.2 Model Validation in Practice">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121-grid-search" class="md-nav__link">
    1.2.1 Grid Search
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S4_Feature_Engineering/" class="md-nav__link">
        Feature Engineering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S5_Unsupervised_Learning/" class="md-nav__link">
        Unsupervised Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S6_Bagging/" class="md-nav__link">
        Bagging
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S7_Boosting/" class="md-nav__link">
        Boosting
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Exercises
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Exercises" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Exercises
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exercises/E1_Descriptive_Statistics_Data_Hunt/" class="md-nav__link">
        Descriptive Statistics Data Hunt
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exercises/E2_Inferential_Statistics_Data_Hunt/" class="md-nav__link">
        Inferential Statisitcs Data Hunt
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exercises/E3_Feature_Engineering/" class="md-nav__link">
        Practice with Feature Engineering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exercises/E4_Supervised_Learners/" class="md-nav__link">
        Practice with Supervised Learners
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exercises/E5_Writing_Unit_Tests/" class="md-nav__link">
        Practice with Writing Unit Tests
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Project
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Project" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Project
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../project/P1_Statistical_Analysis_of_TicTacToe/" class="md-nav__link">
        Statistical Analysis of TicTacToe
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../project/P2_Heuristical_TicTacToe_Agents/" class="md-nav__link">
        Heuristical TicTacToe Agents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../project/P3_1_Step_Look_Ahead_Agents/" class="md-nav__link">
        1-Step Look Ahead Agents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../project/P4_N_Step_Look_Ahead_Agents/" class="md-nav__link">
        N-Step Look Ahead Agents
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#10-preparing-environment-and-importing-data" class="md-nav__link">
    1.0 Preparing Environment and Importing Data
  </a>
  
    <nav class="md-nav" aria-label="1.0 Preparing Environment and Importing Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-import-packages" class="md-nav__link">
    1.0.1 Import Packages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#102-load-dataset" class="md-nav__link">
    1.0.2 Load Dataset
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-model-validation" class="md-nav__link">
    1.1 Model Validation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#110-k-nearest-neighbors" class="md-nav__link">
    1.1.0 K-Nearest Neighbors
  </a>
  
    <nav class="md-nav" aria-label="1.1.0 K-Nearest Neighbors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111-holdout-sets" class="md-nav__link">
    1.1.1 Holdout Sets
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#112-data-leakage-and-cross-validation" class="md-nav__link">
    1.1.2 Data Leakage and Cross-Validation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#113-bias-variance-tradeoff" class="md-nav__link">
    1.1.3 Bias-Variance Tradeoff
  </a>
  
    <nav class="md-nav" aria-label="1.1.3 Bias-Variance Tradeoff">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exercise-1-quantitatively-define-performance" class="md-nav__link">
    üèãÔ∏è Exercise 1: Quantitatively Define Performance
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#114-learning-curves" class="md-nav__link">
    1.1.4 Learning Curves
  </a>
  
    <nav class="md-nav" aria-label="1.1.4 Learning Curves">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1141-considering-model-complexity" class="md-nav__link">
    1.1.4.1 Considering Model Complexity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1142-considering-training-set-size" class="md-nav__link">
    1.1.4.2 Considering Training Set Size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-2-visualization" class="md-nav__link">
    üèãÔ∏è Exercise 2: Visualization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-model-validation-in-practice" class="md-nav__link">
    1.2 Model Validation in Practice
  </a>
  
    <nav class="md-nav" aria-label="1.2 Model Validation in Practice">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121-grid-search" class="md-nav__link">
    1.2.1 Grid Search
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                

<p><a href="https://colab.research.google.com/github/wesleybeckner/data_science_foundations/blob/main/notebooks/S3_Model_Selection_and_Validation.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<h1 id="data-science-foundations-session-3-model-selection-and-validation">Data Science Foundations, Session 3: Model Selection and Validation</h1>
<p><strong>Instructor</strong>: Wesley Beckner</p>
<p><strong>Contact</strong>: wesleybeckner@gmail.com</p>
<hr />
<p><br></p>
<p>At the end of session 1, we saw the basic recipe for creating a supervised machine learning model:</p>
<ol>
<li>Environment setup and importing data</li>
<li>Rudimentary exploratory data analysis</li>
<li>Feature engineering</li>
<li>Choosing and training a model:</li>
<li>choose model</li>
<li>choose hyperparameters</li>
<li>fit using (training) data</li>
<li>predict using (validation) data</li>
</ol>
<p>In session 1, I chose our model and hyperparameters preemptively. How did I do that? In the real world, you won't necessarily have the best intution about how to make these choices. In today's session, we will algorithmize the way we approach choosing and training a model</p>
<p>Note: I will import libraries at the beginning of this notebook, as is good practice, but will reimport them as they are used to remind ourselves where each method came from!</p>
<p><br></p>
<hr />
<p><br></p>
<p><a name='top'></a></p>
<p><a name='x.0'></a></p>
<h2 id="10-preparing-environment-and-importing-data">1.0 Preparing Environment and Importing Data</h2>
<p><a href="#top">back to top</a></p>
<p><a name='x.0.1'></a></p>
<h3 id="101-import-packages">1.0.1 Import Packages</h3>
<p><a href="#top">back to top</a></p>
<pre><code class="language-python"># Pandas library for the pandas dataframes
import pandas as pd
import numpy as np
import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import random
import scipy.stats as stats
from patsy import dmatrices
from statsmodels.stats.outliers_influence import variance_inflation_factor

from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn import metrics
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.datasets import load_iris
</code></pre>
<p><a name='x.0.2'></a></p>
<h3 id="102-load-dataset">1.0.2 Load Dataset</h3>
<p><a href="#top">back to top</a></p>
<p>In course 1 we cursorily discussed why we may need strategies for validating our model. Here we'll discuss it more in depth. </p>
<p>I'm going to take a simple example. In the following, I have a dataset that contains some data about flowers. It's a very famous dataset used in the ML world</p>
<pre><code class="language-python">iris = load_iris()
X = iris.data
y = iris.target
</code></pre>
<pre><code class="language-python">print(X.shape)
print(y.shape)
</code></pre>
<pre><code>(150, 4)
(150,)
</code></pre>
<p>let's go ahead and load our wine dataset as well...</p>
<pre><code class="language-python">wine = pd.read_csv(&quot;https://raw.githubusercontent.com/wesleybeckner/&quot;\
      &quot;ds_for_engineers/main/data/wine_quality/winequalityN.csv&quot;)
</code></pre>
<pre><code class="language-python">wine.dropna(inplace=True)
wine['quality_label'] = wine['quality'].apply(lambda x: 'low' if x &lt;=5 else
                                              'med' if x &lt;= 7 else 'high')
class_tp = {'red': 0, 'white': 1}
y_tp = wine['type'].map(class_tp)
wine['type_encoding'] = y_tp

class_ql = {'low':0, 'med': 1, 'high': 2}
y_ql = wine['quality_label'].map(class_ql)
wine['quality_encoding'] = y_ql

wine.columns = wine.columns.str.replace(' ', '_')
</code></pre>
<p><a name='1.1'></a></p>
<h2 id="11-model-validation">1.1 Model Validation</h2>
<p><a href="#top">back to top</a></p>
<p><em>doing it the wrong way</em><br></p>
<p>While we're here, I'm going to introduce a VERY SIMPLE supervised learning method called K-Nearest Neighbors.</p>
<p><a name='x.1.0'></a></p>
<h2 id="110-k-nearest-neighbors">1.1.0 K-Nearest Neighbors</h2>
<p><a href="#top">back to top</a></p>
<p>K-Nearest Neighbors is perhaps the simplest algorithm of them all. It is essentially a lookup table: We select the hyperparameter K, and when assigning a new value a data label, assign it according to, the majority label in the vicinity of the new datapoint. The vicinity being determined by K, the number of nearest neighbors we are going to assess.</p>
<pre><code class="language-python">knn = KNeighborsRegressor(n_neighbors=1)
knn.fit(X,y)
</code></pre>
<pre><code>KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=None, n_neighbors=1, p=2,
                    weights='uniform')
</code></pre>
<pre><code class="language-python">knn.score(X,y)
</code></pre>
<pre><code>1.0
</code></pre>
<p>Wow! we achieved a model with a perfect score! But is this really how we would expect the model to perform on data it had never seen before? Probably not. How do we actually check the performance of our model?</p>
<p><a name='1.1.1'></a></p>
<h3 id="111-holdout-sets">1.1.1 Holdout Sets</h3>
<p><a href="#top">back to top</a></p>
<p>The way we account for unseen data, in practice, is to leave a portion of the dataset out for testing. This way, we can estimate how our model will perform on entirely new data it may come across in application.</p>
<pre><code class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_state=42)
knn = KNeighborsRegressor(n_neighbors=1)
knn.fit(X_train,y_train)
print(knn.score(X_test, y_test))
</code></pre>
<pre><code>0.9753593429158111
</code></pre>
<p>We see that we get a more reasonable value for our performance!</p>
<p><a name='1.1.2'></a></p>
<h3 id="112-data-leakage-and-cross-validation">1.1.2 Data Leakage and Cross-Validation</h3>
<p><a href="#top">back to top</a></p>
<p>An even more rigorous method to leaving out a single test set, is to perform cross validation. Imagine a situation where we are trying to estimate the best value of K in our KNN algorithm. If we continually train our model with new values of K on our training set, and test with our testing set, "knowledge" of our test set values with leak into our model, as we choose the best value for K based on how it performs on our test set (even though we did not train on this test set). We call this phenomenon <em>data leakage</em>. CV or Cross Validation overcomes this by only evaluating our parameters with our training set.</p>
<p align="center">
<img src='https://scikit-learn.org/stable/_images/grid_search_workflow.png' width=500px></img>

<small>[image src](https://scikit-learn.org/stable/modules/cross_validation.html)</small>
</p>

<p>In this scheme, we don't evaluate our model on the test set until the very end. Rather, we estimate our hyperparameter performances by slicing the training set into cross folds</p>
<p align="center">
<img src='https://scikit-learn.org/stable/_images/grid_search_cross_validation.png' width=500px></img>

<small>[image src](https://scikit-learn.org/stable/modules/cross_validation.html)</small>
</p>

<pre><code class="language-python">from sklearn.model_selection import cross_val_score

scores = cross_val_score(knn, X_train, y_train, cv=5)
</code></pre>
<pre><code class="language-python">scores
</code></pre>
<pre><code>array([0.91666667, 0.81725888, 0.85714286, 1.        , 0.91      ])
</code></pre>
<pre><code class="language-python">print(&quot;%0.2f accuracy with a standard deviation of %0.3f&quot; % (scores.mean(), scores.std()))
</code></pre>
<pre><code>0.90 accuracy with a standard deviation of 0.062
</code></pre>
<p>More information on the cross_val_score method in sklearn can be found <a href="https://scikit-learn.org/stable/modules/cross_validation.html">here</a></p>
<p>An additional topic on cross validation is the extreme leave-one-out validation, you can read more about that <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#Model-validation-via-cross-validation">here</a></p>
<p><a name='1.1.3'></a></p>
<h3 id="113-bias-variance-tradeoff">1.1.3 Bias-Variance Tradeoff</h3>
<p><a href="#top">back to top</a></p>
<p>This next concept will be most easily understood (imo) if we go ahead an make up some data ourselves, I'm going to do that now.</p>
<pre><code class="language-python"># we can throttle the error rate
err = .5
random.seed(42)

# our data has a KNOWN underlying functional form (log(x))
def func(x, err):
  return np.log(x) + err * random.randint(-1,1) * random.random()
x = np.arange(20,100)
y = [func(t, err) for t in x]
plt.plot(x,y, ls='', marker='.')
plt.xlabel('X')
plt.ylabel('Y')
</code></pre>
<pre><code>Text(0, 0.5, 'Y')
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_25_1.png" /></p>
<p>Let's fit to just a portion of this data</p>
<pre><code class="language-python">random.seed(42)
X_train = random.sample(list(x), 10)

indices = [list(x).index(i) for i in X_train]
# we could also do it this way: np.argwhere([i in X_train for i in x])

y_train = [y[i] for i in indices]
</code></pre>
<pre><code class="language-python">plt.plot(X_train,y_train, ls='', marker='.')
</code></pre>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x7f3ea2189f10&gt;]
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_28_1.png" /></p>
<p>Now let's take two extreme scenarios, fitting a linear line and a high order polynomial, to these datapoints. Keeping in mind the larger dataset, as well as the error we introduced in our data generating function, this will really illustrate our point!</p>
<pre><code class="language-python"># solving our training data with a n-degree polynomial
coefs = np.polyfit(X_train, y_train, 9)

# solve the slope and intercept of our 1-degree polynomial ;)
model = LinearRegression()
model.fit(np.array(X_train).reshape(-1,1), y_train)

# create some x data to plot our functions
X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1)

fig, ax = plt.subplots(1,2,figsize=(15,5))
ax[0].plot(X_seq, model.predict(X_seq), c='grey', ls='--')
ax[0].plot(X_train, y_train, ls='', marker='.')
ax[0].set_ylim(min(y_train), max(y_train))
ax[0].set_title(&quot;High Bias Model&quot;)


ax[1].plot(X_seq, np.polyval(coefs, X_seq), c='grey', ls='--')
ax[1].plot(X_train, y_train, ls='', marker='.')
ax[1].set_ylim(min(y_train), max(y_train))
ax[1].set_title(&quot;High Variance Model&quot;)
</code></pre>
<pre><code>Text(0.5, 1.0, 'High Variance Model')
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_30_1.png" /></p>
<p>We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has <em>high bias</em> because we are forcing the functional form without much consideration to the underlying data &mdash; we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has <em>low variance</em> with respect to the underlying data. </p>
<p>On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has <em>low bias</em> because we don't introduce many constraints on the final form of the model. it is <em>high variance</em> because depending on the underlying training data, the final outcome of the model can change quite drastically!</p>
<p>In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further:</p>
<ol>
<li>what happens when we retrain these models on different samples of the data population</li>
<li>and let's use this to better understand what we mean by <em>bias</em> and <em>variance</em></li>
<li>what happens when we tie this back in with the error we introduced to the data generator?</li>
<li>and let's use this to better understand irreducible error</li>
</ol>
<pre><code class="language-python">random.seed(42)
fig, ax = plt.subplots(1,2,figsize=(15,5))
for samples in range(5):
  X_train = random.sample(list(x), 10)
  indices = [list(x).index(i) for i in X_train]
  y_train = [y[i] for i in indices]

  # solving our training data with a n-degree polynomial
  coefs = np.polyfit(X_train, y_train, 9)

  # solve the slope and intercept of our 1-degree polynomial ;)
  model = LinearRegression()
  model.fit(np.array(X_train).reshape(-1,1), y_train)

  # create some x data to plot our functions
  X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1)


  ax[0].plot(X_seq, model.predict(X_seq), alpha=0.5, ls='--')
  ax[0].plot(X_train, y_train, ls='', marker='.')
  ax[0].set_ylim(min(y_train), max(y_train))
  ax[0].set_title(&quot;High Bias Model&quot;)


  ax[1].plot(X_seq, np.polyval(coefs, X_seq), alpha=0.5, ls='--')
  ax[1].plot(X_train, y_train, ls='', marker='.')
  ax[1].set_ylim(min(y_train), max(y_train))
  ax[1].set_title(&quot;High Variance Model&quot;)
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_32_0.png" /></p>
<p>As we can see, depending on what data we train our model on, the <em>high bias</em> model changes relatively slightly, while the <em>high variance</em> model changes a whole awful lot!</p>
<p>The <em>high variance</em> model is prone to something we call <em>overfitting</em>. It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the <em>population</em></p>
<pre><code class="language-python"># solving our training data with a n-degree polynomial
coefs = np.polyfit(X_train, y_train, 9)

# solve the slope and intercept of our 1-degree polynomial ;)
model = LinearRegression()
model.fit(np.array(X_train).reshape(-1,1), y_train)

# create some x data to plot our functions
X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1)

fig, ax = plt.subplots(1,2,figsize=(15,5))
ax[0].plot(X_seq, model.predict(X_seq), c='grey', ls='--')
ax[0].plot(x, y, ls='', marker='*', alpha=0.6)
ax[0].plot(X_train, y_train, ls='', marker='.')
ax[0].set_ylim(min(y), max(y))
ax[0].set_title(&quot;High Bias Model&quot;)


ax[1].plot(X_seq, np.polyval(coefs, X_seq), c='grey', ls='--')
ax[1].plot(x, y, ls='', marker='*', alpha=0.6)
ax[1].plot(X_train, y_train, ls='', marker='.')
ax[1].set_ylim(min(y), max(y))
ax[1].set_title(&quot;High Variance Model&quot;)
</code></pre>
<pre><code>Text(0.5, 1.0, 'High Variance Model')
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_34_1.png" /></p>
<p>In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance models are particuarly prone to the phenomenon of <em>over fitting</em> and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this.</p>
<pre><code class="language-python">x = np.arange(20,100)
y = [func(t, err=0) for t in x]
plt.plot(x,y, ls='', marker='.')
</code></pre>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x7f3ea05cbc90&gt;]
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_36_1.png" /></p>
<pre><code class="language-python">random.seed(42)
X_train = random.sample(list(x), 10)

indices = [list(x).index(i) for i in X_train]
# we could also do it this way: np.argwhere([i in X_train for i in x])

y_train = [y[i] for i in indices]

# solving our training data with a n-degree polynomial
coefs = np.polyfit(X_train, y_train, 9)

# solve the slope and intercept of our 1-degree polynomial ;)
model = LinearRegression()
model.fit(np.array(X_train).reshape(-1,1), y_train)

# create some x data to plot our functions
X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1)

fig, ax = plt.subplots(1,2,figsize=(15,5))
ax[0].plot(X_seq, model.predict(X_seq), c='grey', ls='--')
ax[0].plot(x, y, ls='', marker='o', alpha=0.2)
ax[0].plot(X_train, y_train, ls='', marker='.')
ax[0].set_ylim(min(y), max(y))
ax[0].set_title(&quot;High Bias Model&quot;)


ax[1].plot(X_seq, np.polyval(coefs, X_seq), c='grey', ls='--')
ax[1].plot(x, y, ls='', marker='o', alpha=0.2)
ax[1].plot(X_train, y_train, ls='', marker='.')
ax[1].set_ylim(min(y), max(y))
ax[1].set_title(&quot;High Variance Model&quot;)
</code></pre>
<pre><code>Text(0.5, 1.0, 'High Variance Model')
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_37_1.png" /></p>
<p>This time, our high variance model really <em>gets it</em>! And this is because the data we trained on actually <em>is</em> a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. </p>
<p>I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the <em>bias variance tradeoff</em></p>
<h4 id="exercise-1-quantitatively-define-performance">üèãÔ∏è Exercise 1: Quantitatively Define Performance</h4>
<p>Up until now, we've explored this idea of bias variance tradeoff from a qualitative standpoint. As an exercise, continue with this idea, this time calculating the mean squared error (MSE) and R-square between the model and UNSEEN (non-training data) population data. </p>
<p>error metrics
* <code>r2_score</code> 
* <code>mean_squared_error</code></p>
<p>Do this for a 9th order polynomial and repeat for population data with low, med, and high degrees of error and small and large training fractions. Complete the chart below.</p>
<table>
<thead>
<tr>
<th>error</th>
<th>training fraction</th>
<th>MSE</th>
<th>R2</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Code Cell for Exercise 1
from sklearn.preprocessing import PolynomialFeatures
random.seed(42)

# function to generate data
def func(x, err):
  return np.log(x) + err * random.randint(-1,1) * random.random()
x = np.arange(20,100)

################################################################################
########## CHANGE ERR TO CHANGE THE AMOUNT OF NOISE IN YOUR DATA ###############
################################################################################
err = &lt;YOUR ERR&gt; # change the error (.001 - 1)
y_actual = [func(t, err) for t in x]


################################################################################
### SAMPLE THE DATA FOR TRAINING
################################################################################
x_train, x_test, y_train, y_test = train_test_split(x, 
                                                    y_actual,
                                                    train_size=&lt;YOUR NUMBER&gt;, # change the training size
                                                    random_state=42)

# solving our training data with a 9-degree polynomial
coefs = np.polyfit(x_train, y_train, 9)

# generate y data with 9-degree polynomial model and X_seq
y_pred = np.polyval(coefs, x_test)

################################################################################
### CALCULATE MSE AND R2
################################################################################
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# look at results
print(&quot;mean square error: {:.2f}&quot;.format(mse))
print(&quot;r2: {:.2f}&quot;.format(r2))
</code></pre>
<pre><code>mean square error: 5.20e-01
r2: -0.13
</code></pre>
<p><a name='1.1.4'></a></p>
<h3 id="114-learning-curves">1.1.4 Learning Curves</h3>
<p><a href="#top">back to top</a></p>
<p>To move from qualitative to quantitative understanding of bias-variance tradeoff we need to introduce some metric for model performance. A good one to use here is R-square, a measure of the degree to which predictions match actual values. We can import a tool from sklearn to calculate this for us.</p>
<pre><code class="language-python">from sklearn.metrics import r2_score
</code></pre>
<p><a name='x.1.4.1'></a></p>
<h4 id="1141-considering-model-complexity">1.1.4.1 Considering Model Complexity</h4>
<p><a href="#top">back to top</a></p>
<p>In a learning curve, we will typically plot the training and testing scores together, to give a sense of when we have either too much bias or too much variance in our model.</p>
<p>I'm going to go ahead and recreate the original data distribution we introduced in 1.1.3</p>
<pre><code class="language-python"># we can throttle the error rate
err = .5
random.seed(42)

# our data has a KNOWN underlying functional form (log(x))
def func(x, err):
  return np.log(x) + err * random.randint(-1,1) * random.random()
x = np.arange(20,100)
y = [func(t, err) for t in x]
plt.plot(x,y, ls='', marker='.')
</code></pre>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x7f3ea0305f90&gt;]
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_44_1.png" /></p>
<p>Now let's itteratively introduce more complexity into our model</p>
<pre><code class="language-python">random.seed(42)
fig, ax = plt.subplots(1,2,figsize=(10,5))
X_train = random.sample(list(x), 10)
indices = [list(x).index(i) for i in X_train]
y_train = [y[i] for i in indices]
ax[0].plot(X_train, y_train, ls='', marker='.', color='black')
for complexity in range(1,10):


  # solving our training data with a n-degree polynomial
  coefs = np.polyfit(X_train, y_train, complexity)

    # create some x data to plot our functions
  X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1)

  score = r2_score(np.polyval(coefs, X_train), y_train)
  ax[0].plot(X_seq, np.polyval(coefs, X_seq), alpha=0.5, ls='--')
  ax[0].set_ylim(min(y_train), max(y_train))
  ax[0].set_title(&quot;Predictions with Increasing Model Complexity&quot;)

  ax[1].plot(complexity, score, ls='', marker='.',
          label='{}-poly, {:.2f}-score'.format(complexity, score))
  ax[1].set_title(&quot;Scores with Increasing Model Complexity&quot;)
ax[1].legend()
</code></pre>
<pre><code>&lt;matplotlib.legend.Legend at 0x7f3ea0577d90&gt;
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_46_1.png" /></p>
<p>As we see from both plots, the score on the training data increases with added model complexity. Giving us the expected perfect fit when the order is the same as the number of data points! This is part I of our learning curve. Part II consists of plotting the training data score with the testing data score.</p>
<p>Something else I'm going to do, is define the training portion of the data as a fraction of the overall population size. To keep the comparisons the same as up until now, I will keep this training fraction low at .2</p>
<pre><code class="language-python">random.seed(42)

# defining my training fraction
training_frac = .2

# create test and training data
X_train = random.sample(list(x), int(int(len(x))*training_frac))
train_indices = [list(x).index(i) for i in X_train]
y_train = [y[i] for i in train_indices]
test_indices = [i for i in range(len(x)) if i not in train_indices]
X_test = [x[i] for i in test_indices]
y_test = [y[i] for i in test_indices]

# initialize the plot and display the data
fig, ax = plt.subplots(1,2,figsize=(10,5))
ax[0].plot(X_train, y_train, ls='', marker='.', color='black')
ax[0].plot(X_test, y_test, ls='', marker='.', color='grey', alpha=0.5)
for complexity in range(1,10):

  # solving our training data with a n-degree polynomial
  coefs = np.polyfit(X_train, y_train, complexity)

  # create some x data to plot our functions
  X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1)

  score1 = r2_score(np.polyval(coefs, X_train), y_train)
  score2 = r2_score(np.polyval(coefs, X_test), y_test)
  ax[0].plot(X_seq, np.polyval(coefs, X_seq), alpha=0.5, ls='--',
             label='{}-poly, {:.2f}-score'.format(complexity, score2))
  ax[0].set_ylim(min(y_train), max(y_train))
  ax[0].set_title(&quot;Predictions with Increasing Model Complexity&quot;)

  ax[1].plot(complexity, score1, ls='', marker='.', color='blue',
          label='{}-poly, {:.2f}-score'.format(complexity, score1))
  ax[1].plot(complexity, score2, ls='', marker='o', color='red',
          label='{}-poly, {:.2f}-score'.format(complexity, score2))
  ax[1].set_title(&quot;Scores with Increasing Model Complexity&quot;)

ax[1].legend(['Train $R^2$', 'Test $R^2$'])
ax[0].legend()
</code></pre>
<pre><code>&lt;matplotlib.legend.Legend at 0x7f3ea0278850&gt;
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_48_1.png" /></p>
<p>As we can see, The 2nd order polynomial achieves the greatest best test set data \(R^2\), while the highest order polynomial achieves the best training set data \(R^2\). This learning curve is explanative of what we see generally, namely a divergence after some degree of complexity between training and test set performances. In this case, we would resolve to choose the 2nd order polynomial as the best model for our data.</p>
<p><img src="https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.03-validation-curve.png" width=500px></img></p>
<p><small><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html">img src</a></small></p>
<p><a name='x.1.4.2'></a></p>
<h4 id="1142-considering-training-set-size">1.1.4.2 Considering Training Set Size</h4>
<p><a href="#top">back to top</a></p>
<p>The last piece of the puzzle we require, to fully cover learning curves, is the effect of training data size on the model. This is why I introduced the 'fraction of training data' parameter earlier. Let's explore.</p>
<pre><code class="language-python">random.seed(42)
# initialize the plot and display the data
fig, ax = plt.subplots(1,1,figsize=(10,5))
for training_frac in np.linspace(0.1,.9,50):
  # create test and training data
  X_train = random.sample(list(x), int(int(len(x))*training_frac))
  indices = [list(x).index(i) for i in X_train]
  y_train = [y[i] for i in indices]
  test_indices = [i for i in range(len(x)) if i not in indices]
  X_test = [x[i] for i in test_indices]
  y_test = [y[i] for i in test_indices]

  # solving our training data with a n-degree polynomial
  coefs = np.polyfit(X_train, y_train, 9)

  score1 = r2_score(np.polyval(coefs, X_train), y_train)
  score2 = r2_score(np.polyval(coefs, X_test), y_test)

  ax.plot(training_frac, score1, ls='', marker='.', color='blue',
          label='{}-poly, {:.2f}-score'.format(training_frac, score1))
  ax.plot(training_frac, score2, ls='', marker='o', color='red',
          label='{}-poly, {:.2f}-score'.format(training_frac, score2))
  ax.set_title(&quot;9th-order Polynomial Score with Increasing Training Set Size&quot;)
ax.legend(['Train','Test'])
ax.set_xlabel('Training Fraction')
ax.set_ylabel('$R^2$')
</code></pre>
<pre><code>/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: RankWarning: Polyfit may be poorly conditioned
  exec(code_obj, self.user_global_ns, self.user_ns)
/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: RankWarning: Polyfit may be poorly conditioned
  exec(code_obj, self.user_global_ns, self.user_ns)





Text(0, 0.5, '$R^2$')
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_51_2.png" /></p>
<p>What we see here is a trend that happens generally, as our amount of training data increases, our models handle more complexity. This is illustrated below. </p>
<p><img src="https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.03-learning-curve.png" width=500px></img></p>
<p><small><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html">img src</a></p>
<h4 id="exercise-2-visualization">üèãÔ∏è Exercise 2: Visualization</h4>
<p>Starting with the code below, make a side-by-side plot of a 3rd degree polynomial and a 12th degree polynomial. On the x axis slowly increase the training set size, on the y axis plot the scores for the training and test sets.</p>
<pre><code class="language-python"># Code Cell for Exercise 2
random.seed(42)

# create the figure and axes
fig, ax = plt.subplots(1,1,figsize=(10,5))

for training_frac in np.linspace(0.1,.9,50):

  # create test and training data
  x_train, x_test, y_train, y_test = train_test_split(x, 
                                                    y_actual,
                                                    train_size=training_frac, 
                                                    random_state=42)

  # solving our training data with a n-degree polynomial
  coefs = np.polyfit(x_train, y_train, 9)

  # recording the scores for the training and test sets
  score1 = r2_score(np.polyval(coefs, x_train), y_train)
  score2 = r2_score(np.polyval(coefs, x_test), y_test)

  ax.plot(training_frac, score1, ls='', marker='.', color='blue',
          label='{}-poly, {:.2f}-score'.format(training_frac, score1))
  ax.plot(training_frac, score2, ls='', marker='o', color='red',
          label='{}-poly, {:.2f}-score'.format(training_frac, score2))
  ax.set_title(&quot;9th-order Polynomial Score with Increasing Training Set Size&quot;)

ax.legend(['Train','Test'])
ax.set_xlabel('Training Fraction')
ax.set_ylabel('$R^2$')
</code></pre>
<pre><code>Text(0, 0.5, '$R^2$')
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_54_1.png" /></p>
<blockquote>
<p>As a visualization exercise, how would you attempt to combine the ideas of model performance with increasing training set size and increasing model complexity? Could you create this visualization with something other than a polynomial model?</p>
</blockquote>
<p><a name='1.2'></a></p>
<h2 id="12-model-validation-in-practice">1.2 Model Validation in Practice</h2>
<p><a href="#top">back to top</a></p>
<p>We will now turn our attention to practical implementation.</p>
<p>In practice, there are a wide number of variables (called hyperparameters) to consider when choosing a model. Scikit learn has a useful method called Grid Search that will iterate through every possible combination of a range of hyperparameter settings you provide as input. </p>
<p>Before we get started with grid search, we'll need to switch over from our numpy polynomial fit method to one in sklearn. Here, the caveat is our actual model will solve for the <em>coefficients</em> infront of the polynomials. We will <em>engineer</em> the polynomial features ourselves. This is an example of <em>feature engineering</em> which we will revisit in depth in a later session.</p>
<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

def PolynomialRegression(degree=2, **kwargs):
    return make_pipeline(PolynomialFeatures(degree),
                         LinearRegression(**kwargs))
</code></pre>
<p><a name='1.2.1'></a></p>
<h3 id="121-grid-search">1.2.1 Grid Search</h3>
<p><a href="#top">back to top</a></p>
<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

param_grid = {'polynomialfeatures__degree': np.arange(10),
              'linearregression__fit_intercept': [True, False],
              'linearregression__normalize': [True, False]}

grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)
</code></pre>
<pre><code class="language-python"># create test and training data
random.seed(42)
X_train = random.sample(list(x), int(int(len(x))*.8))
indices = [list(x).index(i) for i in X_train]
y_train = [y[i] for i in indices]
test_indices = [i for i in range(len(x)) if i not in indices]
X_test = [x[i] for i in test_indices]
y_test = [y[i] for i in test_indices]
</code></pre>
<pre><code class="language-python">grid.fit(np.array(X_train).reshape(-1,1), y_train)
</code></pre>
<pre><code>GridSearchCV(cv=7, error_score=nan,
             estimator=Pipeline(memory=None,
                                steps=[('polynomialfeatures',
                                        PolynomialFeatures(degree=2,
                                                           include_bias=True,
                                                           interaction_only=False,
                                                           order='C')),
                                       ('linearregression',
                                        LinearRegression(copy_X=True,
                                                         fit_intercept=True,
                                                         n_jobs=None,
                                                         normalize=False))],
                                verbose=False),
             iid='deprecated', n_jobs=None,
             param_grid={'linearregression__fit_intercept': [True, False],
                         'linearregression__normalize': [True, False],
                         'polynomialfeatures__degree': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])},
             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
             scoring=None, verbose=0)
</code></pre>
<pre><code class="language-python">grid.best_params_
</code></pre>
<pre><code>{'linearregression__fit_intercept': True,
 'linearregression__normalize': False,
 'polynomialfeatures__degree': 3}
</code></pre>
<p>to grab the best model from the CV/search outcome. we use grid.best_estimator</p>
<pre><code class="language-python">model = grid.best_estimator_

# create some x data to plot our functions
X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1)

fig, ax = plt.subplots(1,1,figsize=(15,5))
ax.plot(X_seq, model.predict(X_seq), c='grey', ls='--')
ax.plot(x, y, ls='', marker='*', alpha=0.6)
ax.plot(X_train, y_train, ls='', marker='.')
ax.set_ylim(min(y), max(y))
ax.set_title(&quot;Best Grid Search CV Model&quot;)
</code></pre>
<pre><code>Text(0.5, 1.0, 'Best Grid Search CV Model')
</code></pre>
<p><img alt="png" src="../S3_Model_Selection_and_Validation_files/S3_Model_Selection_and_Validation_64_1.png" /></p>
<p><a name='reference'></a></p>
<h1 id="references">References</h1>
<p><a href="#top">back to top</a></p>
<h2 id="model-validation">Model Validation</h2>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/cross_validation.html">cross_val_score</a></li>
<li><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#Model-validation-via-cross-validation">leave-one-out</a></li>
</ul>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../S2_Inferential_Statistics/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Inferential Statistics" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Inferential Statistics
            </div>
          </div>
        </a>
      
      
        
        <a href="../S4_Feature_Engineering/" class="md-footer__link md-footer__link--next" aria-label="Next: Feature Engineering" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Feature Engineering
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.ca141e46.min.js"}</script>
    
    
      <script src="../assets/javascripts/bundle.6baa0517.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>