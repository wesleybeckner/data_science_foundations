
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.1.4">
    
    
      
        <title>Regression and Analysis - Data Science Foundations</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.bb3983ee.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.e6a45f82.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../extra.css">
    
      <link rel="stylesheet" href="../styles.css">
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  


  <script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-114664473-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){var e;this.value&&(e=document.location.pathname,ga("send","pageview",e+"?q="+this.value))}),"undefined"!=typeof location$&&location$.subscribe(function(e){ga("send","pageview",e.pathname)})})</script>
  <script async src="https://www.google-analytics.com/analytics.js"></script>


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#data-science-foundations-session-1-regression-and-analysis" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Data Science Foundations" class="md-header__button md-logo" aria-label="Data Science Foundations" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Data Science Foundations
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Regression and Analysis
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Data Science Foundations" class="md-nav__button md-logo" aria-label="Data Science Foundations" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Data Science Foundations
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        About
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../about/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Sessions
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Sessions" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Sessions
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Regression and Analysis
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Regression and Analysis
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#10-preparing-environment-and-importing-data" class="md-nav__link">
    1.0 Preparing Environment and Importing Data
  </a>
  
    <nav class="md-nav" aria-label="1.0 Preparing Environment and Importing Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-import-packages" class="md-nav__link">
    1.0.1 Import Packages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#102-load-dataset" class="md-nav__link">
    1.0.2 Load Dataset
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-what-is-regression" class="md-nav__link">
    1.1 What is regression?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-linear-regression-fitting-with-scikit-learn" class="md-nav__link">
    1.2  Linear regression fitting with scikit-learn
  </a>
  
    <nav class="md-nav" aria-label="1.2  Linear regression fitting with scikit-learn">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exercise-1-rudimentary-eda" class="md-nav__link">
    🏋️ Exercise 1: rudimentary EDA
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-1-discussion-around-eda-plot" class="md-nav__link">
    🙋 Question 1: Discussion Around EDA Plot
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122-visualizing-the-data-set-motivating-regression-analysis" class="md-nav__link">
    1.2.2 Visualizing the data set - motivating regression analysis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#123-estimating-the-regression-coefficients" class="md-nav__link">
    1.2.3 Estimating the regression coefficients
  </a>
  
    <nav class="md-nav" aria-label="1.2.3 Estimating the regression coefficients">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-2-linear-regression-loss-function" class="md-nav__link">
    🙋 Question 2: linear regression loss function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-2-drop-null-values-and-practice-pandas-operations" class="md-nav__link">
    🏋️ Exercise 2: drop Null Values (and practice pandas operations)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-3-why-do-we-drop-null-values-across-both-columns" class="md-nav__link">
    🙋 Question 3: why do we drop null values across both columns?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-3-calculating-y_pred" class="md-nav__link">
    🏋️ Exercise 3: calculating y_pred
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-error-and-topics-of-model-fitting-assessing-model-accuracy" class="md-nav__link">
    1.3 Error and topics of model fitting (assessing model accuracy)
  </a>
  
    <nav class="md-nav" aria-label="1.3 Error and topics of model fitting (assessing model accuracy)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#131-measuring-the-quality-of-fit" class="md-nav__link">
    1.3.1 Measuring the quality of fit
  </a>
  
    <nav class="md-nav" aria-label="1.3.1 Measuring the quality of fit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1311-mean-squared-error" class="md-nav__link">
    1.3.1.1 Mean Squared Error
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1312-r-square" class="md-nav__link">
    1.3.1.2 R-square
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-4-lets-understand-r2" class="md-nav__link">
    🙋 Question 4: lets understand \(R^2\)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#132-corollaries-with-classification-models" class="md-nav__link">
    1.3.2 Corollaries with classification models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#133-beyond-a-single-input-feature" class="md-nav__link">
    1.3.3 Beyond a single input feature
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14-multivariate-regression" class="md-nav__link">
    1.4 Multivariate regression
  </a>
  
    <nav class="md-nav" aria-label="1.4 Multivariate regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#141-linear-regression-with-all-input-fields" class="md-nav__link">
    1.4.1 Linear regression with all input fields
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-4-evaluate-the-error" class="md-nav__link">
    🏋️ Exercise 4: evaluate the error
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-5-make-a-plot-of-y-actual-vs-y-predicted" class="md-nav__link">
    🏋️ Exercise 5: make a plot of y actual vs y predicted
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#142-enrichment-splitting-into-train-and-test-sets" class="md-nav__link">
    🍒 1.4.2 Enrichment: Splitting into train and test sets
  </a>
  
    <nav class="md-nav" aria-label="🍒 1.4.2 Enrichment: Splitting into train and test sets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1421-other-data-considerations" class="md-nav__link">
    1.4.2.1 Other data considerations
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#143-enrichment-other-regression-algorithms" class="md-nav__link">
    🍒 1.4.3 Enrichment: Other regression algorithms
  </a>
  
    <nav class="md-nav" aria-label="🍒 1.4.3 Enrichment: Other regression algorithms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exercise-6-tune-hyperparameter-for-ridge-regression" class="md-nav__link">
    🏋️ Exercise 6: Tune Hyperparameter for Ridge Regression
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-enrichment-additional-regression-exercises" class="md-nav__link">
    🍒 1.5 Enrichment: Additional Regression Exercises
  </a>
  
    <nav class="md-nav" aria-label="🍒 1.5 Enrichment: Additional Regression Exercises">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-1-number-and-choice-of-input-features" class="md-nav__link">
    Problem 1) Number and choice of input features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-2-type-of-regression-algorithm" class="md-nav__link">
    Problem 2) Type of regression algorithm
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S2_Inferential_Statistics/" class="md-nav__link">
        Inferential Statistics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S3_Model_Selection_and_Validation/" class="md-nav__link">
        Model Selection and Validation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S4_Feature_Engineering/" class="md-nav__link">
        Feature Engineering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S5_Unsupervised_Learning/" class="md-nav__link">
        Unsupervised Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S6_Bagging/" class="md-nav__link">
        Bagging
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S7_Boosting/" class="md-nav__link">
        Boosting
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Exercises
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Exercises" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Exercises
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exercises/E1_Descriptive_Statistics_Data_Hunt/" class="md-nav__link">
        Descriptive Statistics Data Hunt
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exercises/E2_Inferential_Statistics_Data_Hunt/" class="md-nav__link">
        Inferential Statisitcs Data Hunt
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exercises/E3_Feature_Engineering/" class="md-nav__link">
        Practice with Feature Engineering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exercises/E4_Supervised_Learners/" class="md-nav__link">
        Practice with Supervised Learners
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exercises/E5_Writing_Unit_Tests/" class="md-nav__link">
        Practice with Writing Unit Tests
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Project
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Project" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Project
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../project/P1_Statistical_Analysis_of_TicTacToe/" class="md-nav__link">
        Statistical Analysis of TicTacToe
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../project/P2_Heuristical_TicTacToe_Agents/" class="md-nav__link">
        Heuristical TicTacToe Agents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../project/P3_1_Step_Look_Ahead_Agents/" class="md-nav__link">
        1-Step Look Ahead Agents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../project/P4_N_Step_Look_Ahead_Agents/" class="md-nav__link">
        N-Step Look Ahead Agents
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#10-preparing-environment-and-importing-data" class="md-nav__link">
    1.0 Preparing Environment and Importing Data
  </a>
  
    <nav class="md-nav" aria-label="1.0 Preparing Environment and Importing Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-import-packages" class="md-nav__link">
    1.0.1 Import Packages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#102-load-dataset" class="md-nav__link">
    1.0.2 Load Dataset
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-what-is-regression" class="md-nav__link">
    1.1 What is regression?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-linear-regression-fitting-with-scikit-learn" class="md-nav__link">
    1.2  Linear regression fitting with scikit-learn
  </a>
  
    <nav class="md-nav" aria-label="1.2  Linear regression fitting with scikit-learn">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exercise-1-rudimentary-eda" class="md-nav__link">
    🏋️ Exercise 1: rudimentary EDA
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-1-discussion-around-eda-plot" class="md-nav__link">
    🙋 Question 1: Discussion Around EDA Plot
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122-visualizing-the-data-set-motivating-regression-analysis" class="md-nav__link">
    1.2.2 Visualizing the data set - motivating regression analysis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#123-estimating-the-regression-coefficients" class="md-nav__link">
    1.2.3 Estimating the regression coefficients
  </a>
  
    <nav class="md-nav" aria-label="1.2.3 Estimating the regression coefficients">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-2-linear-regression-loss-function" class="md-nav__link">
    🙋 Question 2: linear regression loss function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-2-drop-null-values-and-practice-pandas-operations" class="md-nav__link">
    🏋️ Exercise 2: drop Null Values (and practice pandas operations)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-3-why-do-we-drop-null-values-across-both-columns" class="md-nav__link">
    🙋 Question 3: why do we drop null values across both columns?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-3-calculating-y_pred" class="md-nav__link">
    🏋️ Exercise 3: calculating y_pred
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-error-and-topics-of-model-fitting-assessing-model-accuracy" class="md-nav__link">
    1.3 Error and topics of model fitting (assessing model accuracy)
  </a>
  
    <nav class="md-nav" aria-label="1.3 Error and topics of model fitting (assessing model accuracy)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#131-measuring-the-quality-of-fit" class="md-nav__link">
    1.3.1 Measuring the quality of fit
  </a>
  
    <nav class="md-nav" aria-label="1.3.1 Measuring the quality of fit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1311-mean-squared-error" class="md-nav__link">
    1.3.1.1 Mean Squared Error
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1312-r-square" class="md-nav__link">
    1.3.1.2 R-square
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-4-lets-understand-r2" class="md-nav__link">
    🙋 Question 4: lets understand \(R^2\)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#132-corollaries-with-classification-models" class="md-nav__link">
    1.3.2 Corollaries with classification models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#133-beyond-a-single-input-feature" class="md-nav__link">
    1.3.3 Beyond a single input feature
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14-multivariate-regression" class="md-nav__link">
    1.4 Multivariate regression
  </a>
  
    <nav class="md-nav" aria-label="1.4 Multivariate regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#141-linear-regression-with-all-input-fields" class="md-nav__link">
    1.4.1 Linear regression with all input fields
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-4-evaluate-the-error" class="md-nav__link">
    🏋️ Exercise 4: evaluate the error
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-5-make-a-plot-of-y-actual-vs-y-predicted" class="md-nav__link">
    🏋️ Exercise 5: make a plot of y actual vs y predicted
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#142-enrichment-splitting-into-train-and-test-sets" class="md-nav__link">
    🍒 1.4.2 Enrichment: Splitting into train and test sets
  </a>
  
    <nav class="md-nav" aria-label="🍒 1.4.2 Enrichment: Splitting into train and test sets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1421-other-data-considerations" class="md-nav__link">
    1.4.2.1 Other data considerations
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#143-enrichment-other-regression-algorithms" class="md-nav__link">
    🍒 1.4.3 Enrichment: Other regression algorithms
  </a>
  
    <nav class="md-nav" aria-label="🍒 1.4.3 Enrichment: Other regression algorithms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exercise-6-tune-hyperparameter-for-ridge-regression" class="md-nav__link">
    🏋️ Exercise 6: Tune Hyperparameter for Ridge Regression
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-enrichment-additional-regression-exercises" class="md-nav__link">
    🍒 1.5 Enrichment: Additional Regression Exercises
  </a>
  
    <nav class="md-nav" aria-label="🍒 1.5 Enrichment: Additional Regression Exercises">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-1-number-and-choice-of-input-features" class="md-nav__link">
    Problem 1) Number and choice of input features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-2-type-of-regression-algorithm" class="md-nav__link">
    Problem 2) Type of regression algorithm
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                

<p><a href="https://colab.research.google.com/github/wesleybeckner/data_science_foundations/blob/main/notebooks/S1_Regression_and_Analysis.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<h1 id="data-science-foundations-session-1-regression-and-analysis">Data Science Foundations Session 1: Regression and Analysis<a class="headerlink" href="#data-science-foundations-session-1-regression-and-analysis" title="Permanent link">&para;</a></h1>
<p><strong>Instructor</strong>: Wesley Beckner</p>
<p><strong>Contact</strong>: <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#119;&#101;&#115;&#108;&#101;&#121;&#98;&#101;&#99;&#107;&#110;&#101;&#114;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#119;&#101;&#115;&#108;&#101;&#121;&#98;&#101;&#99;&#107;&#110;&#101;&#114;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a></p>
<hr />
<p><br></p>
<p>In this session we will look at fitting data to a curve using <strong>regression</strong>. We will also look at using regression to make <strong>predictions</strong> for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error.</p>
<p><br></p>
<hr />
<p><a name='x.0'></a></p>
<h2 id="10-preparing-environment-and-importing-data">1.0 Preparing Environment and Importing Data<a class="headerlink" href="#10-preparing-environment-and-importing-data" title="Permanent link">&para;</a></h2>
<p><a href="#top">back to top</a></p>
<p><a name='x.0.1'></a></p>
<h3 id="101-import-packages">1.0.1 Import Packages<a class="headerlink" href="#101-import-packages" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Import pandas, pyplot, ipywidgets</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>  
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span>

<span class="c1"># Import Scikit-Learn library for the regression models</span>
<span class="kn">import</span> <span class="nn">sklearn</span>         
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>

<span class="c1"># for enrichment topics</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</code></pre></div>

<h3 id="102-load-dataset">1.0.2 Load Dataset<a class="headerlink" href="#102-load-dataset" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>For our discussion on regression and descriptive statistics today we will use a well known dataset of different wines and their quality ratings</p>
<div class="codehilite"><pre><span></span><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/wesleybeckner/&quot;</span>\
                 <span class="s2">&quot;ds_for_engineers/main/data/wine_quality/winequalityN.csv&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>(6497, 13)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>type</th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>white</td>
      <td>7.0</td>
      <td>0.27</td>
      <td>0.36</td>
      <td>20.7</td>
      <td>0.045</td>
      <td>45.0</td>
      <td>170.0</td>
      <td>1.0010</td>
      <td>3.00</td>
      <td>0.45</td>
      <td>8.8</td>
      <td>6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>white</td>
      <td>6.3</td>
      <td>0.30</td>
      <td>0.34</td>
      <td>1.6</td>
      <td>0.049</td>
      <td>14.0</td>
      <td>132.0</td>
      <td>0.9940</td>
      <td>3.30</td>
      <td>0.49</td>
      <td>9.5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>white</td>
      <td>8.1</td>
      <td>0.28</td>
      <td>0.40</td>
      <td>6.9</td>
      <td>0.050</td>
      <td>30.0</td>
      <td>97.0</td>
      <td>0.9951</td>
      <td>3.26</td>
      <td>0.44</td>
      <td>10.1</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>white</td>
      <td>7.2</td>
      <td>0.23</td>
      <td>0.32</td>
      <td>8.5</td>
      <td>0.058</td>
      <td>47.0</td>
      <td>186.0</td>
      <td>0.9956</td>
      <td>3.19</td>
      <td>0.40</td>
      <td>9.9</td>
      <td>6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>white</td>
      <td>7.2</td>
      <td>0.23</td>
      <td>0.32</td>
      <td>8.5</td>
      <td>0.058</td>
      <td>47.0</td>
      <td>186.0</td>
      <td>0.9956</td>
      <td>3.19</td>
      <td>0.40</td>
      <td>9.9</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="11-what-is-regression">1.1 What is regression?<a class="headerlink" href="#11-what-is-regression" title="Permanent link">&para;</a></h2>
<p>It is the process of finding a relationship between <strong><em>dependent</em></strong> and <strong><em>independent</em></strong> variables to find trends in data. This abstract definition means that you have one variable (the dependent variable) which depends on one or more variables (the independent variables). One of the reasons for which we want to regress data is to understand whether there is a trend between two variables. </p>
<p><strong>Housing Prices Example</strong></p>
<p>We can imagine this scenario with housing prices. Envision a <strong><em>mixed</em></strong> dataset of <strong><em>continuous</em></strong> and <strong><em>discrete</em></strong> independent variables. Some features could be continuous, floating point values like location ranking and housing condition. Others could be discrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a <strong><em>regression</em></strong> model.</p>
<p align=center>
<img src="https://raw.githubusercontent.com/wesleybeckner/technology_explorers/main/assets/machine_learning/ML3.png" width=1000px></img>
</p>

<h2 id="12-linear-regression-fitting-with-scikit-learn">1.2  Linear regression fitting with scikit-learn<a class="headerlink" href="#12-linear-regression-fitting-with-scikit-learn" title="Permanent link">&para;</a></h2>
<h4 id="exercise-1-rudimentary-eda">🏋️ Exercise 1: rudimentary EDA<a class="headerlink" href="#exercise-1-rudimentary-eda" title="Permanent link">&para;</a></h4>
<p>What does the data look like? Recall how to visualize data in a pandas dataframe </p>
<ul>
<li>for every column calculate the:<ul>
<li>skew: <code>df.skew()</code></li>
<li>kurtosis: <code>df.kurtosis()</code></li>
<li>pearsons correlation with the dependent variable: <code>df.corr()</code></li>
<li>number of missing entries <code>df.isnull()</code></li>
</ul>
</li>
<li>and organize this into a new dataframe</li>
</ul>
<p><em>note:</em> pearsons is just one type of correlation, another correlation available to us is <strong><em>spearman</em></strong> which differs from pearsons in that it depends on ranked values rather than their direct quantities, you can read more <a href="https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/">here</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>type                     0
fixed acidity           10
volatile acidity         8
citric acid              3
residual sugar           2
chlorides                2
free sulfur dioxide      0
total sulfur dioxide     0
density                  0
pH                       9
sulphates                4
alcohol                  0
quality                  0
dtype: int64
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># Cell for Exercise 1</span>

<span class="c1"># part A </span>
<span class="c1"># using df.&lt;method&gt; define the following four variables with the results from</span>
<span class="c1"># skew(), kurtosis(), corr() (while selecting for quality), and isnull()</span>
<span class="c1"># for isnull() you&#39;ll notice the return is a dataframe of booleans. we would</span>
<span class="c1"># like to simply know the number of null values for each column. change the</span>
<span class="c1"># return of isnull() using the sum() method along the columns</span>

<span class="c1"># skew =</span>
<span class="c1"># kurt =</span>
<span class="c1"># pear =</span>
<span class="c1"># null =</span>

<span class="c1"># part B</span>
<span class="c1"># on line 13, put these results in a list using square brackets and call </span>
<span class="c1"># pd.DataFrame on the list to make your new DataFrame! store it under the</span>
<span class="c1"># variable name dff</span>



<span class="c1"># part C</span>
<span class="c1"># take the transpose of this DataFrame using dff.T. reassign dff to this copy</span>



<span class="c1"># part D</span>
<span class="c1"># set the column names to &#39;skew&#39;, &#39;kurtosis&#39;, &#39;pearsons _quality&#39;, and </span>
<span class="c1"># &#39;null count&#39; using dff.columns</span>



<span class="c1"># Now return dff to the output to view your hand work</span>
<span class="c1"># dff # uncomment this line</span>
</code></pre></div>

<p>I have gone ahead and repeated this exercise with the red vs white wine types:</p>
<div class="codehilite"><pre><span></span><code><span class="n">red</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;red&#39;</span><span class="p">]</span>
<span class="n">wht</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;white&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">get_summary</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
  <span class="n">skew</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">skew</span><span class="p">()</span>
  <span class="n">kurt</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">kurtosis</span><span class="p">()</span>
  <span class="n">pear</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span>
  <span class="n">null</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
  <span class="n">med</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">median</span><span class="p">()</span>
  <span class="n">men</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

  <span class="n">dff</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">skew</span><span class="p">,</span> <span class="n">kurt</span><span class="p">,</span> <span class="n">pear</span><span class="p">,</span> <span class="n">null</span><span class="p">,</span> <span class="n">med</span><span class="p">,</span> <span class="n">men</span><span class="p">])</span>
  <span class="n">dff</span> <span class="o">=</span> <span class="n">dff</span><span class="o">.</span><span class="n">T</span>
  <span class="n">dff</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;skew&#39;</span><span class="p">,</span> <span class="s1">&#39;kurtosis&#39;</span><span class="p">,</span> <span class="s1">&#39;pearsons _quality&#39;</span><span class="p">,</span> <span class="s1">&#39;null count&#39;</span><span class="p">,</span> <span class="s1">&#39;median&#39;</span><span class="p">,</span>
                <span class="s1">&#39;mean&#39;</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">dff</span>

<span class="n">dffr</span> <span class="o">=</span> <span class="n">get_summary</span><span class="p">(</span><span class="n">red</span><span class="p">)</span>
<span class="n">dffw</span> <span class="o">=</span> <span class="n">get_summary</span><span class="p">(</span><span class="n">wht</span><span class="p">)</span>

<span class="n">desc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">dffr</span><span class="p">,</span> <span class="n">dffw</span><span class="p">],</span> <span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;white&#39;</span><span class="p">])</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>/tmp/ipykernel_1419/2387423026.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with &#39;numeric_only=None&#39;) is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.
  skew = df.skew()
/tmp/ipykernel_1419/2387423026.py:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with &#39;numeric_only=None&#39;) is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.
  kurt = df.kurtosis()
/tmp/ipykernel_1419/2387423026.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with &#39;numeric_only=None&#39;) is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.
  med = df.median()
/tmp/ipykernel_1419/2387423026.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with &#39;numeric_only=None&#39;) is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.
  men = df.mean()
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">desc</span>
</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>skew</th>
      <th>kurtosis</th>
      <th>pearsons _quality</th>
      <th>null count</th>
      <th>median</th>
      <th>mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="13" valign="top">red</th>
      <th>fixed acidity</th>
      <td>0.982192</td>
      <td>1.132624</td>
      <td>0.123834</td>
      <td>2.0</td>
      <td>7.90000</td>
      <td>8.322104</td>
    </tr>
    <tr>
      <th>volatile acidity</th>
      <td>0.672862</td>
      <td>1.226846</td>
      <td>-0.390858</td>
      <td>1.0</td>
      <td>0.52000</td>
      <td>0.527738</td>
    </tr>
    <tr>
      <th>citric acid</th>
      <td>0.317891</td>
      <td>-0.788476</td>
      <td>0.226917</td>
      <td>1.0</td>
      <td>0.26000</td>
      <td>0.271145</td>
    </tr>
    <tr>
      <th>residual sugar</th>
      <td>4.540655</td>
      <td>28.617595</td>
      <td>0.013732</td>
      <td>0.0</td>
      <td>2.20000</td>
      <td>2.538806</td>
    </tr>
    <tr>
      <th>chlorides</th>
      <td>5.680347</td>
      <td>41.715787</td>
      <td>-0.128907</td>
      <td>0.0</td>
      <td>0.07900</td>
      <td>0.087467</td>
    </tr>
    <tr>
      <th>free sulfur dioxide</th>
      <td>1.250567</td>
      <td>2.023562</td>
      <td>-0.050656</td>
      <td>0.0</td>
      <td>14.00000</td>
      <td>15.874922</td>
    </tr>
    <tr>
      <th>total sulfur dioxide</th>
      <td>1.515531</td>
      <td>3.809824</td>
      <td>-0.185100</td>
      <td>0.0</td>
      <td>38.00000</td>
      <td>46.467792</td>
    </tr>
    <tr>
      <th>density</th>
      <td>0.071288</td>
      <td>0.934079</td>
      <td>-0.174919</td>
      <td>0.0</td>
      <td>0.99675</td>
      <td>0.996747</td>
    </tr>
    <tr>
      <th>pH</th>
      <td>0.194803</td>
      <td>0.814690</td>
      <td>-0.057094</td>
      <td>2.0</td>
      <td>3.31000</td>
      <td>3.310864</td>
    </tr>
    <tr>
      <th>sulphates</th>
      <td>2.429115</td>
      <td>11.712632</td>
      <td>0.251685</td>
      <td>2.0</td>
      <td>0.62000</td>
      <td>0.658078</td>
    </tr>
    <tr>
      <th>alcohol</th>
      <td>0.860829</td>
      <td>0.200029</td>
      <td>0.476166</td>
      <td>0.0</td>
      <td>10.20000</td>
      <td>10.422983</td>
    </tr>
    <tr>
      <th>quality</th>
      <td>0.217802</td>
      <td>0.296708</td>
      <td>1.000000</td>
      <td>0.0</td>
      <td>6.00000</td>
      <td>5.636023</td>
    </tr>
    <tr>
      <th>type</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="13" valign="top">white</th>
      <th>fixed acidity</th>
      <td>0.647981</td>
      <td>2.176560</td>
      <td>-0.114032</td>
      <td>8.0</td>
      <td>6.80000</td>
      <td>6.855532</td>
    </tr>
    <tr>
      <th>volatile acidity</th>
      <td>1.578595</td>
      <td>5.095526</td>
      <td>-0.194976</td>
      <td>7.0</td>
      <td>0.26000</td>
      <td>0.278252</td>
    </tr>
    <tr>
      <th>citric acid</th>
      <td>1.284217</td>
      <td>6.182036</td>
      <td>-0.009194</td>
      <td>2.0</td>
      <td>0.32000</td>
      <td>0.334250</td>
    </tr>
    <tr>
      <th>residual sugar</th>
      <td>1.076601</td>
      <td>3.469536</td>
      <td>-0.097373</td>
      <td>2.0</td>
      <td>5.20000</td>
      <td>6.393250</td>
    </tr>
    <tr>
      <th>chlorides</th>
      <td>5.023412</td>
      <td>37.560847</td>
      <td>-0.210181</td>
      <td>2.0</td>
      <td>0.04300</td>
      <td>0.045778</td>
    </tr>
    <tr>
      <th>free sulfur dioxide</th>
      <td>1.406745</td>
      <td>11.466342</td>
      <td>0.008158</td>
      <td>0.0</td>
      <td>34.00000</td>
      <td>35.308085</td>
    </tr>
    <tr>
      <th>total sulfur dioxide</th>
      <td>0.390710</td>
      <td>0.571853</td>
      <td>-0.174737</td>
      <td>0.0</td>
      <td>134.00000</td>
      <td>138.360657</td>
    </tr>
    <tr>
      <th>density</th>
      <td>0.977773</td>
      <td>9.793807</td>
      <td>-0.307123</td>
      <td>0.0</td>
      <td>0.99374</td>
      <td>0.994027</td>
    </tr>
    <tr>
      <th>pH</th>
      <td>0.458402</td>
      <td>0.532552</td>
      <td>0.098858</td>
      <td>7.0</td>
      <td>3.18000</td>
      <td>3.188203</td>
    </tr>
    <tr>
      <th>sulphates</th>
      <td>0.977361</td>
      <td>1.589847</td>
      <td>0.053690</td>
      <td>2.0</td>
      <td>0.47000</td>
      <td>0.489835</td>
    </tr>
    <tr>
      <th>alcohol</th>
      <td>0.487342</td>
      <td>-0.698425</td>
      <td>0.435575</td>
      <td>0.0</td>
      <td>10.40000</td>
      <td>10.514267</td>
    </tr>
    <tr>
      <th>quality</th>
      <td>0.155796</td>
      <td>0.216526</td>
      <td>1.000000</td>
      <td>0.0</td>
      <td>6.00000</td>
      <td>5.877909</td>
    </tr>
    <tr>
      <th>type</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">my_fig</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="n">desc</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
  <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">desc</span><span class="p">[</span><span class="n">metric</span><span class="p">])</span><span class="o">.</span><span class="n">unstack</span><span class="p">()[</span><span class="n">metric</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;barh&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># interact(my_fig)</span>
</code></pre></div>

<h4 id="question-1-discussion-around-eda-plot">🙋 Question 1: Discussion Around EDA Plot<a class="headerlink" href="#question-1-discussion-around-eda-plot" title="Permanent link">&para;</a></h4>
<p>What do we think of this plot?</p>
<blockquote>
<p><code>metric = mean</code>, the cholrides values <br>
<code>metric = kurtosis</code>, residual sugar <br>
<code>metric = pearsons _quality</code>, <em>magnitudes</em> and <em>directions</em> <br>
How to improve the plot, what other plots would we like to see?</p>
</blockquote>
<p>For instance, what if we were really curious about the high kurtosis for chlorides content? What more would we like to glean about the distribution of chloride content?</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># we can use df.describe() to take a look at the quantile values and min/max</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;chlorides&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>count    6495.000000
mean        0.056042
std         0.035036
min         0.009000
25%         0.038000
50%         0.047000
75%         0.065000
max         0.611000
Name: chlorides, dtype: float64
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># and see how these values appear in a KDE</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;chlorides&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;kde&#39;</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">.61</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>(0.0, 0.61)
</code></pre></div>

<p><img alt="png" src="../S1_Regression_and_Analysis_files/S1_Regression_and_Analysis_22_1.png" /></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># lastly we may want to look at the raw values themselves. We can sort them </span>
<span class="c1"># too view outliers</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;chlorides&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)[:</span><span class="mi">50</span><span class="p">]</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>5156    0.611
5049    0.610
5004    0.467
4979    0.464
5590    0.422
6268    0.415
6270    0.415
5652    0.415
6217    0.414
5949    0.414
5349    0.413
6158    0.403
4981    0.401
5628    0.387
6063    0.369
4915    0.368
5067    0.360
5179    0.358
484     0.346
5189    0.343
4917    0.341
5124    0.337
4940    0.332
1217    0.301
687     0.290
4473    0.271
5079    0.270
6272    0.267
5138    0.263
1865    0.255
5466    0.250
1034    0.244
5674    0.243
5675    0.241
683     0.240
1638    0.239
5045    0.236
6456    0.235
6468    0.230
5465    0.226
5464    0.226
5564    0.222
2186    0.217
5996    0.216
6333    0.214
5206    0.214
6332    0.214
5205    0.213
4497    0.212
1835    0.211
Name: chlorides, dtype: float64
</code></pre></div>

<h3 id="122-visualizing-the-data-set-motivating-regression-analysis">1.2.2 Visualizing the data set - motivating regression analysis<a class="headerlink" href="#122-visualizing-the-data-set-motivating-regression-analysis" title="Permanent link">&para;</a></h3>
<p>In order to demonstrate simple linear regression with this dataset we will look at two particular features: <code>fixed acidity</code> and <code>density</code>.</p>
<p>We can create a scatter plot of <code>fixed acidity</code> vs <code>density</code> for the red wine in the dataset using <code>df.plot()</code> and see that there appears to be a general trend between the two features:</p>
<div class="codehilite"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;red&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;fixed acidity&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> 
                                 <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>&lt;AxesSubplot:xlabel=&#39;fixed acidity&#39;&gt;
</code></pre></div>

<p><img alt="png" src="../S1_Regression_and_Analysis_files/S1_Regression_and_Analysis_26_1.png" /></p>
<p>Now the question is: How do we quantify this trend?</p>
<h3 id="123-estimating-the-regression-coefficients">1.2.3 Estimating the regression coefficients<a class="headerlink" href="#123-estimating-the-regression-coefficients" title="Permanent link">&para;</a></h3>
<p>It looks like density increases with fixed acidity following a line, maybe something like</p>
<p>
<script type="math/tex; mode=display">y(x)= m \cdot x + b  \;\;\;\;\;\;\;\; \sf{eq. 1}</script>
</p>
<p>with \( y=\sf density \), \(x=\sf fixed \space acidity\), and \(m\) the slope and \(b\) the intercept. </p>
<p>To solve the problem, we need to find the values of \(b\) and \(m\) in equation 1 to best fit the data. This is called <strong>linear regression</strong>.</p>
<p>In linear regression our goal is to minimize the error between computed values of positions \(y^{\sf calc}(x_i)\equiv y^{\sf calc}_i\) and known values \(y^{\sf exact}(x_i)\equiv y^{\sf exact}_i\), i.e. find \(b\) and \(m\) which lead to lowest value of</p>
<p>
<script type="math/tex; mode=display">\epsilon (m,b) =SS_{\sf res}=\sum_{i=1}^{N}\left(y^{\sf exact}_i - y^{\sf calc}_i\right)^2 = \sum_{i=1}^{N}\left(y^{\sf exact}_i - m\cdot x_i - b \right)^2\;\;\;\;\;\;\;\;\;\;\;\sf{eq. 2}</script>
</p>
<p>Otherwise known as the <strong>residual sum of squares</strong></p>
<p>To find out more see e.g. <a href="https://en.wikipedia.org/wiki/Simple_linear_regression">https://en.wikipedia.org/wiki/Simple_linear_regression</a></p>
<h4 id="question-2-linear-regression-loss-function">🙋 Question 2: linear regression loss function<a class="headerlink" href="#question-2-linear-regression-loss-function" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Do we always want <em>m</em> and <em>b</em> to be large positive numbers so as to minimize eq. 2? </p>
</blockquote>
<p>Luckily <a href="https://scikit-learn.org/stable/">scikit-learn</a> contains many functions related to regression including <a href="https://scikit-learn.org/stable/modules/linear_model.html">linear regression</a>. </p>
<p>The function we will use is called <code> LinearRegression() </code>. </p>
<div class="codehilite"><pre><span></span><code># Create linear regression object
model = linear_model.LinearRegression()

# Use model to fit to the data, the x values are densities and the y values are fixed acidity
# Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets)
x = red[&#39;density&#39;].values.reshape(-1, 1)
y = red[&#39;fixed acidity&#39;].values.reshape(-1, 1)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># Create linear regression object</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Use model to fit to the data, the x values are densities and the y values are fixed acidity</span>
<span class="c1"># Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">red</span><span class="p">[</span><span class="s1">&#39;density&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">red</span><span class="p">[</span><span class="s1">&#39;fixed acidity&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>print(red[&#39;density&#39;].values.shape, red[&#39;fixed acidity&#39;].values.shape)
print(x.shape, y.shape)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">red</span><span class="p">[</span><span class="s1">&#39;density&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">red</span><span class="p">[</span><span class="s1">&#39;fixed acidity&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>(1599,) (1599,)
(1599, 1) (1599, 1)
</code></pre></div>

<div class="codehilite"><pre><span></span><code># Fit to the data
model.fit(x, y)

# Extract the values of interest
m = model.coef_[0][0]
b = model.intercept_[0]

# Print the slope m and intercept b
print(&#39;Scikit learn - Slope: &#39;, m , &#39;Intercept: &#39;, b )
</code></pre></div>

<p>What happens when we try to fit the data as is?</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Fit to the data</span>
<span class="c1"># model.fit(x, y)</span>
</code></pre></div>

<h4 id="exercise-2-drop-null-values-and-practice-pandas-operations">🏋️ Exercise 2: drop Null Values (and practice pandas operations)<a class="headerlink" href="#exercise-2-drop-null-values-and-practice-pandas-operations" title="Permanent link">&para;</a></h4>
<p>Let's look back at our dataset description dataframe above, what do we notice, what contains null values?</p>
<p>There are several strategies for dealing with null values. For now let's take the simplest case, and drop rows in our dataframe that contain null</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Cell for Exercise 2</span>
<span class="c1"># For this templated exercise you are going to complete everything in one line </span>
<span class="c1"># of code, but we are going to break it up into steps. So for each part (A, B,</span>
<span class="c1"># etc.) paste your answer from the previous part to begin (your opertaions will</span>
<span class="c1"># read from left to right)</span>

<span class="c1"># step A</span>
<span class="c1"># select the &#39;density&#39; and &#39;fixed acidity&#39; columns of red. make sure the return</span>
<span class="c1"># is a dataframe</span>

<span class="c1"># step B</span>
<span class="c1"># now use the dropna() method on axis 0 (the rows) to drop any null values </span>

<span class="c1"># step B</span>
<span class="c1"># select column &#39;density&#39;</span>

<span class="c1"># step C</span>
<span class="c1"># select the values</span>

<span class="c1"># step D</span>
<span class="c1"># reshape the result with an empty second dimension using .reshape() and store</span>
<span class="c1"># the result under variable x</span>

<span class="c1"># repeat the same process with &#39;fixed acidity&#39; and variable y</span>
</code></pre></div>

<p>Now that we have our x and y arrays we can fit using ScikitLearn</p>
<div class="codehilite"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">red</span><span class="p">[[</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="s1">&#39;fixed acidity&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="s1">&#39;density&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">red</span><span class="p">[[</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="s1">&#39;fixed acidity&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="s1">&#39;fixed acidity&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<h4 id="question-3-why-do-we-drop-null-values-across-both-columns">🙋 Question 3: why do we drop null values across both columns?<a class="headerlink" href="#question-3-why-do-we-drop-null-values-across-both-columns" title="Permanent link">&para;</a></h4>
<p>Notice in the above cell how we selected both <code>density</code> and <code>fixed acidity</code> before calling <code>dropna</code>? Why did we do that? Why didn't we just select <code>density</code> in the <code>x</code> variable case and <code>fixed acidity</code> in the <code>y</code> variable case?</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Fit to the data</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Extract the values of interest</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Print the slope m and intercept b</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Scikit learn - Slope: &#39;</span><span class="p">,</span> <span class="n">m</span> <span class="p">,</span> <span class="s1">&#39;Intercept: &#39;</span><span class="p">,</span> <span class="n">b</span> <span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Scikit learn - Slope:  616.01314280661 Intercept:  -605.6880086750523
</code></pre></div>

<h4 id="exercise-3-calculating-y_pred">🏋️ Exercise 3: calculating y_pred<a class="headerlink" href="#exercise-3-calculating-y_pred" title="Permanent link">&para;</a></h4>
<p>Estimate the values of \(y\) by using your fitted parameters. Hint: Use your <code>model.coef_</code> and <code>model.intercept_</code> parameters to estimate y_pred following equation 1</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># define y_pred in terms of m, x, and b</span>
<span class="c1"># y_pred = </span>

<span class="c1"># uncomment the following lines!</span>
<span class="c1"># fig, ax = plt.subplots(1,1, figsize=(10,10))</span>
<span class="c1"># ax.plot(x, y_pred, ls=&#39;&#39;, marker=&#39;*&#39;)</span>
<span class="c1"># ax.plot(x, y, ls=&#39;&#39;, marker=&#39;.&#39;)</span>
</code></pre></div>

<p>We can also return predictions directly with the model object using the predict() method</p>
<blockquote>
<p>note: it is great to get in the habit of utilizing model outputs this way, as the API will be similar across all scikit-learn models (and sometimes models in other libraries as well!)</p>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># Another way to get this is using the model.predict function</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>[&lt;matplotlib.lines.Line2D at 0x7fdd881875e0&gt;]
</code></pre></div>

<p><img alt="png" src="../S1_Regression_and_Analysis_files/S1_Regression_and_Analysis_46_1.png" /></p>
<h2 id="13-error-and-topics-of-model-fitting-assessing-model-accuracy">1.3 Error and topics of model fitting (assessing model accuracy)<a class="headerlink" href="#13-error-and-topics-of-model-fitting-assessing-model-accuracy" title="Permanent link">&para;</a></h2>
<h3 id="131-measuring-the-quality-of-fit">1.3.1 Measuring the quality of fit<a class="headerlink" href="#131-measuring-the-quality-of-fit" title="Permanent link">&para;</a></h3>
<h4 id="1311-mean-squared-error">1.3.1.1 Mean Squared Error<a class="headerlink" href="#1311-mean-squared-error" title="Permanent link">&para;</a></h4>
<p>The plot in Section 1.2.3 looks good, but numerically what is our error? What is the mean value of $\epsilon$, i.e. the <strong>Mean Squared Error (MSE)</strong>?</p>
<p>
<script type="math/tex; mode=display">{\sf MSE}=\epsilon_{\sf ave} = \frac{\sum_{i=1}^{N_{\sf times}}\left(y^{\sf exact}_i - m\cdot t_i - b \right)^2}{N_{\sf times}}\;\;\;\;\;\sf eq. 3</script>
</p>
<div class="codehilite"><pre><span></span><code># The mean squared error
print(&#39;Mean squared error: %.2f&#39; % mean_squared_error(y, y_pred))
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># The mean squared error</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean squared error: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Mean squared error: 1.68
</code></pre></div>

<h4 id="1312-r-square">1.3.1.2 R-square<a class="headerlink" href="#1312-r-square" title="Permanent link">&para;</a></h4>
<p>Another way to measure error is the regression score, \(R^2\). \(R^2\) is generally defined as the ratio of the total sum of squares \(SS_{\sf tot}\) to the residual sum of squares \(SS_{\sf res}\):</p>
<p>
<script type="math/tex; mode=display">SS_{\sf tot}=\sum_{i=1}^{N} \left(y^{\sf exact}_i-\bar{y}\right)^2\;\;\;\;\; \sf eq. 4</script>
<script type="math/tex; mode=display">SS_{\sf res}=\sum_{i=1}^{N} \left(y^{\sf exact}_i - y^{\sf calc}_i\right)^2\;\;\;\;\; \sf eq. 5</script>
<script type="math/tex; mode=display">R^2 = 1 - {SS_{\sf res}\over SS_{\sf tot}} \;\;\;\;\;\; \sf eq. 6</script>
</p>
<p>In eq. 4, \(\bar{y}=\sum_i y^{\sf exact}_i/N\) is the average value of y for \(N\) points. The best value of \(R^2\) is 1 but it can also take a negative value if the error is large.</p>
<p>See all the different regression metrics <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">here</a>.</p>
<h4 id="question-4-lets-understand-r2">🙋 Question 4: lets understand \(R^2\)<a class="headerlink" href="#question-4-lets-understand-r2" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Do we need a large value of \(SS_{\sf tot}\) to minimize \(R^2\) - is this something which we have the power to control?</p>
</blockquote>
<div class="codehilite"><pre><span></span><code># Print the coefficient of determination - 1 is perfect prediction
print(&#39;Coefficient of determination: %.2f&#39; % r2_score(y, y_pred))
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># Print the coefficient of determination - 1 is perfect prediction</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Coefficient of determination: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Coefficient of determination: 0.45
</code></pre></div>

<h3 id="132-corollaries-with-classification-models">1.3.2 Corollaries with classification models<a class="headerlink" href="#132-corollaries-with-classification-models" title="Permanent link">&para;</a></h3>
<p>For classification tasks, we typically assess accuracy vs MSE or R-square, since we are dealing with categorical rather than numerical predictions.</p>
<p>What is accuracy? It is defined as the ratio of True assignments to all assignments. For a binary positive/negative classification task this can be written as the following:</p>
<p>
<script type="math/tex; mode=display"> Acc = \frac{T_p + T_n}{F_p + F_n + T_p + T_n} </script>
</p>
<p>Where \(T\) is True, \(F\) is false, \(p\) is positive, \(n\) is negative</p>
<p>Just as a quick example, we can perform this type of task on our wine dataset by predicting on quality, which is a discrete 3-9 quality score:</p>
<div class="codehilite"><pre><span></span><code><span class="n">y_train</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># train a logistic regression model on the training set</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># instantiate model</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="c1"># fit model</span>
<span class="n">logreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)





LogisticRegression()
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># make class predictions for the testing set</span>
<span class="n">y_pred_class</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># calculate accuracy</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">))</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>0.7538864091118977
</code></pre></div>

<h3 id="133-beyond-a-single-input-feature">1.3.3 Beyond a single input feature<a class="headerlink" href="#133-beyond-a-single-input-feature" title="Permanent link">&para;</a></h3>
<p>(<em>also: quick appreciative beat for folding in domain area expertise into our models and features</em>)</p>
<p>The <strong>acidity</strong> of the wine (the dependent variable v) could depend on:</p>
<ul>
<li>potassium from the soil (increases alkalinity)</li>
<li>unripe grapes (increases acidity)</li>
<li>grapes grown in colder climates or reduced sunshine create less sugar (increases acidity)</li>
<li>preprocessing such as adding tartaric acid to the grape juice before fermentation (increases acidity)</li>
<li>malolactic fermentation (reduces acidity)</li>
<li>+ others</li>
</ul>
<p>So in our lab today we will look at folding in additional variables in our dataset into the model</p>
<hr style="border:1px solid grey">
</hr>
<h2 id="14-multivariate-regression">1.4 Multivariate regression<a class="headerlink" href="#14-multivariate-regression" title="Permanent link">&para;</a></h2>
<p>Let's now turn our attention to wine quality.</p>
<p>The value we aim to predict or evaluate is the quality of each wine in our dataset. This is our dependent variable. We will look at how this is related to the 12 other independent variables, also known as <em>input features</em>. We're going to do this with only the red wine data</p>
<div class="codehilite"><pre><span></span><code><span class="n">red</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>type</th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4898</th>
      <td>red</td>
      <td>7.4</td>
      <td>0.70</td>
      <td>0.00</td>
      <td>1.9</td>
      <td>0.076</td>
      <td>11.0</td>
      <td>34.0</td>
      <td>0.9978</td>
      <td>3.51</td>
      <td>0.56</td>
      <td>9.4</td>
      <td>5</td>
    </tr>
    <tr>
      <th>4899</th>
      <td>red</td>
      <td>7.8</td>
      <td>0.88</td>
      <td>0.00</td>
      <td>2.6</td>
      <td>0.098</td>
      <td>25.0</td>
      <td>67.0</td>
      <td>0.9968</td>
      <td>3.20</td>
      <td>0.68</td>
      <td>9.8</td>
      <td>5</td>
    </tr>
    <tr>
      <th>4900</th>
      <td>red</td>
      <td>7.8</td>
      <td>0.76</td>
      <td>0.04</td>
      <td>2.3</td>
      <td>0.092</td>
      <td>15.0</td>
      <td>54.0</td>
      <td>0.9970</td>
      <td>3.26</td>
      <td>0.65</td>
      <td>9.8</td>
      <td>5</td>
    </tr>
    <tr>
      <th>4901</th>
      <td>red</td>
      <td>11.2</td>
      <td>0.28</td>
      <td>0.56</td>
      <td>1.9</td>
      <td>0.075</td>
      <td>17.0</td>
      <td>60.0</td>
      <td>0.9980</td>
      <td>3.16</td>
      <td>0.58</td>
      <td>9.8</td>
      <td>6</td>
    </tr>
    <tr>
      <th>4902</th>
      <td>red</td>
      <td>7.4</td>
      <td>0.70</td>
      <td>0.00</td>
      <td>1.9</td>
      <td>0.076</td>
      <td>11.0</td>
      <td>34.0</td>
      <td>0.9978</td>
      <td>3.51</td>
      <td>0.56</td>
      <td>9.4</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="141-linear-regression-with-all-input-fields">1.4.1 Linear regression with all input fields<a class="headerlink" href="#141-linear-regression-with-all-input-fields" title="Permanent link">&para;</a></h3>
<p>For this example, notice we have a categorical data variable in the 'type' column. We will ignore this for now, and only work with our red wines. In the future we will discuss how to deal with categorical variable such as this in a mathematical representation.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># this is a list of all our features or independent variables</span>
<span class="n">features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">red</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

<span class="c1"># we&#39;re going to remove our target or dependent variable, density from this</span>
<span class="c1"># list</span>
<span class="n">features</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>

<span class="c1"># now we define X and y according to these lists of names</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">red</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="n">features</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">red</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="s1">&#39;density&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># we will talk about scaling/centering our data at a later time</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">red</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># we are getting rid of some nasty nulls!</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>type                    0
fixed acidity           2
volatile acidity        1
citric acid             1
residual sugar          0
chlorides               0
free sulfur dioxide     0
total sulfur dioxide    0
density                 0
pH                      2
sulphates               2
alcohol                 0
quality                 0
dtype: int64
</code></pre></div>

<div class="codehilite"><pre><span></span><code># Create linear regression object - note that we are using all the input features
model = linear_model.LinearRegression()
model.fit(X, y)
y_calc = model.predict(X)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># Create linear regression object - note that we are using all the input features</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_calc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>

<p>Let's see what the coefficients look like ... </p>
<div class="codehilite"><pre><span></span><code>print(&quot;Fit coefficients: \n&quot;, model.coef_, &quot;\nNumber of coefficients:&quot;, len(model.coef_))
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fit coefficients: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Number of coefficients:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Fit coefficients: 
 [ 1.64059336e-03  1.23999138e-04  1.16115898e-05  5.83002013e-04
  8.35961822e-05 -9.17472420e-05  8.61246026e-05  7.80966358e-04
  2.24558885e-04 -9.80600257e-04 -1.75587885e-05] 
Number of coefficients: 11
</code></pre></div>

<p>We have 11 !!! That's because we are regressing respect to all <strong>11 independent variables</strong>!!!</p>
<p>So now, <script type="math/tex; mode=display">y_{\sf calc}= m_1x_1 +\, m_2x_2 \,+ \,m_3x_3 \,+\,... \,+ \,b =\sum_{i=1}^{13}m_i x_i + b\;\;\;\;\; \sf eq. 7</script>
</p>
<div class="codehilite"><pre><span></span><code>print(&quot;We have 13 slopes / weights:\n\n&quot;, model.coef_)
print(&quot;\nAnd one intercept: &quot;, model.intercept_)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;We have 11 slopes / weights:</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">And one intercept: &quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>We have 11 slopes / weights:

 [ 1.64059336e-03  1.23999138e-04  1.16115898e-05  5.83002013e-04
  8.35961822e-05 -9.17472420e-05  8.61246026e-05  7.80966358e-04
  2.24558885e-04 -9.80600257e-04 -1.75587885e-05]

And one intercept:  0.9967517451349656
</code></pre></div>

<div class="codehilite"><pre><span></span><code># This size should match the number of columns in X
if len(X[0]) == len(model.coef_):
    print(&quot;All good! The number of coefficients matches the number of input features.&quot;)
else:
    print(&quot;Hmm .. something strange is going on.&quot;)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># This size should match the number of columns in X</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;All good! The number of coefficients matches the number of input features.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hmm .. something strange is going on.&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>All good! The number of coefficients matches the number of input features.
</code></pre></div>

<h3 id="exercise-4-evaluate-the-error">🏋️ Exercise 4: evaluate the error<a class="headerlink" href="#exercise-4-evaluate-the-error" title="Permanent link">&para;</a></h3>
<p>Let's <strong>evaluate the error</strong> by computing the MSE and \(R^2\) metrics (see eq. 3 and 6).</p>
<div class="codehilite"><pre><span></span><code># The mean squared error

# part A 
# calculate the MSE using mean_squared_error()
# mse = 

# part B
# calculate the R square using r2_score()
# r2 = 

print(&#39;Mean squared error: {:.2f}&#39;.format(mse)
print(&#39;Coefficient of determination: {:.2f}&#39;.format(r2)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># The mean squared error</span>

<span class="c1"># part A </span>
<span class="c1"># calculate the MSE using mean_squared_error()</span>
<span class="c1"># mse = </span>

<span class="c1"># part B</span>
<span class="c1"># calculate the R square using r2_score()</span>
<span class="c1"># r2 = </span>

<span class="c1"># print(&#39;Mean squared error: {:.2f}&#39;.format(mse))</span>
<span class="c1"># print(&#39;Coefficient of determination: {:.2f}&#39;.format(r2))</span>
</code></pre></div>

<h3 id="exercise-5-make-a-plot-of-y-actual-vs-y-predicted">🏋️ Exercise 5: make a plot of y actual vs y predicted<a class="headerlink" href="#exercise-5-make-a-plot-of-y-actual-vs-y-predicted" title="Permanent link">&para;</a></h3>
<p>We can also look at how well the computed values match the true values graphically by generating a scatterplot.</p>
<div class="codehilite"><pre><span></span><code># generate a plot of y predicted vs y actual using plt.plot()
# remember you must set ls to an empty string and marker to some marker style

# plt.plot()
plt.title(&quot;Linear regression - computed values on entire data set&quot;, fontsize=16)
plt.xlabel(&quot;y$^{\sf calc}$&quot;)
plt.ylabel(&quot;y$^{\sf true}$&quot;)
plt.show()
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># generate a plot of y predicted vs y actual using plt.plot()</span>
<span class="c1"># remember you must set ls to an empty string and marker to some marker style</span>

<span class="c1"># plt.plot()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Linear regression - computed values on entire data set&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;y$^{\sf calc}$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y$^{\sf true}$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="png" src="../S1_Regression_and_Analysis_files/S1_Regression_and_Analysis_79_0.png" /></p>
<h3 id="142-enrichment-splitting-into-train-and-test-sets">🍒 1.4.2 <strong>Enrichment</strong>: Splitting into train and test sets<a class="headerlink" href="#142-enrichment-splitting-into-train-and-test-sets" title="Permanent link">&para;</a></h3>
<blockquote>
<p>note: more of this topic is covered in <a href="https://wesleybeckner.github.io/data_science_foundations/S3_Model_Selection_and_Validation/"><strong>Model Selection and Validation</strong></a></p>
</blockquote>
<p>To see whether we can predict, we will carry out our regression only on a part, 80%, of the full data set. This part is called the <strong>training</strong> data. We will then test the trained model to predict the rest of the data, 20% - the <strong>test</strong> data. The function which fits won't see the test data until it has to predict it. </p>
<p><strong>We will motivate the use of train/test sets more explicitly in <a href="https://wesleybeckner.github.io/data_science_foundations/S3_Model_Selection_and_Validation/">Model Selection and Validation</a></strong></p>
<p>We start by splitting out data using scikit-learn's <code>train_test_split()</code> function:</p>
<div class="codehilite"><pre><span></span><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
                                                    ```


```python
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.20,
                                                    random_state=42)
</code></pre></div>

<p>Now we check the size of <code> y_train </code> and <code> y_test </code>, the sum should be the size of y! If this works then we move on and carry out regression but we only use the training data!</p>
<div class="codehilite"><pre><span></span><code>if len(y_test)+len(y_train) == len(y):

    print(&#39;All good, ready to to go and regress!\n&#39;)

    # Carry out linear regression
    print(&#39;Running linear regression algorithm on the training set\n&#39;)
    model = linear_model.LinearRegression()
    model.fit(X_train, y_train)
    print(&#39;Fit coefficients and intercept:\n\n&#39;, model.coef_, &#39;\n\n&#39;, model.intercept_ )

    # Predict on the test set
    y_pred_test = model.predict(X_test)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="o">+</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All good, ready to to go and regress!</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="c1"># Carry out linear regression</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running linear regression algorithm on the training set</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Fit coefficients and intercept:</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span> <span class="p">)</span>

    <span class="c1"># Predict on the test set</span>
    <span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>All good, ready to to go and regress!

Running linear regression algorithm on the training set

Fit coefficients and intercept:

 [ 1.62385613e-03  1.10578142e-04  7.75216492e-07  5.87755741e-04
  7.65190323e-05 -1.03490059e-04  8.87357873e-05  7.79083342e-04
  2.23534769e-04 -9.99858829e-04  5.85256438e-06]

 0.9967531628434799
</code></pre></div>

<p>Now we can plot our predicted values to see how accurate we are in predicting. We will generate a scatterplot and computing the MSE and \(R^2\) metrics of error.</p>
<div class="codehilite"><pre><span></span><code>sns.scatterplot(x=y_pred_test, y=y_test, color=&quot;mediumvioletred&quot;, s=50)

plt.title(&quot;Linear regression - predict test set&quot;, fontsize=16)
plt.xlabel(&quot;y$^{\sf calc}$&quot;)
plt.ylabel(&quot;y$^{\sf true}$&quot;)
plt.show()

print(&#39;Mean squared error: %.2f&#39; % mean_squared_error(y_test, y_pred_test))
print(&#39;Coefficient of determination: %.2f&#39; % r2_score(y_test, y_pred_test))
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">y_pred_test</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;mediumvioletred&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Linear regression - predict test set&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;y$^{\sf calc}$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y$^{\sf true}$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean squared error: </span><span class="si">%.2e</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Coefficient of determination: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">))</span>
</code></pre></div>

<p><img alt="png" src="../S1_Regression_and_Analysis_files/S1_Regression_and_Analysis_86_0.png" /></p>
<div class="codehilite"><pre><span></span><code>Mean squared error: 5.45e-07
Coefficient of determination: 0.87
</code></pre></div>

<h4 id="1421-other-data-considerations">1.4.2.1 Other data considerations<a class="headerlink" href="#1421-other-data-considerations" title="Permanent link">&para;</a></h4>
<ul>
<li>Do we need all the independent variables? </li>
<li>Topics of interential statistics covered in a couple sessions</li>
<li>Can we output integer quality scores? </li>
<li>Topics of non-binary classification tasks covered in week 4</li>
</ul>
<h3 id="143-enrichment-other-regression-algorithms">🍒 1.4.3 <strong>Enrichment</strong>: Other regression algorithms<a class="headerlink" href="#143-enrichment-other-regression-algorithms" title="Permanent link">&para;</a></h3>
<p>There are many other regression algorithms the two we want to highlight here are Ridge, LASSO, and Elastic Net. They differ by an added term to the loss function. Let's review. Eq. 2 expanded to multivariate form yields:</p>
<p>
<script type="math/tex; mode=display">\sum_{i=1}^{N}(y_i - \sum_{j=1}^{P}x_{ij}\beta_{j})^2</script>
</p>
<p>for Ridge regression, we add a <strong><em>regularization</em></strong> term known as <strong><em>L2</em></strong> regularization:</p>
<p>
<script type="math/tex; mode=display">\sum_{i=1}^{N}(y_i - \sum_{j=1}^{P}x_{ij}\beta_{j})^2 + \lambda \sum_{j=1}^{P}\beta_{j}^2</script>
</p>
<p>for <strong><em>LASSO</em></strong> (Least Absolute Shrinkage and Selection Operator) we add <strong><em>L1</em></strong> regularization:</p>
<p>
<script type="math/tex; mode=display">\sum_{i=1}^{N}(y_i - \sum_{j=1}^{P}x_{ij}\beta_{j})^2 + \lambda \sum_{j=1}^{P}|\beta_{j}|</script>
</p>
<p>The key difference here is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. <strong><em>Elastic Net</em></strong> is a combination of these two regularization methods.</p>
<div class="codehilite"><pre><span></span><code>model = linear_model.Ridge()
model.fit(X_train, y_train)
print(&#39;Fit coefficients and intercept:\n\n&#39;, model.coef_, &#39;\n\n&#39;, model.intercept_ )

# Predict on the test set
y_calc_test = model.predict(X_test)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Fit coefficients and intercept:</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span> <span class="p">)</span>

<span class="c1"># Predict on the test set</span>
<span class="n">y_calc_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Fit coefficients and intercept:

 [ 1.61930554e-03  1.11227142e-04  2.64709094e-06  5.87271456e-04
  7.58510569e-05 -1.02851782e-04  8.76686650e-05  7.75641517e-04
  2.23315063e-04 -9.98653815e-04  5.26839010e-06]

 0.9967531358810221
</code></pre></div>

<div class="codehilite"><pre><span></span><code>sns.scatterplot(x=y_calc_test, y=y_test, color=&quot;lightseagreen&quot;, s=50)
plt.title(&quot;Ridge regression - predict test set&quot;,fontsize=16)
plt.xlabel(&quot;y$^{\sf calc}$&quot;)
plt.ylabel(&quot;y$^{\sf true}$&quot;)
plt.show()

print(&#39;Mean squared error: %.2f&#39; % mean_squared_error(y_test, y_calc_test))
print(&#39;Coefficient of determination: %.2f&#39; % r2_score(y_test, y_calc_test))
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">y_calc_test</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightseagreen&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Ridge regression - predict test set&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;y$^{\sf calc}$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y$^{\sf true}$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean squared error: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_calc_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Coefficient of determination: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_calc_test</span><span class="p">))</span>
</code></pre></div>

<p><img alt="png" src="../S1_Regression_and_Analysis_files/S1_Regression_and_Analysis_93_0.png" /></p>
<div class="codehilite"><pre><span></span><code>Mean squared error: 0.00
Coefficient of determination: 0.87
</code></pre></div>

<h4 id="exercise-6-tune-hyperparameter-for-ridge-regression">🏋️ Exercise 6: Tune Hyperparameter for Ridge Regression<a class="headerlink" href="#exercise-6-tune-hyperparameter-for-ridge-regression" title="Permanent link">&para;</a></h4>
<p>Use the docstring to peak into the hyperparameters for Ridge Regression. What is the optimal value of lambda?</p>
<p>Plot the \(\beta\) values vs \(\lambda\) from the results of your analysis</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># cell for exercise 3</span>
<span class="n">out_lambdas</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">out_coefs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">out_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="n">lambdas</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">coefs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">lamb</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="mf">5e3</span><span class="p">),</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lamb</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">lambdas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lamb</span><span class="p">)</span>
    <span class="n">coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
    <span class="c1"># print(&#39;MSE: %.4f&#39; % mean_squared_error(y_test, model.predict(X_test)))</span>
    <span class="c1"># print(&#39;R2: %.4f&#39; % r2_score(y_test, model.predict(X_test)))</span>
  <span class="n">out_lambdas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lambdas</span><span class="p">)</span>
  <span class="n">out_coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span>
  <span class="n">out_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">coef_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out_coefs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">coef_stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out_coefs</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">results_means</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">coef_means</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>
<span class="n">results_stds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">coef_stds</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>
<span class="n">results_means</span><span class="p">[</span><span class="s1">&#39;lambda&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">lambdas</span><span class="p">]</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">lambdas</span><span class="p">],</span> <span class="n">results_means</span><span class="p">[</span><span class="n">feat</span><span class="p">],</span> <span class="n">yerr</span><span class="o">=</span><span class="n">results_stds</span><span class="p">[</span><span class="n">feat</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">feat</span><span class="p">)</span>
<span class="c1"># results.plot(&#39;lambda&#39;, &#39;scores&#39;, ax=ax[1])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>&lt;matplotlib.legend.Legend at 0x7fdd72270fd0&gt;
</code></pre></div>

<p><img alt="png" src="../S1_Regression_and_Analysis_files/S1_Regression_and_Analysis_97_1.png" /></p>
<div class="codehilite"><pre><span></span><code><span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">coefs</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>
<span class="n">results</span><span class="p">[</span><span class="s1">&#39;lambda&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">lambdas</span><span class="p">]</span>
<span class="n">results</span><span class="p">[</span><span class="s1">&#39;scores&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scores</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
  <span class="n">results</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;lambda&#39;</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">results</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;lambda&#39;</span><span class="p">,</span> <span class="s1">&#39;scores&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>&lt;AxesSubplot:xlabel=&#39;lambda&#39;&gt;
</code></pre></div>

<p><img alt="png" src="../S1_Regression_and_Analysis_files/S1_Regression_and_Analysis_99_1.png" /></p>
<h2 id="15-enrichment-additional-regression-exercises">🍒 1.5 <strong>Enrichment</strong>: Additional Regression Exercises<a class="headerlink" href="#15-enrichment-additional-regression-exercises" title="Permanent link">&para;</a></h2>
<h3 id="problem-1-number-and-choice-of-input-features">Problem 1) Number and choice of input features<a class="headerlink" href="#problem-1-number-and-choice-of-input-features" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Load the red wine dataset and evaluate how the linear regression predictions changes as you change the <strong>number and choice of input features</strong>. The total number of columns in X  is 11 and each column represents a specific input feature. </p>
</li>
<li>
<p>Estimate the MSE</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><code>print(X_train.shape)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>(1274, 11)
</code></pre></div>

<p>If you want to use the first 5 features you could proceed as following:</p>
<div class="codehilite"><pre><span></span><code>X_train_five = X_train[:,0:5]
X_test_five = X_test[:,0:5]
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">X_train_five</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
<span class="n">X_test_five</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div>

<p>Check that the new variables have the shape your expect</p>
<div class="codehilite"><pre><span></span><code>print(X_train_five.shape)
print(X_test_five.shape)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">X_train_five</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_test_five</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>(1274, 5)
(319, 5)
</code></pre></div>

<p>Now you can use these to train your linear regression model and repeat for different numbers or sets of input features! Note that you do not need to change the output feature! It's size is independent from the number of input features, yet recall that its length is the same as the number of values per input feature.</p>
<p>Questions to think about while you work on this problem
- How many input feature variables does one need? Is there a maximum or minimum number? 
- Could one input feature variable be better than the rest?
- What if values are missing for one of the input feature variables - is it still worth using it?
- Can you use <strong><em>L1</em></strong> or <strong><em>L2</em></strong> to determine these optimum features more quickly?</p>
<h3 id="problem-2-type-of-regression-algorithm">Problem 2) Type of regression algorithm<a class="headerlink" href="#problem-2-type-of-regression-algorithm" title="Permanent link">&para;</a></h3>
<p>Try using other types of linear regression methods on the wine dataset: the LASSO model and the Elastic net model which are described by the </p>
<p><code > sklearn.linear_model.ElasticNet() </code> <br>
<code > sklearn.linear_model.Lasso() </code></p>
<p>scikit-learn functions.</p>
<p>For more detail see <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet">ElasticNet</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso">Lasso</a>.</p>
<p>Questions to think about while you work on this problem
- How does the error change with each model?
- Which model seems to perform best?
- How can you optimize the hyperparameter, \(\lambda\)
- Does one model do better than the other at determining which input features are more important?
- How about non linear regression / what if the data does not follow a line?
- How do the bias and variance change for each model</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">ElasticNet</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">]:</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span>
  <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean squared error: %.ef&#39;</span> <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Coefficient of determination: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
  <span class="nb">print</span><span class="p">()</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>ElasticNet()
Mean squared error: 4e-06f
Coefficient of determination: -0.00

Lasso()
Mean squared error: 4e-06f
Coefficient of determination: -0.00

Ridge()
Mean squared error: 7e-07f
Coefficient of determination: 0.83

LinearRegression()
Mean squared error: 7e-07f
Coefficient of determination: 0.83
</code></pre></div>

<hr style="border:1px solid grey">
</hr>
<h1 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h1>
<ul>
<li>
<p><strong>Linear Regression</strong>
To find out more see <a href="https://en.wikipedia.org/wiki/Simple_linear_regression">simple linear regression</a></p>
</li>
<li>
<p><strong>scikit-learn</strong></p>
<ul>
<li><a href="https://scikit-learn.org/stable/">Scikit-learn</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">Linear regression in scikit-learn</a></li>
<li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html">Metrics of error</a></li>
<li><a href="https://scikit-learn.org/stable/datasets/index.html#boston-dataset">The Boston dataset</a></li>
</ul>
</li>
<li>
<p><strong>Pearson correlation</strong>
To find out more see <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">pearson</a></p>
</li>
<li>
<p><strong>Irreducible error, bias and variance</strong></p>
<ul>
<li>Great Coursera videos <a href="https://www.coursera.org/lecture/ml-regression/irreducible-error-and-bias-qlMrZ">here</a>
and <a href="https://www.coursera.org/lecture/ml-regression/variance-and-the-bias-variance-tradeoff-ZvP40">here</a></li>
</ul>
</li>
</ul>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../about/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Introduction" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Introduction
            </div>
          </div>
        </a>
      
      
        
        <a href="../S2_Inferential_Statistics/" class="md-footer__link md-footer__link--next" aria-label="Next: Inferential Statistics" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Inferential Statistics
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.361d90f1.min.js"}</script>
    
    
      <script src="../assets/javascripts/bundle.289a2a4b.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>