{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wesleybeckner/data_science_foundations/blob/main/notebooks/exercises/E5_Writing_Unit_Tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPmio-GwmEwG"
   },
   "source": [
    "# Data Science Foundations <br> Lab 5: Writing Unit Tests\n",
    "\n",
    "**Instructor**: Wesley Beckner\n",
    "\n",
    "**Contact**: wesleybeckner@gmail.com\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "In this lab, we will try our hand at writing unit tests\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqvpwnmXCD6T"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QbaV8zMBXh9_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from contextlib import contextmanager\n",
    "import sys, os\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGbWAzdgx0bj"
   },
   "source": [
    "## Types of Tests\n",
    "\n",
    "There are two main types of tests we want to distinguish:\n",
    "\n",
    "* **_Unit test_**: an automatic test to test the internal workings of a class or function. It should be a stand-alone test which is not related to other resources.\n",
    "* **_Integration test_**: an automatic test that is done on an environment, it tests the coordination of different classes and functions as well as with the running environment. This usually precedes sending code to a QA team.\n",
    "\n",
    "To this I will add:\n",
    "\n",
    "* **_Acid test_**: extremely rigorous tests that push beyond the intended use cases for your classes/functions. Written when you, like me, cannot afford QA employees to actually test your code. (word origin: [gold acid tests in the 1850s](https://en.wikipedia.org/wiki/Acid_test_(gold)), [acid tests in the 70's](https://en.wikipedia.org/wiki/Acid_Tests))\n",
    "* **_EDIT_**: you could also call this a corner, or an edge case\n",
    "\n",
    "In this lab we will focus on _unit tests_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeELH5izfpcW"
   },
   "source": [
    "## Unit Tests\n",
    "\n",
    "Each unit test should test the smallest portion of your code possible, i.e. a single method or function. Any random number generators should be seeded so that they run the exact same way every time. Unit tests should not rely on any local files or the local environment. \n",
    "\n",
    "Why bother with Unit Tests when we have Integration tests?\n",
    "\n",
    "A major challenge with integration testing is when an integration test fails. It‚Äôs very hard to diagnose a system issue without being able to isolate which part of the system is failing. Here comes the unit test to the rescue. \n",
    "\n",
    "Let's take a simple example. If I wanted to test that the sume of two numbers is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dHJEOhTheT5I"
   },
   "outputs": [],
   "source": [
    "assert sum([2, 5]) == 7, \"should be 7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vapg_Y96jePB"
   },
   "source": [
    "Nothing is sent to the print out because the condition is satisfied. If we run, however:\n",
    "\n",
    "```\n",
    "assert sum([2, 4]) == 7, \"should be 7\"\n",
    "```\n",
    "\n",
    "we get an error message:\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "AssertionError                            Traceback (most recent call last)\n",
    "<ipython-input-3-d5724b127818> in <module>()\n",
    "----> 1 assert sum([2, 4]) == 7, \"should be 7\"\n",
    "\n",
    "AssertionError: should be 7\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_myc2iDDjqmk"
   },
   "source": [
    "To make this a Unit Test, you will want to wrap it in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxdkoYntjt1E",
    "outputId": "1507ce22-9ce7-46c4-ba11-8a2721fa9108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything passed\n"
     ]
    }
   ],
   "source": [
    "def test_sum():\n",
    "    assert sum([1, 2, 3]) == 6, \"Should be 6\"\n",
    "\n",
    "test_sum()\n",
    "print(\"Everything passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QP4xzyYGLJRG"
   },
   "source": [
    "And if we include a test that does not pass:\n",
    "\n",
    "```\n",
    "def test_sum():\n",
    "  assert sum([3, 3]) == 6, \"Should be 6\"\n",
    "\n",
    "def test_my_broken_func():\n",
    "  assert sum([1, 2]) == 5, \"Should be 5\"\n",
    "\n",
    "test_sum()\n",
    "test_my_broken_func()\n",
    "print(\"Everything passed\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDrcx65ymKd4"
   },
   "source": [
    "Here our test fails, because the sum of 1 and 2 is 3 and not 5. We get a traceback that tells us the source of the error:\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "AssertionError                            Traceback (most recent call last)\n",
    "<ipython-input-13-8a552fbf52bd> in <module>()\n",
    "      6 \n",
    "      7 test_sum()\n",
    "----> 8 test_my_broken_func()\n",
    "      9 print(\"Everything passed\")\n",
    "\n",
    "<ipython-input-13-8a552fbf52bd> in test_my_broken_func()\n",
    "      3 \n",
    "      4 def test_my_broken_func():\n",
    "----> 5   assert sum([1, 2]) == 5, \"Should be 5\"\n",
    "      6 \n",
    "      7 test_sum()\n",
    "\n",
    "AssertionError: Should be 5\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUWcSQ3osABW"
   },
   "source": [
    "Before sending us on our merry way to practice writing unit tests, we will want to ask, what do I want to write a test about? Here, we've been testing sum(). There are many behaviors in sum() we could check, such as:\n",
    "\n",
    "* Does it sum a list of whole numbers (integers)?\n",
    "* Can it sum a tuple or set?\n",
    "* Can it sum a list of floats?\n",
    "* What happens if one of the numbers is negative? etc..\n",
    "\n",
    "In the end, what you test is up to you, and depends on your intended use cases. As a general rule of thumb, your unit test should test what is relevant.\n",
    "\n",
    "The only caveat to that, is that many continuous integration services (like [TravisCI](https://travis-ci.com/)) will benchmark you based on the percentage of lines of code you have that are covered by your unit tests (ex: [85% coverage](https://github.com/wesleybeckner/gains))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRwuV7gay0hX"
   },
   "source": [
    "## ‚úçüèΩ L5 Q1 Write a Unit Test\n",
    "\n",
    "Remember our Pokeball discussion in [Python Foundations](https://wesleybeckner.github.io/python_foundations/S4_Object_Oriented_Programming/)? We'll return to that here. This time writing unit tests for our classes.\n",
    "\n",
    "Sometimes when writing unit tests, it can be more complicated than checking the return value of a function. Think back on our pokemon example:\n",
    "\n",
    "<br>\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"https://raw.githubusercontent.com/wesleybeckner/python_foundations/main/assets/pokeballs.png\"></img>\n",
    "</p>\n",
    "\n",
    "```\n",
    "class Pokeball:\n",
    "  def __init__(self, contains=None, type_name=\"poke ball\"):\n",
    "    self.contains = contains\n",
    "    self.type_name = type_name\n",
    "    self.catch_rate = 0.50 # note this attribute is not accessible upon init\n",
    "\n",
    "  # the method catch, will update self.contains, if a catch is successful\n",
    "  # it will also use self.catch_rate to set the performance of the catch\n",
    "  def catch(self, pokemon):\n",
    "    if self.contains == None:\n",
    "      if random.random() < self.catch_rate:\n",
    "        self.contains = pokemon\n",
    "        print(f\"{pokemon} captured!\")\n",
    "      else:\n",
    "        print(f\"{pokemon} escaped!\")\n",
    "        pass\n",
    "    else:\n",
    "      print(\"pokeball is not empty!\")\n",
    "      \n",
    "  def release(self):\n",
    "    if self.contains == None:\n",
    "      print(\"Pokeball is already empty\")\n",
    "    else:\n",
    "      print(self.contains, \"has been released\")\n",
    "      self.contains = None\n",
    "```\n",
    "\n",
    "If I wanted to write a unit test for the release method, I couldn't directly check for the output of a function. I'll have to check for a **_side effect_**, in this case, the change of an attribute belonging to a pokeball object; that is the change to the attribute _contains_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HalrD0nxqBTR"
   },
   "outputs": [],
   "source": [
    "class Pokeball:\n",
    "  def __init__(self, contains=None, type_name=\"poke ball\"):\n",
    "    self.contains = contains\n",
    "    self.type_name = type_name\n",
    "    self.catch_rate = 0.50 # note this attribute is not accessible upon init\n",
    "\n",
    "  # the method catch, will update self.contains, if a catch is successful\n",
    "  # it will also use self.catch_rate to set the performance of the catch\n",
    "  def catch(self, pokemon):\n",
    "    if self.contains == None:\n",
    "      if random.random() < self.catch_rate:\n",
    "        self.contains = pokemon\n",
    "        print(f\"{pokemon} captured!\")\n",
    "      else:\n",
    "        print(f\"{pokemon} escaped!\")\n",
    "        pass\n",
    "    else:\n",
    "      print(\"pokeball is not empty!\")\n",
    "      \n",
    "  def release(self):\n",
    "    if self.contains == None:\n",
    "      print(\"Pokeball is already empty\")\n",
    "    else:\n",
    "      print(self.contains, \"has been released\")\n",
    "      self.contains = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h98l1tNRPKRd"
   },
   "source": [
    "In the following cell, finish the code to test the functionality of the _release_ method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4otRRh71rBj_"
   },
   "outputs": [],
   "source": [
    "def test_release():\n",
    "  ball = Pokeball()\n",
    "  ball.contains = 'Pikachu'\n",
    "  ball.release()\n",
    "  # turn the pseudo code below into an assert statement\n",
    "  \n",
    "  ### YOUR CODE HERE ###\n",
    "  # assert <object.attribute> == <something>, \"some erroneous message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAwXirZ0L_yk",
    "outputId": "85219b06-af0b-4d31-be91-3ed63ec3f9ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pikachu has been released\n"
     ]
    }
   ],
   "source": [
    "test_release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJVclx-MUL0S"
   },
   "source": [
    "## ‚õπÔ∏è L5 Q2 Write a Unit Test for the Catch Rate\n",
    "\n",
    "First, we will check that the succcessful catch is operating correctly. Remember that we depend on `random.random` and condition our success on whether that random value is less than the `catch_rate` of the pokeball:\n",
    "\n",
    "```\n",
    "if self.contains == None:\n",
    "      if random.random() < self.catch_rate:\n",
    "        self.contains = pokemon\n",
    "```\n",
    "\n",
    "so to test whether the successful catch is working we will seed our random number generator with a value that returns less than the `catch_rate` of the pokeball and then write our assert statement:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVnPbFn7Wcxy"
   },
   "outputs": [],
   "source": [
    "def test_successful_catch():\n",
    "  # choose a random seed such that\n",
    "  # we know the catch call should succeed\n",
    "  \n",
    "  ### YOUR CODE BELOW ###\n",
    "  # random.seed(<your number here>)\n",
    "  ball = Pokeball()\n",
    "  ball.catch('Psyduck') # Someone's fave pokemon (bless 'em)\n",
    "\n",
    "  ### YOUR CODE BELOW ###\n",
    "  # <object.attribute> == <something>, \"ball did not catch as expected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJkA7mOCYghS"
   },
   "source": [
    "NICE. Now we will do the same thing again, this time testing for an unsuccessful catch. SO in order to do this, we need to choose a random seed that will cause our catch to fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO3HTccaXQ5t"
   },
   "outputs": [],
   "source": [
    "def test_unsuccessful_catch():\n",
    "  # choose a random seed such that\n",
    "  # we know the catch call should FAIL\n",
    "  \n",
    "  ### YOUR CODE BELOW ###\n",
    "  # random.seed(<your number here>)\n",
    "  ball = Pokeball()\n",
    "  ball.catch('Psyduck') \n",
    "\n",
    "  ### YOUR CODE BELOW ###\n",
    "  # <object.attribute> == <something>, \"ball did not fail as expected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvR_oU1lYtn3"
   },
   "source": [
    "When you are finished test your functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bM_mq6niY429",
    "outputId": "6a76898c-ea4e-4813-ec76-0e820de2245c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Psyduck captured!\n"
     ]
    }
   ],
   "source": [
    "test_unsuccessful_catch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqhIkH0nYyQW",
    "outputId": "8b78aa5e-1bd8-4ddb-9187-ec41483a2162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Psyduck captured!\n"
     ]
    }
   ],
   "source": [
    "test_successful_catch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiQgfk6BZA16"
   },
   "source": [
    "## ‚öñÔ∏è L5 Q3 Write a Unit Test that Checks Whether the Overall Catch Rate is 50/50\n",
    "\n",
    "For this one, we're going to take those same ideas around seeding the random number generator. However, here we'd like to run the catch function multiple times to check whether it is truly creating a 50/50 catch rate situation.\n",
    "\n",
    "Here's a pseudo code outline:\n",
    "\n",
    "1. seed the random number generator\n",
    "2. for 100 iterations: \n",
    "    * create a pokeball\n",
    "    * try to catch something\n",
    "    * log whether it was successful\n",
    "3. check that for the 100 attempts the success was approximately 50/50\n",
    "\n",
    "_note:_ you can use my `suppress stdout()` function to suppress the print statements from `ball.catch`\n",
    "\n",
    "ex:\n",
    "\n",
    "```\n",
    "with suppress_stdout():\n",
    "  print(\"HELLO OUT THERE!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "_quick segway_: what is the actual behavior of `random.seed()`? Does it produce the same number every time we call `random.random()` now? Check for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tH-R1S7-aF9X",
    "outputId": "1034637a-f405-4bc6-c20d-1a6b0570b532"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6394267984578837,\n",
       " 0.025010755222666936,\n",
       " 0.27502931836911926,\n",
       " 0.22321073814882275,\n",
       " 0.7364712141640124]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "[random.random() for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXUHxvQMasoN"
   },
   "source": [
    "We see that it still produces random numbers with each call to `random.random`. However, those numbers are the same with every execution of the cell. What happens when we do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbBLOZgqa1AZ",
    "outputId": "1f04099c-a077-44c8-80ed-14d5aea01cfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6766994874229113,\n",
       " 0.8921795677048454,\n",
       " 0.08693883262941615,\n",
       " 0.4219218196852704,\n",
       " 0.029797219438070344]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[random.random() for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-UWTf1Ta4He"
   },
   "source": [
    "The numbers are different. BUT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZk3tQ5Ja7S0",
    "outputId": "82e18e62-d495-4d26-ce57-1349c92910c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6394267984578837,\n",
       " 0.025010755222666936,\n",
       " 0.27502931836911926,\n",
       " 0.22321073814882275,\n",
       " 0.7364712141640124,\n",
       " 0.6766994874229113,\n",
       " 0.8921795677048454,\n",
       " 0.08693883262941615,\n",
       " 0.4219218196852704,\n",
       " 0.029797219438070344]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "[random.random() for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGh81x6Sa814"
   },
   "source": [
    "We see them here in the bottom half of the list again. So, random.seed() is _seeding_ the random number generator such that it will produce the same sequence of random numbers every time from the given seed. This will reset whenever random.seed() is set again. This behavior is useful because it allows us to continue using random number generation in our code, (for testing, creating examples and demos, etc.) but it will be reproducable each time.\n",
    "\n",
    "_End Segway_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T-5i7ndkU8B_"
   },
   "outputs": [],
   "source": [
    "# 1. seed the random number generator\n",
    "# 2. for 100 iterations: \n",
    "#     * create a pokeball\n",
    "#     * try to catch something\n",
    "#     * log whether it was successful\n",
    "# 3. check that for the 100 attempts the success was approximately 50/50\n",
    "def test_catch_rate():\n",
    "  ### YOUR CODE HERE ###\n",
    "  \n",
    "  ### END YOUR CODE ###\n",
    "  # assert np.abs(np.mean(results) - 0.5) < 0.1, \"catch rate not 50/50\"\n",
    "  pass\n",
    "test_catch_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyAe363HyC5j"
   },
   "source": [
    "## Test Runners\n",
    "\n",
    "When we start to create many tests like this, it can be cumbersome to run them all at once and log which ones fail. To handle our unit tests we use what are called **_test runners_**. We won't dedicate time to any single one here but the three most common are:\n",
    "\n",
    "* unittest\n",
    "* nose2\n",
    "* pytest\n",
    "\n",
    "unittest is built into python. I don't like it because you have to follow a strict class/method structure when writing the tests. nose2 is popular with many useful features and is generally good for high volumes of tests. My favorite is pytest, it's flexible and has an ecosystem of plugins for extensibility. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "E5_Writing_Unit_Tests.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
