
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.1.4">
    
    
      
        <title>Unsupervised Learning - Data Science Foundations</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.bb3983ee.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.e6a45f82.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../extra.css">
    
      <link rel="stylesheet" href="../styles.css">
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  


  <script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-114664473-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){var e;this.value&&(e=document.location.pathname,ga("send","pageview",e+"?q="+this.value))}),"undefined"!=typeof location$&&location$.subscribe(function(e){ga("send","pageview",e.pathname)})})</script>
  <script async src="https://www.google-analytics.com/analytics.js"></script>


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#data-science-foundations-session-5-unsupervised-learning-clustering-and-dimensionality-reduction" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Data Science Foundations" class="md-header__button md-logo" aria-label="Data Science Foundations" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Data Science Foundations
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Unsupervised Learning
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Data Science Foundations" class="md-nav__button md-logo" aria-label="Data Science Foundations" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Data Science Foundations
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        About
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Sessions
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Sessions" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Sessions
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S1_Regression_and_Analysis/" class="md-nav__link">
        Regression and Analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S2_Inferential_Statistics/" class="md-nav__link">
        Inferential Statistics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S3_Model_Selection_and_Validation/" class="md-nav__link">
        Model Selection and Validation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S4_Feature_Engineering/" class="md-nav__link">
        Feature Engineering
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Unsupervised Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Unsupervised Learning
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#50-preparing-environment-and-importing-data" class="md-nav__link">
    5.0 Preparing Environment and Importing Data
  </a>
  
    <nav class="md-nav" aria-label="5.0 Preparing Environment and Importing Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#501-import-packages" class="md-nav__link">
    5.0.1 Import Packages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#502-load-and-process-dataset" class="md-nav__link">
    5.0.2 Load and Process Dataset
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#51-principal-component-analysis" class="md-nav__link">
    5.1 Principal Component Analysis
  </a>
  
    <nav class="md-nav" aria-label="5.1 Principal Component Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#511-the-covariance-matrix" class="md-nav__link">
    5.1.1 The Covariance Matrix
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#512-how-does-the-covariance-matrix-relate-to-the-new-coordinate-system" class="md-nav__link">
    5.1.2 How Does the Covariance Matrix Relate to the New Coordinate System
  </a>
  
    <nav class="md-nav" aria-label="5.1.2 How Does the Covariance Matrix Relate to the New Coordinate System">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#5121-enrichment-deriving-the-eigenvectors-and-eigenvalues" class="md-nav__link">
    🌭 5.1.2.1 Enrichment: Deriving the Eigenvectors and Eigenvalues
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5122-find-the-eigenvalues" class="md-nav__link">
    🌭  5.1.2.2: Find the Eigenvalues
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5123-find-the-eigenvectors" class="md-nav__link">
    🌭  5.1.2.3: Find the Eigenvectors
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#513-projecting-onto-the-principal-components" class="md-nav__link">
    5.1.3 Projecting onto the Principal Components
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#514-cumulative-explained-variance" class="md-nav__link">
    5.1.4 Cumulative Explained Variance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#515-pca-with-scikit-learn" class="md-nav__link">
    5.1.5 PCA with Scikit-Learn
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#516-pca-as-dimensionality-reduction" class="md-nav__link">
    5.1.6 PCA as Dimensionality Reduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#517-pca-for-visualization" class="md-nav__link">
    5.1.7 PCA for visualization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#518-enrichment-pca-as-outlier-removal-and-noise-filtering" class="md-nav__link">
    🌭 5.1.8 Enrichment: PCA as Outlier Removal and Noise Filtering
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#519-pca-for-feature-engineering" class="md-nav__link">
    5.1.9 PCA for Feature Engineering
  </a>
  
    <nav class="md-nav" aria-label="5.1.9 PCA for Feature Engineering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exercise-1-pca-as-preprocessing-for-models" class="md-nav__link">
    🏋️ Exercise 1: PCA as Preprocessing for Models
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#52-k-means-clustering" class="md-nav__link">
    5.2 K-Means Clustering
  </a>
  
    <nav class="md-nav" aria-label="5.2 K-Means Clustering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#521-the-algorithm-expectation-maximization" class="md-nav__link">
    5.2.1 The Algorithm: Expectation-Maximization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#522-limitations" class="md-nav__link">
    5.2.2 Limitations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#523-determining-k-with-the-elbow-method" class="md-nav__link">
    5.2.3 Determining K with the Elbow Method
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-1-comparing-metrics" class="md-nav__link">
    🙋‍♀️ Question 1: Comparing Metrics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#53-gaussian-mixture-models" class="md-nav__link">
    5.3 Gaussian Mixture Models
  </a>
  
    <nav class="md-nav" aria-label="5.3 Gaussian Mixture Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#531-generalizing-e-m-for-gmms" class="md-nav__link">
    5.3.1 Generalizing E-M for GMMs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#532-gmms-as-a-data-generator" class="md-nav__link">
    5.3.2 GMMs as a Data Generator
  </a>
  
    <nav class="md-nav" aria-label="5.3.2 GMMs as a Data Generator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#5321-determining-the-number-of-components" class="md-nav__link">
    5.3.2.1 Determining the number of components
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-2-determine-number-of-components-for-circular-moons" class="md-nav__link">
    🏋️ Exercise 2: Determine Number of Components for Circular Moons
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S6_Bagging/" class="md-nav__link">
        Bagging
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S7_Boosting/" class="md-nav__link">
        Boosting
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Labs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Labs" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Labs
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../labs/L1_Descriptive_Statistics_Data_Hunt/" class="md-nav__link">
        Descriptive Statistics Data Hunt
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../labs/L2_Inferential_Statistics_Data_Hunt/" class="md-nav__link">
        Inferential Statistics Data Hunt
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../labs/L3_Feature_Engineering/" class="md-nav__link">
        Practice with Feature Engineering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../labs/L4_Supervised_Learners/" class="md-nav__link">
        Practice with Supervised Learners
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../labs/L5_Writing_Unit_Tests/" class="md-nav__link">
        Practice with Writing Unit Tests
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Project
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Project" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Project
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../project/P1_Statistical_Analysis_of_TicTacToe/" class="md-nav__link">
        Statistical Analysis of TicTacToe
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../project/P2_Heuristical_TicTacToe_Agents/" class="md-nav__link">
        Heuristical TicTacToe Agents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../project/P3_1_Step_Look_Ahead_Agents/" class="md-nav__link">
        1-Step Look Ahead Agents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../project/P4_N_Step_Look_Ahead_Agents/" class="md-nav__link">
        N-Step Look Ahead Agents
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#50-preparing-environment-and-importing-data" class="md-nav__link">
    5.0 Preparing Environment and Importing Data
  </a>
  
    <nav class="md-nav" aria-label="5.0 Preparing Environment and Importing Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#501-import-packages" class="md-nav__link">
    5.0.1 Import Packages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#502-load-and-process-dataset" class="md-nav__link">
    5.0.2 Load and Process Dataset
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#51-principal-component-analysis" class="md-nav__link">
    5.1 Principal Component Analysis
  </a>
  
    <nav class="md-nav" aria-label="5.1 Principal Component Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#511-the-covariance-matrix" class="md-nav__link">
    5.1.1 The Covariance Matrix
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#512-how-does-the-covariance-matrix-relate-to-the-new-coordinate-system" class="md-nav__link">
    5.1.2 How Does the Covariance Matrix Relate to the New Coordinate System
  </a>
  
    <nav class="md-nav" aria-label="5.1.2 How Does the Covariance Matrix Relate to the New Coordinate System">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#5121-enrichment-deriving-the-eigenvectors-and-eigenvalues" class="md-nav__link">
    🌭 5.1.2.1 Enrichment: Deriving the Eigenvectors and Eigenvalues
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5122-find-the-eigenvalues" class="md-nav__link">
    🌭  5.1.2.2: Find the Eigenvalues
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5123-find-the-eigenvectors" class="md-nav__link">
    🌭  5.1.2.3: Find the Eigenvectors
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#513-projecting-onto-the-principal-components" class="md-nav__link">
    5.1.3 Projecting onto the Principal Components
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#514-cumulative-explained-variance" class="md-nav__link">
    5.1.4 Cumulative Explained Variance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#515-pca-with-scikit-learn" class="md-nav__link">
    5.1.5 PCA with Scikit-Learn
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#516-pca-as-dimensionality-reduction" class="md-nav__link">
    5.1.6 PCA as Dimensionality Reduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#517-pca-for-visualization" class="md-nav__link">
    5.1.7 PCA for visualization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#518-enrichment-pca-as-outlier-removal-and-noise-filtering" class="md-nav__link">
    🌭 5.1.8 Enrichment: PCA as Outlier Removal and Noise Filtering
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#519-pca-for-feature-engineering" class="md-nav__link">
    5.1.9 PCA for Feature Engineering
  </a>
  
    <nav class="md-nav" aria-label="5.1.9 PCA for Feature Engineering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exercise-1-pca-as-preprocessing-for-models" class="md-nav__link">
    🏋️ Exercise 1: PCA as Preprocessing for Models
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#52-k-means-clustering" class="md-nav__link">
    5.2 K-Means Clustering
  </a>
  
    <nav class="md-nav" aria-label="5.2 K-Means Clustering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#521-the-algorithm-expectation-maximization" class="md-nav__link">
    5.2.1 The Algorithm: Expectation-Maximization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#522-limitations" class="md-nav__link">
    5.2.2 Limitations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#523-determining-k-with-the-elbow-method" class="md-nav__link">
    5.2.3 Determining K with the Elbow Method
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-1-comparing-metrics" class="md-nav__link">
    🙋‍♀️ Question 1: Comparing Metrics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#53-gaussian-mixture-models" class="md-nav__link">
    5.3 Gaussian Mixture Models
  </a>
  
    <nav class="md-nav" aria-label="5.3 Gaussian Mixture Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#531-generalizing-e-m-for-gmms" class="md-nav__link">
    5.3.1 Generalizing E-M for GMMs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#532-gmms-as-a-data-generator" class="md-nav__link">
    5.3.2 GMMs as a Data Generator
  </a>
  
    <nav class="md-nav" aria-label="5.3.2 GMMs as a Data Generator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#5321-determining-the-number-of-components" class="md-nav__link">
    5.3.2.1 Determining the number of components
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-2-determine-number-of-components-for-circular-moons" class="md-nav__link">
    🏋️ Exercise 2: Determine Number of Components for Circular Moons
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                

<p><a href="https://colab.research.google.com/github/wesleybeckner/data_science_foundations/blob/main/notebooks/S5_Unsupervised_Learning.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<h1 id="data-science-foundations-session-5-unsupervised-learning-clustering-and-dimensionality-reduction">Data Science Foundations <br> Session 5: Unsupervised Learning: Clustering and Dimensionality Reduction<a class="headerlink" href="#data-science-foundations-session-5-unsupervised-learning-clustering-and-dimensionality-reduction" title="Permanent link">&para;</a></h1>
<p><strong>Instructor</strong>: Wesley Beckner</p>
<p><strong>Contact</strong>: <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#119;&#101;&#115;&#108;&#101;&#121;&#98;&#101;&#99;&#107;&#110;&#101;&#114;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#119;&#101;&#115;&#108;&#101;&#121;&#98;&#101;&#99;&#107;&#110;&#101;&#114;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a></p>
<hr />
<p><br></p>
<p>In the previous session we began our discussion on feature engineering and ended with a sneak peak into dimensionality reduction. This last topic deserves a whole session on its own because its use case is not limited to feature engineering! It can also be used as a tool for visualization, for noise filtering, and much more. In this session we discuss dimensionality reduction along with other unsupervised learning methods. </p>
<p>Up until now, the only learning estimators we've looked at were supervised ones: estimators that predict labels based on training data. Here, however, we are interested in uncovering aspects of the data without reference to any known labels. The usefulness for these learners will become immediately apparent when we revist our wine quality models!</p>
<p><br></p>
<hr />
<p><br></p>
<p><a name='top'></a></p>
<p><a name='3.0'></a></p>
<h2 id="50-preparing-environment-and-importing-data">5.0 Preparing Environment and Importing Data<a class="headerlink" href="#50-preparing-environment-and-importing-data" title="Permanent link">&para;</a></h2>
<p><a href="#top">back to top</a></p>
<p><a name='x.0.1'></a></p>
<h3 id="501-import-packages">5.0.1 Import Packages<a class="headerlink" href="#501-import-packages" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">,</span> <span class="n">silhouette_score</span><span class="p">,</span> <span class="n">calinski_harabasz_score</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">copy</span>
</code></pre></div>

<p><a name='x.0.2'></a></p>
<h3 id="502-load-and-process-dataset">5.0.2 Load and Process Dataset<a class="headerlink" href="#502-load-and-process-dataset" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">wine</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/wesleybeckner/&quot;</span>\
      <span class="s2">&quot;ds_for_engineers/main/data/wine_quality/winequalityN.csv&quot;</span><span class="p">)</span>
<span class="c1"># infer str cols</span>
<span class="n">str_cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">wine</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="s1">&#39;object&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1">#set target col</span>
<span class="n">target</span> <span class="o">=</span> <span class="s1">&#39;density&#39;</span>

<span class="n">enc</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="n">imp</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">()</span>

<span class="n">enc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">wine</span><span class="p">[</span><span class="n">str_cols</span><span class="p">])</span>
<span class="n">X_cat</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">wine</span><span class="p">[</span><span class="n">str_cols</span><span class="p">])</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">str_cols</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">imp</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_wine</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X_cat</span><span class="p">,</span> <span class="n">X</span><span class="p">])</span>

<span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">enc</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()]</span>
<span class="n">cols</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">wine</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">cols</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
<span class="p">[</span><span class="n">cols</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">str_cols</span><span class="p">]</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_wine</span><span class="p">[:,</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_wine</span><span class="p">[:,</span><span class="mi">2</span><span class="p">:])</span>

<span class="n">wine</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_wine</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">cols</span><span class="p">)</span>
<span class="n">wine</span><span class="p">[</span><span class="s1">&#39;density&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
</code></pre></div>

<p><a name='3.1'></a></p>
<h2 id="51-principal-component-analysis">5.1 Principal Component Analysis<a class="headerlink" href="#51-principal-component-analysis" title="Permanent link">&para;</a></h2>
<p><a href="#top">back to top</a></p>
<p>Principle Component Analysis or PCA is one of the most wide spread implementations of dimensionality reduction. In PCA, we find the principle components, or linear recombinations of the dimensions of the data, that best explain the variance of the data. </p>
<p><a name='x.1.1'></a></p>
<h3 id="511-the-covariance-matrix">5.1.1 The Covariance Matrix<a class="headerlink" href="#511-the-covariance-matrix" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>In the cell below, we have plotted acidity and density from our familiar wine dataset.</p>
<div class="codehilite"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">wine</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">wine</span><span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;fixed acidity&#39;</span><span class="p">,</span> 
                                     <span class="n">y</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> 
                                     <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> 
                                     <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>&lt;AxesSubplot:xlabel=&#39;fixed acidity&#39;&gt;
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_9_1.png" /></p>
<div class="codehilite"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">wine</span><span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][[</span><span class="s1">&#39;fixed acidity&#39;</span><span class="p">,</span> <span class="s1">&#39;density&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>array([[0.14156636, 0.9978    ],
       [0.45029132, 0.9968    ],
       [0.45029132, 0.997     ],
       [3.07445349, 0.998     ],
       [0.14156636, 0.9978    ]])
</code></pre></div>

<p>The principal components, are actually the eigenvectors of the covariance matrix of the standardized data. </p>
<p>We will perform this by hand to get an understanding.</p>
<p>First we standardize the data</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">X_std</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># note I&#39;ve already done this in 5.0.2</span>
</code></pre></div>

<p>Then we compute the covariance matrix. There is a nice demonstration of computing covariance on <a href="https://www.youtube.com/watch?v=qtaqvPAeEJY">stats quest</a>.</p>
<p>The covariance can be expressed as:</p>
<p>
<script type="math/tex; mode=display"> cov(X,Y) = \frac{1}{n^2}\sum\sum(x_i - x_j)(y_i - y_j) \;\;\;\;\;\sf eq. 1 </script>
</p>
<p>Every \((x_i - x_j)(y_i - y_j)\) is the area described by the rectangle between points \(i\) and \(j\), and we if we deem to color positive changes as red and negative ones as blue, we get a picture like the following: </p>
<p align=center>
<img src="https://i.stack.imgur.com/XPGjN.png"></img>
</p>

<p>When we view all the pairwise interactions in aggregate we get a sense of how the areas of rectangles made by each observation influence the covariance:</p>
<p align=center>
<img src="https://i.stack.imgur.com/Kfmhn.png"></img>
</p>

<p>As a side note, the covariance term is the numerator in the pearsons correlation we covered last week:</p>
<p>
<script type="math/tex; mode=display">\rho_{x,y} = \frac{cov(X,Y)}{\sigma_x\sigma_y} \;\;\;\;\;\sf eq. 2 </script>
</p>
<p>Extrapolating \(Eq. 1\) across the entire matrix, \(X\) of datapoints:</p>
<p>
<script type="math/tex; mode=display"> C = \frac{1}{n-1}(X - \bar{X})^{T} \cdot (X - \bar{X}) \;\;\;\;\;\sf eq. 3 </script>
</p>
<p>The covariance matrix of our wine data can be obtained from \(Eq. 3\):</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">mean_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cov_mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_std</span> <span class="o">-</span> <span class="n">mean_vec</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">((</span><span class="n">X_std</span> <span class="o">-</span> <span class="n">mean_vec</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">X_std</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Covariance matrix </span><span class="se">\n</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span><span class="n">cov_mat</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Covariance matrix 
[[1.00062578 0.66847772]
 [0.66847772 1.00062578]]
</code></pre></div>

<h3 id="512-how-does-the-covariance-matrix-relate-to-the-new-coordinate-system">5.1.2 How Does the Covariance Matrix Relate to the New Coordinate System<a class="headerlink" href="#512-how-does-the-covariance-matrix-relate-to-the-new-coordinate-system" title="Permanent link">&para;</a></h3>
<ol>
<li>We desire a new coordinate system that has no covariance between its dimensions (thus each dimension can be sorted by explained variance to isolate key dimensions (i.e. principal components)) </li>
<li>Because the covariance matrix in \(Eq. 3\) is a square matrix, we can diagonalize it; the new dimensional space whose covariance matrix is expressed by this diagonolized matrix will have the desired properties explained in point 1 (because everything off the diagonal is zero)</li>
<li>The difficult and unintuitive part of PCA is that the vector that produces this transformation to the new coordinate space is given by the eigenvectors of \(C\). For those who are interested in investigating further I suggest reading this <a href="https://stats.stackexchange.com/questions/217995/what-is-an-intuitive-explanation-for-how-pca-turns-from-a-geometric-problem-wit">answer</a> by <em>amoeba</em> and this <a href="https://stats.stackexchange.com/questions/10251/what-is-the-objective-function-of-pca/10256#10256">answer</a> by <em>cardinal</em>. For a more geometric explanation of the principal components checkout the <a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579">grandparent, spouse, daughter parable</a></li>
</ol>
<blockquote>
<p>These arguments coincide with the <em>Spectral theorem</em> explanation of PCA, and you can read more about it in the links provided above</p>
</blockquote>
<p>In 5.1.2.1-5.1.2.3 I provide a segue into deriving eigenvectors and eigenvalues, feel free to visit these foundational topics, although they are not necessary to reap the value of PCA. </p>
<p>For this particular set of wine data, we will see that the corresponding diagonalized matrix will look like:</p>
<p>
<script type="math/tex; mode=display"> \begin{bmatrix} 1.67 & 0 \\ 0 & 0.33 \end{bmatrix} </script>
</p>
<p>At the end of 5.1.3 we will show that this is also the covariance matrix of our data projected into the new coordinate system!</p>
<h4 id="5121-enrichment-deriving-the-eigenvectors-and-eigenvalues">🌭 5.1.2.1 Enrichment: Deriving the Eigenvectors and Eigenvalues<a class="headerlink" href="#5121-enrichment-deriving-the-eigenvectors-and-eigenvalues" title="Permanent link">&para;</a></h4>
<p>The principal components are found mathematically by determining the eigenvectors of the covariance matrix and sorting them by their egienvalues, i.e. their explained variance. </p>
<p>There is a mathematical proof <a href="https://stats.stackexchange.com/questions/217995/what-is-an-intuitive-explanation-for-how-pca-turns-from-a-geometric-problem-wit">1</a>, <a href="https://stats.stackexchange.com/questions/10251/what-is-the-objective-function-of-pca/10256#10256">2</a> for why this works, but we will not cover that here. </p>
<p>So how do we calculate eigenvalues and their correpsonding eigenvectors? This question is usually posited in the following format:</p>
<p>
<script type="math/tex; mode=display"> A \cdot v = \lambda \cdot v \;\;\;\;\;\sf eq. 3 </script>
</p>
<p>In this equation A is an n-by-n matrix (our covariance matrix in this case), v is a non-zero n-by-1 vector and λ is a scalar (which may be either real or complex).  Any value of λ for which this equation has a solution is known as an eigenvalue of the matrix A.</p>
<blockquote>
<p>In other words v, is an eigenvector of A if there exists a scalar value such that \(A \cdot v\) and \(\lambda \cdot v\) will yield the same result</p>
</blockquote>
<p>In high school or college, we might've solved for these eigenvalues and eigenvectors by hand using simple, integer-bound matrices of A. Here's an example, for this matrix: </p>
<p>
<script type="math/tex; mode=display"> \begin{bmatrix} -6 & 3 \\ 4 & 5\end{bmatrix} </script>
</p>
<p>an eigenvector is:</p>
<p>
<script type="math/tex; mode=display"> \begin{bmatrix} 1 \\ 4 \end{bmatrix} </script>
</p>
<p>with a corresponding eigenvalue of 6.</p>
<p>Taking the requisite dot products for each side of eq. 3, \(A v\) gives us:</p>
<p>
<script type="math/tex; mode=display"> \begin{bmatrix} -6 & 3 & \\ 4 & 5 \end{bmatrix} 
\begin{bmatrix} 1 \\ 4 \end{bmatrix} = 
\begin{bmatrix} 6 \\ 24 \end{bmatrix}</script>
</p>
<p>and \(\lambda v\):</p>
<p>
<script type="math/tex; mode=display"> 6 \begin{bmatrix} 1 \\ 4 \end{bmatrix}  =
\begin{bmatrix} 6 \\ 24\end{bmatrix} </script>
</p>
<h4 id="5122-find-the-eigenvalues">🌭  5.1.2.2: Find the Eigenvalues<a class="headerlink" href="#5122-find-the-eigenvalues" title="Permanent link">&para;</a></h4>
<p>The trick that is employed to decompose these equality statements is to multiply the right hand side of eq. 3 by an identity matrix and then subtract this quantity from both sides of the equation. In the case of \(v\) being non-zero, this becomes the <a href="https://www.mathsisfun.com/algebra/matrix-determinant.html">determinant</a>:</p>
<p>
<script type="math/tex; mode=display"> | A - \lambda I | = 0 </script>
</p>
<p>In the case of our simple example</p>
<p>
<script type="math/tex; mode=display"> \begin{vmatrix}  \begin{bmatrix} -6 & 3  \\ 4 & 5 \end{bmatrix} - \lambda
\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \end{vmatrix}  = 0</script>
</p>
<p>simplifies to</p>
<p>
<script type="math/tex; mode=display"> \begin{vmatrix}  -6-\lambda & 3  \\ 4 & 5-\lambda  \end{vmatrix}  = 0</script>
</p>
<p>writing out the determinant</p>
<p>
<script type="math/tex; mode=display"> (-6-\lambda)(5-\lambda) - 3 x 4 = 0 </script>
</p>
<p>gives the quadratic equation</p>
<p>
<script type="math/tex; mode=display"> \lambda^2 + \lambda - 42 = 0 </script>
</p>
<p>and solving for \(\lambda\)</p>
<p>
<script type="math/tex; mode=display"> \lambda = -7 \space or \space 6 </script>
</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">fsolve</span><span class="p">,</span> <span class="n">leastsq</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># define the determinant</span>
<span class="k">def</span> <span class="nf">det</span><span class="p">(</span><span class="n">lamb</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A: the covariance matrix</span>
<span class="sd">    I: the identity matrix</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">lamb</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">lamb</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>

<span class="n">root</span> <span class="o">=</span> <span class="n">fsolve</span><span class="p">(</span><span class="n">det</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">det</span><span class="p">(</span><span class="n">root</span><span class="p">),</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">root</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>[ True  True]
[-7.  6.]
</code></pre></div>

<h4 id="5123-find-the-eigenvectors">🌭  5.1.2.3: Find the Eigenvectors<a class="headerlink" href="#5123-find-the-eigenvectors" title="Permanent link">&para;</a></h4>
<p>We find the eigenvector for each corresponding eigenvalue one at a time</p>
<p>
<script type="math/tex; mode=display"> \begin{bmatrix} -6 & 3 & \\ 4 & 5 \end{bmatrix} 
\begin{bmatrix} x \\ y \end{bmatrix} = 6
\begin{bmatrix} x \\ y \end{bmatrix}</script>
</p>
<p>multiplying out gives the system of equations</p>
<p>
<script type="math/tex; mode=display"> -6x + 3y = 6x </script>
<script type="math/tex; mode=display"> 4x + 5y = 6y </script>
</p>
<p>bringing to the left hand side</p>
<p>
<script type="math/tex; mode=display"> -12x + 3y = 0 </script>
<script type="math/tex; mode=display"> 4x - 1y = 0 </script>
</p>
<p>solving for either equation yeilds \(y = 4x\) so the eigenvector is</p>
<p>
<script type="math/tex; mode=display"> \begin{bmatrix} 1 \\ 4 \end{bmatrix} </script>
</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">eig</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A: the covariance matrix</span>
<span class="sd">    lamb: the eigen value</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">lamb</span><span class="o">*</span><span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">lamb</span><span class="o">*</span><span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>

<span class="n">lamb</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">root</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">vector</span> <span class="o">=</span> <span class="n">fsolve</span><span class="p">(</span><span class="n">eig</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lamb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">eig</span><span class="p">(</span><span class="n">vector</span><span class="p">),</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]))</span>
<span class="n">vector</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">vector</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span><span class="o">/</span><span class="nb">min</span><span class="p">(</span><span class="n">vector</span><span class="p">)))</span>
<span class="n">vector</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">vector</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="n">lamb</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">root</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">vector</span> <span class="o">=</span> <span class="n">fsolve</span><span class="p">(</span><span class="n">eig</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lamb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">eig</span><span class="p">(</span><span class="n">vector</span><span class="p">),</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]))</span>
<span class="n">vector</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">vector</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span><span class="o">/</span><span class="nb">min</span><span class="p">(</span><span class="n">vector</span><span class="p">)))</span>
<span class="n">vector</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">vector</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>-7
[ True  True]
[-3.  1.]

6
[ True  True]
[1. 4.]
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># we can solve for the eigenvalues/vectors of our covariance</span>
<span class="c1"># matrix using numpy!</span>
<span class="n">eig_vals</span><span class="p">,</span> <span class="n">eig_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Eigenvectors </span><span class="se">\n</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span><span class="n">eig_vecs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Eigenvalues </span><span class="se">\n</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span><span class="n">eig_vals</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Eigenvectors 
[[-0.9486833  -0.24253563]
 [ 0.31622777 -0.9701425 ]]

Eigenvalues 
[-7.  6.]
</code></pre></div>

<h3 id="513-projecting-onto-the-principal-components">5.1.3 Projecting onto the Principal Components<a class="headerlink" href="#513-projecting-onto-the-principal-components" title="Permanent link">&para;</a></h3>
<p>To complete our principal component analysis, we need to project our data onto the eigenvectors of the covariance matrix. We can obtain the eigenvectors and corresponding eigenvalues using <code>np</code> or <code>scipy</code>. Here I've completed the task with <code>np</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="n">cov_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_std</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="c1"># we can solve for the eigenvalues/vectors of our covariance</span>
<span class="c1"># matrix using numpy!</span>
<span class="n">eig_vals</span><span class="p">,</span> <span class="n">eig_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Eigenvectors </span><span class="se">\n</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span><span class="n">eig_vecs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Eigenvalues </span><span class="se">\n</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span><span class="n">eig_vals</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Eigenvectors 
[[-0.70710678 -0.70710678]
 [ 0.70710678 -0.70710678]]

Eigenvalues 
[0.33214806 1.6691035 ]
</code></pre></div>

<p>And now we are just going to sort our vectors by their eigenvalues to get the proper order of principal components:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Make a list of (eigenvalue, eigenvector) tuples</span>
<span class="n">eig_pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">eig_vals</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">eig_vecs</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eig_vals</span><span class="p">))]</span>

<span class="c1"># Sort the (eigenvalue, eigenvector) tuples from high to low</span>
<span class="n">eig_pairs</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Visually confirm that the list is correctly sorted by decreasing eigenvalues</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Eigenvalues in descending order:&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">eig_pairs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Eigenvalues in descending order:
1.669103500110071
0.3321480643454986
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">eig_pairs</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>[(1.669103500110071, array([-0.70710678, -0.70710678])),
 (0.3321480643454986, array([-0.70710678,  0.70710678]))]
</code></pre></div>

<p>For those of us who are familiar with eigenvectors, we should recognize that they are necessarily orthogonal to one another. This is good to know from the PCA point of view, because we wouldn't want the variance along one vector to be explained by another. Let's plot these vectors along with the standardized data:</p>
<div class="codehilite"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">vec</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">eig_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),[</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">]):</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span>
           <span class="p">(</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span><span class="o">*-</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span>
           <span class="p">(</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span><span class="o">*-</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span>
          <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;tab:</span><span class="si">{</span><span class="n">color</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span>
           <span class="p">(</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span><span class="o">*</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span>
           <span class="p">(</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span><span class="o">*</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span>
          <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;tab:</span><span class="si">{</span><span class="n">color</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span><span class="nb">max</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span><span class="nb">max</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>(-2.138871623907465, 4.356979103463171)
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_29_1.png" /></p>
<p>We indeed see that these vectors are orthogonal. </p>
<p>Continuing on with our task of projecting the data onto our principal components, in order to project our data onto the PCs I'll need to reshape <code>eig_pairs</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="n">matrix_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">eig_pairs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
                      <span class="n">eig_pairs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Matrix W:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">matrix_w</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Matrix W:
 [[-0.70710678 -0.70710678]
 [-0.70710678  0.70710678]]
</code></pre></div>

<p>And now taking the dot product:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Y</span> <span class="o">=</span> <span class="n">X_std</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">matrix_w</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">Y</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>&lt;matplotlib.collections.PathCollection at 0x7f681bdf3100&gt;
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_35_1.png" /></p>
<p>We see that our data is dispersed nicely along these PCs. Finally to tie this in with the point made at the end of 5.1.2, we see that the covariance matrix for the data in this new space is described by the diagonalized matrix of the former dimensional space:</p>
<div class="codehilite"><pre><span></span><code><span class="n">mean_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cov_mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">mean_vec</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">((</span><span class="n">Y</span> <span class="o">-</span> <span class="n">mean_vec</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cov_mat</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>array([[1.66910350e+00, 1.76746394e-16],
       [1.76746394e-16, 3.32148064e-01]])
</code></pre></div>

<h3 id="514-cumulative-explained-variance">5.1.4 Cumulative Explained Variance<a class="headerlink" href="#514-cumulative-explained-variance" title="Permanent link">&para;</a></h3>
<p>Often we will need to decide just how many principal components are enough, especially with high dimensional data containing many colinear variables. To assist with this, data scientists will plot the cumulative explained variance. The explained variance is captured by the eigenvalues (this is why we sort by the eigenvalues in the first place). </p>
<div class="codehilite"><pre><span></span><code><span class="n">tot</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">eig_vals</span><span class="p">)</span>
<span class="n">var_exp</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span> <span class="o">/</span> <span class="n">tot</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">eig_vals</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>
<span class="n">cum_var_exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">var_exp</span><span class="p">)</span>

<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="s1">&#39;seaborn-whitegrid&#39;</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">var_exp</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;individual explained variance&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">cum_var_exp</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;mid&#39;</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;cumulative explained variance&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Explained variance ratio&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Principal components&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center right&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_39_0.png" /></p>
<h3 id="515-pca-with-scikit-learn">5.1.5 PCA with Scikit-Learn<a class="headerlink" href="#515-pca-with-scikit-learn" title="Permanent link">&para;</a></h3>
<p>But we can avoid the fancy footwork and do all this in sklearn!</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_std</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>PCA(n_components=2)
</code></pre></div>

<p>We see that the values we get are the same as for the hand-calculated eigenvalues and vectors</p>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>[[ 0.70710678  0.70710678]
 [ 0.70710678 -0.70710678]]
</code></pre></div>

<p>And the eigenvalues are under <code>pca.explained_variance_</code></p>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>[1.6691035  0.33214806]
</code></pre></div>

<p><a name='3.1.1'></a></p>
<h3 id="516-pca-as-dimensionality-reduction">5.1.6 PCA as Dimensionality Reduction<a class="headerlink" href="#516-pca-as-dimensionality-reduction" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>One obvious use case for PCA, is to drop the dimensions with lowest explained variance. Continuing with our acidity/density example this will look like the following</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_std</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>PCA(n_components=2)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">vec</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">eig_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),[</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">]):</span>

  <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span>
           <span class="p">(</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span><span class="o">*-</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span>
           <span class="p">(</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span><span class="o">*-</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span>
          <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;tab:</span><span class="si">{</span><span class="n">color</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

  <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span>
           <span class="p">(</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span><span class="o">*</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span>
           <span class="p">(</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span><span class="o">*</span><span class="n">eig_pairs</span><span class="p">[</span><span class="n">vec</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span>
          <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;tab:</span><span class="si">{</span><span class="n">color</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span><span class="nb">max</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span><span class="nb">max</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Normalized density&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Normalized acidity&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">var_exp</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Individual&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">cum_var_exp</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;mid&#39;</span><span class="p">,</span>
          <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Cumulative&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Explained variance ratio&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Principal components&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>&lt;matplotlib.legend.Legend at 0x7f68189c5520&gt;
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_48_1.png" /></p>
<p>We can capture 80% of the explained variance along just the first principal component. What does this projection look like?</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># we set our components to 1</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_std</span><span class="p">)</span>

<span class="c1"># we then project the data onto the first PC</span>
<span class="c1"># and then rebroadcast this transformation </span>
<span class="c1"># back onto the orginal dimensions to see </span>
<span class="c1"># what this looks like in terms of acidity/density</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_std</span><span class="p">))</span>

<span class="c1"># original data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_std</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_std</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># projected data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>&lt;matplotlib.collections.PathCollection at 0x7f681c292ee0&gt;
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_50_1.png" /></p>
<p><a name='3.1.2'></a></p>
<h3 id="517-pca-for-visualization">5.1.7 PCA for visualization<a class="headerlink" href="#517-pca-for-visualization" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>For classification tasks, PCA lends itself as a useful method for seeing how the classes separate on the highest variance dimensions of the data without consideration to the classes themselves, i.e. do our classes seperate out according to the other variables in the dataset.</p>
<p>With our wine dataset, we see that the wine types fall out nicely along the first two principal components</p>
<div class="codehilite"><pre><span></span><code><span class="n">X_std</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">wine</span><span class="p">)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_std</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_std</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">wine</span><span class="p">[</span><span class="s1">&#39;white&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;First PC&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Second PC&#39;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Text(0, 0.5, &#39;Second PC&#39;)
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_52_1.png" /></p>
<p><a name='3.1.3'></a></p>
<h3 id="518-enrichment-pca-as-outlier-removal-and-noise-filtering">🌭 5.1.8 Enrichment: PCA as Outlier Removal and Noise Filtering<a class="headerlink" href="#518-enrichment-pca-as-outlier-removal-and-noise-filtering" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>In some cases, it can be advantageous to use PCA as a method for outlier removal. There are many caveats to this that we will not discuss here. But for those who are interested, <a href="https://towardsdatascience.com/multivariate-outlier-detection-in-high-dimensional-spectral-data-45878fd0ccb8">this</a> is a great overview article. As for noise filteration, <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html#PCA-as-Noise-Filtering">Vanderplas' DS handbook</a> has a good section on the topic as does <a href="https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/">Guido/Muller's Intro to ML with Python</a> by the same pusblisher.</p>
<p><a name='3.1.4'></a></p>
<h3 id="519-pca-for-feature-engineering">5.1.9 PCA for Feature Engineering<a class="headerlink" href="#519-pca-for-feature-engineering" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>Finally, PCA is actually a commonly used preprocessing technique for supervised machine learning models. In the next exercise, our goal will be to use PCA to generate a new set of features, to feed into our linear model.</p>
<h4 id="exercise-1-pca-as-preprocessing-for-models">🏋️ Exercise 1: PCA as Preprocessing for Models<a class="headerlink" href="#exercise-1-pca-as-preprocessing-for-models" title="Permanent link">&para;</a></h4>
<p>Using the wine data, select any number of the first principal components and attemp to predict density for the red wine data.</p>
<p>Consider that the maximum number of principal components are:</p>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max principal components: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># Code Cell for Exercise 1</span>

<span class="c1">################################################################################</span>
<span class="c1">##### CREATE A SKLEARN-PCA OBJECT, FIT AND TRANSFORM TO THE WINE DATA ##########</span>
<span class="c1">################################################################################</span>

<span class="c1"># as you do this, be sure to remove &#39;density&#39; from the input features</span>

<span class="c1">################################################################################</span>
<span class="c1">############################## UNCOMMENT THE BELOW #############################</span>
<span class="c1">################################################################################</span>
<span class="c1"># plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2, c=wine[&#39;white&#39;].values,</span>
<span class="c1">#             edgecolor=&#39;grey&#39;)</span>
<span class="c1"># plt.xlabel(&#39;First PC&#39;)</span>
<span class="c1"># plt.ylabel(&#39;Second PC&#39;)</span>
<span class="c1"># plt.show()</span>

<span class="c1"># model = LinearRegression()</span>
<span class="c1"># X_train, X_test, y_train, y_test = train_test_split(X_pca, y_wine, train_size=0.8, random_state=42)</span>

<span class="c1"># model.fit(X_train, y_train)</span>
<span class="c1"># y_pred = model.predict(X_test)</span>

<span class="c1"># print(r2_score(y_test, y_pred))</span>
<span class="c1"># print(r2_score(y_train, model.predict(X_train)))</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_57_0.png" /></p>
<div class="codehilite"><pre><span></span><code>0.9634516142421967
0.953295487875815
</code></pre></div>

<p><a name='3.2'></a></p>
<h2 id="52-k-means-clustering">5.2 K-Means Clustering<a class="headerlink" href="#52-k-means-clustering" title="Permanent link">&para;</a></h2>
<p><a href="#top">back to top</a></p>
<p>We now embark on a second class of unsupervised learning techinques: clustering. </p>
<p>The K-means algorithm works under two assumptions: 
  * every cluster can be defined by an arithmetic mean or cluster center
  * each point is closer to one arithmetic center than the other centers</p>
<p>Let's turn back to our wine dataset:</p>
<div class="codehilite"><pre><span></span><code><span class="n">X_std</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_wine</span><span class="p">)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_std</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_std</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;First PC&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Second PC&#39;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Text(0, 0.5, &#39;Second PC&#39;)
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_59_1.png" /></p>
<p>It is fairly obvious under these two dimensions that there are two clusters. The K-means algorithm automatically pics this out</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
<span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>

<span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_61_0.png" /></p>
<p>wa-la! Our simple unsupervised algorithm does a pretty good job of differentiating between the red and white wine clusters.</p>
<p><a name='3.2.1'></a></p>
<h3 id="521-the-algorithm-expectation-maximization">5.2.1 The Algorithm: Expectation-Maximization<a class="headerlink" href="#521-the-algorithm-expectation-maximization" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>We won't go too far into the implementation of the underlying algorithm here. In its basic steps, however, it performs the following:</p>
<ol>
<li>Initialize cluster centers (random guess)</li>
<li>Then repeat:</li>
<li>E-Step: assign points to the nearest center (arithmetic distance)</li>
<li>M-step: set the new center point for each cluster according to the mean of it's datapoint members</li>
</ol>
<p>More information on K-means algorithm can be explored <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html#k-Means-Algorithm:-Expectation%E2%80%93Maximization">here</a></p>
<p><a name='3.2.2'></a></p>
<h3 id="522-limitations">5.2.2 Limitations<a class="headerlink" href="#522-limitations" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>A few brief notes on limitations:</p>
<ol>
<li>the global optimum may not be achieved (no guarantee of finding the overall best solution)</li>
<li>the number of clusters must be guessed beforehand</li>
<li>cluster boundaries are unavoidably linear<ul>
<li>and the cluster assignments are unavoidably circular</li>
</ul>
</li>
<li>can be slow for large datasets</li>
<li>cluster assignments are non probabilistic</li>
</ol>
<p>3 and 5 motivate our next section, Gaussian Mixture Models</p>
<h3 id="523-determining-k-with-the-elbow-method">5.2.3 Determining K with the Elbow Method<a class="headerlink" href="#523-determining-k-with-the-elbow-method" title="Permanent link">&para;</a></h3>
<p>The elbow method is a popular technique for determining the value of <code>k</code>. It involves looping through a range of <code>k</code>'s and assessing some goodness of fit metric. Intuitively, we might presume those metrics involve some measure of the distance of datapoints to their cluster centers. We have options:</p>
<ol>
<li>Distortion: distance (<em>typically Euclidean</em>) from the cluster centers averaged across the respective clusters. </li>
<li>Inertia: the sum of squared distances of samples to their closest cluster center.</li>
<li>Silhouette: calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. <ul>
<li>To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of</li>
<li>The Silhouette Coefficient for a sample is (b-a) / max(b-a)</li>
<li>best value is 1 worst value is -1</li>
</ul>
</li>
<li>Calinski Harabasz Score or Variance Ratio: the ratio between within-cluster dispersion and between-cluster dispersion<ul>
<li><em>should sound familiar to our <a href="https://wesleybeckner.github.io/data_science_foundations/S2_Inferential_Statistics/#steps-for-anova">ANOVA discussion</a></em></li>
<li>higher is better</li>
</ul>
</li>
</ol>
<p>And there are many other <a href="https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation">methods</a> of evaluating cluster assignment performance</p>
<h3 id="question-1-comparing-metrics">🙋‍♀️ Question 1: Comparing Metrics<a class="headerlink" href="#question-1-comparing-metrics" title="Permanent link">&para;</a></h3>
<p>What is the primary difference between Distortion, Inertia vs Silhouette, Calinksi?</p>
<div class="codehilite"><pre><span></span><code><span class="n">distortions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">inertias</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">silhouette</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">variance</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
    <span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span>

    <span class="n">distortions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">cdist</span><span class="p">(</span><span class="n">X_pca</span><span class="p">,</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">,</span>
                                        <span class="s1">&#39;euclidean&#39;</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">inertias</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">silhouette</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">silhouette_score</span><span class="p">(</span><span class="n">X_pca</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">metric</span> <span class="o">=</span> <span class="s1">&#39;euclidean&#39;</span><span class="p">))</span>
        <span class="n">variance</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calinski_harabasz_score</span><span class="p">(</span><span class="n">X_pca</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
</code></pre></div>

<p>We note in the following plots that inertia and distortion asymptotically improve with higher k (because they are unregularized) whereas silhouette and calinski metrics are penalized for inter-cluster relatability</p>
<div class="codehilite"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="p">[[</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">],</span> <span class="p">[</span><span class="n">ax3</span><span class="p">,</span> <span class="n">ax4</span><span class="p">]]</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">distortions</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">inertias</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">silhouette</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">variance</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>[&lt;matplotlib.lines.Line2D at 0x7f67fc5d8bb0&gt;]
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_69_1.png" /></p>
<div class="codehilite"><pre><span></span><code><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
<span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>

<span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_70_0.png" /></p>
<p><a name='3.3'></a></p>
<h2 id="53-gaussian-mixture-models">5.3 Gaussian Mixture Models<a class="headerlink" href="#53-gaussian-mixture-models" title="Permanent link">&para;</a></h2>
<p><a href="#top">back to top</a></p>
<p>in the simplest case, GMMs can be used in the same way as K-means</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>
<span class="n">gmm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> 
            <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">);</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_72_0.png" /></p>
<p>But because GMM is a probablistic model, we can visualize the certainty with which we assign these clusters.</p>
<div class="codehilite"><pre><span></span><code><span class="n">probs</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">5</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>[[0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]]
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># convert probs to 1 dimension</span>
<span class="n">probs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>array([1.        , 0.99999994, 0.99999999, ..., 1.        , 1.        ,
       0.99999189])
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> 
            <span class="n">c</span><span class="o">=</span><span class="n">probs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">);</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_76_0.png" /></p>
<p><a name='3.3.1'></a></p>
<h3 id="531-generalizing-e-m-for-gmms">5.3.1 Generalizing E-M for GMMs<a class="headerlink" href="#531-generalizing-e-m-for-gmms" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>The algorithm for GMMs is very similar to K-means, but now the EM steps are probablistic <a href="https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95">1</a> <a href="https://www.deep-teaching.org/notebooks/graphical-models/directed/exercise-1d-gmm-em">2</a></p>
<ol>
<li>Initialize cluster centers (random guess)</li>
<li>Then repeat:</li>
<li>E-Step: assign points their probability of belonging to every cluster</li>
<li>M-step: set the new center point for each cluster according to the probabilities of <em>all</em> datapoints</li>
</ol>
<p><a name='3.3.2'></a></p>
<h3 id="532-gmms-as-a-data-generator">5.3.2 GMMs as a Data Generator<a class="headerlink" href="#532-gmms-as-a-data-generator" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>One particularly fascinating application of GMMs is to use them to generate new data that is similar to the data on which they are modeled. Let's take an example.</p>
<p>We're going to use sklearn make_circles function to create some arbitrary data that has a complex relationship along two different axes.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># some helper functions borrowed from Jake Vanderplas with a few minor tweaks</span>
<span class="c1"># https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html</span>

<span class="kn">from</span> <span class="nn">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>

<span class="k">def</span> <span class="nf">draw_ellipse</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">covariance</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Draw an ellipse with a given position and covariance&quot;&quot;&quot;</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>

    <span class="c1"># Convert covariance to principal axes</span>
    <span class="k">if</span> <span class="n">covariance</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">covariance</span><span class="p">)</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">degrees</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">U</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">U</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
        <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">covariance</span><span class="p">)</span>

    <span class="c1"># Draw the Ellipse</span>
    <span class="k">for</span> <span class="n">nsig</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">nsig</span> <span class="o">*</span> <span class="n">width</span><span class="p">,</span> <span class="n">nsig</span> <span class="o">*</span> <span class="n">height</span><span class="p">,</span>
                             <span class="n">angle</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">plot_gmm</span><span class="p">(</span><span class="n">gmm</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">data_alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">label</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                   <span class="n">alpha</span><span class="o">=</span><span class="n">data_alpha</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">data_alpha</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

    <span class="n">w_factor</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">/</span> <span class="n">gmm</span><span class="o">.</span><span class="n">weights_</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">covar</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">means_</span><span class="p">,</span> <span class="n">gmm</span><span class="o">.</span><span class="n">covariances_</span><span class="p">,</span> <span class="n">gmm</span><span class="o">.</span><span class="n">weights_</span><span class="p">):</span>
        <span class="n">draw_ellipse</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">covar</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">w</span> <span class="o">*</span> <span class="n">w_factor</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span> <span class="k">as</span> <span class="n">gen</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]);</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_80_0.png" /></p>
<p>As a side note, as a clustering model, the GMM is not particularly useful:</p>
<div class="codehilite"><pre><span></span><code><span class="n">gmm2</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">plot_gmm</span><span class="p">(</span><span class="n">gmm2</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_83_0.png" /></p>
<p>But if we use the GMM as a density estimator on the underlying data, rather than as a clustering algorithm, we find a completely different use case:</p>
<div class="codehilite"><pre><span></span><code><span class="n">gmm16</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plot_gmm</span><span class="p">(</span><span class="n">gmm16</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_85_0.png" /></p>
<p>Now, with the distributions drawn, we can assemble entirely new data:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Xnew</span> <span class="o">=</span> <span class="n">gmm16</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">400</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xnew</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Xnew</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]);</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_87_0.png" /></p>
<p><a name='x.3.2.1'></a></p>
<h4 id="5321-determining-the-number-of-components">5.3.2.1 Determining the number of components<a class="headerlink" href="#5321-determining-the-number-of-components" title="Permanent link">&para;</a></h4>
<p><a href="#top">back to top</a></p>
<p>Let's think back to session 1 on model selection. How might we determine the best number of components?</p>
<p>A couple analytic approaches that we have not much yet discussed, are the <strong>Akaike Information Criterion (AIC)</strong> and <strong>Bayesian Information Criterion (BIC).</strong> The important thing to note from each of these is that they penalize added complexity to the models, and we would like to minimize their estimated prediction error.</p>
<div class="codehilite"><pre><span></span><code><span class="n">n_components</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_components</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">bic</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;BIC&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">aic</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;AIC&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;n_components&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;est. prediction error&#39;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Text(0, 0.5, &#39;est. prediction error&#39;)
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_89_1.png" /></p>
<p>Both AIC and BIC capture a local minimum on the order of 6 Gaussians. This is where the model captures the overall circular modality of the data. However it totally misses the circumscribed nature of the two circles. Increasing this number to 30-40 gaussians captures this feature of the data. The AIC score reflects this while the BIC score (although captures a local minima in the area) does not define this as a global optimum.</p>
<div class="codehilite"><pre><span></span><code><span class="n">gmmNew</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plot_gmm</span><span class="p">(</span><span class="n">gmmNew</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">data_alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_91_0.png" /></p>
<div class="codehilite"><pre><span></span><code><span class="n">Xnew</span> <span class="o">=</span> <span class="n">gmmNew</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">400</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xnew</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Xnew</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]);</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_92_0.png" /></p>
<h4 id="exercise-2-determine-number-of-components-for-circular-moons">🏋️ Exercise 2: Determine Number of Components for Circular Moons<a class="headerlink" href="#exercise-2-determine-number-of-components-for-circular-moons" title="Permanent link">&para;</a></h4>
<p>Repeat the above, this time using <code>sklearn.datasets.make_moons</code></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Code Cell for Exercise 2</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span> <span class="k">as</span> <span class="n">gen</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1">################################################################################</span>
<span class="c1">##### FIT GMM MODEL(S) TO 1-42 CLUSTER CENTERS AND RECORD THE AIC/BIC ##########</span>
<span class="c1">################################################################################</span>


<span class="c1"># uncomment these lines</span>
<span class="c1"># plt.plot(n_components, [m.bic(X) for m in models], label=&#39;BIC&#39;)</span>
<span class="c1"># plt.plot(n_components, [m.aic(X) for m in models], label=&#39;AIC&#39;)</span>
<span class="c1"># plt.legend(loc=&#39;best&#39;)</span>
<span class="c1"># plt.xlabel(&#39;n_components&#39;);</span>
<span class="c1"># plt.ylabel(&#39;est. prediction error&#39;)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Text(0, 0.5, &#39;est. prediction error&#39;)
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_94_1.png" /></p>
<div class="codehilite"><pre><span></span><code><span class="n">gmm_moon</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">plot_gmm</span><span class="p">(</span><span class="n">gmm_moon</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_96_0.png" /></p>
<div class="codehilite"><pre><span></span><code><span class="n">Xnew</span> <span class="o">=</span> <span class="n">gmm_moon</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">400</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xnew</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Xnew</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]);</span>
</code></pre></div>

<p><img alt="png" src="../S5_Unsupervised_Learning_files/S5_Unsupervised_Learning_97_0.png" /></p>
<h1 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h1>
<h2 id="pca">PCA<a class="headerlink" href="#pca" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p><a href="https://stats.stackexchange.com/questions/217995/what-is-an-intuitive-explanation-for-how-pca-turns-from-a-geometric-problem-wit">Intuitive PCA</a></p>
</li>
<li>
<p><a href="https://stats.stackexchange.com/questions/10251/what-is-the-objective-function-of-pca/10256#10256">PCA and Eigenvectors/values</a></p>
</li>
</ul>
<h2 id="gmm">GMM<a class="headerlink" href="#gmm" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p><a href="https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95">GMMs Explained</a> </p>
</li>
<li>
<p><a href="https://www.deep-teaching.org/notebooks/graphical-models/directed/exercise-1d-gmm-em">Derive GMM Exercise</a></p>
</li>
</ul>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../S4_Feature_Engineering/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Feature Engineering" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Feature Engineering
            </div>
          </div>
        </a>
      
      
        
        <a href="../S6_Bagging/" class="md-footer__link md-footer__link--next" aria-label="Next: Bagging" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Bagging
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.361d90f1.min.js"}</script>
    
    
      <script src="../assets/javascripts/bundle.289a2a4b.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>