{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science Foundations Welcome to Data Science Foundations, my name is Wesley . Whether you've landed here by accident or on purpose we (the royal we, man) are glad to have you \ud83d\ude02. Visit the github repo to access the original jupyter notebooks for this class. Happy learning!","title":"Introduction"},{"location":"#data-science-foundations","text":"Welcome to Data Science Foundations, my name is Wesley . Whether you've landed here by accident or on purpose we (the royal we, man) are glad to have you \ud83d\ude02. Visit the github repo to access the original jupyter notebooks for this class. Happy learning!","title":"Data Science Foundations"},{"location":"S1_Regression_and_Analysis/","text":"Data Science Foundations Session 1: Regression and Analysis Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at fitting data to a curve using regression . We will also look at using regression to make predictions for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error. 1.0 Preparing Environment and Importing Data back to top 1.0.1 Import Packages back to top # Import pandas, pyplot, ipywidgets import pandas as pd from matplotlib import pyplot as plt from ipywidgets import interact # Import Scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score # for enrichment topics import seaborn as sns import numpy as np 1.0.2 Load Dataset back to top For our discussion on regression and descriptive statistics today we will use a well known dataset of different wines and their quality ratings df = pd.read_csv(\"https://raw.githubusercontent.com/wesleybeckner/\"\\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\") df.shape (6497, 13) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality count 6487.000000 6489.000000 6494.000000 6495.000000 6495.000000 6497.000000 6497.000000 6497.000000 6488.000000 6493.000000 6497.000000 6497.000000 mean 7.216579 0.339691 0.318722 5.444326 0.056042 30.525319 115.744574 0.994697 3.218395 0.531215 10.491801 5.818378 std 1.296750 0.164649 0.145265 4.758125 0.035036 17.749400 56.521855 0.002999 0.160748 0.148814 1.192712 0.873255 min 3.800000 0.080000 0.000000 0.600000 0.009000 1.000000 6.000000 0.987110 2.720000 0.220000 8.000000 3.000000 25% 6.400000 0.230000 0.250000 1.800000 0.038000 17.000000 77.000000 0.992340 3.110000 0.430000 9.500000 5.000000 50% 7.000000 0.290000 0.310000 3.000000 0.047000 29.000000 118.000000 0.994890 3.210000 0.510000 10.300000 6.000000 75% 7.700000 0.400000 0.390000 8.100000 0.065000 41.000000 156.000000 0.996990 3.320000 0.600000 11.300000 6.000000 max 15.900000 1.580000 1.660000 65.800000 0.611000 289.000000 440.000000 1.038980 4.010000 2.000000 14.900000 9.000000 1.1 What is regression? It is the process of finding a relationship between dependent and independent variables to find trends in data. This abstract definition means that you have one variable (the dependent variable) which depends on one or more variables (the independent variables). One of the reasons for which we want to regress data is to understand whether there is a trend between two variables. Housing Prices Example We can imagine this scenario with housing prices. Envision a mixed dataset of continuous and discrete independent variables. Some features could be continuous, floating point values like location ranking and housing condition. Others could be descrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a regression model. 1.2 Linear regression fitting with scikit-learn \ud83c\udfcb\ufe0f Exercise 1: rudimentary EDA What does the data look like? Recall how to visualize data in a pandas dataframe for every column calculate the * skew: `df.skew()` * kurtosis: `df.kurtosis()` * pearsons correlation with the dependent variable: `df.corr()` * number of missing entries `df.isnull()` and organize this into a new dataframe note: pearsons is just one type of correlation, another available to us spearman which differs from pearsons in that it depends on ranked values rather than their direct quantities, you can read more here df.isnull().sum() type 0 fixed acidity 10 volatile acidity 8 citric acid 3 residual sugar 2 chlorides 2 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 9 sulphates 4 alcohol 0 quality 0 dtype: int64 # Cell for Exercise 1 # part A # using df.<method> define the following four variables with the results from # skew(), kurtosis(), corr() (and selecting for quality), and isnull() # for isnull() you'll notice the return is a dataframe of booleans. we would # like to simply know the number of null values for each column. change the # return of isnull() using the sum() method # skew = # kurt = # pear = # null = # part B # on line 13, put these results in a list using square brackets and call # pandas.DataFrame on the list to make your new DataFrame! store it under the # variable name dff # part C # take the transpose of this DataFrame using dff.T. reassign dff to this copy # part D # set the column names to 'skew', 'kurtosis', 'pearsons _quality', and # 'null count' using dff.columns # Now return dff to the output to view your hand work # dff I have gone ahead and repeated this exercise with the red vs white wine types: red = df.loc[df['type'] == 'red'] wht = df.loc[df['type'] == 'white'] def get_summary(df): skew = df.skew() kurt = df.kurtosis() pear = df.corr()['quality'] null = df.isnull().sum() med = df.median() men = df.mean() dff = pd.DataFrame([skew, kurt, pear, null, med, men]) dff = dff.T dff.columns = ['skew', 'kurtosis', 'pearsons _quality', 'null count', 'median', 'mean'] return dff dffr = get_summary(red) dffw = get_summary(wht) desc = pd.concat([dffr, dffw], keys=['red', 'white']) desc .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count median mean red fixed acidity 0.982192 1.132624 0.123834 2.0 7.90000 8.322104 volatile acidity 0.672862 1.226846 -0.390858 1.0 0.52000 0.527738 citric acid 0.317891 -0.788476 0.226917 1.0 0.26000 0.271145 residual sugar 4.540655 28.617595 0.013732 0.0 2.20000 2.538806 chlorides 5.680347 41.715787 -0.128907 0.0 0.07900 0.087467 free sulfur dioxide 1.250567 2.023562 -0.050656 0.0 14.00000 15.874922 total sulfur dioxide 1.515531 3.809824 -0.185100 0.0 38.00000 46.467792 density 0.071288 0.934079 -0.174919 0.0 0.99675 0.996747 pH 0.194803 0.814690 -0.057094 2.0 3.31000 3.310864 sulphates 2.429115 11.712632 0.251685 2.0 0.62000 0.658078 alcohol 0.860829 0.200029 0.476166 0.0 10.20000 10.422983 quality 0.217802 0.296708 1.000000 0.0 6.00000 5.636023 type NaN NaN NaN 0.0 NaN NaN white fixed acidity 0.647981 2.176560 -0.114032 8.0 6.80000 6.855532 volatile acidity 1.578595 5.095526 -0.194976 7.0 0.26000 0.278252 citric acid 1.284217 6.182036 -0.009194 2.0 0.32000 0.334250 residual sugar 1.076601 3.469536 -0.097373 2.0 5.20000 6.393250 chlorides 5.023412 37.560847 -0.210181 2.0 0.04300 0.045778 free sulfur dioxide 1.406745 11.466342 0.008158 0.0 34.00000 35.308085 total sulfur dioxide 0.390710 0.571853 -0.174737 0.0 134.00000 138.360657 density 0.977773 9.793807 -0.307123 0.0 0.99374 0.994027 pH 0.458402 0.532552 0.098858 7.0 3.18000 3.188203 sulphates 0.977361 1.589847 0.053690 2.0 0.47000 0.489835 alcohol 0.487342 -0.698425 0.435575 0.0 10.40000 10.514267 quality 0.155796 0.216526 1.000000 0.0 6.00000 5.877909 type NaN NaN NaN 0.0 NaN NaN def my_fig(metric=desc.columns): fig, ax = plt.subplots(1, 1, figsize=(10,10)) pd.DataFrame(desc[metric]).unstack()[metric].T.plot(kind='barh', ax=ax) interact(my_fig) interactive(children=(Dropdown(description='metric', options=('skew', 'kurtosis', 'pearsons _quality', 'null c\u2026 <function __main__.my_fig> \ud83d\ude4b Question 1: Discussion Around EDA Plot What do we think of this plot? metric = mean , the cholrides values metric = kurtosis , residual sugar metric = pearsons _quality , magnitudes and directions How to improve the plot, what other plots would we like to see? df['chlorides'].describe() count 6495.000000 mean 0.056042 std 0.035036 min 0.009000 25% 0.038000 50% 0.047000 75% 0.065000 max 0.611000 Name: chlorides, dtype: float64 fig, ax = plt.subplots(1,1,figsize=(10,10)) df['chlorides'].plot(kind='kde',ax=ax) ax.set_xlim(0,.61) (0.0, 0.61) df['chlorides'].sort_values(ascending=False)[:50] 5156 0.611 5049 0.610 5004 0.467 4979 0.464 5590 0.422 5652 0.415 6268 0.415 6270 0.415 6217 0.414 5949 0.414 5349 0.413 6158 0.403 4981 0.401 5628 0.387 6063 0.369 4915 0.368 5067 0.360 5179 0.358 484 0.346 5189 0.343 4917 0.341 5124 0.337 4940 0.332 1217 0.301 687 0.290 4473 0.271 5079 0.270 6272 0.267 5138 0.263 1865 0.255 5466 0.250 1034 0.244 5674 0.243 5675 0.241 683 0.240 1638 0.239 5045 0.236 6456 0.235 6468 0.230 5465 0.226 5464 0.226 5564 0.222 2186 0.217 5996 0.216 6332 0.214 6333 0.214 5206 0.214 5205 0.213 4497 0.212 1835 0.211 Name: chlorides, dtype: float64 1.2.2 Visualizing the data set - motivating regression analysis We can create a scatter plot of fixed acidity vs density of red wine in the dataset using df.plot() and see that there appears to be a general trend between the two features: fig, ax = plt.subplots(1, 1, figsize=(5,5)) df.loc[df['type'] == 'red'].plot(x='fixed acidity', y='density', ax=ax, ls='', marker='.') <matplotlib.axes._subplots.AxesSubplot at 0x7f3158ad7290> 1.2.3 Estimating the regression coefficients It looks like density increases with fixed acidity following a line, maybe something like y(x)= m \\cdot x + b \\;\\;\\;\\;\\;\\;\\;\\; \\sf{eq. 1} with \\( y=\\sf density \\), \\(x=\\sf fixed acidity\\), and \\(m\\) the slope and \\(b\\) the intercept. To solve the problem, we need to find the values of \\(b\\) and \\(m\\) in equation 1 to best fit the data. This is called linear regression . In linear regression our goal is to minimize the error between computed values of positions \\(y^{\\sf calc}(x_i)\\equiv y^{\\sf calc}_i\\) and known values \\(y^{\\sf exact}(x_i)\\equiv y^{\\sf exact}_i\\), i.e. find \\(b\\) and \\(m\\) which lead to lowest value of \\epsilon (m,b) =SS_{\\sf res}=\\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - m\\cdot x_i - b \\right)^2\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sf{eq. 2} To find out more see e.g. https://en.wikipedia.org/wiki/Simple_linear_regression \ud83d\ude4b Question 2: linear regression loss function Do we always want m and b to be large positive numbers so as to minimize eq. 2? Luckily scikit-learn contains many functions related to regression including linear regression . The function we will use is called LinearRegression() . # Create linear regression object model = linear_model.LinearRegression() # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red['density'].values.reshape(-1, 1) y = red['fixed acidity'].values.reshape(-1, 1) # Create linear regression object model = linear_model.LinearRegression() # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red['density'].values.reshape(-1, 1) y = red['fixed acidity'].values.reshape(-1, 1) print(red['density'].values.shape, red['fixed acidity'].values.shape) print(x.shape, y.shape) print(red['density'].values.shape, red['fixed acidity'].values.shape) print(x.shape, y.shape) (1599,) (1599,) (1599, 1) (1599, 1) # Fit to the data model.fit(x, y) # Extract the values of interest m = model.coef_[0][0] b = model.intercept_[0] # Print the slope m and intercept b print('Scikit learn - Slope: ', m , 'Intercept: ', b ) What happens when we try to fit the data as is? # Fit to the data # model.fit(x, y) \ud83c\udfcb\ufe0f Exercise 2: drop Null Values (and practice pandas operations) Let's look back at our dataset description dataframe above, what do we notice, what contains null values? There are several strategies for dealing with null values. For now let's take the simplest case, and drop rows in our dataframe that contain null # Cell for Exercise 2 # For this templated exercise you are going to complete everything in one line # of code, but we are going to break it up into steps. So for each part (A, B, # etc.) paste your answer from the previous part to begin # step A # select the 'density' and 'fixed acidity' columns of red. make sure the return # is a dataframe # step B # now use the dropna() method on axis 0 (the rows) to drop any null values # step B # select column 'density' # step C # select the values # step D # reshape the result with an empty second dimension using .reshape() and store # the result under variable x # repeat the same process with 'fixed acidity' and variable y Now that we have our x and y arrays we can fit using ScikitLearn x = red[['density', 'fixed acidity']].dropna(axis=0)['density'].values.reshape(-1,1) y = red[['density', 'fixed acidity']].dropna(axis=0)['fixed acidity'].values.reshape(-1,1) # Fit to the data model.fit(x, y) # Extract the values of interest m = model.coef_[0][0] b = model.intercept_[0] # Print the slope m and intercept b print('Scikit learn - Slope: ', m , 'Intercept: ', b ) Scikit learn - Slope: 616.0131428066102 Intercept: -605.6880086750525 \ud83c\udfcb\ufe0f Exercise 3: calculating y_pred Estimate the values of \\(y\\) by using your fitted parameters. Hint: Use your model.coef_ and model.intercept_ parameters to estimate y_pred following equation 1 # define y_pred in terms of m, x, and b # y_pred = # fig, ax = plt.subplots(1,1, figsize=(10,10)) # ax.plot(x, y_pred, ls='', marker='*') # ax.plot(x, y, ls='', marker='.') We can also return predictions directly with the model object using the predict() method # Another way to get this is using the model.predict function y_pred = model.predict(x) fig, ax = plt.subplots(1,1, figsize=(10,10)) ax.plot(x, y_pred, ls='', marker='*') ax.plot(x, y, ls='', marker='.') 1.3 Error and topics of model fitting (assessing model accuracy) 1.3.1 Measuring the quality of fit 1.3.1.1 Mean Squared Error The plot in Section 1.2.3 looks good, but numerically what is our error? What is the mean value of $\\epsilon$, i.e. the Mean Squared Error (MSE) ? {\\sf MSE}=\\epsilon_{\\sf ave} = \\frac{\\sum_{i=1}^{N_{\\sf times}}\\left(y^{\\sf exact}_i - m\\cdot t_i - b \\right)^2}{N_{\\sf times}}\\;\\;\\;\\;\\;\\sf eq. 3 # The mean squared error print('Mean squared error: %.2f' % mean_squared_error(y, y_pred)) # The mean squared error print('Mean squared error: %.2f' % mean_squared_error(y, y_pred)) 1.3.1.2 R-square Another way to measure error is the regression score, \\(R^2\\). \\(R^2\\) is generally defined as the ratio of the total sum of squares \\(SS_{\\sf tot}\\) to the residual sum of squares \\(SS_{\\sf res}\\): SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\; \\sf eq. 4 SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2\\;\\;\\;\\;\\; \\sf eq. 5 R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}} \\;\\;\\;\\;\\;\\; \\sf eq. 6 In eq. 4, \\(\\bar{y}=\\sum_i y^{\\sf exact}_i/N\\) is the average value of y for \\(N\\) points. The best value of \\(R^2\\) is 1 but it can also take a negative value if the error is large. See all the different regression metrics here . \ud83d\ude4b Question 3 Do we need a large value of \\(SS_{\\sf tot}\\) to minimize \\(R^2\\) - is this something which we have the power to control? # Print the coefficient of determination - 1 is perfect prediction print('Coefficient of determination: %.2f' % r2_score(y, y_pred)) # Print the coefficient of determination - 1 is perfect prediction print('Coefficient of determination: %.2f' % r2_score(y, y_pred)) 1.3.2 Corollaries with classification models For classification tasks, we typically assess accuracy vs MSE or R-square, since we are dealing with categorical rather than numerical predictions. What is accuracy? It is defined as the ratio of True assignments to all assignments. For a binary positive/negative classification task this can be written as the following: Acc = \\frac{T_p + T_n}{F_p + F_n + T_p + T_n} Where \\(T\\) is True, \\(F\\) is false, \\(p\\) is positive, \\(n\\) is negative Just as a quick example, we can perform this type of task on our wine dataset by predicting on quality, which is a discrete 3-9 quality score: y_train = df['type'].values.reshape(-1,1) x_train = df['quality'].values.reshape(-1,1) # train a logistic regression model on the training set from sklearn.linear_model import LogisticRegression # instantiate model logreg = LogisticRegression() # fit model logreg.fit(x_train, y_train) # make class predictions for the testing set y_pred_class = logreg.predict(x_train) # calculate accuracy from sklearn import metrics print(metrics.accuracy_score(y_train, y_pred_class)) 1.3.3 Beyond a single input feature ( also: quick appreciative beat for folding in domain area expertise into our models and features ) The acidity of the wine (the dependent variable v) could depend on: potassium from the soil (increases alkalinity) unripe grapes (increases acidity) grapes grown in colder climates or reduced sunshine create less sugar (increases acidity) preprocessing such as adding tartaric acid to the grape juice before fermentation (increases acidity) malolactic fermentation (reduces acidity) + others So in our lab today we will look at folding in additional variables in our dataset into the model 1.4 Multivariate regression Let's now turn our attention to wine quality. The value we aim to predict or evaluate is the quality of each wine in our dataset. This is our dependent variable. We will look at how this is related to the 12 other independent variables, also known as input features . We're going to do this is just the red wine data red.head() 1.4.1 Linear regression with all input fields For this example, notice we have a categorical data variable in the 'type' column. We will ignore this for now, and only work with our red wines. In the future we will discuss how to deal with categorical variable such as this in a mathematical representation. # this is a list of all our features or independent variables features = list(red.columns[1:]) # we're going to remove our target or dependent variable, density from this # list features.remove('density') # now we define X and y according to these lists of names X = red.dropna(axis=0)[features].values y = red.dropna(axis=0)['density'].values red.isnull().sum(axis=0) # we are getting rid of some nasty nulls! # Create linear regression object - note that we are using all the input features model = linear_model.LinearRegression() model.fit(X, y) y_calc = model.predict(X) # Create linear regression object - note that we are using all the input features model = linear_model.LinearRegression() model.fit(X, y) y_calc = model.predict(X) Let's see what the coefficients look like ... print(\"Fit coefficients: \\n\", model.coef_, \"\\nNumber of coefficients:\", len(model.coef_)) print(\"Fit coefficients: \\n\", model.coef_, \"\\nNumber of coefficients:\", len(model.coef_)) We have 11 !!! That's because we are regressing respect to all 11 independent variables !!! So now, y_{\\sf calc}= m_1x_1 +\\, m_2x_2 \\,+ \\,m_3x_3 \\,+\\,... \\,+ \\,b =\\sum_{i=1}^{13}m_i x_i + b\\;\\;\\;\\;\\; \\sf eq. 7 print(\"We have 13 slopes / weights:\\n\\n\", model.coef_) print(\"\\nAnd one intercept: \", model.intercept_) print(\"We have 11 slopes / weights:\\n\\n\", model.coef_) print(\"\\nAnd one intercept: \", model.intercept_) # This size should match the number of columns in X if len(X[0]) == len(model.coef_): print(\"All good! The number of coefficients matches the number of input features.\") else: print(\"Hmm .. something strange is going on.\") # This size should match the number of columns in X if len(X[0]) == len(model.coef_): print(\"All good! The number of coefficients matches the number of input features.\") else: print(\"Hmm .. something strange is going on.\") \ud83c\udfcb\ufe0f Exercise 4: evaluate the error Let's evaluate the error by computing the MSE and \\(R^2\\) metrics (see eq. 3 and 6). # The mean squared error # part A # calculate the MSE using mean_squared_error() # mse = # part B # calculate the R square using r2_score() # r2 = print('Mean squared error: {:.2f}'.format(mse) print('Coefficient of determination: {:.2f}'.format(r2) # The mean squared error # part A # calculate the MSE using mean_squared_error() # mse = # part B # calculate the R square using r2_score() # r2 = print('Mean squared error: {:.2f}'.format(mse) print('Coefficient of determination: {:.2f}'.format(r2) \ud83c\udfcb\ufe0f Exercise 5: make a plot of y actual vs y predicted We can also look at how well the computed values match the true values graphically by generating a scatterplot. # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style # plt.plot() plt.title(\"Linear regression - computed values on entire data set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style # plt.plot() plt.title(\"Linear regression - computed values on entire data set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() 1.4.2 Enrichment : Splitting into train and test sets To see whether we can predict, we will carry out our regression only on a part, 80%, of the full data set. This part is called the training data. We will then test the trained model to predict the rest of the data, 20% - the test data. The function which fits won't see the test data until it has to predict it. We will motivate the use of train/test sets more explicitly in Course 2 Session 1 We start by splitting out data using scikit-learn's train_test_split() function: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) ``` ```python X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) Now we check the size of y_train and y_test , the sum should be the size of y! If this works then we move on and carry out regression but we only use the training data! if len(y_test)+len(y_train) == len(y): print('All good, ready to to go and regress!\\n') # Carry out linear regression print('Running linear regression algorithm on the training set\\n') model = linear_model.LinearRegression() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_pred_test = model.predict(X_test) if len(y_test)+len(y_train) == len(y): print('All good, ready to to go and regress!\\n') # Carry out linear regression print('Running linear regression algorithm on the training set\\n') model = linear_model.LinearRegression() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_pred_test = model.predict(X_test) Now we can plot our predicted values to see how accurate we are in predicting. We will generate a scatterplot and computing the MSE and \\(R^2\\) metrics of error. sns.scatterplot(x=y_pred_test, y=y_test, color=\"mediumvioletred\", s=50) plt.title(\"Linear regression - predict test set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred_test)) sns.scatterplot(x=y_pred_test, y=y_test, color=\"mediumvioletred\", s=50) plt.title(\"Linear regression - predict test set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred_test)) 1.4.2.1 Other data considerations Do we need all the independent variables? Topics of interential statistics covered in a couple sessions Can we output integer quality scores? Topics of non-binary classification tasks covered in week 4 1.4.3 Enrichment : Other regression algorithms There are many other regression algorithms the two we want to highlight here are Ridge, LASSO, and Elastic Net. They differ by an added term to the loss function. Let's review. Eq. 2 expanded to multivariate form yields: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 for Ridge regression, we add a regularization term known as L2 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}\\beta_{j}^2 for LASSO (Least Absolute Shrinkage and Selection Operator) we add L1 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}|\\beta_{j}| The key difference here is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. Elastic Net is a combination of these two regularization methods. model = linear_model.Ridge() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_calc_test = model.predict(X_test) model = linear_model.Ridge() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_calc_test = model.predict(X_test) sns.scatterplot(x=y_calc_test, y=y_test, color=\"lightseagreen\", s=50) plt.title(\"Ridge regression - predict test set\",fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_calc_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_calc_test)) sns.scatterplot(x=y_calc_test, y=y_test, color=\"lightseagreen\", s=50) plt.title(\"Ridge regression - predict test set\",fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_calc_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_calc_test)) \ud83c\udfcb\ufe0f Exercise 6: Tune Hyperparameter for Ridge Regression Use the docstring to peak into the hyperparameters for Ridge Regression. What is the optimal value of lambda? Plot the \\(\\beta\\) values vs \\(\\lambda\\) from the results of your analysis # cell for exercise 3 out_lambdas = [] out_coefs = [] out_scores = [] for i in range(10): lambdas = [] coefs = [] scores = [] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) for lamb in range(1,110): model = linear_model.Ridge(alpha=lamb/50, normalize=True) model.fit(X_train, y_train) lambdas.append(lamb) coefs.append(model.coef_) scores.append(r2_score(y_test, model.predict(X_test))) # print('MSE: %.4f' % mean_squared_error(y_test, model.predict(X_test))) # print('R2: %.4f' % r2_score(y_test, model.predict(X_test))) out_lambdas.append(lambdas) out_coefs.append(coefs) out_scores.append(scores) coef_means = np.array(out_coefs).mean(axis=0) coef_stds = np.array(out_coefs).std(axis=0) results_means = pd.DataFrame(coef_means,columns=features) results_stds = pd.DataFrame(coef_stds,columns=features) results_means['lambda'] = [i/50 for i in lambdas] fig, ax = plt.subplots(1,1,figsize=(10,10)) for feat in features: ax.errorbar([i/50 for i in lambdas], results_means[feat], yerr=results_stds[feat], label=feat) # results.plot('lambda', 'scores', ax=ax[1]) ax.legend() results = pd.DataFrame(coefs,columns=features) results['lambda'] = [i/50 for i in lambdas] results['scores'] = scores fig, ax = plt.subplots(1,2,figsize=(10,5)) for feat in features: results.plot('lambda', feat, ax=ax[0]) results.plot('lambda', 'scores', ax=ax[1]) 1.5 Enrichment : Additional Regression Exercises Problem 1) Number and choice of input features Load the red wine dataset and evaluate how the linear regression predictions changes as you change the number and choice of input features . The total number of columns in X is 11 and each column represent a specific input feature. Estimate the MSE print(X_train.shape) print(X_train.shape) If you want to use the first 5 features you could proceed as following: X_train_five = X_train[:,0:5] X_test_five = X_test[:,0:5] X_train_five = X_train[:,0:5] X_test_five = X_test[:,0:5] Check that the new variables have the shape your expect print(X_train_five.shape) print(X_test_five.shape) print(X_train_five.shape) print(X_test_five.shape) Now you can use these to train your linear regression model and repeat for different numbers or sets of input features! Note that you do not need to change the output feature! It's size is independent from the number of input features, yet recall that its length is the same as the number of values per input feature. Questions to think about while you work on this problem - How many input feature variables does one need? Is there a maximum or minimum number? - Could one input feature variable be better than the rest? - What if values are missing for one of the input feature variables - is it still worth using it? - Can you use L1 or L2 to determine these optimum features more quickly? Problem 2) Type of regression algorithm Try using other types of linear regression methods on the wine dataset: the LASSO model and the Elastic net model which are described by the sklearn.linear_model.ElasticNet() sklearn.linear_model.Lasso() scikit-learn functions. For more detail see ElasticNet and Lasso . Questions to think about while you work on this problem - How does the error change with each model? - Which model seems to perform best? - How can you optimize the hyperparameter, \\(\\lambda\\) - Does one model do better than the other at determining which input features are more important? - How about non linear regression / what if the data does not follow a line? - How do the bias and variance change for each model from sklearn.linear_model import ElasticNet from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.linear_model import LinearRegression for model in [ElasticNet, Lasso, Ridge, LinearRegression]: model = model() model.fit(X_train, y_train) print('Mean squared error: %.2f' % mean_squared_error(y_test, model.predict(X_test))) print('Coefficient of determination: %.2f' % r2_score(y_test, model.predict(X_test))) References Linear Regression To find out more see simple linear regression scikit-learn Scikit-learn Linear regression in scikit-learn Metrics of error The Boston dataset Pearson correlation To find out more see pearson Irreducible error, bias and variance Great Coursera videos here and here","title":"Regression and Analysis"},{"location":"S1_Regression_and_Analysis/#data-science-foundations-session-1-regression-and-analysis","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at fitting data to a curve using regression . We will also look at using regression to make predictions for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error.","title":"Data Science Foundations Session 1: Regression and Analysis"},{"location":"S1_Regression_and_Analysis/#10-preparing-environment-and-importing-data","text":"back to top","title":"1.0 Preparing Environment and Importing Data"},{"location":"S1_Regression_and_Analysis/#101-import-packages","text":"back to top # Import pandas, pyplot, ipywidgets import pandas as pd from matplotlib import pyplot as plt from ipywidgets import interact # Import Scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score # for enrichment topics import seaborn as sns import numpy as np","title":"1.0.1 Import Packages"},{"location":"S1_Regression_and_Analysis/#102-load-dataset","text":"back to top For our discussion on regression and descriptive statistics today we will use a well known dataset of different wines and their quality ratings df = pd.read_csv(\"https://raw.githubusercontent.com/wesleybeckner/\"\\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\") df.shape (6497, 13) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality count 6487.000000 6489.000000 6494.000000 6495.000000 6495.000000 6497.000000 6497.000000 6497.000000 6488.000000 6493.000000 6497.000000 6497.000000 mean 7.216579 0.339691 0.318722 5.444326 0.056042 30.525319 115.744574 0.994697 3.218395 0.531215 10.491801 5.818378 std 1.296750 0.164649 0.145265 4.758125 0.035036 17.749400 56.521855 0.002999 0.160748 0.148814 1.192712 0.873255 min 3.800000 0.080000 0.000000 0.600000 0.009000 1.000000 6.000000 0.987110 2.720000 0.220000 8.000000 3.000000 25% 6.400000 0.230000 0.250000 1.800000 0.038000 17.000000 77.000000 0.992340 3.110000 0.430000 9.500000 5.000000 50% 7.000000 0.290000 0.310000 3.000000 0.047000 29.000000 118.000000 0.994890 3.210000 0.510000 10.300000 6.000000 75% 7.700000 0.400000 0.390000 8.100000 0.065000 41.000000 156.000000 0.996990 3.320000 0.600000 11.300000 6.000000 max 15.900000 1.580000 1.660000 65.800000 0.611000 289.000000 440.000000 1.038980 4.010000 2.000000 14.900000 9.000000","title":"1.0.2 Load Dataset"},{"location":"S1_Regression_and_Analysis/#11-what-is-regression","text":"It is the process of finding a relationship between dependent and independent variables to find trends in data. This abstract definition means that you have one variable (the dependent variable) which depends on one or more variables (the independent variables). One of the reasons for which we want to regress data is to understand whether there is a trend between two variables. Housing Prices Example We can imagine this scenario with housing prices. Envision a mixed dataset of continuous and discrete independent variables. Some features could be continuous, floating point values like location ranking and housing condition. Others could be descrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a regression model.","title":"1.1 What is regression?"},{"location":"S1_Regression_and_Analysis/#12-linear-regression-fitting-with-scikit-learn","text":"","title":"1.2  Linear regression fitting with scikit-learn"},{"location":"S1_Regression_and_Analysis/#exercise-1-rudimentary-eda","text":"What does the data look like? Recall how to visualize data in a pandas dataframe for every column calculate the * skew: `df.skew()` * kurtosis: `df.kurtosis()` * pearsons correlation with the dependent variable: `df.corr()` * number of missing entries `df.isnull()` and organize this into a new dataframe note: pearsons is just one type of correlation, another available to us spearman which differs from pearsons in that it depends on ranked values rather than their direct quantities, you can read more here df.isnull().sum() type 0 fixed acidity 10 volatile acidity 8 citric acid 3 residual sugar 2 chlorides 2 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 9 sulphates 4 alcohol 0 quality 0 dtype: int64 # Cell for Exercise 1 # part A # using df.<method> define the following four variables with the results from # skew(), kurtosis(), corr() (and selecting for quality), and isnull() # for isnull() you'll notice the return is a dataframe of booleans. we would # like to simply know the number of null values for each column. change the # return of isnull() using the sum() method # skew = # kurt = # pear = # null = # part B # on line 13, put these results in a list using square brackets and call # pandas.DataFrame on the list to make your new DataFrame! store it under the # variable name dff # part C # take the transpose of this DataFrame using dff.T. reassign dff to this copy # part D # set the column names to 'skew', 'kurtosis', 'pearsons _quality', and # 'null count' using dff.columns # Now return dff to the output to view your hand work # dff I have gone ahead and repeated this exercise with the red vs white wine types: red = df.loc[df['type'] == 'red'] wht = df.loc[df['type'] == 'white'] def get_summary(df): skew = df.skew() kurt = df.kurtosis() pear = df.corr()['quality'] null = df.isnull().sum() med = df.median() men = df.mean() dff = pd.DataFrame([skew, kurt, pear, null, med, men]) dff = dff.T dff.columns = ['skew', 'kurtosis', 'pearsons _quality', 'null count', 'median', 'mean'] return dff dffr = get_summary(red) dffw = get_summary(wht) desc = pd.concat([dffr, dffw], keys=['red', 'white']) desc .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count median mean red fixed acidity 0.982192 1.132624 0.123834 2.0 7.90000 8.322104 volatile acidity 0.672862 1.226846 -0.390858 1.0 0.52000 0.527738 citric acid 0.317891 -0.788476 0.226917 1.0 0.26000 0.271145 residual sugar 4.540655 28.617595 0.013732 0.0 2.20000 2.538806 chlorides 5.680347 41.715787 -0.128907 0.0 0.07900 0.087467 free sulfur dioxide 1.250567 2.023562 -0.050656 0.0 14.00000 15.874922 total sulfur dioxide 1.515531 3.809824 -0.185100 0.0 38.00000 46.467792 density 0.071288 0.934079 -0.174919 0.0 0.99675 0.996747 pH 0.194803 0.814690 -0.057094 2.0 3.31000 3.310864 sulphates 2.429115 11.712632 0.251685 2.0 0.62000 0.658078 alcohol 0.860829 0.200029 0.476166 0.0 10.20000 10.422983 quality 0.217802 0.296708 1.000000 0.0 6.00000 5.636023 type NaN NaN NaN 0.0 NaN NaN white fixed acidity 0.647981 2.176560 -0.114032 8.0 6.80000 6.855532 volatile acidity 1.578595 5.095526 -0.194976 7.0 0.26000 0.278252 citric acid 1.284217 6.182036 -0.009194 2.0 0.32000 0.334250 residual sugar 1.076601 3.469536 -0.097373 2.0 5.20000 6.393250 chlorides 5.023412 37.560847 -0.210181 2.0 0.04300 0.045778 free sulfur dioxide 1.406745 11.466342 0.008158 0.0 34.00000 35.308085 total sulfur dioxide 0.390710 0.571853 -0.174737 0.0 134.00000 138.360657 density 0.977773 9.793807 -0.307123 0.0 0.99374 0.994027 pH 0.458402 0.532552 0.098858 7.0 3.18000 3.188203 sulphates 0.977361 1.589847 0.053690 2.0 0.47000 0.489835 alcohol 0.487342 -0.698425 0.435575 0.0 10.40000 10.514267 quality 0.155796 0.216526 1.000000 0.0 6.00000 5.877909 type NaN NaN NaN 0.0 NaN NaN def my_fig(metric=desc.columns): fig, ax = plt.subplots(1, 1, figsize=(10,10)) pd.DataFrame(desc[metric]).unstack()[metric].T.plot(kind='barh', ax=ax) interact(my_fig) interactive(children=(Dropdown(description='metric', options=('skew', 'kurtosis', 'pearsons _quality', 'null c\u2026 <function __main__.my_fig>","title":"\ud83c\udfcb\ufe0f Exercise 1: rudimentary EDA"},{"location":"S1_Regression_and_Analysis/#question-1-discussion-around-eda-plot","text":"What do we think of this plot? metric = mean , the cholrides values metric = kurtosis , residual sugar metric = pearsons _quality , magnitudes and directions How to improve the plot, what other plots would we like to see? df['chlorides'].describe() count 6495.000000 mean 0.056042 std 0.035036 min 0.009000 25% 0.038000 50% 0.047000 75% 0.065000 max 0.611000 Name: chlorides, dtype: float64 fig, ax = plt.subplots(1,1,figsize=(10,10)) df['chlorides'].plot(kind='kde',ax=ax) ax.set_xlim(0,.61) (0.0, 0.61) df['chlorides'].sort_values(ascending=False)[:50] 5156 0.611 5049 0.610 5004 0.467 4979 0.464 5590 0.422 5652 0.415 6268 0.415 6270 0.415 6217 0.414 5949 0.414 5349 0.413 6158 0.403 4981 0.401 5628 0.387 6063 0.369 4915 0.368 5067 0.360 5179 0.358 484 0.346 5189 0.343 4917 0.341 5124 0.337 4940 0.332 1217 0.301 687 0.290 4473 0.271 5079 0.270 6272 0.267 5138 0.263 1865 0.255 5466 0.250 1034 0.244 5674 0.243 5675 0.241 683 0.240 1638 0.239 5045 0.236 6456 0.235 6468 0.230 5465 0.226 5464 0.226 5564 0.222 2186 0.217 5996 0.216 6332 0.214 6333 0.214 5206 0.214 5205 0.213 4497 0.212 1835 0.211 Name: chlorides, dtype: float64","title":"\ud83d\ude4b Question 1: Discussion Around EDA Plot"},{"location":"S1_Regression_and_Analysis/#122-visualizing-the-data-set-motivating-regression-analysis","text":"We can create a scatter plot of fixed acidity vs density of red wine in the dataset using df.plot() and see that there appears to be a general trend between the two features: fig, ax = plt.subplots(1, 1, figsize=(5,5)) df.loc[df['type'] == 'red'].plot(x='fixed acidity', y='density', ax=ax, ls='', marker='.') <matplotlib.axes._subplots.AxesSubplot at 0x7f3158ad7290>","title":"1.2.2 Visualizing the data set - motivating regression analysis"},{"location":"S1_Regression_and_Analysis/#123-estimating-the-regression-coefficients","text":"It looks like density increases with fixed acidity following a line, maybe something like y(x)= m \\cdot x + b \\;\\;\\;\\;\\;\\;\\;\\; \\sf{eq. 1} with \\( y=\\sf density \\), \\(x=\\sf fixed acidity\\), and \\(m\\) the slope and \\(b\\) the intercept. To solve the problem, we need to find the values of \\(b\\) and \\(m\\) in equation 1 to best fit the data. This is called linear regression . In linear regression our goal is to minimize the error between computed values of positions \\(y^{\\sf calc}(x_i)\\equiv y^{\\sf calc}_i\\) and known values \\(y^{\\sf exact}(x_i)\\equiv y^{\\sf exact}_i\\), i.e. find \\(b\\) and \\(m\\) which lead to lowest value of \\epsilon (m,b) =SS_{\\sf res}=\\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - m\\cdot x_i - b \\right)^2\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sf{eq. 2} To find out more see e.g. https://en.wikipedia.org/wiki/Simple_linear_regression","title":"1.2.3 Estimating the regression coefficients"},{"location":"S1_Regression_and_Analysis/#question-2-linear-regression-loss-function","text":"Do we always want m and b to be large positive numbers so as to minimize eq. 2? Luckily scikit-learn contains many functions related to regression including linear regression . The function we will use is called LinearRegression() . # Create linear regression object model = linear_model.LinearRegression() # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red['density'].values.reshape(-1, 1) y = red['fixed acidity'].values.reshape(-1, 1) # Create linear regression object model = linear_model.LinearRegression() # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red['density'].values.reshape(-1, 1) y = red['fixed acidity'].values.reshape(-1, 1) print(red['density'].values.shape, red['fixed acidity'].values.shape) print(x.shape, y.shape) print(red['density'].values.shape, red['fixed acidity'].values.shape) print(x.shape, y.shape) (1599,) (1599,) (1599, 1) (1599, 1) # Fit to the data model.fit(x, y) # Extract the values of interest m = model.coef_[0][0] b = model.intercept_[0] # Print the slope m and intercept b print('Scikit learn - Slope: ', m , 'Intercept: ', b ) What happens when we try to fit the data as is? # Fit to the data # model.fit(x, y)","title":"\ud83d\ude4b Question 2: linear regression loss function"},{"location":"S1_Regression_and_Analysis/#exercise-2-drop-null-values-and-practice-pandas-operations","text":"Let's look back at our dataset description dataframe above, what do we notice, what contains null values? There are several strategies for dealing with null values. For now let's take the simplest case, and drop rows in our dataframe that contain null # Cell for Exercise 2 # For this templated exercise you are going to complete everything in one line # of code, but we are going to break it up into steps. So for each part (A, B, # etc.) paste your answer from the previous part to begin # step A # select the 'density' and 'fixed acidity' columns of red. make sure the return # is a dataframe # step B # now use the dropna() method on axis 0 (the rows) to drop any null values # step B # select column 'density' # step C # select the values # step D # reshape the result with an empty second dimension using .reshape() and store # the result under variable x # repeat the same process with 'fixed acidity' and variable y Now that we have our x and y arrays we can fit using ScikitLearn x = red[['density', 'fixed acidity']].dropna(axis=0)['density'].values.reshape(-1,1) y = red[['density', 'fixed acidity']].dropna(axis=0)['fixed acidity'].values.reshape(-1,1) # Fit to the data model.fit(x, y) # Extract the values of interest m = model.coef_[0][0] b = model.intercept_[0] # Print the slope m and intercept b print('Scikit learn - Slope: ', m , 'Intercept: ', b ) Scikit learn - Slope: 616.0131428066102 Intercept: -605.6880086750525","title":"\ud83c\udfcb\ufe0f Exercise 2: drop Null Values (and practice pandas operations)"},{"location":"S1_Regression_and_Analysis/#exercise-3-calculating-y_pred","text":"Estimate the values of \\(y\\) by using your fitted parameters. Hint: Use your model.coef_ and model.intercept_ parameters to estimate y_pred following equation 1 # define y_pred in terms of m, x, and b # y_pred = # fig, ax = plt.subplots(1,1, figsize=(10,10)) # ax.plot(x, y_pred, ls='', marker='*') # ax.plot(x, y, ls='', marker='.') We can also return predictions directly with the model object using the predict() method # Another way to get this is using the model.predict function y_pred = model.predict(x) fig, ax = plt.subplots(1,1, figsize=(10,10)) ax.plot(x, y_pred, ls='', marker='*') ax.plot(x, y, ls='', marker='.')","title":"\ud83c\udfcb\ufe0f Exercise 3: calculating y_pred"},{"location":"S1_Regression_and_Analysis/#13-error-and-topics-of-model-fitting-assessing-model-accuracy","text":"","title":"1.3 Error and topics of model fitting (assessing model accuracy)"},{"location":"S1_Regression_and_Analysis/#131-measuring-the-quality-of-fit","text":"","title":"1.3.1 Measuring the quality of fit"},{"location":"S1_Regression_and_Analysis/#1311-mean-squared-error","text":"The plot in Section 1.2.3 looks good, but numerically what is our error? What is the mean value of $\\epsilon$, i.e. the Mean Squared Error (MSE) ? {\\sf MSE}=\\epsilon_{\\sf ave} = \\frac{\\sum_{i=1}^{N_{\\sf times}}\\left(y^{\\sf exact}_i - m\\cdot t_i - b \\right)^2}{N_{\\sf times}}\\;\\;\\;\\;\\;\\sf eq. 3 # The mean squared error print('Mean squared error: %.2f' % mean_squared_error(y, y_pred)) # The mean squared error print('Mean squared error: %.2f' % mean_squared_error(y, y_pred))","title":"1.3.1.1 Mean Squared Error"},{"location":"S1_Regression_and_Analysis/#1312-r-square","text":"Another way to measure error is the regression score, \\(R^2\\). \\(R^2\\) is generally defined as the ratio of the total sum of squares \\(SS_{\\sf tot}\\) to the residual sum of squares \\(SS_{\\sf res}\\): SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\; \\sf eq. 4 SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2\\;\\;\\;\\;\\; \\sf eq. 5 R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}} \\;\\;\\;\\;\\;\\; \\sf eq. 6 In eq. 4, \\(\\bar{y}=\\sum_i y^{\\sf exact}_i/N\\) is the average value of y for \\(N\\) points. The best value of \\(R^2\\) is 1 but it can also take a negative value if the error is large. See all the different regression metrics here .","title":"1.3.1.2 R-square"},{"location":"S1_Regression_and_Analysis/#question-3","text":"Do we need a large value of \\(SS_{\\sf tot}\\) to minimize \\(R^2\\) - is this something which we have the power to control? # Print the coefficient of determination - 1 is perfect prediction print('Coefficient of determination: %.2f' % r2_score(y, y_pred)) # Print the coefficient of determination - 1 is perfect prediction print('Coefficient of determination: %.2f' % r2_score(y, y_pred))","title":"\ud83d\ude4b Question 3"},{"location":"S1_Regression_and_Analysis/#132-corollaries-with-classification-models","text":"For classification tasks, we typically assess accuracy vs MSE or R-square, since we are dealing with categorical rather than numerical predictions. What is accuracy? It is defined as the ratio of True assignments to all assignments. For a binary positive/negative classification task this can be written as the following: Acc = \\frac{T_p + T_n}{F_p + F_n + T_p + T_n} Where \\(T\\) is True, \\(F\\) is false, \\(p\\) is positive, \\(n\\) is negative Just as a quick example, we can perform this type of task on our wine dataset by predicting on quality, which is a discrete 3-9 quality score: y_train = df['type'].values.reshape(-1,1) x_train = df['quality'].values.reshape(-1,1) # train a logistic regression model on the training set from sklearn.linear_model import LogisticRegression # instantiate model logreg = LogisticRegression() # fit model logreg.fit(x_train, y_train) # make class predictions for the testing set y_pred_class = logreg.predict(x_train) # calculate accuracy from sklearn import metrics print(metrics.accuracy_score(y_train, y_pred_class))","title":"1.3.2 Corollaries with classification models"},{"location":"S1_Regression_and_Analysis/#133-beyond-a-single-input-feature","text":"( also: quick appreciative beat for folding in domain area expertise into our models and features ) The acidity of the wine (the dependent variable v) could depend on: potassium from the soil (increases alkalinity) unripe grapes (increases acidity) grapes grown in colder climates or reduced sunshine create less sugar (increases acidity) preprocessing such as adding tartaric acid to the grape juice before fermentation (increases acidity) malolactic fermentation (reduces acidity) + others So in our lab today we will look at folding in additional variables in our dataset into the model","title":"1.3.3 Beyond a single input feature"},{"location":"S1_Regression_and_Analysis/#14-multivariate-regression","text":"Let's now turn our attention to wine quality. The value we aim to predict or evaluate is the quality of each wine in our dataset. This is our dependent variable. We will look at how this is related to the 12 other independent variables, also known as input features . We're going to do this is just the red wine data red.head()","title":"1.4 Multivariate regression"},{"location":"S1_Regression_and_Analysis/#141-linear-regression-with-all-input-fields","text":"For this example, notice we have a categorical data variable in the 'type' column. We will ignore this for now, and only work with our red wines. In the future we will discuss how to deal with categorical variable such as this in a mathematical representation. # this is a list of all our features or independent variables features = list(red.columns[1:]) # we're going to remove our target or dependent variable, density from this # list features.remove('density') # now we define X and y according to these lists of names X = red.dropna(axis=0)[features].values y = red.dropna(axis=0)['density'].values red.isnull().sum(axis=0) # we are getting rid of some nasty nulls! # Create linear regression object - note that we are using all the input features model = linear_model.LinearRegression() model.fit(X, y) y_calc = model.predict(X) # Create linear regression object - note that we are using all the input features model = linear_model.LinearRegression() model.fit(X, y) y_calc = model.predict(X) Let's see what the coefficients look like ... print(\"Fit coefficients: \\n\", model.coef_, \"\\nNumber of coefficients:\", len(model.coef_)) print(\"Fit coefficients: \\n\", model.coef_, \"\\nNumber of coefficients:\", len(model.coef_)) We have 11 !!! That's because we are regressing respect to all 11 independent variables !!! So now, y_{\\sf calc}= m_1x_1 +\\, m_2x_2 \\,+ \\,m_3x_3 \\,+\\,... \\,+ \\,b =\\sum_{i=1}^{13}m_i x_i + b\\;\\;\\;\\;\\; \\sf eq. 7 print(\"We have 13 slopes / weights:\\n\\n\", model.coef_) print(\"\\nAnd one intercept: \", model.intercept_) print(\"We have 11 slopes / weights:\\n\\n\", model.coef_) print(\"\\nAnd one intercept: \", model.intercept_) # This size should match the number of columns in X if len(X[0]) == len(model.coef_): print(\"All good! The number of coefficients matches the number of input features.\") else: print(\"Hmm .. something strange is going on.\") # This size should match the number of columns in X if len(X[0]) == len(model.coef_): print(\"All good! The number of coefficients matches the number of input features.\") else: print(\"Hmm .. something strange is going on.\")","title":"1.4.1 Linear regression with all input fields"},{"location":"S1_Regression_and_Analysis/#exercise-4-evaluate-the-error","text":"Let's evaluate the error by computing the MSE and \\(R^2\\) metrics (see eq. 3 and 6). # The mean squared error # part A # calculate the MSE using mean_squared_error() # mse = # part B # calculate the R square using r2_score() # r2 = print('Mean squared error: {:.2f}'.format(mse) print('Coefficient of determination: {:.2f}'.format(r2) # The mean squared error # part A # calculate the MSE using mean_squared_error() # mse = # part B # calculate the R square using r2_score() # r2 = print('Mean squared error: {:.2f}'.format(mse) print('Coefficient of determination: {:.2f}'.format(r2)","title":"\ud83c\udfcb\ufe0f Exercise 4: evaluate the error"},{"location":"S1_Regression_and_Analysis/#exercise-5-make-a-plot-of-y-actual-vs-y-predicted","text":"We can also look at how well the computed values match the true values graphically by generating a scatterplot. # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style # plt.plot() plt.title(\"Linear regression - computed values on entire data set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style # plt.plot() plt.title(\"Linear regression - computed values on entire data set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show()","title":"\ud83c\udfcb\ufe0f Exercise 5: make a plot of y actual vs y predicted"},{"location":"S1_Regression_and_Analysis/#142-enrichment-splitting-into-train-and-test-sets","text":"To see whether we can predict, we will carry out our regression only on a part, 80%, of the full data set. This part is called the training data. We will then test the trained model to predict the rest of the data, 20% - the test data. The function which fits won't see the test data until it has to predict it. We will motivate the use of train/test sets more explicitly in Course 2 Session 1 We start by splitting out data using scikit-learn's train_test_split() function: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) ``` ```python X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) Now we check the size of y_train and y_test , the sum should be the size of y! If this works then we move on and carry out regression but we only use the training data! if len(y_test)+len(y_train) == len(y): print('All good, ready to to go and regress!\\n') # Carry out linear regression print('Running linear regression algorithm on the training set\\n') model = linear_model.LinearRegression() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_pred_test = model.predict(X_test) if len(y_test)+len(y_train) == len(y): print('All good, ready to to go and regress!\\n') # Carry out linear regression print('Running linear regression algorithm on the training set\\n') model = linear_model.LinearRegression() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_pred_test = model.predict(X_test) Now we can plot our predicted values to see how accurate we are in predicting. We will generate a scatterplot and computing the MSE and \\(R^2\\) metrics of error. sns.scatterplot(x=y_pred_test, y=y_test, color=\"mediumvioletred\", s=50) plt.title(\"Linear regression - predict test set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred_test)) sns.scatterplot(x=y_pred_test, y=y_test, color=\"mediumvioletred\", s=50) plt.title(\"Linear regression - predict test set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred_test))","title":"1.4.2 Enrichment: Splitting into train and test sets"},{"location":"S1_Regression_and_Analysis/#1421-other-data-considerations","text":"Do we need all the independent variables? Topics of interential statistics covered in a couple sessions Can we output integer quality scores? Topics of non-binary classification tasks covered in week 4","title":"1.4.2.1 Other data considerations"},{"location":"S1_Regression_and_Analysis/#143-enrichment-other-regression-algorithms","text":"There are many other regression algorithms the two we want to highlight here are Ridge, LASSO, and Elastic Net. They differ by an added term to the loss function. Let's review. Eq. 2 expanded to multivariate form yields: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 for Ridge regression, we add a regularization term known as L2 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}\\beta_{j}^2 for LASSO (Least Absolute Shrinkage and Selection Operator) we add L1 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}|\\beta_{j}| The key difference here is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. Elastic Net is a combination of these two regularization methods. model = linear_model.Ridge() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_calc_test = model.predict(X_test) model = linear_model.Ridge() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_calc_test = model.predict(X_test) sns.scatterplot(x=y_calc_test, y=y_test, color=\"lightseagreen\", s=50) plt.title(\"Ridge regression - predict test set\",fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_calc_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_calc_test)) sns.scatterplot(x=y_calc_test, y=y_test, color=\"lightseagreen\", s=50) plt.title(\"Ridge regression - predict test set\",fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_calc_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_calc_test))","title":"1.4.3 Enrichment: Other regression algorithms"},{"location":"S1_Regression_and_Analysis/#exercise-6-tune-hyperparameter-for-ridge-regression","text":"Use the docstring to peak into the hyperparameters for Ridge Regression. What is the optimal value of lambda? Plot the \\(\\beta\\) values vs \\(\\lambda\\) from the results of your analysis # cell for exercise 3 out_lambdas = [] out_coefs = [] out_scores = [] for i in range(10): lambdas = [] coefs = [] scores = [] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) for lamb in range(1,110): model = linear_model.Ridge(alpha=lamb/50, normalize=True) model.fit(X_train, y_train) lambdas.append(lamb) coefs.append(model.coef_) scores.append(r2_score(y_test, model.predict(X_test))) # print('MSE: %.4f' % mean_squared_error(y_test, model.predict(X_test))) # print('R2: %.4f' % r2_score(y_test, model.predict(X_test))) out_lambdas.append(lambdas) out_coefs.append(coefs) out_scores.append(scores) coef_means = np.array(out_coefs).mean(axis=0) coef_stds = np.array(out_coefs).std(axis=0) results_means = pd.DataFrame(coef_means,columns=features) results_stds = pd.DataFrame(coef_stds,columns=features) results_means['lambda'] = [i/50 for i in lambdas] fig, ax = plt.subplots(1,1,figsize=(10,10)) for feat in features: ax.errorbar([i/50 for i in lambdas], results_means[feat], yerr=results_stds[feat], label=feat) # results.plot('lambda', 'scores', ax=ax[1]) ax.legend() results = pd.DataFrame(coefs,columns=features) results['lambda'] = [i/50 for i in lambdas] results['scores'] = scores fig, ax = plt.subplots(1,2,figsize=(10,5)) for feat in features: results.plot('lambda', feat, ax=ax[0]) results.plot('lambda', 'scores', ax=ax[1])","title":"\ud83c\udfcb\ufe0f Exercise 6: Tune Hyperparameter for Ridge Regression"},{"location":"S1_Regression_and_Analysis/#15-enrichment-additional-regression-exercises","text":"","title":"1.5 Enrichment: Additional Regression Exercises"},{"location":"S1_Regression_and_Analysis/#problem-1-number-and-choice-of-input-features","text":"Load the red wine dataset and evaluate how the linear regression predictions changes as you change the number and choice of input features . The total number of columns in X is 11 and each column represent a specific input feature. Estimate the MSE print(X_train.shape) print(X_train.shape) If you want to use the first 5 features you could proceed as following: X_train_five = X_train[:,0:5] X_test_five = X_test[:,0:5] X_train_five = X_train[:,0:5] X_test_five = X_test[:,0:5] Check that the new variables have the shape your expect print(X_train_five.shape) print(X_test_five.shape) print(X_train_five.shape) print(X_test_five.shape) Now you can use these to train your linear regression model and repeat for different numbers or sets of input features! Note that you do not need to change the output feature! It's size is independent from the number of input features, yet recall that its length is the same as the number of values per input feature. Questions to think about while you work on this problem - How many input feature variables does one need? Is there a maximum or minimum number? - Could one input feature variable be better than the rest? - What if values are missing for one of the input feature variables - is it still worth using it? - Can you use L1 or L2 to determine these optimum features more quickly?","title":"Problem 1) Number and choice of input features"},{"location":"S1_Regression_and_Analysis/#problem-2-type-of-regression-algorithm","text":"Try using other types of linear regression methods on the wine dataset: the LASSO model and the Elastic net model which are described by the sklearn.linear_model.ElasticNet() sklearn.linear_model.Lasso() scikit-learn functions. For more detail see ElasticNet and Lasso . Questions to think about while you work on this problem - How does the error change with each model? - Which model seems to perform best? - How can you optimize the hyperparameter, \\(\\lambda\\) - Does one model do better than the other at determining which input features are more important? - How about non linear regression / what if the data does not follow a line? - How do the bias and variance change for each model from sklearn.linear_model import ElasticNet from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.linear_model import LinearRegression for model in [ElasticNet, Lasso, Ridge, LinearRegression]: model = model() model.fit(X_train, y_train) print('Mean squared error: %.2f' % mean_squared_error(y_test, model.predict(X_test))) print('Coefficient of determination: %.2f' % r2_score(y_test, model.predict(X_test)))","title":"Problem 2) Type of regression algorithm"},{"location":"S1_Regression_and_Analysis/#references","text":"Linear Regression To find out more see simple linear regression scikit-learn Scikit-learn Linear regression in scikit-learn Metrics of error The Boston dataset Pearson correlation To find out more see pearson Irreducible error, bias and variance Great Coursera videos here and here","title":"References"},{"location":"S2_Inferential_Statistics/","text":"Data Science Foundations, Session 2: Inferential Statistics Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at the utility of EDA combined with inferential statistics. 2.0 Preparing Environment and Importing Data back to top 2.0.1 Import Packages back to top # The modules we've seen before import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns # our stats modules import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy 2.0.2 Load Dataset back to top For this session, we will use dummy datasets from sklearn. df = pd.read_csv('https://raw.githubusercontent.com/wesleybeckner/'\\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... ... ... ... ... ... 1663 Tiramisu Chocolate Outer Doughnut Pear Amethyst Fickelgruber 12/2020 38128.802589 0.420111 1664 Tiramisu Chocolate Outer Doughnut Pear Burgundy Zebrabar 12/2020 108.642857 0.248659 1665 Tiramisu Chocolate Outer Doughnut Pear Teal Zebrabar 12/2020 3517.933333 0.378501 1666 Tiramisu Chocolate Outer Doughnut Rock and Rye Amethyst Slugworth 12/2020 10146.898432 0.213149 1667 Tiramisu Chocolate Outer Doughnut Rock and Rye Burgundy Zebrabar 12/2020 1271.904762 0.431813 1668 rows \u00d7 9 columns descriptors = df.columns[:-2] for col in df.columns[:-5]: print(col) print(df[col].unique()) print() Base Cake ['Butter' 'Cheese' 'Chiffon' 'Pound' 'Sponge' 'Tiramisu'] Truffle Type ['Candy Outer' 'Chocolate Outer' 'Jelly Filled'] Primary Flavor ['Butter Pecan' 'Ginger Lime' 'Margarita' 'Pear' 'Pink Lemonade' 'Raspberry Ginger Ale' 'Sassafras' 'Spice' 'Wild Cherry Cream' 'Cream Soda' 'Horchata' 'Kettle Corn' 'Lemon Bar' 'Orange Pineapple\\tP' 'Plum' 'Orange' 'Butter Toffee' 'Lemon' 'Acai Berry' 'Apricot' 'Birch Beer' 'Cherry Cream Spice' 'Creme de Menthe' 'Fruit Punch' 'Ginger Ale' 'Grand Mariner' 'Orange Brandy' 'Pecan' 'Toasted Coconut' 'Watermelon' 'Wintergreen' 'Vanilla' 'Bavarian Cream' 'Black Licorice' 'Caramel Cream' 'Cheesecake' 'Cherry Cola' 'Coffee' 'Irish Cream' 'Lemon Custard' 'Mango' 'Sour' 'Amaretto' 'Blueberry' 'Butter Milk' 'Chocolate Mint' 'Coconut' 'Dill Pickle' 'Gingersnap' 'Chocolate' 'Doughnut'] Secondary Flavor ['Toffee' 'Banana' 'Rum' 'Tutti Frutti' 'Vanilla' 'Mixed Berry' 'Whipped Cream' 'Apricot' 'Passion Fruit' 'Peppermint' 'Dill Pickle' 'Black Cherry' 'Wild Cherry Cream' 'Papaya' 'Mango' 'Cucumber' 'Egg Nog' 'Pear' 'Rock and Rye' 'Tangerine' 'Apple' 'Black Currant' 'Kiwi' 'Lemon' 'Hazelnut' 'Butter Rum' 'Fuzzy Navel' 'Mojito' 'Ginger Beer'] 2.1 Many Flavors of Statistical Tests https://luminousmen.com/post/descriptive-and-inferential-statistics Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (\u201cinferences\u201d) from that data. With inferential statistics, you take data from samples and make generalizations about a population statshowto Moods Median Test Kruskal-Wallis Test (Another comparison of Medians test) T-Test Analysis of Variance (ANOVA) One Way ANOVA Two Way ANOVA MANOVA Factorial ANOVA When do I use each of these? We will talk about this as we proceed through the examples. This page from minitab has good rules of thumb on the subject. 2.1.1 What is Mood's Median? You can use Chi-Square to test for a goodness of fit (whether a sample of data represents a distribution) or whether two variables are related (using a contingency table, which we will create below!) A special case of Pearon's Chi-Squared Test: We create a table that counts the observations above and below the global median for two different groups. We then perform a chi-squared test of significance on this contingency table Null hypothesis: the Medians are all equal The chi-square test statistic: x^2 = \\sum{\\frac{(O-E)^2}{E}} Where \\(O\\) is the observed frequency and \\(E\\) is the expected frequency. Let's take an example , say we have three shifts with the following production rates: np.random.seed(42) shift_one = [round(i) for i in np.random.normal(16, 3, 10)] shift_two = [round(i) for i in np.random.normal(21, 3, 10)] print(shift_one) print(shift_two) [17, 16, 18, 21, 15, 15, 21, 18, 15, 18] [20, 20, 22, 15, 16, 19, 18, 22, 18, 17] stat, p, m, table = scipy.stats.median_test(shift_one, shift_two, correction=False) what is median_test returning? print(\"The perasons chi-square test statistic: {:.2f}\".format(stat)) print(\"p-value of the test: {:.3f}\".format(p)) print(\"the grand median: {}\".format(m)) The perasons chi-square test statistic: 1.98 p-value of the test: 0.160 the grand median: 18.0 Let's evaluate that test statistic ourselves by taking a look at the contingency table: table array([[2, 5], [8, 5]]) This is easier to make sense of if we order the shift times shift_one.sort() shift_one [15, 15, 15, 16, 17, 18, 18, 18, 21, 21] When we look at shift one, we see that 8 values are at or below the grand median. shift_two.sort() shift_two [15, 16, 17, 18, 18, 19, 20, 20, 22, 22] For shift two, only two are at or below the grand median. Since the sample sizes are the same, the expected value for both groups is the same, 5 above and 5 below the grand median. The chi-square is then: X^2 = \\frac{(2-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(2-5)^2}{5} (3-5)**2/5 + (7-5)**2/5 + (7-5)**2/5 + (3-5)**2/5 3.2 Our p-value, or the probability of observing the null-hypothsis, is under 0.05. We can conclude that these shift performances were drawn under seperate distributions. For comparison, let's do this analysis again with shifts of equal performances np.random.seed(3) shift_three = [round(i) for i in np.random.normal(16, 3, 10)] shift_four = [round(i) for i in np.random.normal(16, 3, 10)] stat, p, m, table = scipy.stats.median_test(shift_three, shift_four, correction=False) print(\"The pearsons chi-square test statistic: {:.2f}\".format(stat)) print(\"p-value of the test: {:.3f}\".format(p)) print(\"the grand median: {}\".format(m)) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000 the grand median: 15.5 and the shift raw values: shift_three.sort() shift_four.sort() print(shift_three) print(shift_four) [10, 14, 15, 15, 15, 16, 16, 16, 17, 21] [11, 12, 13, 14, 15, 16, 19, 19, 19, 21] table array([[5, 5], [5, 5]]) 2.1.2 When to Use Mood's? Mood's Median Test is highly flexible but has the following assumptions: Considers only one categorical factor Response variable is continuous (our shift rates) Data does not need to be normally distributed But the distributions are similarly shaped Sample sizes can be unequal and small (less than 20 observations) Other considerations: Not as powerful as Kruskal-Wallis Test but still useful for small sample sizes or when there are outliers \ud83c\udfcb\ufe0f Exercise 1: Use Mood's Median Test Part A Perform moods median test on Base Cake in Truffle data We're also going to get some practice with pandas groupby. # what is returned by this groupby? gp = df.groupby('Base Cake') How do we find out? We could iterate through it: # seems to be a tuple of some sort for i in gp: print(i) break ('Butter', Base Cake Truffle Type ... KG EBITDA/KG 0 Butter Candy Outer ... 53770.342593 0.500424 1 Butter Candy Outer ... 466477.578125 0.220395 2 Butter Candy Outer ... 80801.728070 0.171014 3 Butter Candy Outer ... 18046.111111 0.233025 4 Butter Candy Outer ... 19147.454268 0.480689 ... ... ... ... ... ... 1562 Butter Chocolate Outer ... 9772.200521 0.158279 1563 Butter Chocolate Outer ... 10861.245675 -0.159275 1564 Butter Chocolate Outer ... 3578.592163 0.431328 1565 Butter Jelly Filled ... 21438.187500 0.105097 1566 Butter Jelly Filled ... 15617.489115 0.185070 [456 rows x 9 columns]) # the first object appears to be the group print(i[0]) # the second object appears to be the df belonging to that group print(i[1]) Butter Base Cake Truffle Type ... KG EBITDA/KG 0 Butter Candy Outer ... 53770.342593 0.500424 1 Butter Candy Outer ... 466477.578125 0.220395 2 Butter Candy Outer ... 80801.728070 0.171014 3 Butter Candy Outer ... 18046.111111 0.233025 4 Butter Candy Outer ... 19147.454268 0.480689 ... ... ... ... ... ... 1562 Butter Chocolate Outer ... 9772.200521 0.158279 1563 Butter Chocolate Outer ... 10861.245675 -0.159275 1564 Butter Chocolate Outer ... 3578.592163 0.431328 1565 Butter Jelly Filled ... 21438.187500 0.105097 1566 Butter Jelly Filled ... 15617.489115 0.185070 [456 rows x 9 columns] going back to our diagram from our earlier pandas session. It looks like whenever we split in the groupby method, we create separate dataframes as well as their group label: Ok, so we know gp is separate dataframes. How do we turn them into arrays to then pass to median_test ? # complete this for loop for i, j in gp: # turn j into an array using the .values attribute # print this to the screen After you've completed the previous step, turn this into a list comprehension and pass the result to a variable called margins # complete the code below # margins = [# YOUR LIST COMPREHENSION HERE] Remember the list unpacking we did for the tic tac toe project? We're going to do the same thing here. Unpack the margins list for median_test and run the cell below! # complete the following line # stat, p, m, table = scipy.stats.median_test(<UNPACK MARGINS HERE>, correction=False) print(\"The pearsons chi-square test statistic: {:.2f}\".format(stat)) print(\"p-value of the test: {:.2e}\".format(p)) print(\"the grand median: {:.2e}\".format(m)) The pearsons chi-square test statistic: 448.81 p-value of the test: 8.85e-95 the grand median: 2.16e-01 Part B View the distributions of the data using matplotlib and seaborn What a fantastic statistical result we found! Can we affirm our result with some visualizations? I hope so! Create a boxplot below using pandas. In your call to df.boxplot() the by parameter should be set to Base Cake and the column parameter should be set to EBITDA/KG # YOUR BOXPLOT HERE /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray <matplotlib.axes._subplots.AxesSubplot at 0x7fb512f782d0> For comparison, I've shown the boxplot below using seaborn! fig, ax = plt.subplots(figsize=(10,7)) ax = sns.boxplot(x='Base Cake', y='EBITDA/KG', data=df, color='#A0cbe8') Part C Perform Moods Median on all the other groups # Recall the other descriptors we have descriptors Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') for desc in descriptors: # YOUR CODE FORM MARGINS BELOW # margins = [<YOUR LIST COMPREHENSION>] # UNPACK MARGINS INTO MEDIAN_TEST # stat, p, m, table = scipy.stats.median_test(<YOUR UNPACKING METHOD>, correction=False) print(desc) print(\"The pearsons chi-square test statistic: {:.2f}\".format(stat)) print(\"p-value of the test: {:e}\".format(p)) print(\"the grand median: {}\".format(m), end='\\n\\n') Base Cake The pearsons chi-square test statistic: 448.81 p-value of the test: 8.851450e-95 the grand median: 0.2160487288076019 Truffle Type The pearsons chi-square test statistic: 22.86 p-value of the test: 1.088396e-05 the grand median: 0.2160487288076019 Primary Flavor The pearsons chi-square test statistic: 638.99 p-value of the test: 3.918933e-103 the grand median: 0.2160487288076019 Secondary Flavor The pearsons chi-square test statistic: 323.13 p-value of the test: 6.083210e-52 the grand median: 0.2160487288076019 Color Group The pearsons chi-square test statistic: 175.18 p-value of the test: 1.011412e-31 the grand median: 0.2160487288076019 Customer The pearsons chi-square test statistic: 5.66 p-value of the test: 2.257760e-01 the grand median: 0.2160487288076019 Date The pearsons chi-square test statistic: 5.27 p-value of the test: 9.175929e-01 the grand median: 0.2160487288076019 Part D Many boxplots And finally, we will confirm these visually. Complete the Boxplot for each group: for desc in descriptors: fig, ax = plt.subplots(figsize=(10,5)) # sns.boxplot(x=<YOUR X VARIABLE HERE>, y='EBITDA/KG', data=df, color='#A0cbe8', ax=ax) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 9 missing from current font. /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 9 missing from current font. 2.1.3 Enrichment : What is a T-test? There are 1-sample and 2-sample T-tests (note: we would use a 1-sample T-test just to determine if the sample mean is equal to a hypothesized population mean) Within 2-sample T-tests we have independent and dependent T-tests (uncorrelated or correlated samples) For independent, two-sample T-tests: Equal variance (or pooled) T-test scipy.stats.ttest_ind(equal_var=True) Unequal variance T-test scipy.stats.ttest_ind(equal_var=False) also called Welch's T-test For dependent T-tests: * Paired (or correlated) T-test * scipy.stats.ttest_rel A full discussion on T-tests is outside the scope of this session, but we can refer to wikipedia for more information, including formulas on how each statistic is computed: * student's T-test 2.1.4 Enrichment : Demonstration of T-tests back to top We'll assume our shifts are of equal variance and proceed with the appropriate independent two-sample T-test... print(shift_one) print(shift_two) [15, 15, 15, 16, 17, 18, 18, 18, 21, 21] [15, 16, 17, 18, 18, 19, 20, 20, 22, 22] To calculate the T-test, we follow a slightly different statistical formula: $T=\\frac{\\mu_1 - \\mu_2}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$ where $\\mu$ are the means of the two groups, $n$ are the sample sizes and $s$ is the pooled standard deviation, also known as the cummulative variance (depending on if you square it or not): $s= \\sqrt{\\frac{(n_1-1)\\sigma_1^2 + (n_2-1)\\sigma_2^2}{n_1 + n_2 - 2}}$ where $\\sigma$ are the standard deviations. What you'll notice here is we are combining the two variances, we can only do this if we assume the variances are somewhat equal, this is known as the equal variances t-test. mean_shift_one = np.mean(shift_one) mean_shift_two = np.mean(shift_two) print(mean_shift_one, mean_shift_two) 17.4 18.7 com_var = ((np.sum([(i - mean_shift_one)**2 for i in shift_one]) + np.sum([(i - mean_shift_two)**2 for i in shift_two])) / (len(shift_one) + len(shift_two)-2)) print(com_var) 5.361111111111111 T = (np.abs(mean_shift_one - mean_shift_two) / ( np.sqrt(com_var/len(shift_one) + com_var/len(shift_two)))) T 1.2554544209603191 We see that this hand-computed result matches that of the scipy module: scipy.stats.ttest_ind(shift_two, shift_one, equal_var=True) Ttest_indResult(statistic=1.2554544209603191, pvalue=0.22536782778843117) Enrichment : 6.1.5 What are F-statistics and the F-test? The F-statistic is simply a ratio of two variances, or the ratio of mean squares mean squares is the estimate of population variance that accounts for the degrees of freedom to compute that estimate. We will explore this in the context of ANOVA 2.1.6 Enrichment : What is Analysis of Variance? ANOVA uses the F-test to determine whether the variability between group means is larger than the variability within the groups. If that statistic is large enough, you can conclude that the means of the groups are not equal. The caveat is that ANOVA tells us whether there is a difference in means but it does not tell us where the difference is. To find where the difference is between the groups, we have to conduct post-hoc tests. There are two main types: * One-way (one factor) and * Two-way (two factor) where factor is an indipendent variable Ind A Ind B Dep X H 10 X I 12 Y I 11 Y H 20 ANOVA Hypotheses Null hypothesis : group means are equal Alternative hypothesis : at least one group mean is different form the other groups ANOVA Assumptions Residuals (experimental error) are normally distributed (test with Shapiro-Wilk) Homogeneity of variances (variances are equal between groups) (test with Bartlett's) Observations are sampled independently from each other Note: ANOVA assumptions can be checked using test statistics (e.g. Shapiro-Wilk, Bartlett\u2019s, Levene\u2019s test) and the visual approaches such as residual plots (e.g. QQ-plots) and histograms. Steps for ANOVA Check sample sizes: equal observations must be in each group Calculate Sum of Square between groups and within groups (\\(SS_B, SS_E\\)) Calculate Mean Square between groups and within groups (\\(MS_B, MS_E\\)) Calculate F value (\\(MS_B/MS_E\\)) This might be easier to see in a table: Source of Variation degree of freedom (Df) Sum of squares (SS) Mean square (MS) F value Between Groups Df_b = P-1 SS_B MS_B = SS_B / Df_B MS_B / MS_E Within Groups Df_E = P(N-1) SS_E MS_E = SS_E / Df_E total Df_T = PN-1 SS_T Where: SS_B = \\sum_{i}^{P}{(\\bar{y}_i-\\bar{y})^2} SS_E = \\sum_{ik}^{PN}{(\\bar{y}_{ik}-\\bar{y}_i)^2} SS_T = SS_B + SS_E Let's go back to our shift data to take an example: shifts = pd.DataFrame([shift_one, shift_two, shift_three, shift_four]).T shifts.columns = ['A', 'B', 'C', 'D'] shifts.boxplot() <matplotlib.axes._subplots.AxesSubplot at 0x7fb5128c6690> 2.1.6.0 Enrichment : SNS Boxplot this is another great way to view boxplot data. Notice how sns also shows us the raw data alongside the box and whiskers using a swarmplot . shift_melt = pd.melt(shifts.reset_index(), id_vars=['index'], value_vars=['A', 'B', 'C', 'D']) shift_melt.columns = ['index', 'shift', 'rate'] ax = sns.boxplot(x='shift', y='rate', data=shift_melt, color='#A0cbe8') ax = sns.swarmplot(x=\"shift\", y=\"rate\", data=shift_melt, color='#79706e') Anyway back to ANOVA... fvalue, pvalue = stats.f_oneway(shifts['A'], shifts['B'], shifts['C'], shifts['D']) print(fvalue, pvalue) 2.8666172656539457 0.04998066540684099 We can get this in the format of the table we saw above: # get ANOVA table import statsmodels.api as sm from statsmodels.formula.api import ols # Ordinary Least Squares (OLS) model model = ols('rate ~ C(shift)', data=shift_melt).fit() anova_table = sm.stats.anova_lm(model, typ=2) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(shift) 64.475 3.0 2.866617 0.049981 Residual 269.900 36.0 NaN NaN The Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w, pvalue = stats.shapiro(model.resid) print(w, pvalue) 0.9800916314125061 0.6929556727409363 We can use Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. w, pvalue = stats.bartlett(shifts['A'], shifts['B'], shifts['C'], shifts['D']) print(w, pvalue) 1.9492677462621584 0.5830028540285896 2.1.6.1 ANOVA Interpretation The p value form ANOVA analysis is significant ( p < 0.05) and we can conclude there are significant difference between the shifts. But we do not know which shift(s) are different. For this we need to perform a post hoc test. There are a multitude of these that are beyond the scope of this discussion ( Tukey-kramer is one such test) 2.1.7 Putting it all together In summary, there are many statistical tests at our disposal when performing inferential statistical analysis. In times like these, a simple decision tree can be extraordinarily useful! source: scribbr 2.2 Evaluate statistical significance of product margin: a snake in the garden 2.2.1 Mood's Median on product descriptors The first issue we run into with moods is... what? We can only perform moods on two groups at a time. How can we get around this? Let's take a look at the category with the fewest descriptors. If we remember, this was the Truffle Types. df.columns Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date', 'KG', 'EBITDA/KG'], dtype='object') df['Truffle Type'].unique() array(['Candy Outer', 'Chocolate Outer', 'Jelly Filled'], dtype=object) col = 'Truffle Type' moodsdf = pd.DataFrame() for truff in df[col].unique(): # for each group = df.loc[df[col] == truff]['EBITDA/KG'] pop = df.loc[~(df[col] == truff)]['EBITDA/KG'] stat, p, m, table = scipy.stats.median_test(group, pop) median = np.median(group) mean = np.mean(group) size = len(group) print(\"{}: N={}\".format(truff, size)) print(\"Welch's T-Test for Unequal Variances\") print(scipy.stats.ttest_ind(group, pop, equal_var=False)) welchp = scipy.stats.ttest_ind(group, pop, equal_var=False).pvalue print() moodsdf = pd.concat([moodsdf, pd.DataFrame([truff, stat, p, m, mean, median, size, welchp, table]).T]) moodsdf.columns = [col, 'pearsons_chi_square', 'p_value', 'grand_median', 'group_mean', 'group_median', 'size', 'welch p', 'table'] Candy Outer: N=288 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=-2.7615297773427527, pvalue=0.005911048922657976) Chocolate Outer: N=1356 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=4.409449025092911, pvalue=1.1932685612874952e-05) Jelly Filled: N=24 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=-8.4142523067935, pvalue=7.929912531660173e-09) Question 1: Moods Results on Truffle Type What do we notice about the resultant table? p-values Most are quite small (really low probability of achieving these table results under a single distribution) group sizes: our Jelly Filled group is relatively small moodsdf.sort_values('p_value') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Truffle Type pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Jelly Filled 18.6432 1.57604e-05 0.216049 0.0513823 0.0179334 24 7.92991e-09 [[1, 833], [23, 811]] 0 Chocolate Outer 6.6275 0.0100416 0.216049 0.262601 0.225562 1356 1.19327e-05 [[699, 135], [657, 177]] 0 Candy Outer 1.51507 0.218368 0.216049 0.230075 0.204264 288 0.00591105 [[134, 700], [154, 680]] We can go ahead and repeat this analysis for all of our product categories: df.columns[:5] Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group'], dtype='object') moodsdf = pd.DataFrame() for col in df.columns[:5]: for truff in df[col].unique(): group = df.loc[df[col] == truff]['EBITDA/KG'] pop = df.loc[~(df[col] == truff)]['EBITDA/KG'] stat, p, m, table = scipy.stats.median_test(group, pop) median = np.median(group) mean = np.mean(group) size = len(group) welchp = scipy.stats.ttest_ind(group, pop, equal_var=False).pvalue moodsdf = pd.concat([moodsdf, pd.DataFrame([col, truff, stat, p, m, mean, median, size, welchp, table]).T]) moodsdf.columns = ['descriptor', 'group', 'pearsons_chi_square', 'p_value', 'grand_median', 'group_mean', 'group_median', 'size', 'welch p', 'table'] print(moodsdf.shape) (101, 10) moodsdf = moodsdf.loc[(moodsdf['welch p'] < 0.005) & (moodsdf['p_value'] < 0.005)].sort_values('group_median') moodsdf = moodsdf.sort_values('group_median').reset_index(drop=True) print(moodsdf.shape) (51, 10) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Secondary Flavor Papaya 18.6432 1.57604e-05 0.216049 0.0167466 0.00245839 24 1.04879e-10 [[1, 833], [23, 811]] 1 Primary Flavor Orange Pineapple\\tP 18.6432 1.57604e-05 0.216049 0.0167466 0.00245839 24 1.04879e-10 [[1, 833], [23, 811]] 2 Primary Flavor Cherry Cream Spice 10.1564 0.00143801 0.216049 0.0187023 0.00970093 12 7.15389e-07 [[0, 834], [12, 822]] 3 Secondary Flavor Cucumber 18.6432 1.57604e-05 0.216049 0.0513823 0.0179334 24 7.92991e-09 [[1, 833], [23, 811]] 4 Truffle Type Jelly Filled 18.6432 1.57604e-05 0.216049 0.0513823 0.0179334 24 7.92991e-09 [[1, 833], [23, 811]] 5 Primary Flavor Orange 18.6432 1.57604e-05 0.216049 0.0513823 0.0179334 24 7.92991e-09 [[1, 833], [23, 811]] 6 Primary Flavor Toasted Coconut 15.2613 9.36173e-05 0.216049 0.0370021 0.0283916 24 3.13722e-08 [[2, 832], [22, 812]] 7 Secondary Flavor Apricot 15.2613 9.36173e-05 0.216049 0.0603122 0.0374225 24 4.5952e-08 [[2, 832], [22, 812]] 8 Primary Flavor Kettle Corn 29.0621 7.00962e-08 0.216049 0.0554518 0.045891 60 6.3013e-18 [[9, 825], [51, 783]] 9 Primary Flavor Acai Berry 18.6432 1.57604e-05 0.216049 0.0365051 0.0494656 24 1.49539e-10 [[1, 833], [23, 811]] 10 Primary Flavor Pink Lemonade 10.1564 0.00143801 0.216049 0.0398622 0.0563492 12 1.06677e-05 [[0, 834], [12, 822]] 11 Secondary Flavor Black Cherry 58.9004 1.65861e-14 0.216049 0.0559745 0.0628979 96 6.65441e-31 [[11, 823], [85, 749]] 12 Primary Flavor Watermelon 15.2613 9.36173e-05 0.216049 0.0440497 0.0678958 24 5.20636e-08 [[2, 832], [22, 812]] 13 Primary Flavor Plum 34.8516 3.55816e-09 0.216049 0.0849632 0.0799934 72 1.20739e-16 [[11, 823], [61, 773]] 14 Secondary Flavor Dill Pickle 10.1564 0.00143801 0.216049 0.0370421 0.0824944 12 7.45639e-06 [[0, 834], [12, 822]] 15 Primary Flavor Horchata 10.1564 0.00143801 0.216049 0.0370421 0.0824944 12 7.45639e-06 [[0, 834], [12, 822]] 16 Primary Flavor Lemon Custard 12.2175 0.000473444 0.216049 0.0793894 0.087969 24 6.19493e-06 [[3, 831], [21, 813]] 17 Primary Flavor Fruit Punch 10.1564 0.00143801 0.216049 0.0789353 0.0903256 12 7.61061e-05 [[0, 834], [12, 822]] 18 Base Cake Chiffon 117.046 2.80454e-27 0.216049 0.127851 0.125775 288 6.79655e-43 [[60, 774], [228, 606]] 19 Base Cake Butter 134.367 4.54085e-31 0.216049 0.142082 0.139756 456 9.8457e-52 [[122, 712], [334, 500]] 20 Secondary Flavor Banana 10.8053 0.00101207 0.216049 0.163442 0.15537 60 7.23966e-09 [[17, 817], [43, 791]] 21 Primary Flavor Cream Soda 9.51186 0.00204148 0.216049 0.150265 0.163455 24 2.17977e-06 [[4, 830], [20, 814]] 22 Secondary Flavor Peppermint 9.51186 0.00204148 0.216049 0.150265 0.163455 24 2.17977e-06 [[4, 830], [20, 814]] 23 Primary Flavor Grand Mariner 10.5818 0.00114208 0.216049 0.197463 0.165529 72 0.000828508 [[22, 812], [50, 784]] 24 Color Group Amethyst 20.4883 5.99977e-06 0.216049 0.195681 0.167321 300 4.0427e-07 [[114, 720], [186, 648]] 25 Color Group Burgundy 10.9997 0.000911278 0.216049 0.193048 0.171465 120 0.000406312 [[42, 792], [78, 756]] 26 Color Group White 35.7653 2.22582e-09 0.216049 0.19 0.177264 432 1.56475e-16 [[162, 672], [270, 564]] 27 Color Group Opal 11.5872 0.000664086 0.216049 0.317878 0.259304 324 3.94098e-07 [[190, 644], [134, 700]] 28 Secondary Flavor Apple 27.2833 1.75723e-07 0.216049 0.326167 0.293876 36 0.00117635 [[34, 800], [2, 832]] 29 Secondary Flavor Tangerine 32.6264 1.11688e-08 0.216049 0.342314 0.319273 48 0.000112572 [[44, 790], [4, 830]] 30 Secondary Flavor Black Currant 34.7784 3.69452e-09 0.216049 0.357916 0.332449 36 9.48594e-08 [[36, 798], [0, 834]] 31 Secondary Flavor Pear 16.6143 4.58043e-05 0.216049 0.373034 0.33831 60 3.05948e-05 [[46, 788], [14, 820]] 32 Primary Flavor Vanilla 34.7784 3.69452e-09 0.216049 0.378053 0.341626 36 1.00231e-06 [[36, 798], [0, 834]] 33 Color Group Citrine 10.1564 0.00143801 0.216049 0.390728 0.342512 12 0.00192513 [[12, 822], [0, 834]] 34 Color Group Teal 13.5397 0.000233572 0.216049 0.323955 0.3446 96 0.00120954 [[66, 768], [30, 804]] 35 Base Cake Tiramisu 52.3606 4.61894e-13 0.216049 0.388267 0.362102 144 7.98485e-12 [[114, 720], [30, 804]] 36 Primary Flavor Doughnut 74.9353 4.86406e-18 0.216049 0.439721 0.379361 108 2.47855e-15 [[98, 736], [10, 824]] 37 Secondary Flavor Ginger Beer 22.3634 2.25628e-06 0.216049 0.444895 0.382283 24 0.000480693 [[24, 810], [0, 834]] 38 Color Group Rose 18.6432 1.57604e-05 0.216049 0.42301 0.407061 24 6.16356e-05 [[23, 811], [1, 833]] 39 Base Cake Cheese 66.8047 2.99776e-16 0.216049 0.450934 0.435638 84 4.5139e-18 [[79, 755], [5, 829]] 40 Primary Flavor Butter Toffee 60.1815 8.65028e-15 0.216049 0.50366 0.456343 60 2.09848e-19 [[60, 774], [0, 834]] 41 Color Group Slate 10.1564 0.00143801 0.216049 0.540214 0.483138 12 1.69239e-05 [[12, 822], [0, 834]] 42 Primary Flavor Gingersnap 22.3634 2.25628e-06 0.216049 0.643218 0.623627 24 9.02005e-16 [[24, 810], [0, 834]] 43 Primary Flavor Dill Pickle 22.3634 2.25628e-06 0.216049 0.642239 0.655779 24 5.14558e-16 [[24, 810], [0, 834]] 44 Color Group Olive 44.9675 2.00328e-11 0.216049 0.637627 0.670186 60 3.71909e-20 [[56, 778], [4, 830]] 45 Primary Flavor Butter Milk 10.1564 0.00143801 0.216049 0.699284 0.688601 12 2.76773e-07 [[12, 822], [0, 834]] 46 Base Cake Sponge 127.156 1.71707e-29 0.216049 0.698996 0.699355 120 2.5584e-80 [[120, 714], [0, 834]] 47 Primary Flavor Chocolate Mint 10.1564 0.00143801 0.216049 0.685546 0.699666 12 1.48794e-07 [[12, 822], [0, 834]] 48 Primary Flavor Coconut 10.1564 0.00143801 0.216049 0.732777 0.717641 12 3.06716e-14 [[12, 822], [0, 834]] 49 Primary Flavor Blueberry 22.3634 2.25628e-06 0.216049 0.759643 0.72536 24 9.11605e-14 [[24, 810], [0, 834]] 50 Primary Flavor Amaretto 10.1564 0.00143801 0.216049 0.782156 0.764845 12 8.53642e-10 [[12, 822], [0, 834]] 2.2.2 Enrichment : Broad Analysis of Categories: ANOVA Recall our \"melted\" shift data. It will be useful to think of getting our Truffle data in this format: shift_melt.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index shift rate 0 0 A 15 1 1 A 15 2 2 A 15 3 3 A 16 4 4 A 17 df.columns = df.columns.str.replace(' ', '_') df.columns = df.columns.str.replace('/', '_') # get ANOVA table # Ordinary Least Squares (OLS) model model = ols('EBITDA_KG ~ C(Truffle_Type)', data=df).fit() anova_table = sm.stats.anova_lm(model, typ=2) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Recall the Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w, pvalue = stats.shapiro(model.resid) print(w, pvalue) 0.9576056599617004 1.2598073820281984e-21 And the Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. gb = df.groupby('Truffle_Type')['EBITDA_KG'] gb <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fb5127814d0> w, pvalue = stats.bartlett(*[gb.get_group(x) for x in gb.groups]) print(w, pvalue) 109.93252546442552 1.344173733366234e-24 Wow it looks like our data is not drawn from a normal distribution! Let's check this for other categories... We can wrap these in a for loop: for col in df.columns[:5]: print(col) model = ols('EBITDA_KG ~ C({})'.format(col), data=df).fit() anova_table = sm.stats.anova_lm(model, typ=2) display(anova_table) w, pvalue = stats.shapiro(model.resid) print(\"Shapiro: \", w, pvalue) gb = df.groupby(col)['EBITDA_KG'] w, pvalue = stats.bartlett(*[gb.get_group(x) for x in gb.groups]) print(\"Bartlett: \", w, pvalue) print() Base_Cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Base_Cake) 39.918103 5.0 314.869955 1.889884e-237 Residual 42.140500 1662.0 NaN NaN Shapiro: 0.9634131193161011 4.1681337029688696e-20 Bartlett: 69.83288886114195 1.1102218566053728e-13 Truffle_Type .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Shapiro: 0.9576056599617004 1.2598073820281984e-21 Bartlett: 109.93252546442552 1.344173733366234e-24 Primary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Primary_Flavor) 50.270639 50.0 51.143649 1.153434e-292 Residual 31.787964 1617.0 NaN NaN Shapiro: 0.948470413684845 9.90281706784179e-24 Bartlett: 210.15130419114894 1.5872504991231547e-21 Secondary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Secondary_Flavor) 15.088382 28.0 13.188089 1.929302e-54 Residual 66.970220 1639.0 NaN NaN Shapiro: 0.9548103213310242 2.649492974953278e-22 Bartlett: 420.6274502894803 1.23730070350945e-71 Color_Group .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Color_Group) 16.079685 11.0 36.689347 6.544980e-71 Residual 65.978918 1656.0 NaN NaN Shapiro: 0.969061017036438 1.8926407335144587e-18 Bartlett: 136.55525281340468 8.164787784033709e-24 2.2.3 Enrichment : Visual Analysis of Residuals: QQ-Plots This can be distressing and is often why we want visual methods to see what is going on with our data! model = ols('EBITDA_KG ~ C(Truffle_Type)', data=df).fit() #create instance of influence influence = model.get_influence() #obtain standardized residuals standardized_residuals = influence.resid_studentized_internal # res.anova_std_residuals are standardized residuals obtained from ANOVA (check above) sm.qqplot(standardized_residuals, line='45') plt.xlabel(\"Theoretical Quantiles\") plt.ylabel(\"Standardized Residuals\") plt.show() # histogram plt.hist(model.resid, bins='auto', histtype='bar', ec='k') plt.xlabel(\"Residuals\") plt.ylabel('Frequency') plt.show() We see that a lot of our data is swayed by extremely high and low values, so what can we conclude? You need the right test statistic for the right job, in this case, we are littered with unequal variance in our groupings so we use the moods median and welch (unequal variance t-test) to make conclusions about our data References Renesh Bedre ANOVA Minitab ANOVA Analytics Vidhya ANOVA Renesh Bedre Hypothesis Testing Real Statistics Turkey-kramer Mutual Information","title":"Inferential Statistics"},{"location":"S2_Inferential_Statistics/#data-science-foundations-session-2-inferential-statistics","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at the utility of EDA combined with inferential statistics.","title":"Data Science Foundations, Session 2: Inferential Statistics"},{"location":"S2_Inferential_Statistics/#20-preparing-environment-and-importing-data","text":"back to top","title":"2.0 Preparing Environment and Importing Data"},{"location":"S2_Inferential_Statistics/#201-import-packages","text":"back to top # The modules we've seen before import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns # our stats modules import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy","title":"2.0.1 Import Packages"},{"location":"S2_Inferential_Statistics/#202-load-dataset","text":"back to top For this session, we will use dummy datasets from sklearn. df = pd.read_csv('https://raw.githubusercontent.com/wesleybeckner/'\\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... ... ... ... ... ... 1663 Tiramisu Chocolate Outer Doughnut Pear Amethyst Fickelgruber 12/2020 38128.802589 0.420111 1664 Tiramisu Chocolate Outer Doughnut Pear Burgundy Zebrabar 12/2020 108.642857 0.248659 1665 Tiramisu Chocolate Outer Doughnut Pear Teal Zebrabar 12/2020 3517.933333 0.378501 1666 Tiramisu Chocolate Outer Doughnut Rock and Rye Amethyst Slugworth 12/2020 10146.898432 0.213149 1667 Tiramisu Chocolate Outer Doughnut Rock and Rye Burgundy Zebrabar 12/2020 1271.904762 0.431813 1668 rows \u00d7 9 columns descriptors = df.columns[:-2] for col in df.columns[:-5]: print(col) print(df[col].unique()) print() Base Cake ['Butter' 'Cheese' 'Chiffon' 'Pound' 'Sponge' 'Tiramisu'] Truffle Type ['Candy Outer' 'Chocolate Outer' 'Jelly Filled'] Primary Flavor ['Butter Pecan' 'Ginger Lime' 'Margarita' 'Pear' 'Pink Lemonade' 'Raspberry Ginger Ale' 'Sassafras' 'Spice' 'Wild Cherry Cream' 'Cream Soda' 'Horchata' 'Kettle Corn' 'Lemon Bar' 'Orange Pineapple\\tP' 'Plum' 'Orange' 'Butter Toffee' 'Lemon' 'Acai Berry' 'Apricot' 'Birch Beer' 'Cherry Cream Spice' 'Creme de Menthe' 'Fruit Punch' 'Ginger Ale' 'Grand Mariner' 'Orange Brandy' 'Pecan' 'Toasted Coconut' 'Watermelon' 'Wintergreen' 'Vanilla' 'Bavarian Cream' 'Black Licorice' 'Caramel Cream' 'Cheesecake' 'Cherry Cola' 'Coffee' 'Irish Cream' 'Lemon Custard' 'Mango' 'Sour' 'Amaretto' 'Blueberry' 'Butter Milk' 'Chocolate Mint' 'Coconut' 'Dill Pickle' 'Gingersnap' 'Chocolate' 'Doughnut'] Secondary Flavor ['Toffee' 'Banana' 'Rum' 'Tutti Frutti' 'Vanilla' 'Mixed Berry' 'Whipped Cream' 'Apricot' 'Passion Fruit' 'Peppermint' 'Dill Pickle' 'Black Cherry' 'Wild Cherry Cream' 'Papaya' 'Mango' 'Cucumber' 'Egg Nog' 'Pear' 'Rock and Rye' 'Tangerine' 'Apple' 'Black Currant' 'Kiwi' 'Lemon' 'Hazelnut' 'Butter Rum' 'Fuzzy Navel' 'Mojito' 'Ginger Beer']","title":"2.0.2 Load Dataset"},{"location":"S2_Inferential_Statistics/#21-many-flavors-of-statistical-tests","text":"https://luminousmen.com/post/descriptive-and-inferential-statistics Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (\u201cinferences\u201d) from that data. With inferential statistics, you take data from samples and make generalizations about a population statshowto Moods Median Test Kruskal-Wallis Test (Another comparison of Medians test) T-Test Analysis of Variance (ANOVA) One Way ANOVA Two Way ANOVA MANOVA Factorial ANOVA When do I use each of these? We will talk about this as we proceed through the examples. This page from minitab has good rules of thumb on the subject.","title":"2.1 Many Flavors of Statistical Tests"},{"location":"S2_Inferential_Statistics/#211-what-is-moods-median","text":"You can use Chi-Square to test for a goodness of fit (whether a sample of data represents a distribution) or whether two variables are related (using a contingency table, which we will create below!) A special case of Pearon's Chi-Squared Test: We create a table that counts the observations above and below the global median for two different groups. We then perform a chi-squared test of significance on this contingency table Null hypothesis: the Medians are all equal The chi-square test statistic: x^2 = \\sum{\\frac{(O-E)^2}{E}} Where \\(O\\) is the observed frequency and \\(E\\) is the expected frequency. Let's take an example , say we have three shifts with the following production rates: np.random.seed(42) shift_one = [round(i) for i in np.random.normal(16, 3, 10)] shift_two = [round(i) for i in np.random.normal(21, 3, 10)] print(shift_one) print(shift_two) [17, 16, 18, 21, 15, 15, 21, 18, 15, 18] [20, 20, 22, 15, 16, 19, 18, 22, 18, 17] stat, p, m, table = scipy.stats.median_test(shift_one, shift_two, correction=False) what is median_test returning? print(\"The perasons chi-square test statistic: {:.2f}\".format(stat)) print(\"p-value of the test: {:.3f}\".format(p)) print(\"the grand median: {}\".format(m)) The perasons chi-square test statistic: 1.98 p-value of the test: 0.160 the grand median: 18.0 Let's evaluate that test statistic ourselves by taking a look at the contingency table: table array([[2, 5], [8, 5]]) This is easier to make sense of if we order the shift times shift_one.sort() shift_one [15, 15, 15, 16, 17, 18, 18, 18, 21, 21] When we look at shift one, we see that 8 values are at or below the grand median. shift_two.sort() shift_two [15, 16, 17, 18, 18, 19, 20, 20, 22, 22] For shift two, only two are at or below the grand median. Since the sample sizes are the same, the expected value for both groups is the same, 5 above and 5 below the grand median. The chi-square is then: X^2 = \\frac{(2-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(2-5)^2}{5} (3-5)**2/5 + (7-5)**2/5 + (7-5)**2/5 + (3-5)**2/5 3.2 Our p-value, or the probability of observing the null-hypothsis, is under 0.05. We can conclude that these shift performances were drawn under seperate distributions. For comparison, let's do this analysis again with shifts of equal performances np.random.seed(3) shift_three = [round(i) for i in np.random.normal(16, 3, 10)] shift_four = [round(i) for i in np.random.normal(16, 3, 10)] stat, p, m, table = scipy.stats.median_test(shift_three, shift_four, correction=False) print(\"The pearsons chi-square test statistic: {:.2f}\".format(stat)) print(\"p-value of the test: {:.3f}\".format(p)) print(\"the grand median: {}\".format(m)) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000 the grand median: 15.5 and the shift raw values: shift_three.sort() shift_four.sort() print(shift_three) print(shift_four) [10, 14, 15, 15, 15, 16, 16, 16, 17, 21] [11, 12, 13, 14, 15, 16, 19, 19, 19, 21] table array([[5, 5], [5, 5]])","title":"2.1.1 What is Mood's Median?"},{"location":"S2_Inferential_Statistics/#212-when-to-use-moods","text":"Mood's Median Test is highly flexible but has the following assumptions: Considers only one categorical factor Response variable is continuous (our shift rates) Data does not need to be normally distributed But the distributions are similarly shaped Sample sizes can be unequal and small (less than 20 observations) Other considerations: Not as powerful as Kruskal-Wallis Test but still useful for small sample sizes or when there are outliers","title":"2.1.2 When to Use Mood's?"},{"location":"S2_Inferential_Statistics/#exercise-1-use-moods-median-test","text":"","title":"\ud83c\udfcb\ufe0f Exercise 1: Use Mood's Median Test"},{"location":"S2_Inferential_Statistics/#part-a-perform-moods-median-test-on-base-cake-in-truffle-data","text":"We're also going to get some practice with pandas groupby. # what is returned by this groupby? gp = df.groupby('Base Cake') How do we find out? We could iterate through it: # seems to be a tuple of some sort for i in gp: print(i) break ('Butter', Base Cake Truffle Type ... KG EBITDA/KG 0 Butter Candy Outer ... 53770.342593 0.500424 1 Butter Candy Outer ... 466477.578125 0.220395 2 Butter Candy Outer ... 80801.728070 0.171014 3 Butter Candy Outer ... 18046.111111 0.233025 4 Butter Candy Outer ... 19147.454268 0.480689 ... ... ... ... ... ... 1562 Butter Chocolate Outer ... 9772.200521 0.158279 1563 Butter Chocolate Outer ... 10861.245675 -0.159275 1564 Butter Chocolate Outer ... 3578.592163 0.431328 1565 Butter Jelly Filled ... 21438.187500 0.105097 1566 Butter Jelly Filled ... 15617.489115 0.185070 [456 rows x 9 columns]) # the first object appears to be the group print(i[0]) # the second object appears to be the df belonging to that group print(i[1]) Butter Base Cake Truffle Type ... KG EBITDA/KG 0 Butter Candy Outer ... 53770.342593 0.500424 1 Butter Candy Outer ... 466477.578125 0.220395 2 Butter Candy Outer ... 80801.728070 0.171014 3 Butter Candy Outer ... 18046.111111 0.233025 4 Butter Candy Outer ... 19147.454268 0.480689 ... ... ... ... ... ... 1562 Butter Chocolate Outer ... 9772.200521 0.158279 1563 Butter Chocolate Outer ... 10861.245675 -0.159275 1564 Butter Chocolate Outer ... 3578.592163 0.431328 1565 Butter Jelly Filled ... 21438.187500 0.105097 1566 Butter Jelly Filled ... 15617.489115 0.185070 [456 rows x 9 columns] going back to our diagram from our earlier pandas session. It looks like whenever we split in the groupby method, we create separate dataframes as well as their group label: Ok, so we know gp is separate dataframes. How do we turn them into arrays to then pass to median_test ? # complete this for loop for i, j in gp: # turn j into an array using the .values attribute # print this to the screen After you've completed the previous step, turn this into a list comprehension and pass the result to a variable called margins # complete the code below # margins = [# YOUR LIST COMPREHENSION HERE] Remember the list unpacking we did for the tic tac toe project? We're going to do the same thing here. Unpack the margins list for median_test and run the cell below! # complete the following line # stat, p, m, table = scipy.stats.median_test(<UNPACK MARGINS HERE>, correction=False) print(\"The pearsons chi-square test statistic: {:.2f}\".format(stat)) print(\"p-value of the test: {:.2e}\".format(p)) print(\"the grand median: {:.2e}\".format(m)) The pearsons chi-square test statistic: 448.81 p-value of the test: 8.85e-95 the grand median: 2.16e-01","title":"Part A Perform moods median test on Base Cake in Truffle data"},{"location":"S2_Inferential_Statistics/#part-b-view-the-distributions-of-the-data-using-matplotlib-and-seaborn","text":"What a fantastic statistical result we found! Can we affirm our result with some visualizations? I hope so! Create a boxplot below using pandas. In your call to df.boxplot() the by parameter should be set to Base Cake and the column parameter should be set to EBITDA/KG # YOUR BOXPLOT HERE /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray <matplotlib.axes._subplots.AxesSubplot at 0x7fb512f782d0> For comparison, I've shown the boxplot below using seaborn! fig, ax = plt.subplots(figsize=(10,7)) ax = sns.boxplot(x='Base Cake', y='EBITDA/KG', data=df, color='#A0cbe8')","title":"Part B View the distributions of the data using matplotlib and seaborn"},{"location":"S2_Inferential_Statistics/#part-c-perform-moods-median-on-all-the-other-groups","text":"# Recall the other descriptors we have descriptors Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') for desc in descriptors: # YOUR CODE FORM MARGINS BELOW # margins = [<YOUR LIST COMPREHENSION>] # UNPACK MARGINS INTO MEDIAN_TEST # stat, p, m, table = scipy.stats.median_test(<YOUR UNPACKING METHOD>, correction=False) print(desc) print(\"The pearsons chi-square test statistic: {:.2f}\".format(stat)) print(\"p-value of the test: {:e}\".format(p)) print(\"the grand median: {}\".format(m), end='\\n\\n') Base Cake The pearsons chi-square test statistic: 448.81 p-value of the test: 8.851450e-95 the grand median: 0.2160487288076019 Truffle Type The pearsons chi-square test statistic: 22.86 p-value of the test: 1.088396e-05 the grand median: 0.2160487288076019 Primary Flavor The pearsons chi-square test statistic: 638.99 p-value of the test: 3.918933e-103 the grand median: 0.2160487288076019 Secondary Flavor The pearsons chi-square test statistic: 323.13 p-value of the test: 6.083210e-52 the grand median: 0.2160487288076019 Color Group The pearsons chi-square test statistic: 175.18 p-value of the test: 1.011412e-31 the grand median: 0.2160487288076019 Customer The pearsons chi-square test statistic: 5.66 p-value of the test: 2.257760e-01 the grand median: 0.2160487288076019 Date The pearsons chi-square test statistic: 5.27 p-value of the test: 9.175929e-01 the grand median: 0.2160487288076019","title":"Part C Perform Moods Median on all the other groups"},{"location":"S2_Inferential_Statistics/#part-d-many-boxplots","text":"And finally, we will confirm these visually. Complete the Boxplot for each group: for desc in descriptors: fig, ax = plt.subplots(figsize=(10,5)) # sns.boxplot(x=<YOUR X VARIABLE HERE>, y='EBITDA/KG', data=df, color='#A0cbe8', ax=ax) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 9 missing from current font. /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 9 missing from current font.","title":"Part D Many boxplots"},{"location":"S2_Inferential_Statistics/#213-enrichment-what-is-a-t-test","text":"There are 1-sample and 2-sample T-tests (note: we would use a 1-sample T-test just to determine if the sample mean is equal to a hypothesized population mean) Within 2-sample T-tests we have independent and dependent T-tests (uncorrelated or correlated samples) For independent, two-sample T-tests: Equal variance (or pooled) T-test scipy.stats.ttest_ind(equal_var=True) Unequal variance T-test scipy.stats.ttest_ind(equal_var=False) also called Welch's T-test For dependent T-tests: * Paired (or correlated) T-test * scipy.stats.ttest_rel A full discussion on T-tests is outside the scope of this session, but we can refer to wikipedia for more information, including formulas on how each statistic is computed: * student's T-test","title":"2.1.3 Enrichment: What is a T-test?"},{"location":"S2_Inferential_Statistics/#214-enrichment-demonstration-of-t-tests","text":"back to top We'll assume our shifts are of equal variance and proceed with the appropriate independent two-sample T-test... print(shift_one) print(shift_two) [15, 15, 15, 16, 17, 18, 18, 18, 21, 21] [15, 16, 17, 18, 18, 19, 20, 20, 22, 22] To calculate the T-test, we follow a slightly different statistical formula: $T=\\frac{\\mu_1 - \\mu_2}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$ where $\\mu$ are the means of the two groups, $n$ are the sample sizes and $s$ is the pooled standard deviation, also known as the cummulative variance (depending on if you square it or not): $s= \\sqrt{\\frac{(n_1-1)\\sigma_1^2 + (n_2-1)\\sigma_2^2}{n_1 + n_2 - 2}}$ where $\\sigma$ are the standard deviations. What you'll notice here is we are combining the two variances, we can only do this if we assume the variances are somewhat equal, this is known as the equal variances t-test. mean_shift_one = np.mean(shift_one) mean_shift_two = np.mean(shift_two) print(mean_shift_one, mean_shift_two) 17.4 18.7 com_var = ((np.sum([(i - mean_shift_one)**2 for i in shift_one]) + np.sum([(i - mean_shift_two)**2 for i in shift_two])) / (len(shift_one) + len(shift_two)-2)) print(com_var) 5.361111111111111 T = (np.abs(mean_shift_one - mean_shift_two) / ( np.sqrt(com_var/len(shift_one) + com_var/len(shift_two)))) T 1.2554544209603191 We see that this hand-computed result matches that of the scipy module: scipy.stats.ttest_ind(shift_two, shift_one, equal_var=True) Ttest_indResult(statistic=1.2554544209603191, pvalue=0.22536782778843117)","title":"2.1.4 Enrichment: Demonstration of T-tests"},{"location":"S2_Inferential_Statistics/#enrichment-615-what-are-f-statistics-and-the-f-test","text":"The F-statistic is simply a ratio of two variances, or the ratio of mean squares mean squares is the estimate of population variance that accounts for the degrees of freedom to compute that estimate. We will explore this in the context of ANOVA","title":"Enrichment: 6.1.5 What are F-statistics and the F-test?"},{"location":"S2_Inferential_Statistics/#216-enrichment-what-is-analysis-of-variance","text":"ANOVA uses the F-test to determine whether the variability between group means is larger than the variability within the groups. If that statistic is large enough, you can conclude that the means of the groups are not equal. The caveat is that ANOVA tells us whether there is a difference in means but it does not tell us where the difference is. To find where the difference is between the groups, we have to conduct post-hoc tests. There are two main types: * One-way (one factor) and * Two-way (two factor) where factor is an indipendent variable Ind A Ind B Dep X H 10 X I 12 Y I 11 Y H 20","title":"2.1.6 Enrichment: What is Analysis of Variance?"},{"location":"S2_Inferential_Statistics/#anova-hypotheses","text":"Null hypothesis : group means are equal Alternative hypothesis : at least one group mean is different form the other groups","title":"ANOVA Hypotheses"},{"location":"S2_Inferential_Statistics/#anova-assumptions","text":"Residuals (experimental error) are normally distributed (test with Shapiro-Wilk) Homogeneity of variances (variances are equal between groups) (test with Bartlett's) Observations are sampled independently from each other Note: ANOVA assumptions can be checked using test statistics (e.g. Shapiro-Wilk, Bartlett\u2019s, Levene\u2019s test) and the visual approaches such as residual plots (e.g. QQ-plots) and histograms.","title":"ANOVA Assumptions"},{"location":"S2_Inferential_Statistics/#steps-for-anova","text":"Check sample sizes: equal observations must be in each group Calculate Sum of Square between groups and within groups (\\(SS_B, SS_E\\)) Calculate Mean Square between groups and within groups (\\(MS_B, MS_E\\)) Calculate F value (\\(MS_B/MS_E\\)) This might be easier to see in a table: Source of Variation degree of freedom (Df) Sum of squares (SS) Mean square (MS) F value Between Groups Df_b = P-1 SS_B MS_B = SS_B / Df_B MS_B / MS_E Within Groups Df_E = P(N-1) SS_E MS_E = SS_E / Df_E total Df_T = PN-1 SS_T Where: SS_B = \\sum_{i}^{P}{(\\bar{y}_i-\\bar{y})^2} SS_E = \\sum_{ik}^{PN}{(\\bar{y}_{ik}-\\bar{y}_i)^2} SS_T = SS_B + SS_E Let's go back to our shift data to take an example: shifts = pd.DataFrame([shift_one, shift_two, shift_three, shift_four]).T shifts.columns = ['A', 'B', 'C', 'D'] shifts.boxplot() <matplotlib.axes._subplots.AxesSubplot at 0x7fb5128c6690>","title":"Steps for ANOVA"},{"location":"S2_Inferential_Statistics/#2160-enrichment-sns-boxplot","text":"this is another great way to view boxplot data. Notice how sns also shows us the raw data alongside the box and whiskers using a swarmplot . shift_melt = pd.melt(shifts.reset_index(), id_vars=['index'], value_vars=['A', 'B', 'C', 'D']) shift_melt.columns = ['index', 'shift', 'rate'] ax = sns.boxplot(x='shift', y='rate', data=shift_melt, color='#A0cbe8') ax = sns.swarmplot(x=\"shift\", y=\"rate\", data=shift_melt, color='#79706e') Anyway back to ANOVA... fvalue, pvalue = stats.f_oneway(shifts['A'], shifts['B'], shifts['C'], shifts['D']) print(fvalue, pvalue) 2.8666172656539457 0.04998066540684099 We can get this in the format of the table we saw above: # get ANOVA table import statsmodels.api as sm from statsmodels.formula.api import ols # Ordinary Least Squares (OLS) model model = ols('rate ~ C(shift)', data=shift_melt).fit() anova_table = sm.stats.anova_lm(model, typ=2) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(shift) 64.475 3.0 2.866617 0.049981 Residual 269.900 36.0 NaN NaN The Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w, pvalue = stats.shapiro(model.resid) print(w, pvalue) 0.9800916314125061 0.6929556727409363 We can use Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. w, pvalue = stats.bartlett(shifts['A'], shifts['B'], shifts['C'], shifts['D']) print(w, pvalue) 1.9492677462621584 0.5830028540285896","title":"2.1.6.0 Enrichment: SNS Boxplot"},{"location":"S2_Inferential_Statistics/#2161-anova-interpretation","text":"The p value form ANOVA analysis is significant ( p < 0.05) and we can conclude there are significant difference between the shifts. But we do not know which shift(s) are different. For this we need to perform a post hoc test. There are a multitude of these that are beyond the scope of this discussion ( Tukey-kramer is one such test)","title":"2.1.6.1 ANOVA Interpretation"},{"location":"S2_Inferential_Statistics/#217-putting-it-all-together","text":"In summary, there are many statistical tests at our disposal when performing inferential statistical analysis. In times like these, a simple decision tree can be extraordinarily useful! source: scribbr","title":"2.1.7 Putting it all together"},{"location":"S2_Inferential_Statistics/#22-evaluate-statistical-significance-of-product-margin-a-snake-in-the-garden","text":"","title":"2.2 Evaluate statistical significance of product margin: a snake in the garden"},{"location":"S2_Inferential_Statistics/#221-moods-median-on-product-descriptors","text":"The first issue we run into with moods is... what? We can only perform moods on two groups at a time. How can we get around this? Let's take a look at the category with the fewest descriptors. If we remember, this was the Truffle Types. df.columns Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date', 'KG', 'EBITDA/KG'], dtype='object') df['Truffle Type'].unique() array(['Candy Outer', 'Chocolate Outer', 'Jelly Filled'], dtype=object) col = 'Truffle Type' moodsdf = pd.DataFrame() for truff in df[col].unique(): # for each group = df.loc[df[col] == truff]['EBITDA/KG'] pop = df.loc[~(df[col] == truff)]['EBITDA/KG'] stat, p, m, table = scipy.stats.median_test(group, pop) median = np.median(group) mean = np.mean(group) size = len(group) print(\"{}: N={}\".format(truff, size)) print(\"Welch's T-Test for Unequal Variances\") print(scipy.stats.ttest_ind(group, pop, equal_var=False)) welchp = scipy.stats.ttest_ind(group, pop, equal_var=False).pvalue print() moodsdf = pd.concat([moodsdf, pd.DataFrame([truff, stat, p, m, mean, median, size, welchp, table]).T]) moodsdf.columns = [col, 'pearsons_chi_square', 'p_value', 'grand_median', 'group_mean', 'group_median', 'size', 'welch p', 'table'] Candy Outer: N=288 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=-2.7615297773427527, pvalue=0.005911048922657976) Chocolate Outer: N=1356 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=4.409449025092911, pvalue=1.1932685612874952e-05) Jelly Filled: N=24 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=-8.4142523067935, pvalue=7.929912531660173e-09)","title":"2.2.1 Mood's Median on product descriptors"},{"location":"S2_Inferential_Statistics/#question-1-moods-results-on-truffle-type","text":"What do we notice about the resultant table? p-values Most are quite small (really low probability of achieving these table results under a single distribution) group sizes: our Jelly Filled group is relatively small moodsdf.sort_values('p_value') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Truffle Type pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Jelly Filled 18.6432 1.57604e-05 0.216049 0.0513823 0.0179334 24 7.92991e-09 [[1, 833], [23, 811]] 0 Chocolate Outer 6.6275 0.0100416 0.216049 0.262601 0.225562 1356 1.19327e-05 [[699, 135], [657, 177]] 0 Candy Outer 1.51507 0.218368 0.216049 0.230075 0.204264 288 0.00591105 [[134, 700], [154, 680]] We can go ahead and repeat this analysis for all of our product categories: df.columns[:5] Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group'], dtype='object') moodsdf = pd.DataFrame() for col in df.columns[:5]: for truff in df[col].unique(): group = df.loc[df[col] == truff]['EBITDA/KG'] pop = df.loc[~(df[col] == truff)]['EBITDA/KG'] stat, p, m, table = scipy.stats.median_test(group, pop) median = np.median(group) mean = np.mean(group) size = len(group) welchp = scipy.stats.ttest_ind(group, pop, equal_var=False).pvalue moodsdf = pd.concat([moodsdf, pd.DataFrame([col, truff, stat, p, m, mean, median, size, welchp, table]).T]) moodsdf.columns = ['descriptor', 'group', 'pearsons_chi_square', 'p_value', 'grand_median', 'group_mean', 'group_median', 'size', 'welch p', 'table'] print(moodsdf.shape) (101, 10) moodsdf = moodsdf.loc[(moodsdf['welch p'] < 0.005) & (moodsdf['p_value'] < 0.005)].sort_values('group_median') moodsdf = moodsdf.sort_values('group_median').reset_index(drop=True) print(moodsdf.shape) (51, 10) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Secondary Flavor Papaya 18.6432 1.57604e-05 0.216049 0.0167466 0.00245839 24 1.04879e-10 [[1, 833], [23, 811]] 1 Primary Flavor Orange Pineapple\\tP 18.6432 1.57604e-05 0.216049 0.0167466 0.00245839 24 1.04879e-10 [[1, 833], [23, 811]] 2 Primary Flavor Cherry Cream Spice 10.1564 0.00143801 0.216049 0.0187023 0.00970093 12 7.15389e-07 [[0, 834], [12, 822]] 3 Secondary Flavor Cucumber 18.6432 1.57604e-05 0.216049 0.0513823 0.0179334 24 7.92991e-09 [[1, 833], [23, 811]] 4 Truffle Type Jelly Filled 18.6432 1.57604e-05 0.216049 0.0513823 0.0179334 24 7.92991e-09 [[1, 833], [23, 811]] 5 Primary Flavor Orange 18.6432 1.57604e-05 0.216049 0.0513823 0.0179334 24 7.92991e-09 [[1, 833], [23, 811]] 6 Primary Flavor Toasted Coconut 15.2613 9.36173e-05 0.216049 0.0370021 0.0283916 24 3.13722e-08 [[2, 832], [22, 812]] 7 Secondary Flavor Apricot 15.2613 9.36173e-05 0.216049 0.0603122 0.0374225 24 4.5952e-08 [[2, 832], [22, 812]] 8 Primary Flavor Kettle Corn 29.0621 7.00962e-08 0.216049 0.0554518 0.045891 60 6.3013e-18 [[9, 825], [51, 783]] 9 Primary Flavor Acai Berry 18.6432 1.57604e-05 0.216049 0.0365051 0.0494656 24 1.49539e-10 [[1, 833], [23, 811]] 10 Primary Flavor Pink Lemonade 10.1564 0.00143801 0.216049 0.0398622 0.0563492 12 1.06677e-05 [[0, 834], [12, 822]] 11 Secondary Flavor Black Cherry 58.9004 1.65861e-14 0.216049 0.0559745 0.0628979 96 6.65441e-31 [[11, 823], [85, 749]] 12 Primary Flavor Watermelon 15.2613 9.36173e-05 0.216049 0.0440497 0.0678958 24 5.20636e-08 [[2, 832], [22, 812]] 13 Primary Flavor Plum 34.8516 3.55816e-09 0.216049 0.0849632 0.0799934 72 1.20739e-16 [[11, 823], [61, 773]] 14 Secondary Flavor Dill Pickle 10.1564 0.00143801 0.216049 0.0370421 0.0824944 12 7.45639e-06 [[0, 834], [12, 822]] 15 Primary Flavor Horchata 10.1564 0.00143801 0.216049 0.0370421 0.0824944 12 7.45639e-06 [[0, 834], [12, 822]] 16 Primary Flavor Lemon Custard 12.2175 0.000473444 0.216049 0.0793894 0.087969 24 6.19493e-06 [[3, 831], [21, 813]] 17 Primary Flavor Fruit Punch 10.1564 0.00143801 0.216049 0.0789353 0.0903256 12 7.61061e-05 [[0, 834], [12, 822]] 18 Base Cake Chiffon 117.046 2.80454e-27 0.216049 0.127851 0.125775 288 6.79655e-43 [[60, 774], [228, 606]] 19 Base Cake Butter 134.367 4.54085e-31 0.216049 0.142082 0.139756 456 9.8457e-52 [[122, 712], [334, 500]] 20 Secondary Flavor Banana 10.8053 0.00101207 0.216049 0.163442 0.15537 60 7.23966e-09 [[17, 817], [43, 791]] 21 Primary Flavor Cream Soda 9.51186 0.00204148 0.216049 0.150265 0.163455 24 2.17977e-06 [[4, 830], [20, 814]] 22 Secondary Flavor Peppermint 9.51186 0.00204148 0.216049 0.150265 0.163455 24 2.17977e-06 [[4, 830], [20, 814]] 23 Primary Flavor Grand Mariner 10.5818 0.00114208 0.216049 0.197463 0.165529 72 0.000828508 [[22, 812], [50, 784]] 24 Color Group Amethyst 20.4883 5.99977e-06 0.216049 0.195681 0.167321 300 4.0427e-07 [[114, 720], [186, 648]] 25 Color Group Burgundy 10.9997 0.000911278 0.216049 0.193048 0.171465 120 0.000406312 [[42, 792], [78, 756]] 26 Color Group White 35.7653 2.22582e-09 0.216049 0.19 0.177264 432 1.56475e-16 [[162, 672], [270, 564]] 27 Color Group Opal 11.5872 0.000664086 0.216049 0.317878 0.259304 324 3.94098e-07 [[190, 644], [134, 700]] 28 Secondary Flavor Apple 27.2833 1.75723e-07 0.216049 0.326167 0.293876 36 0.00117635 [[34, 800], [2, 832]] 29 Secondary Flavor Tangerine 32.6264 1.11688e-08 0.216049 0.342314 0.319273 48 0.000112572 [[44, 790], [4, 830]] 30 Secondary Flavor Black Currant 34.7784 3.69452e-09 0.216049 0.357916 0.332449 36 9.48594e-08 [[36, 798], [0, 834]] 31 Secondary Flavor Pear 16.6143 4.58043e-05 0.216049 0.373034 0.33831 60 3.05948e-05 [[46, 788], [14, 820]] 32 Primary Flavor Vanilla 34.7784 3.69452e-09 0.216049 0.378053 0.341626 36 1.00231e-06 [[36, 798], [0, 834]] 33 Color Group Citrine 10.1564 0.00143801 0.216049 0.390728 0.342512 12 0.00192513 [[12, 822], [0, 834]] 34 Color Group Teal 13.5397 0.000233572 0.216049 0.323955 0.3446 96 0.00120954 [[66, 768], [30, 804]] 35 Base Cake Tiramisu 52.3606 4.61894e-13 0.216049 0.388267 0.362102 144 7.98485e-12 [[114, 720], [30, 804]] 36 Primary Flavor Doughnut 74.9353 4.86406e-18 0.216049 0.439721 0.379361 108 2.47855e-15 [[98, 736], [10, 824]] 37 Secondary Flavor Ginger Beer 22.3634 2.25628e-06 0.216049 0.444895 0.382283 24 0.000480693 [[24, 810], [0, 834]] 38 Color Group Rose 18.6432 1.57604e-05 0.216049 0.42301 0.407061 24 6.16356e-05 [[23, 811], [1, 833]] 39 Base Cake Cheese 66.8047 2.99776e-16 0.216049 0.450934 0.435638 84 4.5139e-18 [[79, 755], [5, 829]] 40 Primary Flavor Butter Toffee 60.1815 8.65028e-15 0.216049 0.50366 0.456343 60 2.09848e-19 [[60, 774], [0, 834]] 41 Color Group Slate 10.1564 0.00143801 0.216049 0.540214 0.483138 12 1.69239e-05 [[12, 822], [0, 834]] 42 Primary Flavor Gingersnap 22.3634 2.25628e-06 0.216049 0.643218 0.623627 24 9.02005e-16 [[24, 810], [0, 834]] 43 Primary Flavor Dill Pickle 22.3634 2.25628e-06 0.216049 0.642239 0.655779 24 5.14558e-16 [[24, 810], [0, 834]] 44 Color Group Olive 44.9675 2.00328e-11 0.216049 0.637627 0.670186 60 3.71909e-20 [[56, 778], [4, 830]] 45 Primary Flavor Butter Milk 10.1564 0.00143801 0.216049 0.699284 0.688601 12 2.76773e-07 [[12, 822], [0, 834]] 46 Base Cake Sponge 127.156 1.71707e-29 0.216049 0.698996 0.699355 120 2.5584e-80 [[120, 714], [0, 834]] 47 Primary Flavor Chocolate Mint 10.1564 0.00143801 0.216049 0.685546 0.699666 12 1.48794e-07 [[12, 822], [0, 834]] 48 Primary Flavor Coconut 10.1564 0.00143801 0.216049 0.732777 0.717641 12 3.06716e-14 [[12, 822], [0, 834]] 49 Primary Flavor Blueberry 22.3634 2.25628e-06 0.216049 0.759643 0.72536 24 9.11605e-14 [[24, 810], [0, 834]] 50 Primary Flavor Amaretto 10.1564 0.00143801 0.216049 0.782156 0.764845 12 8.53642e-10 [[12, 822], [0, 834]]","title":"Question 1: Moods Results on Truffle Type"},{"location":"S2_Inferential_Statistics/#222-enrichment-broad-analysis-of-categories-anova","text":"Recall our \"melted\" shift data. It will be useful to think of getting our Truffle data in this format: shift_melt.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index shift rate 0 0 A 15 1 1 A 15 2 2 A 15 3 3 A 16 4 4 A 17 df.columns = df.columns.str.replace(' ', '_') df.columns = df.columns.str.replace('/', '_') # get ANOVA table # Ordinary Least Squares (OLS) model model = ols('EBITDA_KG ~ C(Truffle_Type)', data=df).fit() anova_table = sm.stats.anova_lm(model, typ=2) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Recall the Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w, pvalue = stats.shapiro(model.resid) print(w, pvalue) 0.9576056599617004 1.2598073820281984e-21 And the Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. gb = df.groupby('Truffle_Type')['EBITDA_KG'] gb <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fb5127814d0> w, pvalue = stats.bartlett(*[gb.get_group(x) for x in gb.groups]) print(w, pvalue) 109.93252546442552 1.344173733366234e-24 Wow it looks like our data is not drawn from a normal distribution! Let's check this for other categories... We can wrap these in a for loop: for col in df.columns[:5]: print(col) model = ols('EBITDA_KG ~ C({})'.format(col), data=df).fit() anova_table = sm.stats.anova_lm(model, typ=2) display(anova_table) w, pvalue = stats.shapiro(model.resid) print(\"Shapiro: \", w, pvalue) gb = df.groupby(col)['EBITDA_KG'] w, pvalue = stats.bartlett(*[gb.get_group(x) for x in gb.groups]) print(\"Bartlett: \", w, pvalue) print() Base_Cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Base_Cake) 39.918103 5.0 314.869955 1.889884e-237 Residual 42.140500 1662.0 NaN NaN Shapiro: 0.9634131193161011 4.1681337029688696e-20 Bartlett: 69.83288886114195 1.1102218566053728e-13 Truffle_Type .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Shapiro: 0.9576056599617004 1.2598073820281984e-21 Bartlett: 109.93252546442552 1.344173733366234e-24 Primary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Primary_Flavor) 50.270639 50.0 51.143649 1.153434e-292 Residual 31.787964 1617.0 NaN NaN Shapiro: 0.948470413684845 9.90281706784179e-24 Bartlett: 210.15130419114894 1.5872504991231547e-21 Secondary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Secondary_Flavor) 15.088382 28.0 13.188089 1.929302e-54 Residual 66.970220 1639.0 NaN NaN Shapiro: 0.9548103213310242 2.649492974953278e-22 Bartlett: 420.6274502894803 1.23730070350945e-71 Color_Group .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Color_Group) 16.079685 11.0 36.689347 6.544980e-71 Residual 65.978918 1656.0 NaN NaN Shapiro: 0.969061017036438 1.8926407335144587e-18 Bartlett: 136.55525281340468 8.164787784033709e-24","title":"2.2.2 Enrichment: Broad Analysis of Categories: ANOVA"},{"location":"S2_Inferential_Statistics/#223-enrichment-visual-analysis-of-residuals-qq-plots","text":"This can be distressing and is often why we want visual methods to see what is going on with our data! model = ols('EBITDA_KG ~ C(Truffle_Type)', data=df).fit() #create instance of influence influence = model.get_influence() #obtain standardized residuals standardized_residuals = influence.resid_studentized_internal # res.anova_std_residuals are standardized residuals obtained from ANOVA (check above) sm.qqplot(standardized_residuals, line='45') plt.xlabel(\"Theoretical Quantiles\") plt.ylabel(\"Standardized Residuals\") plt.show() # histogram plt.hist(model.resid, bins='auto', histtype='bar', ec='k') plt.xlabel(\"Residuals\") plt.ylabel('Frequency') plt.show() We see that a lot of our data is swayed by extremely high and low values, so what can we conclude? You need the right test statistic for the right job, in this case, we are littered with unequal variance in our groupings so we use the moods median and welch (unequal variance t-test) to make conclusions about our data","title":"2.2.3 Enrichment: Visual Analysis of Residuals: QQ-Plots"},{"location":"S2_Inferential_Statistics/#references","text":"Renesh Bedre ANOVA Minitab ANOVA Analytics Vidhya ANOVA Renesh Bedre Hypothesis Testing Real Statistics Turkey-kramer Mutual Information","title":"References"},{"location":"S3_Model_Selection_and_Validation/","text":"Data Science Foundations, Session 3: Model Selection and Validation Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com At the end of session 1, we saw the basic recipe for creating a supervised machine learning model: Environment setup and importing data Rudimentary exploratory data analysis Feature engineering Choosing and training a model: choose model choose hyperparameters fit using (training) data predict using (validation) data In session 1, I chose our model and hyperparameters preemptively. How did I do that? In the real world, you won't necessarily have the best intution about how to make these choices. In today's session, we will algorithmize the way we approach choosing and training a model Note: I will import libraries at the beginning of this notebook, as is good practice, but will reimport them as they are used to remind ourselves where each method came from! 3.0 Preparing Environment and Importing Data back to top 3.0.1 Import Packages back to top # Pandas library for the pandas dataframes import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import seaborn as sns import random import scipy.stats as stats from patsy import dmatrices from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import LabelEncoder, StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression, LinearRegression from sklearn import metrics from sklearn.metrics import r2_score, mean_squared_error from sklearn.datasets import load_iris 3.0.2 Load Dataset back to top In course 1 we cursorily discussed why we may need strategies for validating our model. Here we'll discuss it more in depth. I'm going to take a simple example. In the following, I have a dataset that contains some data about flowers. It's a very famous dataset used in the ML world iris = load_iris() X = iris.data y = iris.target print(X.shape) print(y.shape) (150, 4) (150,) let's go ahead and load our wine dataset as well... wine = pd.read_csv(\"https://raw.githubusercontent.com/wesleybeckner/\"\\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\") wine.dropna(inplace=True) wine['quality_label'] = wine['quality'].apply(lambda x: 'low' if x <=5 else 'med' if x <= 7 else 'high') class_tp = {'red': 0, 'white': 1} y_tp = wine['type'].map(class_tp) wine['type_encoding'] = y_tp class_ql = {'low':0, 'med': 1, 'high': 2} y_ql = wine['quality_label'].map(class_ql) wine['quality_encoding'] = y_ql wine.columns = wine.columns.str.replace(' ', '_') 3.1 Model Validation back to top doing it the wrong way While we're here, I'm going to introduce a VERY SIMPLE supervised learning method called K-Nearest Neighbors. 3.1.0 K-Nearest Neighbors back to top K-Nearest Neighbors is perhaps the simplest algorithm of them all. It is essentially a lookup table: We select the hyperparameter K, and when assigning a new value a data label, assign it according to, the majority label in the vicinity of the new datapoint. The vicinity being determined by K, the number of nearest neighbors we are going to assess. knn = KNeighborsRegressor(n_neighbors=1) knn.fit(X,y) KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=1, p=2, weights='uniform') knn.score(X,y) 1.0 Wow! we achieved a model with a perfect score! But is this really how we would expect the model to perform on data it had never seen before? Probably not. How do we actually check the performance of our model? 3.1.1 Holdout Sets back to top The way we account for unseen data, in practice, is to leave a portion of the dataset out for testing. This way, we can estimate how our model will perform on entirely new data it may come across in application. X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_state=42) knn = KNeighborsRegressor(n_neighbors=1) knn.fit(X_train,y_train) print(knn.score(X_test, y_test)) 0.9753593429158111 We see that we get a more reasonable value for our performance! 3.1.2 Data Leakage and Cross-Validation back to top An even more rigorous method to leaving out a single test set, is to perform cross validation. Imagine a situation where we are trying to estimate the best value of K in our KNN algorithm. If we continually train our model with new values of K on our training set, and test with our testing set, \"knowledge\" of our test set values with leak into our model, as we choose the best value for K based on how it performs on our test set (even though we did not train on this test set). We call this phenomenon data leakage . CV or Cross Validation overcomes this by only evaluating our parameters with our training set. [image src](https://scikit-learn.org/stable/modules/cross_validation.html) In this scheme, we don't evaluate our model on the test set until the very end. Rather, we estimate our hyperparameter performances by slicing the training set into cross folds [image src](https://scikit-learn.org/stable/modules/cross_validation.html) from sklearn.model_selection import cross_val_score scores = cross_val_score(knn, X_train, y_train, cv=5) scores array([0.91666667, 0.81725888, 0.85714286, 1. , 0.91 ]) print(\"%0.2f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std())) 0.90 accuracy with a standard deviation of 0.062 More information on the cross_val_score method in sklearn can be found here An additional topic on cross validation is the extreme leave-one-out validation, you can read more about that here 3.1.3 Bias-Variance Tradeoff back to top This next concept will be most easily understood (imo) if we go ahead an make up some data ourselves, I'm going to do that now. # we can throttle the error rate err = .5 random.seed(42) # our data has a KNOWN underlying functional form (log(x)) def func(x, err): return np.log(x) + err * random.randint(-1,1) * random.random() x = np.arange(20,100) y = [func(t, err) for t in x] plt.plot(x,y, ls='', marker='.') plt.xlabel('X') plt.ylabel('Y') Text(0, 0.5, 'Y') Let's fit to just a portion of this data random.seed(42) X_train = random.sample(list(x), 10) indices = [list(x).index(i) for i in X_train] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [y[i] for i in indices] plt.plot(X_train,y_train, ls='', marker='.') [<matplotlib.lines.Line2D at 0x7f3ea2189f10>] Now let's take two extreme scenarios, fitting a linear line and a high order polynomial, to these datapoints. Keeping in mind the larger dataset, as well as the error we introduced in our data generating function, this will really illustrate our point! # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, 9) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression() model.fit(np.array(X_train).reshape(-1,1), y_train) # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) fig, ax = plt.subplots(1,2,figsize=(15,5)) ax[0].plot(X_seq, model.predict(X_seq), c='grey', ls='--') ax[0].plot(X_train, y_train, ls='', marker='.') ax[0].set_ylim(min(y_train), max(y_train)) ax[0].set_title(\"High Bias Model\") ax[1].plot(X_seq, np.polyval(coefs, X_seq), c='grey', ls='--') ax[1].plot(X_train, y_train, ls='', marker='.') ax[1].set_ylim(min(y_train), max(y_train)) ax[1].set_title(\"High Variance Model\") Text(0.5, 1.0, 'High Variance Model') We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has high bias because we are forcing the functional form without much consideration to the underlying data \u2014 we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has low variance with respect to the underlying data. On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has low bias because we don't introduce many constraints on the final form of the model. it is high variance because depending on the underlying training data, the final outcome of the model can change quite drastically! In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further: what happens when we retrain these models on different samples of the data population and let's use this to better understand what we mean by bias and variance what happens when we tie this back in with the error we introduced to the data generator? and let's use this to better understand irreducible error random.seed(42) fig, ax = plt.subplots(1,2,figsize=(15,5)) for samples in range(5): X_train = random.sample(list(x), 10) indices = [list(x).index(i) for i in X_train] y_train = [y[i] for i in indices] # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, 9) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression() model.fit(np.array(X_train).reshape(-1,1), y_train) # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) ax[0].plot(X_seq, model.predict(X_seq), alpha=0.5, ls='--') ax[0].plot(X_train, y_train, ls='', marker='.') ax[0].set_ylim(min(y_train), max(y_train)) ax[0].set_title(\"High Bias Model\") ax[1].plot(X_seq, np.polyval(coefs, X_seq), alpha=0.5, ls='--') ax[1].plot(X_train, y_train, ls='', marker='.') ax[1].set_ylim(min(y_train), max(y_train)) ax[1].set_title(\"High Variance Model\") As we can see, depending on what data we train our model on, the high bias model changes relatively slightly, while the high variance model changes a whole awful lot! The high variance model is prone to something we call overfitting . It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the population # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, 9) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression() model.fit(np.array(X_train).reshape(-1,1), y_train) # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) fig, ax = plt.subplots(1,2,figsize=(15,5)) ax[0].plot(X_seq, model.predict(X_seq), c='grey', ls='--') ax[0].plot(x, y, ls='', marker='*', alpha=0.6) ax[0].plot(X_train, y_train, ls='', marker='.') ax[0].set_ylim(min(y), max(y)) ax[0].set_title(\"High Bias Model\") ax[1].plot(X_seq, np.polyval(coefs, X_seq), c='grey', ls='--') ax[1].plot(x, y, ls='', marker='*', alpha=0.6) ax[1].plot(X_train, y_train, ls='', marker='.') ax[1].set_ylim(min(y), max(y)) ax[1].set_title(\"High Variance Model\") Text(0.5, 1.0, 'High Variance Model') In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance models are particuarly prone to the phenomenon of over fitting and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this. x = np.arange(20,100) y = [func(t, err=0) for t in x] plt.plot(x,y, ls='', marker='.') [<matplotlib.lines.Line2D at 0x7f3ea05cbc90>] random.seed(42) X_train = random.sample(list(x), 10) indices = [list(x).index(i) for i in X_train] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [y[i] for i in indices] # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, 9) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression() model.fit(np.array(X_train).reshape(-1,1), y_train) # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) fig, ax = plt.subplots(1,2,figsize=(15,5)) ax[0].plot(X_seq, model.predict(X_seq), c='grey', ls='--') ax[0].plot(x, y, ls='', marker='o', alpha=0.2) ax[0].plot(X_train, y_train, ls='', marker='.') ax[0].set_ylim(min(y), max(y)) ax[0].set_title(\"High Bias Model\") ax[1].plot(X_seq, np.polyval(coefs, X_seq), c='grey', ls='--') ax[1].plot(x, y, ls='', marker='o', alpha=0.2) ax[1].plot(X_train, y_train, ls='', marker='.') ax[1].set_ylim(min(y), max(y)) ax[1].set_title(\"High Variance Model\") Text(0.5, 1.0, 'High Variance Model') This time, our high variance model really gets it ! And this is because the data we trained on actually is a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the bias variance tradeoff \ud83c\udfcb\ufe0f Exercise 1: Quantitatively Define Performance Up until now, we've explored this idea of bias variance tradeoff from a qualitative standpoint. As an exercise, continue with this idea, this time calculating the mean squared error (MSE) and R-square between the model and UNSEEN (non-training data) population data. error metrics * r2_score * mean_squared_error Do this for a 9th order polynomial and repeat for population data with low, med, and high degrees of error and small and large training fractions. Complete the chart below. error training fraction MSE R2 # Code Cell for Exercise 1 from sklearn.preprocessing import PolynomialFeatures random.seed(42) # function to generate data def func(x, err): return np.log(x) + err * random.randint(-1,1) * random.random() x = np.arange(20,100) ################################################################################ ########## CHANGE ERR TO CHANGE THE AMOUNT OF NOISE IN YOUR DATA ############### ################################################################################ err = <YOUR ERR> # change the error (.001 - 1) y_actual = [func(t, err) for t in x] ################################################################################ ### SAMPLE THE DATA FOR TRAINING ################################################################################ x_train, x_test, y_train, y_test = train_test_split(x, y_actual, train_size=<YOUR NUMBER>, # change the training size random_state=42) # solving our training data with a 9-degree polynomial coefs = np.polyfit(x_train, y_train, 9) # generate y data with 9-degree polynomial model and X_seq y_pred = np.polyval(coefs, x_test) ################################################################################ ### CALCULATE MSE AND R2 ################################################################################ mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) # look at results print(\"mean square error: {:.2f}\".format(mse)) print(\"r2: {:.2f}\".format(r2)) mean square error: 5.20e-01 r2: -0.13 3.1.4 Learning Curves back to top To move from qualitative to quantitative understanding of bias-variance tradeoff we need to introduce some metric for model performance. A good one to use here is R-square, a measure of the degree to which predictions match actual values. We can import a tool from sklearn to calculate this for us. from sklearn.metrics import r2_score 3.1.4.1 Considering Model Complexity back to top In a learning curve, we will typically plot the training and testing scores together, to give a sense of when we have either too much bias or too much variance in our model. I'm going to go ahead and recreate the original data distribution we introduced in 1.1.3 # we can throttle the error rate err = .5 random.seed(42) # our data has a KNOWN underlying functional form (log(x)) def func(x, err): return np.log(x) + err * random.randint(-1,1) * random.random() x = np.arange(20,100) y = [func(t, err) for t in x] plt.plot(x,y, ls='', marker='.') [<matplotlib.lines.Line2D at 0x7f3ea0305f90>] Now let's itteratively introduce more complexity into our model random.seed(42) fig, ax = plt.subplots(1,2,figsize=(10,5)) X_train = random.sample(list(x), 10) indices = [list(x).index(i) for i in X_train] y_train = [y[i] for i in indices] ax[0].plot(X_train, y_train, ls='', marker='.', color='black') for complexity in range(1,10): # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, complexity) # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) score = r2_score(np.polyval(coefs, X_train), y_train) ax[0].plot(X_seq, np.polyval(coefs, X_seq), alpha=0.5, ls='--') ax[0].set_ylim(min(y_train), max(y_train)) ax[0].set_title(\"Predictions with Increasing Model Complexity\") ax[1].plot(complexity, score, ls='', marker='.', label='{}-poly, {:.2f}-score'.format(complexity, score)) ax[1].set_title(\"Scores with Increasing Model Complexity\") ax[1].legend() <matplotlib.legend.Legend at 0x7f3ea0577d90> As we see from both plots, the score on the training data increases with added model complexity. Giving us the expected perfect fit when the order is the same as the number of data points! This is part I of our learning curve. Part II consists of plotting the training data score with the testing data score. Something else I'm going to do, is define the training portion of the data as a fraction of the overall population size. To keep the comparisons the same as up until now, I will keep this training fraction low at .2 random.seed(42) # defining my training fraction training_frac = .2 # create test and training data X_train = random.sample(list(x), int(int(len(x))*training_frac)) train_indices = [list(x).index(i) for i in X_train] y_train = [y[i] for i in train_indices] test_indices = [i for i in range(len(x)) if i not in train_indices] X_test = [x[i] for i in test_indices] y_test = [y[i] for i in test_indices] # initialize the plot and display the data fig, ax = plt.subplots(1,2,figsize=(10,5)) ax[0].plot(X_train, y_train, ls='', marker='.', color='black') ax[0].plot(X_test, y_test, ls='', marker='.', color='grey', alpha=0.5) for complexity in range(1,10): # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, complexity) # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) score1 = r2_score(np.polyval(coefs, X_train), y_train) score2 = r2_score(np.polyval(coefs, X_test), y_test) ax[0].plot(X_seq, np.polyval(coefs, X_seq), alpha=0.5, ls='--', label='{}-poly, {:.2f}-score'.format(complexity, score2)) ax[0].set_ylim(min(y_train), max(y_train)) ax[0].set_title(\"Predictions with Increasing Model Complexity\") ax[1].plot(complexity, score1, ls='', marker='.', color='blue', label='{}-poly, {:.2f}-score'.format(complexity, score1)) ax[1].plot(complexity, score2, ls='', marker='o', color='red', label='{}-poly, {:.2f}-score'.format(complexity, score2)) ax[1].set_title(\"Scores with Increasing Model Complexity\") ax[1].legend(['Train $R^2$', 'Test $R^2$']) ax[0].legend() <matplotlib.legend.Legend at 0x7f3ea0278850> As we can see, The 2nd order polynomial achieves the greatest best test set data \\(R^2\\), while the highest order polynomial achieves the best training set data \\(R^2\\). This learning curve is explanative of what we see generally, namely a divergence after some degree of complexity between training and test set performances. In this case, we would resolve to choose the 2nd order polynomial as the best model for our data. img src 3.1.4.2 Considering Training Set Size back to top The last piece of the puzzle we require, to fully cover learning curves, is the effect of training data size on the model. This is why I introduced the 'fraction of training data' parameter earlier. Let's explore. random.seed(42) # initialize the plot and display the data fig, ax = plt.subplots(1,1,figsize=(10,5)) for training_frac in np.linspace(0.1,.9,50): # create test and training data X_train = random.sample(list(x), int(int(len(x))*training_frac)) indices = [list(x).index(i) for i in X_train] y_train = [y[i] for i in indices] test_indices = [i for i in range(len(x)) if i not in indices] X_test = [x[i] for i in test_indices] y_test = [y[i] for i in test_indices] # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, 9) score1 = r2_score(np.polyval(coefs, X_train), y_train) score2 = r2_score(np.polyval(coefs, X_test), y_test) ax.plot(training_frac, score1, ls='', marker='.', color='blue', label='{}-poly, {:.2f}-score'.format(training_frac, score1)) ax.plot(training_frac, score2, ls='', marker='o', color='red', label='{}-poly, {:.2f}-score'.format(training_frac, score2)) ax.set_title(\"9th-order Polynomial Score with Increasing Training Set Size\") ax.legend(['Train','Test']) ax.set_xlabel('Training Fraction') ax.set_ylabel('$R^2$') /usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: RankWarning: Polyfit may be poorly conditioned exec(code_obj, self.user_global_ns, self.user_ns) /usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: RankWarning: Polyfit may be poorly conditioned exec(code_obj, self.user_global_ns, self.user_ns) Text(0, 0.5, '$R^2$') What we see here is a trend that happens generally, as our amount of training data increases, our models handle more complexity. This is illustrated below. img src \ud83c\udfcb\ufe0f Exercise 2: Visualization Starting with the code below, make a side-by-side plot of a 3rd degree polynomial and a 12th degree polynomial. On the x axis slowly increase the training set size, on the y axis plot the scores for the training and test sets. # Code Cell for Exercise 2 random.seed(42) # create the figure and axes fig, ax = plt.subplots(1,1,figsize=(10,5)) for training_frac in np.linspace(0.1,.9,50): # create test and training data x_train, x_test, y_train, y_test = train_test_split(x, y_actual, train_size=training_frac, random_state=42) # solving our training data with a n-degree polynomial coefs = np.polyfit(x_train, y_train, 9) # recording the scores for the training and test sets score1 = r2_score(np.polyval(coefs, x_train), y_train) score2 = r2_score(np.polyval(coefs, x_test), y_test) ax.plot(training_frac, score1, ls='', marker='.', color='blue', label='{}-poly, {:.2f}-score'.format(training_frac, score1)) ax.plot(training_frac, score2, ls='', marker='o', color='red', label='{}-poly, {:.2f}-score'.format(training_frac, score2)) ax.set_title(\"9th-order Polynomial Score with Increasing Training Set Size\") ax.legend(['Train','Test']) ax.set_xlabel('Training Fraction') ax.set_ylabel('$R^2$') Text(0, 0.5, '$R^2$') As a visualization exercise, how would you attempt to combine the ideas of model performance with increasing training set size and increasing model complexity? Could you create this visualization with something other than a polynomial model? 3.2 Model Validation in Practice back to top We will now turn our attention to practical implementation. In practice, there are a wide number of variables (called hyperparameters) to consider when choosing a model. Scikit learn has a useful method called Grid Search that will iterate through every possible combination of a range of hyperparameter settings you provide as input. Before we get started with grid search, we'll need to switch over from our numpy polynomial fit method to one in sklearn. Here, the caveat is our actual model will solve for the coefficients infront of the polynomials. We will engineer the polynomial features ourselves. This is an example of feature engineering which we will revisit in depth in a later session. from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression(degree=2, **kwargs): return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs)) 3.2.1 Grid Search back to top from sklearn.model_selection import GridSearchCV param_grid = {'polynomialfeatures__degree': np.arange(10), 'linearregression__fit_intercept': [True, False], 'linearregression__normalize': [True, False]} grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7) # create test and training data random.seed(42) X_train = random.sample(list(x), int(int(len(x))*.8)) indices = [list(x).index(i) for i in X_train] y_train = [y[i] for i in indices] test_indices = [i for i in range(len(x)) if i not in indices] X_test = [x[i] for i in test_indices] y_test = [y[i] for i in test_indices] grid.fit(np.array(X_train).reshape(-1,1), y_train) GridSearchCV(cv=7, error_score=nan, estimator=Pipeline(memory=None, steps=[('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False, order='C')), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))], verbose=False), iid='deprecated', n_jobs=None, param_grid={'linearregression__fit_intercept': [True, False], 'linearregression__normalize': [True, False], 'polynomialfeatures__degree': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=0) grid.best_params_ {'linearregression__fit_intercept': True, 'linearregression__normalize': False, 'polynomialfeatures__degree': 3} to grab the best model from the CV/search outcome. we use grid.best_estimator model = grid.best_estimator_ # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) fig, ax = plt.subplots(1,1,figsize=(15,5)) ax.plot(X_seq, model.predict(X_seq), c='grey', ls='--') ax.plot(x, y, ls='', marker='*', alpha=0.6) ax.plot(X_train, y_train, ls='', marker='.') ax.set_ylim(min(y), max(y)) ax.set_title(\"Best Grid Search CV Model\") Text(0.5, 1.0, 'Best Grid Search CV Model') References back to top Model Validation cross_val_score leave-one-out","title":"Model Selection and Validation"},{"location":"S3_Model_Selection_and_Validation/#data-science-foundations-session-3-model-selection-and-validation","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com At the end of session 1, we saw the basic recipe for creating a supervised machine learning model: Environment setup and importing data Rudimentary exploratory data analysis Feature engineering Choosing and training a model: choose model choose hyperparameters fit using (training) data predict using (validation) data In session 1, I chose our model and hyperparameters preemptively. How did I do that? In the real world, you won't necessarily have the best intution about how to make these choices. In today's session, we will algorithmize the way we approach choosing and training a model Note: I will import libraries at the beginning of this notebook, as is good practice, but will reimport them as they are used to remind ourselves where each method came from!","title":"Data Science Foundations, Session 3: Model Selection and Validation"},{"location":"S3_Model_Selection_and_Validation/#30-preparing-environment-and-importing-data","text":"back to top","title":"3.0 Preparing Environment and Importing Data"},{"location":"S3_Model_Selection_and_Validation/#301-import-packages","text":"back to top # Pandas library for the pandas dataframes import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import seaborn as sns import random import scipy.stats as stats from patsy import dmatrices from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import LabelEncoder, StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression, LinearRegression from sklearn import metrics from sklearn.metrics import r2_score, mean_squared_error from sklearn.datasets import load_iris","title":"3.0.1 Import Packages"},{"location":"S3_Model_Selection_and_Validation/#302-load-dataset","text":"back to top In course 1 we cursorily discussed why we may need strategies for validating our model. Here we'll discuss it more in depth. I'm going to take a simple example. In the following, I have a dataset that contains some data about flowers. It's a very famous dataset used in the ML world iris = load_iris() X = iris.data y = iris.target print(X.shape) print(y.shape) (150, 4) (150,) let's go ahead and load our wine dataset as well... wine = pd.read_csv(\"https://raw.githubusercontent.com/wesleybeckner/\"\\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\") wine.dropna(inplace=True) wine['quality_label'] = wine['quality'].apply(lambda x: 'low' if x <=5 else 'med' if x <= 7 else 'high') class_tp = {'red': 0, 'white': 1} y_tp = wine['type'].map(class_tp) wine['type_encoding'] = y_tp class_ql = {'low':0, 'med': 1, 'high': 2} y_ql = wine['quality_label'].map(class_ql) wine['quality_encoding'] = y_ql wine.columns = wine.columns.str.replace(' ', '_')","title":"3.0.2 Load Dataset"},{"location":"S3_Model_Selection_and_Validation/#31-model-validation","text":"back to top doing it the wrong way While we're here, I'm going to introduce a VERY SIMPLE supervised learning method called K-Nearest Neighbors.","title":"3.1 Model Validation"},{"location":"S3_Model_Selection_and_Validation/#310-k-nearest-neighbors","text":"back to top K-Nearest Neighbors is perhaps the simplest algorithm of them all. It is essentially a lookup table: We select the hyperparameter K, and when assigning a new value a data label, assign it according to, the majority label in the vicinity of the new datapoint. The vicinity being determined by K, the number of nearest neighbors we are going to assess. knn = KNeighborsRegressor(n_neighbors=1) knn.fit(X,y) KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=1, p=2, weights='uniform') knn.score(X,y) 1.0 Wow! we achieved a model with a perfect score! But is this really how we would expect the model to perform on data it had never seen before? Probably not. How do we actually check the performance of our model?","title":"3.1.0 K-Nearest Neighbors"},{"location":"S3_Model_Selection_and_Validation/#311-holdout-sets","text":"back to top The way we account for unseen data, in practice, is to leave a portion of the dataset out for testing. This way, we can estimate how our model will perform on entirely new data it may come across in application. X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_state=42) knn = KNeighborsRegressor(n_neighbors=1) knn.fit(X_train,y_train) print(knn.score(X_test, y_test)) 0.9753593429158111 We see that we get a more reasonable value for our performance!","title":"3.1.1 Holdout Sets"},{"location":"S3_Model_Selection_and_Validation/#312-data-leakage-and-cross-validation","text":"back to top An even more rigorous method to leaving out a single test set, is to perform cross validation. Imagine a situation where we are trying to estimate the best value of K in our KNN algorithm. If we continually train our model with new values of K on our training set, and test with our testing set, \"knowledge\" of our test set values with leak into our model, as we choose the best value for K based on how it performs on our test set (even though we did not train on this test set). We call this phenomenon data leakage . CV or Cross Validation overcomes this by only evaluating our parameters with our training set. [image src](https://scikit-learn.org/stable/modules/cross_validation.html) In this scheme, we don't evaluate our model on the test set until the very end. Rather, we estimate our hyperparameter performances by slicing the training set into cross folds [image src](https://scikit-learn.org/stable/modules/cross_validation.html) from sklearn.model_selection import cross_val_score scores = cross_val_score(knn, X_train, y_train, cv=5) scores array([0.91666667, 0.81725888, 0.85714286, 1. , 0.91 ]) print(\"%0.2f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std())) 0.90 accuracy with a standard deviation of 0.062 More information on the cross_val_score method in sklearn can be found here An additional topic on cross validation is the extreme leave-one-out validation, you can read more about that here","title":"3.1.2 Data Leakage and Cross-Validation"},{"location":"S3_Model_Selection_and_Validation/#313-bias-variance-tradeoff","text":"back to top This next concept will be most easily understood (imo) if we go ahead an make up some data ourselves, I'm going to do that now. # we can throttle the error rate err = .5 random.seed(42) # our data has a KNOWN underlying functional form (log(x)) def func(x, err): return np.log(x) + err * random.randint(-1,1) * random.random() x = np.arange(20,100) y = [func(t, err) for t in x] plt.plot(x,y, ls='', marker='.') plt.xlabel('X') plt.ylabel('Y') Text(0, 0.5, 'Y') Let's fit to just a portion of this data random.seed(42) X_train = random.sample(list(x), 10) indices = [list(x).index(i) for i in X_train] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [y[i] for i in indices] plt.plot(X_train,y_train, ls='', marker='.') [<matplotlib.lines.Line2D at 0x7f3ea2189f10>] Now let's take two extreme scenarios, fitting a linear line and a high order polynomial, to these datapoints. Keeping in mind the larger dataset, as well as the error we introduced in our data generating function, this will really illustrate our point! # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, 9) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression() model.fit(np.array(X_train).reshape(-1,1), y_train) # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) fig, ax = plt.subplots(1,2,figsize=(15,5)) ax[0].plot(X_seq, model.predict(X_seq), c='grey', ls='--') ax[0].plot(X_train, y_train, ls='', marker='.') ax[0].set_ylim(min(y_train), max(y_train)) ax[0].set_title(\"High Bias Model\") ax[1].plot(X_seq, np.polyval(coefs, X_seq), c='grey', ls='--') ax[1].plot(X_train, y_train, ls='', marker='.') ax[1].set_ylim(min(y_train), max(y_train)) ax[1].set_title(\"High Variance Model\") Text(0.5, 1.0, 'High Variance Model') We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has high bias because we are forcing the functional form without much consideration to the underlying data \u2014 we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has low variance with respect to the underlying data. On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has low bias because we don't introduce many constraints on the final form of the model. it is high variance because depending on the underlying training data, the final outcome of the model can change quite drastically! In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further: what happens when we retrain these models on different samples of the data population and let's use this to better understand what we mean by bias and variance what happens when we tie this back in with the error we introduced to the data generator? and let's use this to better understand irreducible error random.seed(42) fig, ax = plt.subplots(1,2,figsize=(15,5)) for samples in range(5): X_train = random.sample(list(x), 10) indices = [list(x).index(i) for i in X_train] y_train = [y[i] for i in indices] # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, 9) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression() model.fit(np.array(X_train).reshape(-1,1), y_train) # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) ax[0].plot(X_seq, model.predict(X_seq), alpha=0.5, ls='--') ax[0].plot(X_train, y_train, ls='', marker='.') ax[0].set_ylim(min(y_train), max(y_train)) ax[0].set_title(\"High Bias Model\") ax[1].plot(X_seq, np.polyval(coefs, X_seq), alpha=0.5, ls='--') ax[1].plot(X_train, y_train, ls='', marker='.') ax[1].set_ylim(min(y_train), max(y_train)) ax[1].set_title(\"High Variance Model\") As we can see, depending on what data we train our model on, the high bias model changes relatively slightly, while the high variance model changes a whole awful lot! The high variance model is prone to something we call overfitting . It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the population # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, 9) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression() model.fit(np.array(X_train).reshape(-1,1), y_train) # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) fig, ax = plt.subplots(1,2,figsize=(15,5)) ax[0].plot(X_seq, model.predict(X_seq), c='grey', ls='--') ax[0].plot(x, y, ls='', marker='*', alpha=0.6) ax[0].plot(X_train, y_train, ls='', marker='.') ax[0].set_ylim(min(y), max(y)) ax[0].set_title(\"High Bias Model\") ax[1].plot(X_seq, np.polyval(coefs, X_seq), c='grey', ls='--') ax[1].plot(x, y, ls='', marker='*', alpha=0.6) ax[1].plot(X_train, y_train, ls='', marker='.') ax[1].set_ylim(min(y), max(y)) ax[1].set_title(\"High Variance Model\") Text(0.5, 1.0, 'High Variance Model') In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance models are particuarly prone to the phenomenon of over fitting and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this. x = np.arange(20,100) y = [func(t, err=0) for t in x] plt.plot(x,y, ls='', marker='.') [<matplotlib.lines.Line2D at 0x7f3ea05cbc90>] random.seed(42) X_train = random.sample(list(x), 10) indices = [list(x).index(i) for i in X_train] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [y[i] for i in indices] # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, 9) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression() model.fit(np.array(X_train).reshape(-1,1), y_train) # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) fig, ax = plt.subplots(1,2,figsize=(15,5)) ax[0].plot(X_seq, model.predict(X_seq), c='grey', ls='--') ax[0].plot(x, y, ls='', marker='o', alpha=0.2) ax[0].plot(X_train, y_train, ls='', marker='.') ax[0].set_ylim(min(y), max(y)) ax[0].set_title(\"High Bias Model\") ax[1].plot(X_seq, np.polyval(coefs, X_seq), c='grey', ls='--') ax[1].plot(x, y, ls='', marker='o', alpha=0.2) ax[1].plot(X_train, y_train, ls='', marker='.') ax[1].set_ylim(min(y), max(y)) ax[1].set_title(\"High Variance Model\") Text(0.5, 1.0, 'High Variance Model') This time, our high variance model really gets it ! And this is because the data we trained on actually is a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the bias variance tradeoff","title":"3.1.3 Bias-Variance Tradeoff"},{"location":"S3_Model_Selection_and_Validation/#exercise-1-quantitatively-define-performance","text":"Up until now, we've explored this idea of bias variance tradeoff from a qualitative standpoint. As an exercise, continue with this idea, this time calculating the mean squared error (MSE) and R-square between the model and UNSEEN (non-training data) population data. error metrics * r2_score * mean_squared_error Do this for a 9th order polynomial and repeat for population data with low, med, and high degrees of error and small and large training fractions. Complete the chart below. error training fraction MSE R2 # Code Cell for Exercise 1 from sklearn.preprocessing import PolynomialFeatures random.seed(42) # function to generate data def func(x, err): return np.log(x) + err * random.randint(-1,1) * random.random() x = np.arange(20,100) ################################################################################ ########## CHANGE ERR TO CHANGE THE AMOUNT OF NOISE IN YOUR DATA ############### ################################################################################ err = <YOUR ERR> # change the error (.001 - 1) y_actual = [func(t, err) for t in x] ################################################################################ ### SAMPLE THE DATA FOR TRAINING ################################################################################ x_train, x_test, y_train, y_test = train_test_split(x, y_actual, train_size=<YOUR NUMBER>, # change the training size random_state=42) # solving our training data with a 9-degree polynomial coefs = np.polyfit(x_train, y_train, 9) # generate y data with 9-degree polynomial model and X_seq y_pred = np.polyval(coefs, x_test) ################################################################################ ### CALCULATE MSE AND R2 ################################################################################ mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) # look at results print(\"mean square error: {:.2f}\".format(mse)) print(\"r2: {:.2f}\".format(r2)) mean square error: 5.20e-01 r2: -0.13","title":"\ud83c\udfcb\ufe0f Exercise 1: Quantitatively Define Performance"},{"location":"S3_Model_Selection_and_Validation/#314-learning-curves","text":"back to top To move from qualitative to quantitative understanding of bias-variance tradeoff we need to introduce some metric for model performance. A good one to use here is R-square, a measure of the degree to which predictions match actual values. We can import a tool from sklearn to calculate this for us. from sklearn.metrics import r2_score","title":"3.1.4 Learning Curves"},{"location":"S3_Model_Selection_and_Validation/#3141-considering-model-complexity","text":"back to top In a learning curve, we will typically plot the training and testing scores together, to give a sense of when we have either too much bias or too much variance in our model. I'm going to go ahead and recreate the original data distribution we introduced in 1.1.3 # we can throttle the error rate err = .5 random.seed(42) # our data has a KNOWN underlying functional form (log(x)) def func(x, err): return np.log(x) + err * random.randint(-1,1) * random.random() x = np.arange(20,100) y = [func(t, err) for t in x] plt.plot(x,y, ls='', marker='.') [<matplotlib.lines.Line2D at 0x7f3ea0305f90>] Now let's itteratively introduce more complexity into our model random.seed(42) fig, ax = plt.subplots(1,2,figsize=(10,5)) X_train = random.sample(list(x), 10) indices = [list(x).index(i) for i in X_train] y_train = [y[i] for i in indices] ax[0].plot(X_train, y_train, ls='', marker='.', color='black') for complexity in range(1,10): # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, complexity) # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) score = r2_score(np.polyval(coefs, X_train), y_train) ax[0].plot(X_seq, np.polyval(coefs, X_seq), alpha=0.5, ls='--') ax[0].set_ylim(min(y_train), max(y_train)) ax[0].set_title(\"Predictions with Increasing Model Complexity\") ax[1].plot(complexity, score, ls='', marker='.', label='{}-poly, {:.2f}-score'.format(complexity, score)) ax[1].set_title(\"Scores with Increasing Model Complexity\") ax[1].legend() <matplotlib.legend.Legend at 0x7f3ea0577d90> As we see from both plots, the score on the training data increases with added model complexity. Giving us the expected perfect fit when the order is the same as the number of data points! This is part I of our learning curve. Part II consists of plotting the training data score with the testing data score. Something else I'm going to do, is define the training portion of the data as a fraction of the overall population size. To keep the comparisons the same as up until now, I will keep this training fraction low at .2 random.seed(42) # defining my training fraction training_frac = .2 # create test and training data X_train = random.sample(list(x), int(int(len(x))*training_frac)) train_indices = [list(x).index(i) for i in X_train] y_train = [y[i] for i in train_indices] test_indices = [i for i in range(len(x)) if i not in train_indices] X_test = [x[i] for i in test_indices] y_test = [y[i] for i in test_indices] # initialize the plot and display the data fig, ax = plt.subplots(1,2,figsize=(10,5)) ax[0].plot(X_train, y_train, ls='', marker='.', color='black') ax[0].plot(X_test, y_test, ls='', marker='.', color='grey', alpha=0.5) for complexity in range(1,10): # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, complexity) # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) score1 = r2_score(np.polyval(coefs, X_train), y_train) score2 = r2_score(np.polyval(coefs, X_test), y_test) ax[0].plot(X_seq, np.polyval(coefs, X_seq), alpha=0.5, ls='--', label='{}-poly, {:.2f}-score'.format(complexity, score2)) ax[0].set_ylim(min(y_train), max(y_train)) ax[0].set_title(\"Predictions with Increasing Model Complexity\") ax[1].plot(complexity, score1, ls='', marker='.', color='blue', label='{}-poly, {:.2f}-score'.format(complexity, score1)) ax[1].plot(complexity, score2, ls='', marker='o', color='red', label='{}-poly, {:.2f}-score'.format(complexity, score2)) ax[1].set_title(\"Scores with Increasing Model Complexity\") ax[1].legend(['Train $R^2$', 'Test $R^2$']) ax[0].legend() <matplotlib.legend.Legend at 0x7f3ea0278850> As we can see, The 2nd order polynomial achieves the greatest best test set data \\(R^2\\), while the highest order polynomial achieves the best training set data \\(R^2\\). This learning curve is explanative of what we see generally, namely a divergence after some degree of complexity between training and test set performances. In this case, we would resolve to choose the 2nd order polynomial as the best model for our data. img src","title":"3.1.4.1 Considering Model Complexity"},{"location":"S3_Model_Selection_and_Validation/#3142-considering-training-set-size","text":"back to top The last piece of the puzzle we require, to fully cover learning curves, is the effect of training data size on the model. This is why I introduced the 'fraction of training data' parameter earlier. Let's explore. random.seed(42) # initialize the plot and display the data fig, ax = plt.subplots(1,1,figsize=(10,5)) for training_frac in np.linspace(0.1,.9,50): # create test and training data X_train = random.sample(list(x), int(int(len(x))*training_frac)) indices = [list(x).index(i) for i in X_train] y_train = [y[i] for i in indices] test_indices = [i for i in range(len(x)) if i not in indices] X_test = [x[i] for i in test_indices] y_test = [y[i] for i in test_indices] # solving our training data with a n-degree polynomial coefs = np.polyfit(X_train, y_train, 9) score1 = r2_score(np.polyval(coefs, X_train), y_train) score2 = r2_score(np.polyval(coefs, X_test), y_test) ax.plot(training_frac, score1, ls='', marker='.', color='blue', label='{}-poly, {:.2f}-score'.format(training_frac, score1)) ax.plot(training_frac, score2, ls='', marker='o', color='red', label='{}-poly, {:.2f}-score'.format(training_frac, score2)) ax.set_title(\"9th-order Polynomial Score with Increasing Training Set Size\") ax.legend(['Train','Test']) ax.set_xlabel('Training Fraction') ax.set_ylabel('$R^2$') /usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: RankWarning: Polyfit may be poorly conditioned exec(code_obj, self.user_global_ns, self.user_ns) /usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: RankWarning: Polyfit may be poorly conditioned exec(code_obj, self.user_global_ns, self.user_ns) Text(0, 0.5, '$R^2$') What we see here is a trend that happens generally, as our amount of training data increases, our models handle more complexity. This is illustrated below. img src","title":"3.1.4.2 Considering Training Set Size"},{"location":"S3_Model_Selection_and_Validation/#exercise-2-visualization","text":"Starting with the code below, make a side-by-side plot of a 3rd degree polynomial and a 12th degree polynomial. On the x axis slowly increase the training set size, on the y axis plot the scores for the training and test sets. # Code Cell for Exercise 2 random.seed(42) # create the figure and axes fig, ax = plt.subplots(1,1,figsize=(10,5)) for training_frac in np.linspace(0.1,.9,50): # create test and training data x_train, x_test, y_train, y_test = train_test_split(x, y_actual, train_size=training_frac, random_state=42) # solving our training data with a n-degree polynomial coefs = np.polyfit(x_train, y_train, 9) # recording the scores for the training and test sets score1 = r2_score(np.polyval(coefs, x_train), y_train) score2 = r2_score(np.polyval(coefs, x_test), y_test) ax.plot(training_frac, score1, ls='', marker='.', color='blue', label='{}-poly, {:.2f}-score'.format(training_frac, score1)) ax.plot(training_frac, score2, ls='', marker='o', color='red', label='{}-poly, {:.2f}-score'.format(training_frac, score2)) ax.set_title(\"9th-order Polynomial Score with Increasing Training Set Size\") ax.legend(['Train','Test']) ax.set_xlabel('Training Fraction') ax.set_ylabel('$R^2$') Text(0, 0.5, '$R^2$') As a visualization exercise, how would you attempt to combine the ideas of model performance with increasing training set size and increasing model complexity? Could you create this visualization with something other than a polynomial model?","title":"\ud83c\udfcb\ufe0f Exercise 2: Visualization"},{"location":"S3_Model_Selection_and_Validation/#32-model-validation-in-practice","text":"back to top We will now turn our attention to practical implementation. In practice, there are a wide number of variables (called hyperparameters) to consider when choosing a model. Scikit learn has a useful method called Grid Search that will iterate through every possible combination of a range of hyperparameter settings you provide as input. Before we get started with grid search, we'll need to switch over from our numpy polynomial fit method to one in sklearn. Here, the caveat is our actual model will solve for the coefficients infront of the polynomials. We will engineer the polynomial features ourselves. This is an example of feature engineering which we will revisit in depth in a later session. from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression(degree=2, **kwargs): return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))","title":"3.2 Model Validation in Practice"},{"location":"S3_Model_Selection_and_Validation/#321-grid-search","text":"back to top from sklearn.model_selection import GridSearchCV param_grid = {'polynomialfeatures__degree': np.arange(10), 'linearregression__fit_intercept': [True, False], 'linearregression__normalize': [True, False]} grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7) # create test and training data random.seed(42) X_train = random.sample(list(x), int(int(len(x))*.8)) indices = [list(x).index(i) for i in X_train] y_train = [y[i] for i in indices] test_indices = [i for i in range(len(x)) if i not in indices] X_test = [x[i] for i in test_indices] y_test = [y[i] for i in test_indices] grid.fit(np.array(X_train).reshape(-1,1), y_train) GridSearchCV(cv=7, error_score=nan, estimator=Pipeline(memory=None, steps=[('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False, order='C')), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))], verbose=False), iid='deprecated', n_jobs=None, param_grid={'linearregression__fit_intercept': [True, False], 'linearregression__normalize': [True, False], 'polynomialfeatures__degree': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=0) grid.best_params_ {'linearregression__fit_intercept': True, 'linearregression__normalize': False, 'polynomialfeatures__degree': 3} to grab the best model from the CV/search outcome. we use grid.best_estimator model = grid.best_estimator_ # create some x data to plot our functions X_seq = np.linspace(min(X_train),max(X_train),300).reshape(-1,1) fig, ax = plt.subplots(1,1,figsize=(15,5)) ax.plot(X_seq, model.predict(X_seq), c='grey', ls='--') ax.plot(x, y, ls='', marker='*', alpha=0.6) ax.plot(X_train, y_train, ls='', marker='.') ax.set_ylim(min(y), max(y)) ax.set_title(\"Best Grid Search CV Model\") Text(0.5, 1.0, 'Best Grid Search CV Model')","title":"3.2.1 Grid Search"},{"location":"S3_Model_Selection_and_Validation/#references","text":"back to top","title":"References"},{"location":"S3_Model_Selection_and_Validation/#model-validation","text":"cross_val_score leave-one-out","title":"Model Validation"},{"location":"S4_Feature_Engineering/","text":"Data Science Foundations, Session 4: Feature Engineering Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we talked about model pipelines and conveniently began with a suitable set of input data. In the real world, this is hardly ever the case! What is constant is this: at the end of the day, our models need numbers. Not only this, but a suitable set of numbers. What does that mean? The answer to that question is the subject of our session today. 4.0 Preparing Environment and Importing Data back to top 4.0.1 Import Packages back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats as stats from scipy.stats import gamma from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor import seaborn as sns; sns.set() from sklearn.datasets import load_iris from sklearn.metrics import mean_squared_error, r2_score 4.0.2 Load Dataset back to top margin = pd.read_csv('https://raw.githubusercontent.com/wesleybeckner/'\\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv') orders = pd.read_csv('https://raw.githubusercontent.com/wesleybeckner/'\\ 'ds_for_engineers/main/data/truffle_margin/truffle_orders.csv') time_cols = [i for i in orders.columns if '/' in i] margin.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 dfcat = margin.columns[:-2] dfcat Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') orders.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer 1/2020 2/2020 3/2020 4/2020 5/2020 6/2020 7/2020 8/2020 9/2020 10/2020 11/2020 12/2020 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 4.1 Categorical Features back to top At the end of the day, our algorithms operate on numerical values. How do you get from a series of string values to numerical values? margin['Customer'].unique() array(['Slugworth', 'Perk-a-Cola', 'Fickelgruber', 'Zebrabar', \"Dandy's Candies\"], dtype=object) A naive way to do it would be to assign a number to every entry 'Slugworth' = 1 'Perk-a-Cola' = 2 'Dandy's Candies' = 3 but we would inadvertently end up with some weird mathematical relationships between these variables, e.g. Dandy's Candies - Perk-a-Cola = Slugworth (3 - 2 = 1). A work around for this is to think multi-dimensionally we express our categorical values as vectors in a hyperspace where they cannot be expressed in terms of one another, i.e. they are orthogonal 'Slugworth' = [1,0,0] 'Perk-a-Cola' = [0,1,0] 'Dandy's Candies' = [0,0,1] such a scheme, in machine learning vernacular, is termed one-hot encoding. 4.1.1 One-Hot Encoding back to top sklearn has a couple useful libraries for one-hot encoding. let's start with the OneHotEncoder class in its preprocessing library from sklearn.preprocessing import OneHotEncoder # create the encoder object enc = OneHotEncoder() # grab the columns we want to convert from strings X_cat = margin['Customer'].values.reshape(-1,1) # fit our encoder to this data enc.fit(X_cat) OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>, handle_unknown='error', sparse=True) After fitting our encoder, we can then use this object to create our training array. # as a reference here's our original data display(X_cat[:10]) print(X_cat.shape, end='\\n\\n') onehotlabels = enc.transform(X_cat).toarray() print(onehotlabels.shape, end='\\n\\n') # And here is our new data onehotlabels[:10] array([['Slugworth'], ['Slugworth'], ['Perk-a-Cola'], ['Fickelgruber'], ['Fickelgruber'], ['Fickelgruber'], ['Slugworth'], ['Zebrabar'], ['Slugworth'], ['Zebrabar']], dtype=object) (1668, 1) (1668, 5) array([[0., 0., 0., 1., 0.], [0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]]) We have our customer information one-hot encoded, we need to do this for all our variables and concatenate them with our regular numerical variables in our original dataframe. # create the encoder object enc = OneHotEncoder() # grab the columns we want to convert from strings X_cat = margin[dfcat].values # fit our encoder to this data enc.fit(X_cat) onehotlabels = enc.transform(X_cat).toarray() X_num = margin[\"KG\"] print(X_num.shape) X = np.concatenate((onehotlabels, X_num.values.reshape(-1,1)),axis=1) X.shape (1668,) (1668, 119) And now we grab our EBITDA (margin) data for prediction y = margin[\"EBITDA/KG\"] \ud83c\udfcb\ufe0f Exercise 1: Create a simple linear model Using the X and Y sets, use train_test_split and LinearRegression to make a baseline model based on what we've learned so far. Assess your model performance visually by plottying y_test vs y_test_pred # Cell for Exercise 1 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression [<matplotlib.lines.Line2D at 0x7fb3d1bb0890>] \ud83d\ude4b Question 1: How can we assess the relative feature importance of the features in our model? We could be tempted to inspect the coefficients ( linear.coef_ ) of our model to evaluate the relative feature importance, but in order to do this our features need to be scaled (so that the relative coefficient sizes are meaningful). What other issues might there be (think categorical vs continuous variables). 4.2 Derived Features back to top Can we recall an example of where we've seen this previously? That's right earlier on in our first session we derived some polynomial features to create our polynomial model using the linear regression class in sklearn. We actually see this a lot in engineering, where we will describe log relationships or some other transformation of the original variable. Actually let me see if I can find an example in my handy BSL... concentration profiles in continous stirred tank vs plug flow reactors. Notice the y-axis is log scale.Thank's Bird, Stewart, Lightfoot! Can we think of other examples where we would like to derive features from our input data? 4.2.1 Creating Polynomials back to top Let's revisit our example from the previous session, right before we introduced Grid Search in sklearn # from Model Selection and Validation, 1.2.1 from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression(degree=2, **kwargs): return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs)) in the above, we use sklearn's convenient tool, make_pipeline to join together the preprocessing tool PolynomialFeatures and the basic model LinearRegression . Let's take a look at what PolynomialFeatures does to some simple data x = np.arange(1,11) y = x**3 print(x) print(y) [ 1 2 3 4 5 6 7 8 9 10] [ 1 8 27 64 125 216 343 512 729 1000] features = PolynomialFeatures(degree=3) X2 = features.fit_transform(x.reshape(-1,1)) we see our new feature set contains our original features, plus new features up to the nth-degree polynomial we set when creating the features object from PolynomialFeatures print(X2) [[ 1. 1. 1. 1.] [ 1. 2. 4. 8.] [ 1. 3. 9. 27.] [ 1. 4. 16. 64.] [ 1. 5. 25. 125.] [ 1. 6. 36. 216.] [ 1. 7. 49. 343.] [ 1. 8. 64. 512.] [ 1. 9. 81. 729.] [ 1. 10. 100. 1000.]] model = LinearRegression().fit(X2, y) yhat = model.predict(X2) plt.scatter(x, y) plt.plot(x, yhat); 4.2.2 Dealing with Time Series back to top Often, we will be dealing with time series data, whether its data generated by machinery, reactors, or sales and customers. In the following we discuss some simple practices for dealing with time series data. Enrichment : 2.2.2.1 Fast Fourier Transform back to top Sometimes we'll want to create a more sophisticated transformation of our input data. As engineers, this can often have to do with some empirical knowledge we understand about our process. When working with equipment and machinery, we will often want to convert a signal from the time to frequency domain. Let's cover how we can do that with numpy! [img src](https://www.nti-audio.com/en/support/know-how/fast-fourier-transform-fft#:~:text=The%20%22Fast%20Fourier%20Transform%22%20(,frequency%20information%20about%20the%20signal.) What I've drawn here in the following is called a square-wave signal t = np.linspace(0,5,200) w = 5 h = 4 s = 4 * h / np.pi * (np.sin(w*t) + np.sin(3*w*t)/3 + np.sin(5*w*t)/5) # here is the call to numpy FFT F = np.fft.fft(s) freq = np.fft.fftfreq(t.shape[-1]) # amplitudes amps = [max(np.sin(w*t)), max(np.sin(w*t*3)/3), max(np.sin(w*t*5)/5)] fig, ax = plt.subplots(1,2,figsize=(10,5)) ax[0].plot(t,s) ax[0].plot(t,np.sin(w*t), ls='--') ax[0].plot(t,np.sin(w*t*3)/3, ls='--') ax[0].plot(t,np.sin(w*t*5)/5, ls='--') ax[0].set_title('Time Domain') # tells us about the amplitude of the component at the # corresponding frequency magnitude = np.sqrt(F.real**2 + F.imag**2) ax[1].plot(freq, magnitude) ax[1].set_xlim(0,.15) ax[1].set_title('Frequency Domain') Text(0.5, 1.0, 'Frequency Domain') We can construct a similar plot with 4 signals contributing to the square-wave: t = np.linspace(0,5,200) w = 5 h = 4 s = 4 * h / np.pi * (np.sin(w*t) + np.sin(3*w*t)/3 + np.sin(5*w*t)/5 + np.sin(10*w*t)/10) F = np.fft.fft(s) freq = np.fft.fftfreq(t.shape[-1]) fig, ax = plt.subplots(1,2,figsize=(10,5)) ax[0].plot(t,s) ax[0].plot(t,np.sin(w*t), ls='--') ax[0].plot(t,np.sin(w*t*3)/3, ls='--') ax[0].plot(t,np.sin(w*t*5)/5, ls='--') ax[0].plot(t,np.sin(10*w*t)/10, ls='--') ax[0].set_title('Time Domain') # tells us about the amplitude of the component at the # corresponding frequency magnitude = np.sqrt(F.real**2 + F.imag**2) ax[1].plot(freq, magnitude) ax[1].set_xlim(0) ax[1].set_title('Frequency Domain') Text(0.5, 1.0, 'Frequency Domain') 4.2.2.2 Rolling Windows back to top One powerful technique for dealing with time series data, is to create a rolling window of features based on the historical data. The proper window size can usually be determined by trial and error, or constraints around access to the data itself. In the above gif, we have a window size of 7. What that means is for whatever time step units we are in (that could be minutes, days, months, etc.) we will have 7 of them included in a single instance or observation. This instance or observation is then interpreted by our model and used to assess the target value, typically the quantity in the very next time step after the window (the green bar in the gif). \ud83c\udfcb\ufe0f Exercise 2: Optimize Rolling Window Size for Customer Forecasts For this exercise, you will use the process_data function below to help you optimize the window size for predicting the order quantity in any given month. create a model using a window size of 3 and predict the order quantity for the month immediately following the window create a model for window sizes 1-11 and report the \\(R^2\\) for each model def process_data(Xy, time_cols=12, window=3, remove_null=False): \"\"\" This function splits your time series data into the proper windows Parameters ---------- Xy: array The input data. If there are non-time series columns, assumes they are on the left and time columns are on the right. time_cols: int The number of time columns, default 12 window: int The time window size, default 3 Returns ------- X_: array The independent variables, includes time and non-time series columns with the new window y_: array The dependent variable, selected from the time columns at the end of the window labels: The time series labels, can be used in subsequent plot \"\"\" # separate the non-time series columns X_cat = Xy[:,:-time_cols] # select the columns to apply the sweeping window X = Xy[:,-time_cols:] X_ = [] y = [] for i in range(X.shape[1]-window): # after attaching the current window to the non-time series # columns, add it to a growing list X_.append(np.concatenate((X_cat, X[:, i:i+window]), axis=1)) # add the next time delta after the window to the list of y # values y.append(X[:, i+window]) # X_ is 3D: [number of replicates from sweeping window, # length of input data, # size of new feature with categories and time] # we want to reshape X_ so that the replicates due to the sweeping window is # a part of the same dimension as the instances of the input data X_ = np.array(X_).reshape(X.shape[0]*np.array(X_).shape[0],window+X_cat.shape[1]) y = np.array(y).reshape(X.shape[0]*np.array(y).shape[0],) if remove_null: # remove training data where the target is 0 (may be unfair advantage) X_ = X_[np.where(~np.isnan(y.astype(float)))[0]] y = y[np.where(~np.isnan(y.astype(float)))[0]] # create labels that show the previous month values used to train the model labels = [] for row in X_: labels.append(\"X: {}\".format(np.array2string(row[-window:].astype(float).round()))) return X_, y, labels # Code Cell for Exercise 2 # use data and the function process_data to create your X, y arrays # then use train_test_split to create train and test portions data = orders.values[:,6:] ### YOUR CODE HERE ### # USE y_test and y_pred for your actual and true test data # name your labels for the test set labels_test # change only window parameter in process_data() 1 0.762506752772391 2 0.8895992010134899 3 0.9413336982898548 4 0.7532142077720143 5 0.9675986952925033 6 0.9900851583059013 7 0.9994996102278398 8 1.0 9 1.0 10 1.0 11 1.0 #### RUN AFTER EXERCISE 2.2.2.2.1 #### fig = px.scatter(x=y_test, y=y_pred, labels={ \"y\": \"Prediction\", \"x\": \"Actual\" }) fig.update_layout( autosize=False, width=800, height=500, title='R2: {:.3f}'.format(r2_score(y_test, y_pred)) ) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"d46fe430-b481-4c6b-8c71-3cb9cfb98a5e\")) { Plotly.newPlot( 'd46fe430-b481-4c6b-8c71-3cb9cfb98a5e', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Actual=%{x}<br>Prediction=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [108.64285714285714, 2355.267295597484, 1432.2712418300653, 19569.69230769231, 1702.1929824561405, 58020.911949685535, 1968.8904494382025, 16569.26100628931, 20.098039215686274, 13504.20634920635, 191.11111111111111, 8608.709677419354, 289.8888888888889, 3665.8097686375318, 440.859375, 712.4603174603176, 16918.132716049382, 8104.40251572327, 38128.80258899677, 36.37096774193548, 21438.1875, 19129.33333333333, 1529.7752808988766, 12274.009146341465, 14.663461538461537, 538.8571428571429, 4057.832278481013, 1630.1966292134832], \"xaxis\": \"x\", \"y\": [108.6428571428318, 2355.26729559746, 1432.2712418300403, 19569.692307692298, 1702.1929824561164, 58020.91194968554, 1968.8904494381782, 16569.261006289296, 20.098039215660815, 13504.206349206332, 191.11111111108582, 8608.709677419334, 289.88888888886396, 3665.809768637509, 440.8593749999747, 712.4603174602926, 16918.132716049364, 8104.402515723249, 38128.802588996754, 36.370967741910036, 21438.187499999993, 19129.33333333332, 1529.7752808988523, 12274.009146341446, 14.663461538436076, 538.8571428571178, 4057.8322784809898, 1630.196629213459], \"yaxis\": \"y\"}], {\"autosize\": false, \"height\": 500, \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"R2: 1.000\"}, \"width\": 800, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Actual\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Prediction\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('d46fe430-b481-4c6b-8c71-3cb9cfb98a5e'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; 4.2.3 Image Preprocessing back to top Image preprocessing is beyond the scope of this session. We will cover this in C4. For now, know that there is a wealth of considerations for how to handle images, and they all fit within the realm of feature engineering. 4.3 Transformed Features back to top Transformed features, are features that we would like to augment based on their relationship within their own distribution or to other (allegedly) independent data within our training set. e.g. we're not deriving new features based on some empirical knowledge of the data, rather we are changing them due to statistical properties that we can assess based on the data itself. 4.3.1 Skewness back to top Skewed data can lead to imbalances in our model prediction. Why? Skewed values in the distribution will bias the mean. When assigning weights to this input feature, therefore, the model will give preferential treatment to these values. To demonstrate, I'm going to use scipy to create some skewed data. from scipy.stats import skewnorm a = 10 x = np.linspace(skewnorm.ppf(0.01, a), skewnorm.ppf(0.99, a), 100) plt.plot(x, skewnorm.pdf(x, a), 'r-', lw=5, alpha=0.6, label='skewnorm pdf') [<matplotlib.lines.Line2D at 0x7fb3cb441910>] We can now generate a random population based on this distribution r = skewnorm.rvs(a, size=1000) plt.hist(r) (array([143., 290., 244., 160., 96., 43., 13., 7., 3., 1.]), array([-0.24457186, 0.18369502, 0.61196191, 1.04022879, 1.46849568, 1.89676256, 2.32502945, 2.75329633, 3.18156322, 3.60983011, 4.03809699]), <a list of 10 Patch objects>) Unskewed data will return something close to 0 from calling df.skew() . When dealing with actual data, we can use df.skew() to determine whether we should transform our data. x = pd.DataFrame(r, columns=['Skewed Data']) x['Skewed Data'].skew() 0.9914234810526167 There are a handful of ways to deal with skewed data: log transform square root transform Box-Cox transform Let's try the first two print('square root transformed skew: {:.4f}'.format(np.sqrt(x['Skewed Data']).skew())) print('log transformed skew: {:.4f}'.format(np.log(x['Skewed Data']).skew())) fig, ax = plt.subplots(1, 1, figsize=(10,10)) ax.hist(x['Skewed Data'], alpha=0.5, label='original: {:.2f}'. format((x['Skewed Data']).skew())) ax.hist(np.sqrt(x['Skewed Data']), alpha=0.5, label='sqrt: {:.2f}'. format(np.sqrt(x['Skewed Data']).skew())) ax.hist(np.log(x['Skewed Data']), alpha=0.5, label='log: {:.2f}'. format(np.log(x['Skewed Data']).skew())) ax.legend() square root transformed skew: 0.1112 log transformed skew: -1.9834 /usr/local/lib/python3.7/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in sqrt /usr/local/lib/python3.7/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log <matplotlib.legend.Legend at 0x7fb3cb3525d0> We see we didn't get much traction with the log transform, and the log transform will not be able to handle 0 values, and so we will sometimes have to code exceptions for those. Boxplot is often a good route to go, but it has the added restriction that the data has to all be above 0. Let's create a new distribution with this added restriction a = 6 r = skewnorm.rvs(a, size=1000) r = [i for i in r if i > 0] plt.hist(r) (array([186., 194., 186., 134., 107., 70., 41., 18., 7., 6.]), array([4.86688792e-04, 2.93460209e-01, 5.86433729e-01, 8.79407250e-01, 1.17238077e+00, 1.46535429e+00, 1.75832781e+00, 2.05130133e+00, 2.34427485e+00, 2.63724837e+00, 2.93022189e+00]), <a list of 10 Patch objects>) from scipy import stats x = pd.DataFrame(r, columns=['Skewed Data']) fig, ax = plt.subplots(1, 1, figsize=(10,10)) ax.hist(x['Skewed Data'], alpha=0.5, label='original: {:.2f}'. format((x['Skewed Data']).skew())) ax.hist(np.sqrt(x['Skewed Data']), alpha=0.5, label='sqrt: {:.2f}'. format(np.sqrt(x['Skewed Data']).skew())) ax.hist(np.log(x['Skewed Data']), alpha=0.5, label='log: {:.2f}'. format(np.log(x['Skewed Data']).skew())) ax.hist(stats.boxcox(x['Skewed Data'])[0], alpha=0.5, label='box-cox: {:.2f}'. format(pd.DataFrame(stats.boxcox(x['Skewed Data'])[0])[0].skew())) ax.legend() <matplotlib.legend.Legend at 0x7fb3cb35e310> \ud83c\udfcb\ufe0f Exercise 3: Transform data from a gamma distribution Repeat section 2.3.1, this time synthesizing a gamma distribution and transforming it. Which transformation best reduces the skew? Do this for a dataset that does not contain values at or below 0. # code cell for exercise 3 from scipy.stats import gamma <matplotlib.legend.Legend at 0x7fb3cb31fe90> 4.3.2 Colinearity back to top Colinearity can also affect the performance of your machine learning model. In particular, if features are colinear, it can be easy for your model to overfit to your training dataset. This is often mitigated by regularization. If you're curious you can read more about it on this discussion from StackExchange . We will still explore it explicitly here by calculating the Variance Inflation Factor (VIF) on some hypothetical data. VIF = \\frac{1}{1-R^2} 4.3.2.1 Detecting Colinearity back to top from statsmodels.stats.outliers_influence import variance_inflation_factor Step 1: Make some data # we can throttle the error rate random.seed(42) # x2 will be sqrt of x1 plus some error def func(x, err): return x**.5 + (err * random.randint(-1,1) * random.random() * x) x0 = range(100) x1 = [func(i, .05) for i in x0] x2 = [func(i, 1) for i in x0] x3 = [random.randint(0,100) for i in x0] # take a look fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.plot(x0, x1, label='x1') ax.plot(x0, x2, label='x2') ax.plot(x0, x3, label='x3') ax.legend() <matplotlib.legend.Legend at 0x7fb3cbab8c90> To calculate the colinearities I'm going to aggregate these x's into a dataframe: colin = pd.DataFrame([x0,x1,x2,x3]).T colin.columns = ['x0','x1','x2','x3'] colin.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 0 0.0 0.000000 0.000000 29.0 1 1.0 1.013751 0.721523 28.0 2 2.0 1.400260 1.414214 3.0 3 3.0 1.630546 -0.438007 84.0 4 4.0 2.017388 4.304847 24.0 Step 2: Calculate VIF factors # calculate VIF factors vif = pd.DataFrame() vif[\"VIF Factor\"] = [variance_inflation_factor(colin.values, i) for i in range(colin.shape[1])] vif[\"features\"] = colin.columns Step 3: Inspect VIF factors # inspect VIF factors display(vif) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 12.555415 x0 1 15.823872 x1 2 1.030609 x2 3 3.559468 x3 4.3.2.2 Fixing Colinearity back to top It is good to aknowledge where colinearity exists as this will influence the interpretability of your model. In most cases, however, it won't have a heavy influence on the performance of your model. A simple method of dealing with colinearity, is to remove the highest VIF features from your model, iteratively, assessing the performance and determining whether to keep the variable or not. Another method is to create some linear combination of the correlated variables. This is encapsulated in the section on dimensionality reduction. 4.3.3 Normalization back to top Many learning algorithms require zero mean and unit variance to behave optimally. Sklearn preprocessing library contains a very usefull class, StandardScaler for handling this automatically for us. from sklearn.preprocessing import StandardScaler scaler = StandardScaler() normed = scaler.fit_transform(colin) colin[['x0','x1','x2','x3']].plot(kind='kde') <matplotlib.axes._subplots.AxesSubplot at 0x7fb3cb102750> pd.DataFrame(normed, columns = [['x0','x1','x2','x3']]).plot(kind='kde') <matplotlib.axes._subplots.AxesSubplot at 0x7fb3cb102990> \ud83c\udfcb\ufe0f Exercise 4: Normalization affect on VIF In the above, we saw how to scale and center variables. How does this affect VIF? Calculate the VIF for the raw and scaled-centered data # Code Cell for Exercise 4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 3.286048 x0 1 3.296881 x1 2 1.015805 x2 3 1.035537 x3 4.3.4 Dimensionality Reduction back to top Dimensionality reduction is an awesome way to do feature engineering. It is very commonly used. Because it is also an unsupervised machine learning technique, we will visit this topic in that section. 4.4 Missing Data back to top We will often have missing data in our datasets. How do we deal with this? Let's start by making some data with missing data. We'll use a numpy nan datatype to do this from numpy import nan X = np.array([[ nan, 0, 3 ], [ 3, 7, 9 ], [ 3, 5, 2 ], [ 4, nan, 6 ], [ 8, 8, 1 ]]) y = np.array([14, 16, -1, 8, -5]) 4.4.1 Imputation back to top A very common strategy is to impute or fill in the missing data, based on basic statistical descriptions of the feature column (mode, mean, and median) from sklearn.impute import SimpleImputer # strategy = 'mean' will replace nan's with mean value # of the column # others are median and most_frequent (mode) imp = SimpleImputer(strategy='mean') X2 = imp.fit_transform(X) X2 array([[4.5, 0. , 3. ], [3. , 7. , 9. ], [3. , 5. , 2. ], [4. , 5. , 6. ], [8. , 8. , 1. ]]) 4.4.2 Other Strategies back to top Depending on the severity of missing data, you will sometimes opt to remove the whole column, or perhaps apply some simple learning to fill in the missing data. This is a great article on more advanced strategies for handling missing data. References back to top * Box Cox * Multicolinearity * Missing Data","title":"Feature Engineering"},{"location":"S4_Feature_Engineering/#data-science-foundations-session-4-feature-engineering","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we talked about model pipelines and conveniently began with a suitable set of input data. In the real world, this is hardly ever the case! What is constant is this: at the end of the day, our models need numbers. Not only this, but a suitable set of numbers. What does that mean? The answer to that question is the subject of our session today.","title":"Data Science Foundations, Session 4: Feature Engineering"},{"location":"S4_Feature_Engineering/#40-preparing-environment-and-importing-data","text":"back to top","title":"4.0 Preparing Environment and Importing Data"},{"location":"S4_Feature_Engineering/#401-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats as stats from scipy.stats import gamma from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor import seaborn as sns; sns.set() from sklearn.datasets import load_iris from sklearn.metrics import mean_squared_error, r2_score","title":"4.0.1 Import Packages"},{"location":"S4_Feature_Engineering/#402-load-dataset","text":"back to top margin = pd.read_csv('https://raw.githubusercontent.com/wesleybeckner/'\\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv') orders = pd.read_csv('https://raw.githubusercontent.com/wesleybeckner/'\\ 'ds_for_engineers/main/data/truffle_margin/truffle_orders.csv') time_cols = [i for i in orders.columns if '/' in i] margin.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 dfcat = margin.columns[:-2] dfcat Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') orders.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer 1/2020 2/2020 3/2020 4/2020 5/2020 6/2020 7/2020 8/2020 9/2020 10/2020 11/2020 12/2020 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146","title":"4.0.2 Load Dataset"},{"location":"S4_Feature_Engineering/#41-categorical-features","text":"back to top At the end of the day, our algorithms operate on numerical values. How do you get from a series of string values to numerical values? margin['Customer'].unique() array(['Slugworth', 'Perk-a-Cola', 'Fickelgruber', 'Zebrabar', \"Dandy's Candies\"], dtype=object) A naive way to do it would be to assign a number to every entry 'Slugworth' = 1 'Perk-a-Cola' = 2 'Dandy's Candies' = 3 but we would inadvertently end up with some weird mathematical relationships between these variables, e.g. Dandy's Candies - Perk-a-Cola = Slugworth (3 - 2 = 1). A work around for this is to think multi-dimensionally we express our categorical values as vectors in a hyperspace where they cannot be expressed in terms of one another, i.e. they are orthogonal 'Slugworth' = [1,0,0] 'Perk-a-Cola' = [0,1,0] 'Dandy's Candies' = [0,0,1] such a scheme, in machine learning vernacular, is termed one-hot encoding.","title":"4.1 Categorical Features"},{"location":"S4_Feature_Engineering/#411-one-hot-encoding","text":"back to top sklearn has a couple useful libraries for one-hot encoding. let's start with the OneHotEncoder class in its preprocessing library from sklearn.preprocessing import OneHotEncoder # create the encoder object enc = OneHotEncoder() # grab the columns we want to convert from strings X_cat = margin['Customer'].values.reshape(-1,1) # fit our encoder to this data enc.fit(X_cat) OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>, handle_unknown='error', sparse=True) After fitting our encoder, we can then use this object to create our training array. # as a reference here's our original data display(X_cat[:10]) print(X_cat.shape, end='\\n\\n') onehotlabels = enc.transform(X_cat).toarray() print(onehotlabels.shape, end='\\n\\n') # And here is our new data onehotlabels[:10] array([['Slugworth'], ['Slugworth'], ['Perk-a-Cola'], ['Fickelgruber'], ['Fickelgruber'], ['Fickelgruber'], ['Slugworth'], ['Zebrabar'], ['Slugworth'], ['Zebrabar']], dtype=object) (1668, 1) (1668, 5) array([[0., 0., 0., 1., 0.], [0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]]) We have our customer information one-hot encoded, we need to do this for all our variables and concatenate them with our regular numerical variables in our original dataframe. # create the encoder object enc = OneHotEncoder() # grab the columns we want to convert from strings X_cat = margin[dfcat].values # fit our encoder to this data enc.fit(X_cat) onehotlabels = enc.transform(X_cat).toarray() X_num = margin[\"KG\"] print(X_num.shape) X = np.concatenate((onehotlabels, X_num.values.reshape(-1,1)),axis=1) X.shape (1668,) (1668, 119) And now we grab our EBITDA (margin) data for prediction y = margin[\"EBITDA/KG\"]","title":"4.1.1 One-Hot Encoding"},{"location":"S4_Feature_Engineering/#exercise-1-create-a-simple-linear-model","text":"Using the X and Y sets, use train_test_split and LinearRegression to make a baseline model based on what we've learned so far. Assess your model performance visually by plottying y_test vs y_test_pred # Cell for Exercise 1 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression [<matplotlib.lines.Line2D at 0x7fb3d1bb0890>]","title":"\ud83c\udfcb\ufe0f Exercise 1: Create a simple linear model"},{"location":"S4_Feature_Engineering/#question-1","text":"How can we assess the relative feature importance of the features in our model? We could be tempted to inspect the coefficients ( linear.coef_ ) of our model to evaluate the relative feature importance, but in order to do this our features need to be scaled (so that the relative coefficient sizes are meaningful). What other issues might there be (think categorical vs continuous variables).","title":"\ud83d\ude4b Question 1:"},{"location":"S4_Feature_Engineering/#42-derived-features","text":"back to top Can we recall an example of where we've seen this previously? That's right earlier on in our first session we derived some polynomial features to create our polynomial model using the linear regression class in sklearn. We actually see this a lot in engineering, where we will describe log relationships or some other transformation of the original variable. Actually let me see if I can find an example in my handy BSL... concentration profiles in continous stirred tank vs plug flow reactors. Notice the y-axis is log scale.Thank's Bird, Stewart, Lightfoot! Can we think of other examples where we would like to derive features from our input data?","title":"4.2 Derived Features"},{"location":"S4_Feature_Engineering/#421-creating-polynomials","text":"back to top Let's revisit our example from the previous session, right before we introduced Grid Search in sklearn # from Model Selection and Validation, 1.2.1 from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression(degree=2, **kwargs): return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs)) in the above, we use sklearn's convenient tool, make_pipeline to join together the preprocessing tool PolynomialFeatures and the basic model LinearRegression . Let's take a look at what PolynomialFeatures does to some simple data x = np.arange(1,11) y = x**3 print(x) print(y) [ 1 2 3 4 5 6 7 8 9 10] [ 1 8 27 64 125 216 343 512 729 1000] features = PolynomialFeatures(degree=3) X2 = features.fit_transform(x.reshape(-1,1)) we see our new feature set contains our original features, plus new features up to the nth-degree polynomial we set when creating the features object from PolynomialFeatures print(X2) [[ 1. 1. 1. 1.] [ 1. 2. 4. 8.] [ 1. 3. 9. 27.] [ 1. 4. 16. 64.] [ 1. 5. 25. 125.] [ 1. 6. 36. 216.] [ 1. 7. 49. 343.] [ 1. 8. 64. 512.] [ 1. 9. 81. 729.] [ 1. 10. 100. 1000.]] model = LinearRegression().fit(X2, y) yhat = model.predict(X2) plt.scatter(x, y) plt.plot(x, yhat);","title":"4.2.1 Creating Polynomials"},{"location":"S4_Feature_Engineering/#422-dealing-with-time-series","text":"back to top Often, we will be dealing with time series data, whether its data generated by machinery, reactors, or sales and customers. In the following we discuss some simple practices for dealing with time series data.","title":"4.2.2 Dealing with Time Series"},{"location":"S4_Feature_Engineering/#enrichment-2221-fast-fourier-transform","text":"back to top Sometimes we'll want to create a more sophisticated transformation of our input data. As engineers, this can often have to do with some empirical knowledge we understand about our process. When working with equipment and machinery, we will often want to convert a signal from the time to frequency domain. Let's cover how we can do that with numpy! [img src](https://www.nti-audio.com/en/support/know-how/fast-fourier-transform-fft#:~:text=The%20%22Fast%20Fourier%20Transform%22%20(,frequency%20information%20about%20the%20signal.) What I've drawn here in the following is called a square-wave signal t = np.linspace(0,5,200) w = 5 h = 4 s = 4 * h / np.pi * (np.sin(w*t) + np.sin(3*w*t)/3 + np.sin(5*w*t)/5) # here is the call to numpy FFT F = np.fft.fft(s) freq = np.fft.fftfreq(t.shape[-1]) # amplitudes amps = [max(np.sin(w*t)), max(np.sin(w*t*3)/3), max(np.sin(w*t*5)/5)] fig, ax = plt.subplots(1,2,figsize=(10,5)) ax[0].plot(t,s) ax[0].plot(t,np.sin(w*t), ls='--') ax[0].plot(t,np.sin(w*t*3)/3, ls='--') ax[0].plot(t,np.sin(w*t*5)/5, ls='--') ax[0].set_title('Time Domain') # tells us about the amplitude of the component at the # corresponding frequency magnitude = np.sqrt(F.real**2 + F.imag**2) ax[1].plot(freq, magnitude) ax[1].set_xlim(0,.15) ax[1].set_title('Frequency Domain') Text(0.5, 1.0, 'Frequency Domain') We can construct a similar plot with 4 signals contributing to the square-wave: t = np.linspace(0,5,200) w = 5 h = 4 s = 4 * h / np.pi * (np.sin(w*t) + np.sin(3*w*t)/3 + np.sin(5*w*t)/5 + np.sin(10*w*t)/10) F = np.fft.fft(s) freq = np.fft.fftfreq(t.shape[-1]) fig, ax = plt.subplots(1,2,figsize=(10,5)) ax[0].plot(t,s) ax[0].plot(t,np.sin(w*t), ls='--') ax[0].plot(t,np.sin(w*t*3)/3, ls='--') ax[0].plot(t,np.sin(w*t*5)/5, ls='--') ax[0].plot(t,np.sin(10*w*t)/10, ls='--') ax[0].set_title('Time Domain') # tells us about the amplitude of the component at the # corresponding frequency magnitude = np.sqrt(F.real**2 + F.imag**2) ax[1].plot(freq, magnitude) ax[1].set_xlim(0) ax[1].set_title('Frequency Domain') Text(0.5, 1.0, 'Frequency Domain')","title":"Enrichment: 2.2.2.1 Fast Fourier Transform"},{"location":"S4_Feature_Engineering/#4222-rolling-windows","text":"back to top One powerful technique for dealing with time series data, is to create a rolling window of features based on the historical data. The proper window size can usually be determined by trial and error, or constraints around access to the data itself. In the above gif, we have a window size of 7. What that means is for whatever time step units we are in (that could be minutes, days, months, etc.) we will have 7 of them included in a single instance or observation. This instance or observation is then interpreted by our model and used to assess the target value, typically the quantity in the very next time step after the window (the green bar in the gif).","title":"4.2.2.2 Rolling Windows"},{"location":"S4_Feature_Engineering/#exercise-2-optimize-rolling-window-size-for-customer-forecasts","text":"For this exercise, you will use the process_data function below to help you optimize the window size for predicting the order quantity in any given month. create a model using a window size of 3 and predict the order quantity for the month immediately following the window create a model for window sizes 1-11 and report the \\(R^2\\) for each model def process_data(Xy, time_cols=12, window=3, remove_null=False): \"\"\" This function splits your time series data into the proper windows Parameters ---------- Xy: array The input data. If there are non-time series columns, assumes they are on the left and time columns are on the right. time_cols: int The number of time columns, default 12 window: int The time window size, default 3 Returns ------- X_: array The independent variables, includes time and non-time series columns with the new window y_: array The dependent variable, selected from the time columns at the end of the window labels: The time series labels, can be used in subsequent plot \"\"\" # separate the non-time series columns X_cat = Xy[:,:-time_cols] # select the columns to apply the sweeping window X = Xy[:,-time_cols:] X_ = [] y = [] for i in range(X.shape[1]-window): # after attaching the current window to the non-time series # columns, add it to a growing list X_.append(np.concatenate((X_cat, X[:, i:i+window]), axis=1)) # add the next time delta after the window to the list of y # values y.append(X[:, i+window]) # X_ is 3D: [number of replicates from sweeping window, # length of input data, # size of new feature with categories and time] # we want to reshape X_ so that the replicates due to the sweeping window is # a part of the same dimension as the instances of the input data X_ = np.array(X_).reshape(X.shape[0]*np.array(X_).shape[0],window+X_cat.shape[1]) y = np.array(y).reshape(X.shape[0]*np.array(y).shape[0],) if remove_null: # remove training data where the target is 0 (may be unfair advantage) X_ = X_[np.where(~np.isnan(y.astype(float)))[0]] y = y[np.where(~np.isnan(y.astype(float)))[0]] # create labels that show the previous month values used to train the model labels = [] for row in X_: labels.append(\"X: {}\".format(np.array2string(row[-window:].astype(float).round()))) return X_, y, labels # Code Cell for Exercise 2 # use data and the function process_data to create your X, y arrays # then use train_test_split to create train and test portions data = orders.values[:,6:] ### YOUR CODE HERE ### # USE y_test and y_pred for your actual and true test data # name your labels for the test set labels_test # change only window parameter in process_data() 1 0.762506752772391 2 0.8895992010134899 3 0.9413336982898548 4 0.7532142077720143 5 0.9675986952925033 6 0.9900851583059013 7 0.9994996102278398 8 1.0 9 1.0 10 1.0 11 1.0 #### RUN AFTER EXERCISE 2.2.2.2.1 #### fig = px.scatter(x=y_test, y=y_pred, labels={ \"y\": \"Prediction\", \"x\": \"Actual\" }) fig.update_layout( autosize=False, width=800, height=500, title='R2: {:.3f}'.format(r2_score(y_test, y_pred)) ) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"d46fe430-b481-4c6b-8c71-3cb9cfb98a5e\")) { Plotly.newPlot( 'd46fe430-b481-4c6b-8c71-3cb9cfb98a5e', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Actual=%{x}<br>Prediction=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [108.64285714285714, 2355.267295597484, 1432.2712418300653, 19569.69230769231, 1702.1929824561405, 58020.911949685535, 1968.8904494382025, 16569.26100628931, 20.098039215686274, 13504.20634920635, 191.11111111111111, 8608.709677419354, 289.8888888888889, 3665.8097686375318, 440.859375, 712.4603174603176, 16918.132716049382, 8104.40251572327, 38128.80258899677, 36.37096774193548, 21438.1875, 19129.33333333333, 1529.7752808988766, 12274.009146341465, 14.663461538461537, 538.8571428571429, 4057.832278481013, 1630.1966292134832], \"xaxis\": \"x\", \"y\": [108.6428571428318, 2355.26729559746, 1432.2712418300403, 19569.692307692298, 1702.1929824561164, 58020.91194968554, 1968.8904494381782, 16569.261006289296, 20.098039215660815, 13504.206349206332, 191.11111111108582, 8608.709677419334, 289.88888888886396, 3665.809768637509, 440.8593749999747, 712.4603174602926, 16918.132716049364, 8104.402515723249, 38128.802588996754, 36.370967741910036, 21438.187499999993, 19129.33333333332, 1529.7752808988523, 12274.009146341446, 14.663461538436076, 538.8571428571178, 4057.8322784809898, 1630.196629213459], \"yaxis\": \"y\"}], {\"autosize\": false, \"height\": 500, \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"R2: 1.000\"}, \"width\": 800, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Actual\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Prediction\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('d46fe430-b481-4c6b-8c71-3cb9cfb98a5e'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) };","title":"\ud83c\udfcb\ufe0f Exercise 2: Optimize Rolling Window Size for Customer Forecasts"},{"location":"S4_Feature_Engineering/#423-image-preprocessing","text":"back to top Image preprocessing is beyond the scope of this session. We will cover this in C4. For now, know that there is a wealth of considerations for how to handle images, and they all fit within the realm of feature engineering.","title":"4.2.3 Image Preprocessing"},{"location":"S4_Feature_Engineering/#43-transformed-features","text":"back to top Transformed features, are features that we would like to augment based on their relationship within their own distribution or to other (allegedly) independent data within our training set. e.g. we're not deriving new features based on some empirical knowledge of the data, rather we are changing them due to statistical properties that we can assess based on the data itself.","title":"4.3 Transformed Features"},{"location":"S4_Feature_Engineering/#431-skewness","text":"back to top Skewed data can lead to imbalances in our model prediction. Why? Skewed values in the distribution will bias the mean. When assigning weights to this input feature, therefore, the model will give preferential treatment to these values. To demonstrate, I'm going to use scipy to create some skewed data. from scipy.stats import skewnorm a = 10 x = np.linspace(skewnorm.ppf(0.01, a), skewnorm.ppf(0.99, a), 100) plt.plot(x, skewnorm.pdf(x, a), 'r-', lw=5, alpha=0.6, label='skewnorm pdf') [<matplotlib.lines.Line2D at 0x7fb3cb441910>] We can now generate a random population based on this distribution r = skewnorm.rvs(a, size=1000) plt.hist(r) (array([143., 290., 244., 160., 96., 43., 13., 7., 3., 1.]), array([-0.24457186, 0.18369502, 0.61196191, 1.04022879, 1.46849568, 1.89676256, 2.32502945, 2.75329633, 3.18156322, 3.60983011, 4.03809699]), <a list of 10 Patch objects>) Unskewed data will return something close to 0 from calling df.skew() . When dealing with actual data, we can use df.skew() to determine whether we should transform our data. x = pd.DataFrame(r, columns=['Skewed Data']) x['Skewed Data'].skew() 0.9914234810526167 There are a handful of ways to deal with skewed data: log transform square root transform Box-Cox transform Let's try the first two print('square root transformed skew: {:.4f}'.format(np.sqrt(x['Skewed Data']).skew())) print('log transformed skew: {:.4f}'.format(np.log(x['Skewed Data']).skew())) fig, ax = plt.subplots(1, 1, figsize=(10,10)) ax.hist(x['Skewed Data'], alpha=0.5, label='original: {:.2f}'. format((x['Skewed Data']).skew())) ax.hist(np.sqrt(x['Skewed Data']), alpha=0.5, label='sqrt: {:.2f}'. format(np.sqrt(x['Skewed Data']).skew())) ax.hist(np.log(x['Skewed Data']), alpha=0.5, label='log: {:.2f}'. format(np.log(x['Skewed Data']).skew())) ax.legend() square root transformed skew: 0.1112 log transformed skew: -1.9834 /usr/local/lib/python3.7/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in sqrt /usr/local/lib/python3.7/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log <matplotlib.legend.Legend at 0x7fb3cb3525d0> We see we didn't get much traction with the log transform, and the log transform will not be able to handle 0 values, and so we will sometimes have to code exceptions for those. Boxplot is often a good route to go, but it has the added restriction that the data has to all be above 0. Let's create a new distribution with this added restriction a = 6 r = skewnorm.rvs(a, size=1000) r = [i for i in r if i > 0] plt.hist(r) (array([186., 194., 186., 134., 107., 70., 41., 18., 7., 6.]), array([4.86688792e-04, 2.93460209e-01, 5.86433729e-01, 8.79407250e-01, 1.17238077e+00, 1.46535429e+00, 1.75832781e+00, 2.05130133e+00, 2.34427485e+00, 2.63724837e+00, 2.93022189e+00]), <a list of 10 Patch objects>) from scipy import stats x = pd.DataFrame(r, columns=['Skewed Data']) fig, ax = plt.subplots(1, 1, figsize=(10,10)) ax.hist(x['Skewed Data'], alpha=0.5, label='original: {:.2f}'. format((x['Skewed Data']).skew())) ax.hist(np.sqrt(x['Skewed Data']), alpha=0.5, label='sqrt: {:.2f}'. format(np.sqrt(x['Skewed Data']).skew())) ax.hist(np.log(x['Skewed Data']), alpha=0.5, label='log: {:.2f}'. format(np.log(x['Skewed Data']).skew())) ax.hist(stats.boxcox(x['Skewed Data'])[0], alpha=0.5, label='box-cox: {:.2f}'. format(pd.DataFrame(stats.boxcox(x['Skewed Data'])[0])[0].skew())) ax.legend() <matplotlib.legend.Legend at 0x7fb3cb35e310>","title":"4.3.1 Skewness"},{"location":"S4_Feature_Engineering/#exercise-3-transform-data-from-a-gamma-distribution","text":"Repeat section 2.3.1, this time synthesizing a gamma distribution and transforming it. Which transformation best reduces the skew? Do this for a dataset that does not contain values at or below 0. # code cell for exercise 3 from scipy.stats import gamma <matplotlib.legend.Legend at 0x7fb3cb31fe90>","title":"\ud83c\udfcb\ufe0f Exercise 3: Transform data from a gamma distribution"},{"location":"S4_Feature_Engineering/#432-colinearity","text":"back to top Colinearity can also affect the performance of your machine learning model. In particular, if features are colinear, it can be easy for your model to overfit to your training dataset. This is often mitigated by regularization. If you're curious you can read more about it on this discussion from StackExchange . We will still explore it explicitly here by calculating the Variance Inflation Factor (VIF) on some hypothetical data. VIF = \\frac{1}{1-R^2}","title":"4.3.2 Colinearity"},{"location":"S4_Feature_Engineering/#4321-detecting-colinearity","text":"back to top from statsmodels.stats.outliers_influence import variance_inflation_factor Step 1: Make some data # we can throttle the error rate random.seed(42) # x2 will be sqrt of x1 plus some error def func(x, err): return x**.5 + (err * random.randint(-1,1) * random.random() * x) x0 = range(100) x1 = [func(i, .05) for i in x0] x2 = [func(i, 1) for i in x0] x3 = [random.randint(0,100) for i in x0] # take a look fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.plot(x0, x1, label='x1') ax.plot(x0, x2, label='x2') ax.plot(x0, x3, label='x3') ax.legend() <matplotlib.legend.Legend at 0x7fb3cbab8c90> To calculate the colinearities I'm going to aggregate these x's into a dataframe: colin = pd.DataFrame([x0,x1,x2,x3]).T colin.columns = ['x0','x1','x2','x3'] colin.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 0 0.0 0.000000 0.000000 29.0 1 1.0 1.013751 0.721523 28.0 2 2.0 1.400260 1.414214 3.0 3 3.0 1.630546 -0.438007 84.0 4 4.0 2.017388 4.304847 24.0 Step 2: Calculate VIF factors # calculate VIF factors vif = pd.DataFrame() vif[\"VIF Factor\"] = [variance_inflation_factor(colin.values, i) for i in range(colin.shape[1])] vif[\"features\"] = colin.columns Step 3: Inspect VIF factors # inspect VIF factors display(vif) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 12.555415 x0 1 15.823872 x1 2 1.030609 x2 3 3.559468 x3","title":"4.3.2.1 Detecting Colinearity"},{"location":"S4_Feature_Engineering/#4322-fixing-colinearity","text":"back to top It is good to aknowledge where colinearity exists as this will influence the interpretability of your model. In most cases, however, it won't have a heavy influence on the performance of your model. A simple method of dealing with colinearity, is to remove the highest VIF features from your model, iteratively, assessing the performance and determining whether to keep the variable or not. Another method is to create some linear combination of the correlated variables. This is encapsulated in the section on dimensionality reduction.","title":"4.3.2.2 Fixing Colinearity"},{"location":"S4_Feature_Engineering/#433-normalization","text":"back to top Many learning algorithms require zero mean and unit variance to behave optimally. Sklearn preprocessing library contains a very usefull class, StandardScaler for handling this automatically for us. from sklearn.preprocessing import StandardScaler scaler = StandardScaler() normed = scaler.fit_transform(colin) colin[['x0','x1','x2','x3']].plot(kind='kde') <matplotlib.axes._subplots.AxesSubplot at 0x7fb3cb102750> pd.DataFrame(normed, columns = [['x0','x1','x2','x3']]).plot(kind='kde') <matplotlib.axes._subplots.AxesSubplot at 0x7fb3cb102990>","title":"4.3.3 Normalization"},{"location":"S4_Feature_Engineering/#exercise-4-normalization-affect-on-vif","text":"In the above, we saw how to scale and center variables. How does this affect VIF? Calculate the VIF for the raw and scaled-centered data # Code Cell for Exercise 4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 3.286048 x0 1 3.296881 x1 2 1.015805 x2 3 1.035537 x3","title":"\ud83c\udfcb\ufe0f Exercise 4: Normalization affect on VIF"},{"location":"S4_Feature_Engineering/#434-dimensionality-reduction","text":"back to top Dimensionality reduction is an awesome way to do feature engineering. It is very commonly used. Because it is also an unsupervised machine learning technique, we will visit this topic in that section.","title":"4.3.4 Dimensionality Reduction"},{"location":"S4_Feature_Engineering/#44-missing-data","text":"back to top We will often have missing data in our datasets. How do we deal with this? Let's start by making some data with missing data. We'll use a numpy nan datatype to do this from numpy import nan X = np.array([[ nan, 0, 3 ], [ 3, 7, 9 ], [ 3, 5, 2 ], [ 4, nan, 6 ], [ 8, 8, 1 ]]) y = np.array([14, 16, -1, 8, -5])","title":"4.4 Missing Data"},{"location":"S4_Feature_Engineering/#441-imputation","text":"back to top A very common strategy is to impute or fill in the missing data, based on basic statistical descriptions of the feature column (mode, mean, and median) from sklearn.impute import SimpleImputer # strategy = 'mean' will replace nan's with mean value # of the column # others are median and most_frequent (mode) imp = SimpleImputer(strategy='mean') X2 = imp.fit_transform(X) X2 array([[4.5, 0. , 3. ], [3. , 7. , 9. ], [3. , 5. , 2. ], [4. , 5. , 6. ], [8. , 8. , 1. ]])","title":"4.4.1 Imputation"},{"location":"S4_Feature_Engineering/#442-other-strategies","text":"back to top Depending on the severity of missing data, you will sometimes opt to remove the whole column, or perhaps apply some simple learning to fill in the missing data. This is a great article on more advanced strategies for handling missing data.","title":"4.4.2 Other Strategies"},{"location":"S4_Feature_Engineering/#references","text":"back to top * Box Cox * Multicolinearity * Missing Data","title":"References"},{"location":"S5_Unsupervised_Learning/","text":"Data Science Foundations, Session 5: Unsupervised Learning: Clustering and Dimensionality Reduction Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we began our discussion on feature engineering and ended with a sneak peak into dimensionality reduction. This last topic deserves a whole session on its own because its use case is not limited to feature engineering! It can also be used as a tool for visualization, for noise filtering, and much more. In this session we discuss dimensionality reduction along with other unsupervised learning methods. Up until now, the only learning estimators we've looked at were supervised ones: estimators that predict labels based on training data. Here, however, we are interested in uncovering aspects of the data without reference to any known labels. The usefulness for these learners will become immediately apparent when we revist our wine quality models from Course 1, Session 7! 5.0 Preparing Environment and Importing Data back to top 5.0.1 Import Packages back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score from sklearn.mixture import GaussianMixture import seaborn as sns; sns.set() import copy /usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. 5.0.2 Load Dataset back to top wine = pd.read_csv(\"https://raw.githubusercontent.com/wesleybeckner/\"\\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\") wine.dropna(inplace=True) wine['quality_label'] = wine['quality'].apply(lambda x: 'low' if x <=5 else 'med' if x <= 7 else 'high') wine['type_encoding'] = wine['type'].map({'red': 0, 'white': 1}) wine['quality_encoding'] = wine['quality_label'].map({'low':0, 'med': 1, 'high': 2}) wine.columns = wine.columns.str.replace(' ', '_') features = list(wine.columns[1:-1].values) features.remove('quality_label') features.remove('quality') 5.1 Principal Component Analysis back to top Principle Component Analysis or PCA is one of the most wide spread implementations of dimensionality reduction. In PCA, we find the principle components, or linear recombinations of the dimensions of the data, that best explain the variance of the data. There are mathematical arguments abound for describing how we analytically solve for the principle components and how they relate to other concepts in mathematics (like pythagorean theorem). We'll sidestep that conversation for now, and proceed to our pragmatic demonstrations, as we have done in the past. 5.1.1 Introduction to PCA back to top In the cell below, we have plotted acidity and density from our familiar wine dataset. fig, ax = plt.subplots(1, 1, figsize=(5,5)) wine.loc[wine['type'] == 'red'].plot(x='fixed_acidity', y='density', ax=ax, ls='', marker='.') <matplotlib.axes._subplots.AxesSubplot at 0x7f8cd1401910> X = wine.loc[wine['type'] == 'red'][['fixed_acidity', 'density']].values X[:5] array([[ 7.4 , 0.9978], [ 7.8 , 0.9968], [ 7.8 , 0.997 ], [11.2 , 0.998 ], [ 7.4 , 0.9978]]) The principal components, are actually the eigenvectors of the covariance matrix of the standardized data. We will perform this by hand to get an understanding. First we standardize the data from sklearn.preprocessing import StandardScaler X_std = StandardScaler().fit_transform(X) Then we compute the covariance matrix. There is a nice demonstration of computing covariance on stats quest . The covariance can be expressed as: cov(X,Y) = \\frac{1}{n^2}\\sum\\sum(x_i - x_j)(y_i - y_j) Every \\((x_i - x_j)(y_i - y_j)\\) is the area described by the rectangle between points \\(i\\) and \\(j\\), and we if we deem to color positive changes as red and negative ones as blue, we get a picture like the following: When we view all the pairwise interactions in aggregate we get a sense of how the areas of rectangles made by each observation influence the covariance: As a side note, the covariance term is the numerator in the pearsons correlation we covered last week: \\rho_{x,y} = \\frac{cov(X,Y)}{\\sigma_x\\sigma_y} import numpy as np mean_vec = np.mean(X_std, axis=0) cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1) print('Covariance matrix \\n%s' %cov_mat) Covariance matrix [[1.00062814 0.66831213] [0.66831213 1.00062814]] The principal components are found mathematically by determining the eigenvectors of the covariance matrix and sorting them by their egienvalues, or their explained variance. There is a mathematical proof 1 , 2 for why this works, but we will not cover that here. cov_mat = np.cov(X_std.T) # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals, eig_vecs = np.linalg.eig(cov_mat) print('Eigenvectors \\n%s' %eig_vecs) print('\\nEigenvalues \\n%s' %eig_vals) Eigenvectors [[-0.70710678 -0.70710678] [ 0.70710678 -0.70710678]] Eigenvalues [0.33231601 1.66894027] And now we are just going to sort our vectors by their eigenvalues to get the proper order of principal components: # Make a list of (eigenvalue, eigenvector) tuples eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))] # Sort the (eigenvalue, eigenvector) tuples from high to low eig_pairs.sort(key=lambda x: x[0], reverse=True) # Visually confirm that the list is correctly sorted by decreasing eigenvalues print('Eigenvalues in descending order:') for i in eig_pairs: print(i[0]) Eigenvalues in descending order: 1.6689402736104237 0.33231600779661163 For those of us who are familiar with eigenvectors, we should recognize that they are necessarily orthogonal to one another. This is good to know from the PCA point of view, because we wouldn't want the variance along one vector to be explained by another. Let's plot these vectors along with the standardized data: fig, ax = plt.subplots(1,1,figsize=(7,7)) ax.plot(X_std[:,0],X_std[:,1],ls='',marker='.', alpha=0.5) for vec, color in zip(range(eig_vecs.shape[0]),['orange', 'green']): ax.plot([np.mean(X_std[:,0]), (eig_pairs[vec][1][0]+np.mean(X_std[:,0]))*-eig_pairs[vec][0]], [np.mean(X_std[:,1]), (eig_pairs[vec][1][1]+np.mean(X_std[:,1]))*-eig_pairs[vec][0]], color=f'tab:{color}', linewidth=4) ax.plot([np.mean(X_std[:,0]), (eig_pairs[vec][1][0]+np.mean(X_std[:,0]))*eig_pairs[vec][0]], [np.mean(X_std[:,1]), (eig_pairs[vec][1][1]+np.mean(X_std[:,1]))*eig_pairs[vec][0]], color=f'tab:{color}', linewidth=4) ax.set_aspect('equal') ax.set_ylim(min(X_std[:,1]),max(X_std[:,1])) ax.set_xlim(min(X_std[:,0]),max(X_std[:,0])) (-2.141423705153984, 4.352327069615032) grandparent, spouse, daughter parable We indeed see that these vectors are orthogonal tot = sum(eig_vals) var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)] cum_var_exp = np.cumsum(var_exp) with plt.style.context('seaborn-whitegrid'): plt.figure(figsize=(7, 4)) plt.bar(range(2), var_exp, alpha=0.5, align='center', label='individual explained variance') plt.step(range(2), cum_var_exp, where='mid', label='cumulative explained variance') plt.ylabel('Explained variance ratio') plt.xlabel('Principal components') plt.legend(loc='center right') plt.tight_layout() Here I'm just putting my eigenvectors into the proper shape: matrix_w = np.hstack((eig_pairs[0][1].reshape(2,1), eig_pairs[1][1].reshape(2,1))) print('Matrix W:\\n', matrix_w) Matrix W: [[-0.70710678 -0.70710678] [-0.70710678 0.70710678]] In order to take the dot product, project, X_std onto the first two principal components: Y = X_std.dot(matrix_w) plt.scatter(Y[:,0],Y[:,1]) <matplotlib.collections.PathCollection at 0x7f8cc392c390> But we can avoid the fancy footwork and do all this in sklearn! from sklearn.decomposition import PCA pca = PCA(n_components=2) pca.fit(X_std) PCA(copy=True, iterated_power='auto', n_components=2, random_state=None, svd_solver='auto', tol=0.0, whiten=False) We see that the values we get are the same as for the hand-calculated eigenvalues and vectors print(pca.components_) [[ 0.70710678 0.70710678] [ 0.70710678 -0.70710678]] And the eigenvalues are under pca.explained_variance_ print(pca.explained_variance_) [1.66894027 0.33231601] 5.1.2 PCA as Dimensionality Reduction back to top One obvious use case for PCA, is to drop the dimensions with lowest explained variance. Continuing with our acidity/density example this will look like the following from sklearn.decomposition import PCA pca = PCA(n_components=2) pca.fit(X_std) PCA(copy=True, iterated_power='auto', n_components=2, random_state=None, svd_solver='auto', tol=0.0, whiten=False) fig, ax = plt.subplots(1,2,figsize=(10,5)) ax[0].plot(X_std[:,0],X_std[:,1],ls='',marker='.', alpha=0.5) for vec, color in zip(range(eig_vecs.shape[0]),['orange', 'green']): ax[0].plot([np.mean(X_std[:,0]), (eig_pairs[vec][1][0]+np.mean(X_std[:,0]))*-eig_pairs[vec][0]], [np.mean(X_std[:,1]), (eig_pairs[vec][1][1]+np.mean(X_std[:,1]))*-eig_pairs[vec][0]], color=f'tab:{color}', linewidth=4) ax[0].plot([np.mean(X_std[:,0]), (eig_pairs[vec][1][0]+np.mean(X_std[:,0]))*eig_pairs[vec][0]], [np.mean(X_std[:,1]), (eig_pairs[vec][1][1]+np.mean(X_std[:,1]))*eig_pairs[vec][0]], color=f'tab:{color}', linewidth=4) ax[0].set_aspect('equal') ax[0].set_ylim(min(X_std[:,1]),max(X_std[:,1])) ax[0].set_xlim(min(X_std[:,0]),max(X_std[:,0])) ax[0].set_ylabel('Normalized density') ax[0].set_xlabel('Normalized acidity') ax[1].bar(range(2), var_exp, alpha=0.5, align='center', label='Individual') ax[1].step(range(2), cum_var_exp, where='mid', label='Cumulative') ax[1].set_ylabel('Explained variance ratio') ax[1].set_xlabel('Principal components') ax[1].legend() <matplotlib.legend.Legend at 0x7f8cc1869810> We can capture 80% of the explained variance along just the first principal component. What does this projection look like? # we set our components to 1 pca = PCA(n_components=1) pca.fit(X_std) # we then project the data onto the first PC # and then rebroadcast this transformation # back onto the orginal dimensions to see # what this looks like in terms of acidity/density X_pca = pca.inverse_transform(pca.transform(X_std)) # original data plt.scatter(X_std[:, 0], X_std[:, 1], alpha=0.2) # projected data plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.8) <matplotlib.collections.PathCollection at 0x7f8cc179d5d0> X_pca array([[-1.86755381, 3.11192011], [-0.38665917, -0.56358304], [-0.46921039, 0.03417659], ..., [ 2.49750969, -1.40757438], [ 3.20760565, -0.19795515], [ 1.57595901, -1.23607745]]) 5.1.3 PCA for visualization back to top For classification tasks, PCA lends itself as a useful method for seeing how the classes separate on the highest variance dimensions of the data without consideration to the classes themselves, i.e. do our classes seperate out according to the other variables in the dataset. With our wine dataset, we see that the wine types fall out nicely along the first two principal components X = wine.select_dtypes(exclude=['object']).values X_std = StandardScaler().fit_transform(X) pca = PCA(n_components=2) pca.fit(X_std) X_pca = pca.transform(X_std) plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2, c=wine['type'].values, edgecolor='grey') plt.xlabel('First PC') plt.ylabel('Second PC') Text(0, 0.5, 'Second PC') 5.1.4 Enrichment: PCA as Outlier Removal and Noise Filtering back to top In some cases, it can be advantageous to use PCA as a method for outlier removal. There are many caveats to this that we will not discuss here. But for those who are interested, this is a great overview article. As for noise filteration, Vanderplas' DS handbook has a good section on the topic as does Guido/Muller's Intro to ML with Python by the same pusblisher. 5.1.5 PCA for Feature Engineering back to top Finally, PCA is actually a commonly used preprocessing technique for supervised machine learning models. In the next exercise, our goal will be to use PCA to generate a new set of features, to feed into our linear model. \ud83c\udfcb\ufe0f Exercise 1: PCA as Preprocessing for Models Using the wine data, select any number of the first principal components and attemp to predict density for the red wine data. features.remove('type_encoding') wine[features] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide pH sulphates alcohol 0 7.0 0.270 0.36 20.7 0.045 45.0 170.0 3.00 0.45 8.8 1 6.3 0.300 0.34 1.6 0.049 14.0 132.0 3.30 0.49 9.5 2 8.1 0.280 0.40 6.9 0.050 30.0 97.0 3.26 0.44 10.1 3 7.2 0.230 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 4 7.2 0.230 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 ... ... ... ... ... ... ... ... ... ... ... 6491 6.8 0.620 0.08 1.9 0.068 28.0 38.0 3.42 0.82 9.5 6492 6.2 0.600 0.08 2.0 0.090 32.0 44.0 3.45 0.58 10.5 6494 6.3 0.510 0.13 2.3 0.076 29.0 40.0 3.42 0.75 11.0 6495 5.9 0.645 0.12 2.0 0.075 32.0 44.0 3.57 0.71 10.2 6496 6.0 0.310 0.47 3.6 0.067 18.0 42.0 3.39 0.66 11.0 6463 rows \u00d7 10 columns # Code Cell for Exercise 1 X = wine[features].values X_std = StandardScaler().fit_transform(X) pca = PCA(n_components=2) pca.fit(X_std) X_pca = pca.transform(X_std) plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2, c=wine['type'].values, edgecolor='grey') plt.xlabel('First PC') plt.ylabel('Second PC') Text(0, 0.5, 'Second PC') y = StandardScaler().fit_transform(wine['density'].values.reshape(-1,1)) lr = LinearRegression().fit(X_pca, y) r2_score(y, lr.predict(X_pca)), 0.382963127385076 5.2 K-Means Clustering back to top We now embark on a second class of unsupervised learning techinques: clustering. The K-means algorithm works under two assumptions: * every cluster can be defined by an arithmetic mean or cluster center * each point is closer to one arithmetic center than the other centers Let's turn back to our wine dataset: X = wine.select_dtypes(exclude=['object']).values X_std = StandardScaler().fit_transform(X) pca = PCA(n_components=2) pca.fit(X_std) X_pca = pca.transform(X_std) plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2, c='grey') plt.xlabel('First PC') plt.ylabel('Second PC') Text(0, 0.5, 'Second PC') It is fairly obvious under these two dimensions that there are two clusters. The K-means algorithm automatically pics this out from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=2) kmeans.fit(X_pca) y_kmeans = kmeans.predict(X_pca) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, s=50, alpha=0.5, edgecolor='grey', cmap='viridis') centers = kmeans.cluster_centers_ plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5); wa-la! Our simple unsupervised algorithm does a pretty good job of differentiating between the red and white wine clusters. 5.2.1 The Algorithm: Expectation-Maximization back to top We won't go too far into the implementation of the underlying algorithm here. In its basic steps, however, it performs the following: Initialize cluster centers (random guess) Then repeat: E-Step: assign points to the nearest center (arithmetic distance) M-step: set the new center point for each cluster according to the mean of it's datapoint members More information on K-means algorithm can be explored here 5.2.2 Limitations back to top A few brief notes on limitations: the global optimum may not be achieved (no guarantee of finding the overall best solution) the number of clusters must be guessed beforehand cluster boundaries are unavoidably linear and the cluster assignments are unavoidably circular can be slow for large datasets cluster assignments are non probabilistic 3 and 5 motivate our next section, Gaussian Mixture Models 5.3 Gaussian Mixture Models back to top in the simplest case, GMMs can be used in the same way as K-means from sklearn.mixture import GaussianMixture gmm = GaussianMixture(n_components=2).fit(X_pca) labels = gmm.predict(X_pca) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=40, cmap='viridis', alpha=0.2, edgecolor='grey'); But because GMM is a probablistic model, we can visualize the certainty with which we assign these clusters. probs = gmm.predict_proba(X_pca) print(probs[5:20].round(3)) [[1. 0. ] [1. 0. ] [1. 0. ] [1. 0. ] [1. 0. ] [0.992 0.008] [1. 0. ] [0.99 0.01 ] [1. 0. ] [1. 0. ] [1. 0. ] [0.999 0.001] [1. 0. ] [1. 0. ] [1. 0. ]] # convert probs to 1 dimension probs.max(1) array([1. , 0.99985029, 0.99996753, ..., 0.99997744, 0.99999985, 0.98796517]) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=probs.max(1), s=40, cmap='Blues', alpha=0.5, edgecolor='grey'); 5.3.1 Generalizing E-M for GMMs back to top The algorithm for GMMs is very similar to K-means, but now the EM steps are probablistic 1 2 Initialize cluster centers (random guess) Then repeat: E-Step: assign points their probability of belonging to every cluster M-step: set the new center point for each cluster according to the probabilities of all datapoints 5.3.2 GMMs as a Data Generator back to top One particularly fascinating application of GMMs is to use them to generate new data that is similar to the data on which they are modeled. Let's take an example. We're going to use sklearn make_circles function to create some arbitrary data that has a complex relationship along two different axes. # some helper functions borrowed from Jake Vanderplas with a few minor tweaks # https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html from matplotlib.patches import Ellipse def draw_ellipse(position, covariance, ax=None, **kwargs): \"\"\"Draw an ellipse with a given position and covariance\"\"\" ax = ax or plt.gca() # Convert covariance to principal axes if covariance.shape == (2, 2): U, s, Vt = np.linalg.svd(covariance) angle = np.degrees(np.arctan2(U[1, 0], U[0, 0])) width, height = 2 * np.sqrt(s) else: angle = 0 width, height = 2 * np.sqrt(covariance) # Draw the Ellipse for nsig in range(1, 4): ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs)) def plot_gmm(gmm, X, label=True, ax=None, data_alpha=1): ax = ax or plt.gca() labels = gmm.fit(X).predict(X) if label: ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2, alpha=data_alpha) else: ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2, alpha=data_alpha) ax.axis('equal') w_factor = 0.2 / gmm.weights_.max() for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_): draw_ellipse(pos, covar, alpha=w * w_factor) from sklearn.datasets import make_circles as gen X, y = gen(200, noise=0.02, random_state=42) plt.scatter(X[:, 0], X[:, 1]); As a side note, as a clustering model, the GMM is not particularly useful: gmm2 = GaussianMixture(n_components=2, covariance_type='full', random_state=0) plot_gmm(gmm2, X) But if we use the GMM as a density estimator on the underlying data, rather than as a clustering algorithm, we find a completely different use case: gmm16 = GaussianMixture(n_components=16, covariance_type='full', random_state=0) plot_gmm(gmm16, X, label=False) Now, with the distributions drawn, we can assemble entirely new data: Xnew = gmm16.sample(400)[0] plt.scatter(Xnew[:, 0], Xnew[:, 1]); 5.3.2.1 Determining the number of components back to top Let's think back to session 1 on model selection. How might we determine the best number of components? A couple analytic approaches that we have not much yet discussed, are the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The important thing to note from each of these is that they penalize added complexity to the models, and we would like to minimize their estimated prediction error. n_components = np.arange(1, 42) models = [GaussianMixture(n, covariance_type='full', random_state=42).fit(X) for n in n_components] plt.plot(n_components, [m.bic(X) for m in models], label='BIC') plt.plot(n_components, [m.aic(X) for m in models], label='AIC') plt.legend(loc='best') plt.xlabel('n_components'); plt.ylabel('est. prediction error') Text(0, 0.5, 'est. prediction error') Both AIC and BIC capture a local minimum on the order of 6 Gaussians. This is where the model captures the overall circular modality of the data. However it totally misses the circumscribed nature of the two circles. Increasing this number to 30-40 gaussians captures this feature of the data. The AIC score reflects this while the BIC score (although captures a local minima in the area) does not define this as a global optimum. gmmNew = GaussianMixture(n_components=40, covariance_type='full', random_state=0) plot_gmm(gmmNew, X, label=True, data_alpha=0) Xnew = gmmNew.sample(400)[0] plt.scatter(Xnew[:, 0], Xnew[:, 1]); \ud83c\udfcb\ufe0f Exercise 2: Determine Number of Components for Circular Moons Repeat the above, this time using sklearn.datasets.make_moons # Code Cell for Exercise 2 from sklearn.datasets import make_moons as gen X, y = gen(200, noise=0.02, random_state=42) n_components = np.arange(1, 42) models = [GaussianMixture(n, covariance_type='full', random_state=42).fit(X) for n in n_components] plt.plot(n_components, [m.bic(X) for m in models], label='BIC') plt.plot(n_components, [m.aic(X) for m in models], label='AIC') plt.legend(loc='best') plt.xlabel('n_components'); plt.ylabel('est. prediction error') Text(0, 0.5, 'est. prediction error') gmm_moon = GaussianMixture(n_components=40, covariance_type='full', random_state=0) plot_gmm(gmm_moon, X) Xnew = gmm_moon.sample(400)[0] plt.scatter(Xnew[:, 0], Xnew[:, 1]); References PCA Intuitive PCA PCA and Eigenvectors/values GMM GMMs Explained Derive GMM Exercise","title":"Unsupervised Learning"},{"location":"S5_Unsupervised_Learning/#data-science-foundations-session-5-unsupervised-learning-clustering-and-dimensionality-reduction","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we began our discussion on feature engineering and ended with a sneak peak into dimensionality reduction. This last topic deserves a whole session on its own because its use case is not limited to feature engineering! It can also be used as a tool for visualization, for noise filtering, and much more. In this session we discuss dimensionality reduction along with other unsupervised learning methods. Up until now, the only learning estimators we've looked at were supervised ones: estimators that predict labels based on training data. Here, however, we are interested in uncovering aspects of the data without reference to any known labels. The usefulness for these learners will become immediately apparent when we revist our wine quality models from Course 1, Session 7!","title":"Data Science Foundations, Session 5: Unsupervised Learning: Clustering and Dimensionality Reduction"},{"location":"S5_Unsupervised_Learning/#50-preparing-environment-and-importing-data","text":"back to top","title":"5.0 Preparing Environment and Importing Data"},{"location":"S5_Unsupervised_Learning/#501-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score from sklearn.mixture import GaussianMixture import seaborn as sns; sns.set() import copy /usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.","title":"5.0.1 Import Packages"},{"location":"S5_Unsupervised_Learning/#502-load-dataset","text":"back to top wine = pd.read_csv(\"https://raw.githubusercontent.com/wesleybeckner/\"\\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\") wine.dropna(inplace=True) wine['quality_label'] = wine['quality'].apply(lambda x: 'low' if x <=5 else 'med' if x <= 7 else 'high') wine['type_encoding'] = wine['type'].map({'red': 0, 'white': 1}) wine['quality_encoding'] = wine['quality_label'].map({'low':0, 'med': 1, 'high': 2}) wine.columns = wine.columns.str.replace(' ', '_') features = list(wine.columns[1:-1].values) features.remove('quality_label') features.remove('quality')","title":"5.0.2 Load Dataset"},{"location":"S5_Unsupervised_Learning/#51-principal-component-analysis","text":"back to top Principle Component Analysis or PCA is one of the most wide spread implementations of dimensionality reduction. In PCA, we find the principle components, or linear recombinations of the dimensions of the data, that best explain the variance of the data. There are mathematical arguments abound for describing how we analytically solve for the principle components and how they relate to other concepts in mathematics (like pythagorean theorem). We'll sidestep that conversation for now, and proceed to our pragmatic demonstrations, as we have done in the past.","title":"5.1 Principal Component Analysis"},{"location":"S5_Unsupervised_Learning/#511-introduction-to-pca","text":"back to top In the cell below, we have plotted acidity and density from our familiar wine dataset. fig, ax = plt.subplots(1, 1, figsize=(5,5)) wine.loc[wine['type'] == 'red'].plot(x='fixed_acidity', y='density', ax=ax, ls='', marker='.') <matplotlib.axes._subplots.AxesSubplot at 0x7f8cd1401910> X = wine.loc[wine['type'] == 'red'][['fixed_acidity', 'density']].values X[:5] array([[ 7.4 , 0.9978], [ 7.8 , 0.9968], [ 7.8 , 0.997 ], [11.2 , 0.998 ], [ 7.4 , 0.9978]]) The principal components, are actually the eigenvectors of the covariance matrix of the standardized data. We will perform this by hand to get an understanding. First we standardize the data from sklearn.preprocessing import StandardScaler X_std = StandardScaler().fit_transform(X) Then we compute the covariance matrix. There is a nice demonstration of computing covariance on stats quest . The covariance can be expressed as: cov(X,Y) = \\frac{1}{n^2}\\sum\\sum(x_i - x_j)(y_i - y_j) Every \\((x_i - x_j)(y_i - y_j)\\) is the area described by the rectangle between points \\(i\\) and \\(j\\), and we if we deem to color positive changes as red and negative ones as blue, we get a picture like the following: When we view all the pairwise interactions in aggregate we get a sense of how the areas of rectangles made by each observation influence the covariance: As a side note, the covariance term is the numerator in the pearsons correlation we covered last week: \\rho_{x,y} = \\frac{cov(X,Y)}{\\sigma_x\\sigma_y} import numpy as np mean_vec = np.mean(X_std, axis=0) cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1) print('Covariance matrix \\n%s' %cov_mat) Covariance matrix [[1.00062814 0.66831213] [0.66831213 1.00062814]] The principal components are found mathematically by determining the eigenvectors of the covariance matrix and sorting them by their egienvalues, or their explained variance. There is a mathematical proof 1 , 2 for why this works, but we will not cover that here. cov_mat = np.cov(X_std.T) # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals, eig_vecs = np.linalg.eig(cov_mat) print('Eigenvectors \\n%s' %eig_vecs) print('\\nEigenvalues \\n%s' %eig_vals) Eigenvectors [[-0.70710678 -0.70710678] [ 0.70710678 -0.70710678]] Eigenvalues [0.33231601 1.66894027] And now we are just going to sort our vectors by their eigenvalues to get the proper order of principal components: # Make a list of (eigenvalue, eigenvector) tuples eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))] # Sort the (eigenvalue, eigenvector) tuples from high to low eig_pairs.sort(key=lambda x: x[0], reverse=True) # Visually confirm that the list is correctly sorted by decreasing eigenvalues print('Eigenvalues in descending order:') for i in eig_pairs: print(i[0]) Eigenvalues in descending order: 1.6689402736104237 0.33231600779661163 For those of us who are familiar with eigenvectors, we should recognize that they are necessarily orthogonal to one another. This is good to know from the PCA point of view, because we wouldn't want the variance along one vector to be explained by another. Let's plot these vectors along with the standardized data: fig, ax = plt.subplots(1,1,figsize=(7,7)) ax.plot(X_std[:,0],X_std[:,1],ls='',marker='.', alpha=0.5) for vec, color in zip(range(eig_vecs.shape[0]),['orange', 'green']): ax.plot([np.mean(X_std[:,0]), (eig_pairs[vec][1][0]+np.mean(X_std[:,0]))*-eig_pairs[vec][0]], [np.mean(X_std[:,1]), (eig_pairs[vec][1][1]+np.mean(X_std[:,1]))*-eig_pairs[vec][0]], color=f'tab:{color}', linewidth=4) ax.plot([np.mean(X_std[:,0]), (eig_pairs[vec][1][0]+np.mean(X_std[:,0]))*eig_pairs[vec][0]], [np.mean(X_std[:,1]), (eig_pairs[vec][1][1]+np.mean(X_std[:,1]))*eig_pairs[vec][0]], color=f'tab:{color}', linewidth=4) ax.set_aspect('equal') ax.set_ylim(min(X_std[:,1]),max(X_std[:,1])) ax.set_xlim(min(X_std[:,0]),max(X_std[:,0])) (-2.141423705153984, 4.352327069615032) grandparent, spouse, daughter parable We indeed see that these vectors are orthogonal tot = sum(eig_vals) var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)] cum_var_exp = np.cumsum(var_exp) with plt.style.context('seaborn-whitegrid'): plt.figure(figsize=(7, 4)) plt.bar(range(2), var_exp, alpha=0.5, align='center', label='individual explained variance') plt.step(range(2), cum_var_exp, where='mid', label='cumulative explained variance') plt.ylabel('Explained variance ratio') plt.xlabel('Principal components') plt.legend(loc='center right') plt.tight_layout() Here I'm just putting my eigenvectors into the proper shape: matrix_w = np.hstack((eig_pairs[0][1].reshape(2,1), eig_pairs[1][1].reshape(2,1))) print('Matrix W:\\n', matrix_w) Matrix W: [[-0.70710678 -0.70710678] [-0.70710678 0.70710678]] In order to take the dot product, project, X_std onto the first two principal components: Y = X_std.dot(matrix_w) plt.scatter(Y[:,0],Y[:,1]) <matplotlib.collections.PathCollection at 0x7f8cc392c390> But we can avoid the fancy footwork and do all this in sklearn! from sklearn.decomposition import PCA pca = PCA(n_components=2) pca.fit(X_std) PCA(copy=True, iterated_power='auto', n_components=2, random_state=None, svd_solver='auto', tol=0.0, whiten=False) We see that the values we get are the same as for the hand-calculated eigenvalues and vectors print(pca.components_) [[ 0.70710678 0.70710678] [ 0.70710678 -0.70710678]] And the eigenvalues are under pca.explained_variance_ print(pca.explained_variance_) [1.66894027 0.33231601]","title":"5.1.1 Introduction to PCA"},{"location":"S5_Unsupervised_Learning/#512-pca-as-dimensionality-reduction","text":"back to top One obvious use case for PCA, is to drop the dimensions with lowest explained variance. Continuing with our acidity/density example this will look like the following from sklearn.decomposition import PCA pca = PCA(n_components=2) pca.fit(X_std) PCA(copy=True, iterated_power='auto', n_components=2, random_state=None, svd_solver='auto', tol=0.0, whiten=False) fig, ax = plt.subplots(1,2,figsize=(10,5)) ax[0].plot(X_std[:,0],X_std[:,1],ls='',marker='.', alpha=0.5) for vec, color in zip(range(eig_vecs.shape[0]),['orange', 'green']): ax[0].plot([np.mean(X_std[:,0]), (eig_pairs[vec][1][0]+np.mean(X_std[:,0]))*-eig_pairs[vec][0]], [np.mean(X_std[:,1]), (eig_pairs[vec][1][1]+np.mean(X_std[:,1]))*-eig_pairs[vec][0]], color=f'tab:{color}', linewidth=4) ax[0].plot([np.mean(X_std[:,0]), (eig_pairs[vec][1][0]+np.mean(X_std[:,0]))*eig_pairs[vec][0]], [np.mean(X_std[:,1]), (eig_pairs[vec][1][1]+np.mean(X_std[:,1]))*eig_pairs[vec][0]], color=f'tab:{color}', linewidth=4) ax[0].set_aspect('equal') ax[0].set_ylim(min(X_std[:,1]),max(X_std[:,1])) ax[0].set_xlim(min(X_std[:,0]),max(X_std[:,0])) ax[0].set_ylabel('Normalized density') ax[0].set_xlabel('Normalized acidity') ax[1].bar(range(2), var_exp, alpha=0.5, align='center', label='Individual') ax[1].step(range(2), cum_var_exp, where='mid', label='Cumulative') ax[1].set_ylabel('Explained variance ratio') ax[1].set_xlabel('Principal components') ax[1].legend() <matplotlib.legend.Legend at 0x7f8cc1869810> We can capture 80% of the explained variance along just the first principal component. What does this projection look like? # we set our components to 1 pca = PCA(n_components=1) pca.fit(X_std) # we then project the data onto the first PC # and then rebroadcast this transformation # back onto the orginal dimensions to see # what this looks like in terms of acidity/density X_pca = pca.inverse_transform(pca.transform(X_std)) # original data plt.scatter(X_std[:, 0], X_std[:, 1], alpha=0.2) # projected data plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.8) <matplotlib.collections.PathCollection at 0x7f8cc179d5d0> X_pca array([[-1.86755381, 3.11192011], [-0.38665917, -0.56358304], [-0.46921039, 0.03417659], ..., [ 2.49750969, -1.40757438], [ 3.20760565, -0.19795515], [ 1.57595901, -1.23607745]])","title":"5.1.2 PCA as Dimensionality Reduction"},{"location":"S5_Unsupervised_Learning/#513-pca-for-visualization","text":"back to top For classification tasks, PCA lends itself as a useful method for seeing how the classes separate on the highest variance dimensions of the data without consideration to the classes themselves, i.e. do our classes seperate out according to the other variables in the dataset. With our wine dataset, we see that the wine types fall out nicely along the first two principal components X = wine.select_dtypes(exclude=['object']).values X_std = StandardScaler().fit_transform(X) pca = PCA(n_components=2) pca.fit(X_std) X_pca = pca.transform(X_std) plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2, c=wine['type'].values, edgecolor='grey') plt.xlabel('First PC') plt.ylabel('Second PC') Text(0, 0.5, 'Second PC')","title":"5.1.3 PCA for visualization"},{"location":"S5_Unsupervised_Learning/#514-enrichment-pca-as-outlier-removal-and-noise-filtering","text":"back to top In some cases, it can be advantageous to use PCA as a method for outlier removal. There are many caveats to this that we will not discuss here. But for those who are interested, this is a great overview article. As for noise filteration, Vanderplas' DS handbook has a good section on the topic as does Guido/Muller's Intro to ML with Python by the same pusblisher.","title":"5.1.4 Enrichment: PCA as Outlier Removal and Noise Filtering"},{"location":"S5_Unsupervised_Learning/#515-pca-for-feature-engineering","text":"back to top Finally, PCA is actually a commonly used preprocessing technique for supervised machine learning models. In the next exercise, our goal will be to use PCA to generate a new set of features, to feed into our linear model.","title":"5.1.5 PCA for Feature Engineering"},{"location":"S5_Unsupervised_Learning/#exercise-1-pca-as-preprocessing-for-models","text":"Using the wine data, select any number of the first principal components and attemp to predict density for the red wine data. features.remove('type_encoding') wine[features] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide pH sulphates alcohol 0 7.0 0.270 0.36 20.7 0.045 45.0 170.0 3.00 0.45 8.8 1 6.3 0.300 0.34 1.6 0.049 14.0 132.0 3.30 0.49 9.5 2 8.1 0.280 0.40 6.9 0.050 30.0 97.0 3.26 0.44 10.1 3 7.2 0.230 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 4 7.2 0.230 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 ... ... ... ... ... ... ... ... ... ... ... 6491 6.8 0.620 0.08 1.9 0.068 28.0 38.0 3.42 0.82 9.5 6492 6.2 0.600 0.08 2.0 0.090 32.0 44.0 3.45 0.58 10.5 6494 6.3 0.510 0.13 2.3 0.076 29.0 40.0 3.42 0.75 11.0 6495 5.9 0.645 0.12 2.0 0.075 32.0 44.0 3.57 0.71 10.2 6496 6.0 0.310 0.47 3.6 0.067 18.0 42.0 3.39 0.66 11.0 6463 rows \u00d7 10 columns # Code Cell for Exercise 1 X = wine[features].values X_std = StandardScaler().fit_transform(X) pca = PCA(n_components=2) pca.fit(X_std) X_pca = pca.transform(X_std) plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2, c=wine['type'].values, edgecolor='grey') plt.xlabel('First PC') plt.ylabel('Second PC') Text(0, 0.5, 'Second PC') y = StandardScaler().fit_transform(wine['density'].values.reshape(-1,1)) lr = LinearRegression().fit(X_pca, y) r2_score(y, lr.predict(X_pca)), 0.382963127385076","title":"\ud83c\udfcb\ufe0f Exercise 1: PCA as Preprocessing for Models"},{"location":"S5_Unsupervised_Learning/#52-k-means-clustering","text":"back to top We now embark on a second class of unsupervised learning techinques: clustering. The K-means algorithm works under two assumptions: * every cluster can be defined by an arithmetic mean or cluster center * each point is closer to one arithmetic center than the other centers Let's turn back to our wine dataset: X = wine.select_dtypes(exclude=['object']).values X_std = StandardScaler().fit_transform(X) pca = PCA(n_components=2) pca.fit(X_std) X_pca = pca.transform(X_std) plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2, c='grey') plt.xlabel('First PC') plt.ylabel('Second PC') Text(0, 0.5, 'Second PC') It is fairly obvious under these two dimensions that there are two clusters. The K-means algorithm automatically pics this out from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=2) kmeans.fit(X_pca) y_kmeans = kmeans.predict(X_pca) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, s=50, alpha=0.5, edgecolor='grey', cmap='viridis') centers = kmeans.cluster_centers_ plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5); wa-la! Our simple unsupervised algorithm does a pretty good job of differentiating between the red and white wine clusters.","title":"5.2 K-Means Clustering"},{"location":"S5_Unsupervised_Learning/#521-the-algorithm-expectation-maximization","text":"back to top We won't go too far into the implementation of the underlying algorithm here. In its basic steps, however, it performs the following: Initialize cluster centers (random guess) Then repeat: E-Step: assign points to the nearest center (arithmetic distance) M-step: set the new center point for each cluster according to the mean of it's datapoint members More information on K-means algorithm can be explored here","title":"5.2.1 The Algorithm: Expectation-Maximization"},{"location":"S5_Unsupervised_Learning/#522-limitations","text":"back to top A few brief notes on limitations: the global optimum may not be achieved (no guarantee of finding the overall best solution) the number of clusters must be guessed beforehand cluster boundaries are unavoidably linear and the cluster assignments are unavoidably circular can be slow for large datasets cluster assignments are non probabilistic 3 and 5 motivate our next section, Gaussian Mixture Models","title":"5.2.2 Limitations"},{"location":"S5_Unsupervised_Learning/#53-gaussian-mixture-models","text":"back to top in the simplest case, GMMs can be used in the same way as K-means from sklearn.mixture import GaussianMixture gmm = GaussianMixture(n_components=2).fit(X_pca) labels = gmm.predict(X_pca) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=40, cmap='viridis', alpha=0.2, edgecolor='grey'); But because GMM is a probablistic model, we can visualize the certainty with which we assign these clusters. probs = gmm.predict_proba(X_pca) print(probs[5:20].round(3)) [[1. 0. ] [1. 0. ] [1. 0. ] [1. 0. ] [1. 0. ] [0.992 0.008] [1. 0. ] [0.99 0.01 ] [1. 0. ] [1. 0. ] [1. 0. ] [0.999 0.001] [1. 0. ] [1. 0. ] [1. 0. ]] # convert probs to 1 dimension probs.max(1) array([1. , 0.99985029, 0.99996753, ..., 0.99997744, 0.99999985, 0.98796517]) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=probs.max(1), s=40, cmap='Blues', alpha=0.5, edgecolor='grey');","title":"5.3 Gaussian Mixture Models"},{"location":"S5_Unsupervised_Learning/#531-generalizing-e-m-for-gmms","text":"back to top The algorithm for GMMs is very similar to K-means, but now the EM steps are probablistic 1 2 Initialize cluster centers (random guess) Then repeat: E-Step: assign points their probability of belonging to every cluster M-step: set the new center point for each cluster according to the probabilities of all datapoints","title":"5.3.1 Generalizing E-M for GMMs"},{"location":"S5_Unsupervised_Learning/#532-gmms-as-a-data-generator","text":"back to top One particularly fascinating application of GMMs is to use them to generate new data that is similar to the data on which they are modeled. Let's take an example. We're going to use sklearn make_circles function to create some arbitrary data that has a complex relationship along two different axes. # some helper functions borrowed from Jake Vanderplas with a few minor tweaks # https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html from matplotlib.patches import Ellipse def draw_ellipse(position, covariance, ax=None, **kwargs): \"\"\"Draw an ellipse with a given position and covariance\"\"\" ax = ax or plt.gca() # Convert covariance to principal axes if covariance.shape == (2, 2): U, s, Vt = np.linalg.svd(covariance) angle = np.degrees(np.arctan2(U[1, 0], U[0, 0])) width, height = 2 * np.sqrt(s) else: angle = 0 width, height = 2 * np.sqrt(covariance) # Draw the Ellipse for nsig in range(1, 4): ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs)) def plot_gmm(gmm, X, label=True, ax=None, data_alpha=1): ax = ax or plt.gca() labels = gmm.fit(X).predict(X) if label: ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2, alpha=data_alpha) else: ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2, alpha=data_alpha) ax.axis('equal') w_factor = 0.2 / gmm.weights_.max() for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_): draw_ellipse(pos, covar, alpha=w * w_factor) from sklearn.datasets import make_circles as gen X, y = gen(200, noise=0.02, random_state=42) plt.scatter(X[:, 0], X[:, 1]); As a side note, as a clustering model, the GMM is not particularly useful: gmm2 = GaussianMixture(n_components=2, covariance_type='full', random_state=0) plot_gmm(gmm2, X) But if we use the GMM as a density estimator on the underlying data, rather than as a clustering algorithm, we find a completely different use case: gmm16 = GaussianMixture(n_components=16, covariance_type='full', random_state=0) plot_gmm(gmm16, X, label=False) Now, with the distributions drawn, we can assemble entirely new data: Xnew = gmm16.sample(400)[0] plt.scatter(Xnew[:, 0], Xnew[:, 1]);","title":"5.3.2 GMMs as a Data Generator"},{"location":"S5_Unsupervised_Learning/#5321-determining-the-number-of-components","text":"back to top Let's think back to session 1 on model selection. How might we determine the best number of components? A couple analytic approaches that we have not much yet discussed, are the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The important thing to note from each of these is that they penalize added complexity to the models, and we would like to minimize their estimated prediction error. n_components = np.arange(1, 42) models = [GaussianMixture(n, covariance_type='full', random_state=42).fit(X) for n in n_components] plt.plot(n_components, [m.bic(X) for m in models], label='BIC') plt.plot(n_components, [m.aic(X) for m in models], label='AIC') plt.legend(loc='best') plt.xlabel('n_components'); plt.ylabel('est. prediction error') Text(0, 0.5, 'est. prediction error') Both AIC and BIC capture a local minimum on the order of 6 Gaussians. This is where the model captures the overall circular modality of the data. However it totally misses the circumscribed nature of the two circles. Increasing this number to 30-40 gaussians captures this feature of the data. The AIC score reflects this while the BIC score (although captures a local minima in the area) does not define this as a global optimum. gmmNew = GaussianMixture(n_components=40, covariance_type='full', random_state=0) plot_gmm(gmmNew, X, label=True, data_alpha=0) Xnew = gmmNew.sample(400)[0] plt.scatter(Xnew[:, 0], Xnew[:, 1]);","title":"5.3.2.1 Determining the number of components"},{"location":"S5_Unsupervised_Learning/#exercise-2-determine-number-of-components-for-circular-moons","text":"Repeat the above, this time using sklearn.datasets.make_moons # Code Cell for Exercise 2 from sklearn.datasets import make_moons as gen X, y = gen(200, noise=0.02, random_state=42) n_components = np.arange(1, 42) models = [GaussianMixture(n, covariance_type='full', random_state=42).fit(X) for n in n_components] plt.plot(n_components, [m.bic(X) for m in models], label='BIC') plt.plot(n_components, [m.aic(X) for m in models], label='AIC') plt.legend(loc='best') plt.xlabel('n_components'); plt.ylabel('est. prediction error') Text(0, 0.5, 'est. prediction error') gmm_moon = GaussianMixture(n_components=40, covariance_type='full', random_state=0) plot_gmm(gmm_moon, X) Xnew = gmm_moon.sample(400)[0] plt.scatter(Xnew[:, 0], Xnew[:, 1]);","title":"\ud83c\udfcb\ufe0f Exercise 2: Determine Number of Components for Circular Moons"},{"location":"S5_Unsupervised_Learning/#references","text":"","title":"References"},{"location":"S5_Unsupervised_Learning/#pca","text":"Intuitive PCA PCA and Eigenvectors/values","title":"PCA"},{"location":"S5_Unsupervised_Learning/#gmm","text":"GMMs Explained Derive GMM Exercise","title":"GMM"},{"location":"S6_Bagging/","text":"Data Science Foundations, Session 6: Bagging: Decision Trees and Random Forests Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're going back to the topic of supervised learning models. These models however, belong to a special class of methods called bagging, or bootstrap aggregation. Bagging is an ensemble learning method. In this method, many weak classifiers cast their votes in a general election for the final prediction. The weak learners that random forests are made of, are called decision trees. 6.0 Preparing Environment and Importing Data back to top 6.0.1 Import Packages back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier import seaborn as sns; sns.set() import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact, interactive, widgets from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn import metrics 6.0.2 Load Dataset back to top margin = pd.read_csv('https://raw.githubusercontent.com/wesleybeckner/'\\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv') print(margin.shape, end='\\n\\n') display(margin.head()) (1668, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 We're going to recreate the same operations we employed in Session 2, Feature Engineering: # identify categorical columns cat_cols = margin.columns[:7] # create the encoder object enc = OneHotEncoder() # grab the columns we want to convert from strings X_cat = margin[cat_cols] # fit our encoder to this data enc.fit(X_cat) onehotlabels = enc.transform(X_cat).toarray() X_num = margin[['KG']] X_truf = np.concatenate((onehotlabels, X_num.values),axis=1) # grab our y data y_truf = margin['EBITDA/KG'].values Lastly, to create a classification task, we're going to identify high, med, and low value products: print('bad less than: {:.2f}'.format(margin[margin.columns[-1]].quantile(.25)), end='\\n\\n') print('low less than: {:.2f}'.format(margin[margin.columns[-1]].quantile(.5)), end='\\n\\n') print('med less than: {:.2f}'.format(margin[margin.columns[-1]].quantile(.75)), end='\\n\\n') pd.DataFrame(margin[margin.columns[-2]]).boxplot(showfliers=False) bad less than: 0.12 low less than: 0.22 med less than: 0.35 <matplotlib.axes._subplots.AxesSubplot at 0x7eff2a001b90> margin['profitability'] = margin[margin.columns[-1]].apply( lambda x: 'bad' if x <= margin[margin.columns[-1]].quantile(.25) else 'low' if x <= margin[margin.columns[-1]].quantile(.50) else 'med' if x <= margin[margin.columns[-1]].quantile(.75) else 'high') margin['profitability'].hist() <matplotlib.axes._subplots.AxesSubplot at 0x7eff29f68b90> class_profit = {'bad': 0, 'low': 1, 'med': 2, 'high': 3} y_truf_class = margin['profitability'].map(class_profit).values margin['profitability_encoding'] = y_truf_class margin.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG profitability profitability_encoding 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 high 3 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 med 2 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 low 1 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 med 2 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 high 3 6.1 Decision Trees back to top In essence, a decision tree is a series of binary questions. Let's begin this discussion by talking about how we make decision trees in sklearn. 6.1.1 Creating a Decision Tree back to top from sklearn import tree X = [[0, 0], [1, 1]] y = [0, 1] clf = tree.DecisionTreeClassifier() clf = clf.fit(X, y) After fitting the model we can use the predict method to show the output for a sample clf.predict([[2., 2.]]) array([1]) Similar to what we saw with GMMs, we also have access to the probabilities of the outcomes: clf.predict_proba([[2., 2.]]) array([[0., 1.]]) Let's now go on to using visual strategies to interpreting trees. 6.1.2 Interpreting a Decision Tree back to top Throughout today, we will discuss many ways to view both a single tree and a random forest of trees. 6.1.2.1 Node & Branch Diagram back to top We can visualize the decision tree: tree.plot_tree(clf) [Text(167.4, 163.07999999999998, 'X[1] <= 0.5\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'), Text(83.7, 54.360000000000014, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'), Text(251.10000000000002, 54.360000000000014, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]')] or, more prettily: import graphviz dot_data = tree.export_graphviz(clf, out_file=None) graph = graphviz.Source(dot_data) graph The gini label, also known as Gini impurity , is a measure of how often a sample passing through the node would be incorrectly labeled if it was randomly assigned a label based on the proportion of all labels passing through the node. So it is a measure of the progress of our tree. Let's take a more complex example from sklearn.datasets import make_classification as gen X, y = gen(random_state=42) Let's inspect our generated data: print(X.shape) print(y.shape) y[:5] # a binary classification (100, 20) (100,) array([0, 0, 1, 1, 0]) And now let's train our tree: clf = tree.DecisionTreeClassifier() clf = clf.fit(X, y) How do we interpret this graph? dot_data = tree.export_graphviz(clf, out_file=None) graph = graphviz.Source(dot_data) graph Can we confirm the observations in the tree by manually inspecting X and y? y[X[:,10] < .203] array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) We can confirm the gini score of the top left node by hand... scr = [] for j in range(1000): y_pred = [0 if random.random() > ( 3/52 ) else 1 for i in range(52)] y_true = [0 if random.random() > ( 3/52 ) else 1 for i in range(52)] scr.append(mean_squared_error(y_pred,y_true)) np.mean(scr) 0.10994230769230771 Let's take a look at this with our truffle dataset Vary the parameter max_depth what do you notice? Does the term greedy mean anything to you? clf = tree.DecisionTreeClassifier(max_depth=1) clf.fit(X_truf, y_truf_class) DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=1, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=None, splitter='best') And now lets look at the graph: dot_data = tree.export_graphviz(clf, out_file=None) graph = graphviz.Source(dot_data) graph What is X[4] ??? # It's those tasty sponge cake truffles! enc.get_feature_names()[4] 'x0_Sponge' This is one great aspect of decision trees, their interpretability . We will perform this analysis again, for now, let's proceed with simpler datasets while exploring the features of decision trees. 6.1.2.1 Decision Boundaries back to top Let's make some random blobs from sklearn.datasets import make_blobs as gen X, y = gen(random_state=42) plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis') <matplotlib.collections.PathCollection at 0x7eff29e6ea90> Let's call up our Classifier again, this time setting the max_depth to two clf = tree.DecisionTreeClassifier(max_depth=2, random_state=42) clf = clf.fit(X, y) # Parameters plot_step = 0.02 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) cs = plt.contourf(xx, yy, Z, cmap='viridis', alpha=0.2) plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', edgecolor='grey', alpha=0.9) <matplotlib.collections.PathCollection at 0x7eff2a0a4bd0> dot_data = tree.export_graphviz(clf, out_file=None) graph = graphviz.Source(dot_data) graph We can see from the output of this graph, that the tree attempts to create the class boundaries as far from the cluster centers as possible. What happens when these clusters overlap? X, y = gen(random_state=42, cluster_std=3) plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis') <matplotlib.collections.PathCollection at 0x7eff2a114150> Let's go ahead and write our plot into a function def plot_tree(X, clf): plot_step = 0.02 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) cs = plt.contourf(xx, yy, Z, cmap='viridis', alpha=0.2) plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', edgecolor='grey', alpha=0.9) return plt We see that the boundaries mislabel some points fig = plot_tree(X, clf) 6.1.3 Overfitting a Decision Tree back to top Let's increase the max_depth clf = tree.DecisionTreeClassifier(max_depth=5, random_state=42) clf = clf.fit(X, y) plot_tree(X, clf) <module 'matplotlib.pyplot' from '/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py'> What we notice is that while the model accurately predicts the training data, we see some spurious labels, noteably the trailing purple bar that extends into the otherwise green region of the data. This is a well known fact about decision trees, that they tend to overfit their training data. In fact, this is a major motivation for why decision trees, a weak classifier, are conveniently packaged into ensembles. We combine the idea of bootstrapping, with decision trees, to come up with an overall better classifier. \ud83c\udfcb\ufe0f Exercise 1: Minimize Overfitting Repeat 4.1.3 with different max_depth settings, also read the docstring and play with any other hyperparameters available to you. What settings do you feel minimize overfitting? # Code Cell for 1 6.2 Random Forests and Bagging back to top 6.2.1 What is Bagging? back to top Bagging , or Bootstrap AGGregation is the process of creating subsets of your data and training separate models on them, and using the aggregate votes of the models to make a final prediction. Bootstrapping is a topic in and of itself that we will just touch on here. Without going through the statistical rigor of proof, bootstrapping, or sampling from your observations with replacement, simulates having drawn additional data from the true population. We use this method to create many new datasets that are then used to train separate learners in parallel. This overall approach is called Bagging . A Random Forest is an instance of bagging where the separate learners are decision trees. 6.2.2 Random Forests for Classification back to top from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier tree = DecisionTreeClassifier() bag = BaggingClassifier(tree, n_estimators=10, max_samples=0.8, random_state=1) bag.fit(X, y) plot_tree(X, bag) <module 'matplotlib.pyplot' from '/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py'> In the above, we have bootstrapped by providing each individual tree with 80% of the population data. In practice, Random Forests can achieve even better results by randomizing how the individual classifiers are constructed. In fact there are many unique methods of training individual trees and you can learn more about them here . For now, know that it is better in practice to implement the RandomForests method in sklearn rather than bag individual trees yourself. from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier(n_estimators=10, random_state=2) clf = clf.fit(X, y) plot_tree(X, clf) <module 'matplotlib.pyplot' from '/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py'> 6.2.2.1 Interpreting a Random Forest back to top Let's revisit our truffle dataset again, this time with random forests # fit the model clf = RandomForestClassifier(n_estimators=10, min_samples_leaf=6) clf = clf.fit(X_truf, y_truf_class) We get a fairly high accuracy when our min_samples_leaf is low and an accuracy that leaves room for improvement when min_samples_leaf is high. This indicates to us the model may be prown to overfitting if we are not careful: accuracy_score(clf.predict(X_truf), y_truf_class) 0.6007194244604317 We need to do some housekeeping to get back the names and categories the features were pulled from: feat_dict = dict(zip([f'x{i}' for i in range(len(cat_cols))], cat_cols.str.replace(' ', '_'))) print(feat_dict) feats = [] # enc.get_feature_names unpacks like so: `x4_White` where # x<N> is an artifact of the original dimensions fed to enc() # I am unpacking the feature names with the original dataframe header for row in enc.get_feature_names(): feats.append('{} {}'.format(feat_dict[row.split('_')[0]], row.split('_')[1])) # feats will be used to visualize the feature importances feats[:5] {'x0': 'Base_Cake', 'x1': 'Truffle_Type', 'x2': 'Primary_Flavor', 'x3': 'Secondary_Flavor', 'x4': 'Color_Group', 'x5': 'Customer', 'x6': 'Date'} ['Base_Cake Butter', 'Base_Cake Cheese', 'Base_Cake Chiffon', 'Base_Cake Pound', 'Base_Cake Sponge'] The feature importances are stored in clf.feature_importances_ . These are calculated from the Mean Decrease in Impurity or MDI also called the Gini Importance . It is the sum of the number of nodes across all trees that include the feature, weighted by the number of samples passing through the node. One downside of estimating feature importance in this way is that it doesn't play well with highly cardinal features (features with many unique values such as mailing addresses, are highly cardinal features) # grab feature importances imp = clf.feature_importances_ # their std std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0) # create new dataframe feat = pd.DataFrame([feats + [\"KG\"], imp, std]).T feat.columns = ['name', 'importance', 'std'] feat = feat.sort_values('importance', ascending=False) feat = feat.reset_index(drop=True) feat.columns = ['feature', 'importance', 'std'] I'm going to use plotly to create this chart: How does feature importance change when we change the minimum leaf size from 2 to 6? px.bar(feat, x='feature', y='importance', error_y='std') if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"318eb430-e0f4-4b77-aa05-3c077a236bea\")) { Plotly.newPlot( '318eb430-e0f4-4b77-aa05-3c077a236bea', [{\"alignmentgroup\": \"True\", \"error_y\": {\"array\": [0.07555343650428877, 0.04063238643408586, 0.05961762914014548, 0.036511772297213245, 0.0290208966535553, 0.027395688661575626, 0.020452704347556663, 0.03393435310671577, 0.028642943310511235, 0.012532926656412787, 0.03556066576596446, 0.01583846311062793, 0.015769424865656922, 0.022456972121037738, 0.02357794745152038, 0.01442052676018209, 0.016310850227497925, 0.020135987095791785, 0.01966912273385237, 0.0172252292386212, 0.016188291852075806, 0.011232957255776955, 0.011752740590544949, 0.014215519364083693, 0.013231561004786502, 0.011128839190837566, 0.006571133069294302, 0.009489276856556898, 0.007960061906913968, 0.010480916770311037, 0.01077200787919219, 0.00994444700093636, 0.01698793108921301, 0.012224243018118919, 0.0074065694102534866, 0.01145262818011683, 0.014493721007728682, 0.008521135565720254, 0.012128874609229678, 0.00618660142472066, 0.008750592417062431, 0.01083766822715062, 0.006589032274060321, 0.005665682686641886, 0.007104402814719823, 0.009642691848896686, 0.005820366803373852, 0.004582819751796309, 0.006191175003169528, 0.010780497029991285, 0.009383117993507828, 0.01064557976533365, 0.007971400075341879, 0.007626220709434621, 0.004528508932574483, 0.007978753956889023, 0.00598655286524879, 0.004724575698947667, 0.006907133033972632, 0.012019731255860346, 0.0069698073631939425, 0.0042979530563582525, 0.00880028649397044, 0.006169496245711782, 0.011543630256147902, 0.011489894256293558, 0.004546766268214796, 0.0039653606347905285, 0.005142839080282742, 0.004941368568974321, 0.004792637324806003, 0.00421720688017499, 0.0035935515587245948, 0.008488082736005341, 0.006269385473890268, 0.006598567537142401, 0.003498034328041033, 0.003873584657620095, 0.0037805701429140014, 0.002515794877177775, 0.006673028932914227, 0.0023320341584585798, 0.00708673630649601, 0.004711100803629174, 0.003424765080306881, 0.0026921725725499285, 0.006302660607068695, 0.0032923995185687254, 0.0030791354423140693, 0.0030119835753018034, 0.0031442289102776937, 0.0020600908472405127, 0.002793480291008204, 0.00193979455345233, 0.0030857822778379788, 0.0035748079758318985, 0.0032705986017104887, 0.002272192880069807, 0.0022907382568384494, 0.0032671244214112233, 0.0019998154929131923, 0.0018820524312719476, 0.0019111629917143336, 0.0017978767817748138, 0.002340223707926282, 0.0016899808466311807, 0.0016614874356055674, 0.0015772015772983537, 0.0009475672660661238, 0.0011080745981432952, 0.0008970952531810846, 0.0005748636888217817, 0.0005470026863239088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"feature=%{x}<br>importance=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"Base_Cake Sponge\", \"Base_Cake Chiffon\", \"Base_Cake Pound\", \"KG\", \"Primary_Flavor Butter Toffee\", \"Base_Cake Tiramisu\", \"Secondary_Flavor Egg Nog\", \"Base_Cake Butter\", \"Base_Cake Cheese\", \"Customer Slugworth\", \"Color_Group Olive\", \"Customer Perk-a-Cola\", \"Color_Group White\", \"Secondary_Flavor Black Cherry\", \"Primary_Flavor Doughnut\", \"Customer Zebrabar\", \"Truffle_Type Candy Outer\", \"Primary_Flavor Orange Pineapple\\tP\", \"Primary_Flavor Kettle Corn\", \"Color_Group Amethyst\", \"Truffle_Type Chocolate Outer\", \"Secondary_Flavor Black Currant\", \"Color_Group Teal\", \"Secondary_Flavor Wild Cherry Cream\", \"Secondary_Flavor Cucumber\", \"Primary_Flavor Toasted Coconut\", \"Color_Group Opal\", \"Customer Fickelgruber\", \"Color_Group Black\", \"Primary_Flavor Lemon Custard\", \"Secondary_Flavor Tangerine\", \"Secondary_Flavor Lemon\", \"Primary_Flavor Dill Pickle\", \"Primary_Flavor Spice\", \"Customer Dandy's Candies\", \"Secondary_Flavor Whipped Cream\", \"Primary_Flavor Orange\", \"Primary_Flavor Ginger Lime\", \"Secondary_Flavor Papaya\", \"Primary_Flavor Irish Cream\", \"Secondary_Flavor Apple\", \"Primary_Flavor Plum\", \"Primary_Flavor Mango\", \"Secondary_Flavor Rock and Rye\", \"Color_Group Tiffany\", \"Secondary_Flavor Peppermint\", \"Primary_Flavor Grand Mariner\", \"Primary_Flavor Caramel Cream\", \"Primary_Flavor Cheesecake\", \"Truffle_Type Jelly Filled\", \"Primary_Flavor Sassafras\", \"Primary_Flavor Horchata\", \"Primary_Flavor Acai Berry\", \"Primary_Flavor Watermelon\", \"Date 12/2020\", \"Primary_Flavor Lemon\", \"Primary_Flavor Pecan\", \"Secondary_Flavor Kiwi\", \"Primary_Flavor Cream Soda\", \"Primary_Flavor Gingersnap\", \"Primary_Flavor Ginger Ale\", \"Secondary_Flavor Banana\", \"Primary_Flavor Pink Lemonade\", \"Secondary_Flavor Pear\", \"Primary_Flavor Amaretto\", \"Primary_Flavor Blueberry\", \"Primary_Flavor Bavarian Cream\", \"Primary_Flavor Black Licorice\", \"Primary_Flavor Cherry Cola\", \"Primary_Flavor Coffee\", \"Color_Group Burgundy\", \"Date 7/2020\", \"Date 11/2020\", \"Secondary_Flavor Rum\", \"Secondary_Flavor Apricot\", \"Primary_Flavor Vanilla\", \"Date 10/2020\", \"Date 8/2020\", \"Secondary_Flavor Vanilla\", \"Date 9/2020\", \"Color_Group Rose\", \"Date 4/2020\", \"Primary_Flavor Chocolate Mint\", \"Primary_Flavor Orange Brandy\", \"Primary_Flavor Sour\", \"Date 2/2020\", \"Color_Group Slate\", \"Date 3/2020\", \"Secondary_Flavor Mojito\", \"Date 5/2020\", \"Secondary_Flavor Mixed Berry\", \"Secondary_Flavor Fuzzy Navel\", \"Primary_Flavor Raspberry Ginger Ale\", \"Primary_Flavor Cherry Cream Spice\", \"Primary_Flavor Lemon Bar\", \"Primary_Flavor Wild Cherry Cream\", \"Secondary_Flavor Ginger Beer\", \"Primary_Flavor Apricot\", \"Secondary_Flavor Dill Pickle\", \"Secondary_Flavor Mango\", \"Primary_Flavor Creme de Menthe\", \"Date 1/2020\", \"Date 6/2020\", \"Secondary_Flavor Butter Rum\", \"Primary_Flavor Chocolate\", \"Primary_Flavor Butter Pecan\", \"Secondary_Flavor Passion Fruit\", \"Primary_Flavor Wintergreen\", \"Primary_Flavor Pear\", \"Primary_Flavor Fruit Punch\", \"Secondary_Flavor Toffee\", \"Primary_Flavor Margarita\", \"Color_Group Taupe\", \"Primary_Flavor Birch Beer\", \"Primary_Flavor Butter Milk\", \"Color_Group Citrine\", \"Primary_Flavor Coconut\", \"Secondary_Flavor Tutti Frutti\", \"Secondary_Flavor Hazelnut\"], \"xaxis\": \"x\", \"y\": [0.11070519650695783, 0.0639051536976566, 0.0548603760743442, 0.04690552959956567, 0.030875729545272637, 0.02964743757949146, 0.02903731620006769, 0.027549820016608686, 0.024187978949675365, 0.023614207046533706, 0.023004895620422038, 0.020589909133373552, 0.019963584487051135, 0.014793374515086494, 0.014393892077033574, 0.013853128581716494, 0.013244615846047074, 0.012690554084720291, 0.01259021885646476, 0.012404368567974087, 0.0113187384045495, 0.011071708910919128, 0.010741822613075441, 0.010696258810503114, 0.01036397151172899, 0.010153740726406372, 0.009873025937418815, 0.009624755083125741, 0.00939787547337474, 0.009324827381141128, 0.009223258704344116, 0.00920180252864671, 0.009030231263778518, 0.008748134672159761, 0.008650390754265581, 0.00857621547469525, 0.008182984461145851, 0.007935677327919664, 0.007710024862021607, 0.007655626597286043, 0.006909301064429637, 0.0068089928959846535, 0.006280078656786092, 0.0062527552424586595, 0.005937366089078937, 0.005569142846762498, 0.005395143076710668, 0.005349940638283773, 0.0053246203141319795, 0.005162744809445722, 0.00513549758800123, 0.005131023056676936, 0.00485863529267419, 0.004618572850759596, 0.004385632341887418, 0.004138155071736413, 0.004092217766380393, 0.004063384055128564, 0.004010104372789467, 0.004006577085286783, 0.00400548765617934, 0.00388998413390825, 0.0038868227387336947, 0.003858150268472716, 0.0038478767520493035, 0.0038428634866371617, 0.00373893601154158, 0.003709740677574182, 0.003654009077597494, 0.00348183888390368, 0.0034621232538648568, 0.0034131418308362077, 0.003344958987577941, 0.0032455102404200156, 0.003051238798750663, 0.0028849097281198836, 0.0028289898361587365, 0.0026498763977929355, 0.0026473717061639046, 0.002544288234247985, 0.002536787060152542, 0.0023816342255907124, 0.0023622454354986703, 0.002355487721202827, 0.0023118590899479387, 0.002239091551568516, 0.002100886869022899, 0.002075493235042351, 0.001917413173718334, 0.0018839487672445655, 0.001580612376283172, 0.0013055871972177259, 0.0012863009671007484, 0.0012487062252696925, 0.0012279317440148873, 0.0011916026586106328, 0.0011551536899490362, 0.0011197945637304875, 0.0011095070716949365, 0.0010890414738037415, 0.0010807649217126744, 0.0009599570923495142, 0.0009534468564891095, 0.0008997626619987524, 0.0007904775752950208, 0.0005633269488770604, 0.000553829145201856, 0.0005257338590994512, 0.0004349081329954505, 0.00036935819938109846, 0.0002990317510603616, 0.00019162122960726061, 0.00018233422877463634, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"yaxis\": \"y\"}], {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"feature\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"importance\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('318eb430-e0f4-4b77-aa05-3c077a236bea'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; We can then go and look at the different EBITDAs when selecting for each of these features: def ebitda_comp(feature=feat['feature']): if len(feature.split(' ')) > 1: group = feature.split(' ')[0].replace('_', ' ') sel = \" \".join(feature.split(' ')[1:]) pos = margin.loc[(margin[group] == sel)]['EBITDA/KG'].median() neg = margin.loc[~(margin[group] == sel)]['EBITDA/KG'].median() print(\"with: {:.2e}\".format(pos)) print(\"without: {:.2e}\".format(neg)) else: pass interact(ebitda_comp) interactive(children=(Dropdown(description='feature', options=('Base_Cake Sponge', 'Base_Cake Chiffon', 'Base_\u2026 <function __main__.ebitda_comp> This is an example of how the interpretability of the random forest ensemble approach can be highly valuable. 6.2.3 Random Forests for Regression back to top from sklearn.ensemble import RandomForestRegressor clf = RandomForestRegressor(n_estimators=10) Because our labels on our blob data were numerical, we can apply and view the estimator in the same way: clf = clf.fit(X, y) plot_tree(X, clf) <module 'matplotlib.pyplot' from '/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py'> I want to revisit a dataset we brought up in Session 2 on feature engineering: t = np.linspace(0,5,200) w = 5 h = 4 s = 4 * h / np.pi * (np.sin(w*t) + np.sin(3*w*t)/3 + np.sin(5*w*t)/5) F = np.fft.fft(s) freq = np.fft.fftfreq(t.shape[-1]) fig, ax = plt.subplots(1,2,figsize=(10,5)) ax[0].plot(t,s) ax[0].plot(t,np.sin(w*t), ls='--') ax[0].plot(t,np.sin(w*t*3)/3, ls='--') ax[0].plot(t,np.sin(w*t*5)/5, ls='--') ax[0].set_title('Time Domain') # tells us about the amplitude of the component at the # corresponding frequency magnitude = np.sqrt(F.real**2 + F.imag**2) ax[1].plot(freq, magnitude) ax[1].set_xlim(0,.15) ax[1].set_title('Frequency Domain') Text(0.5, 1.0, 'Frequency Domain') Let's see if a random forest regression model can capture the wave behavior of the time-series data clf = RandomForestRegressor(n_estimators=10) clf.fit(t.reshape(-1,1),s) RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) fig, ax = plt.subplots(1,1,figsize=(10,5)) ax.plot(t,s) ax.plot(t,clf.predict(t.reshape(-1,1))) [<matplotlib.lines.Line2D at 0x7eff29588690>] Nice! without specifying any perdiodicity, the random forest does a good job of embedding this periodicity in the final output. \ud83c\udfcb\ufe0f Extended Exercise: Practice with Random Forests With the wine dataset: predict: quality, quality group use train_test_split create a learning curve of train/test score vs model complexity for your random forest model(s) # Code Cell for Exercise 4.2.4 wine = pd.read_csv(\"https://raw.githubusercontent.com/wesleybeckner/\"\\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\") wine.dropna(inplace=True) wine['quality_label'] = wine['quality'].apply(lambda x: 'low' if x <=5 else 'med' if x <= 7 else 'high') wine['type_encoding'] = wine['type'].map({'red': 0, 'white': 1}) wine['quality_encoding'] = wine['quality_label'].map({'low':0, 'med': 1, 'high': 2}) wine.columns = wine.columns.str.replace(' ', '_') features = list(wine.columns[1:-1].values) features.remove('quality_label') features.remove('quality')","title":"Bagging"},{"location":"S6_Bagging/#data-science-foundations-session-6-bagging-decision-trees-and-random-forests","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're going back to the topic of supervised learning models. These models however, belong to a special class of methods called bagging, or bootstrap aggregation. Bagging is an ensemble learning method. In this method, many weak classifiers cast their votes in a general election for the final prediction. The weak learners that random forests are made of, are called decision trees.","title":"Data Science Foundations, Session 6: Bagging: Decision Trees and Random Forests"},{"location":"S6_Bagging/#60-preparing-environment-and-importing-data","text":"back to top","title":"6.0 Preparing Environment and Importing Data"},{"location":"S6_Bagging/#601-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier import seaborn as sns; sns.set() import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact, interactive, widgets from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn import metrics","title":"6.0.1 Import Packages"},{"location":"S6_Bagging/#602-load-dataset","text":"back to top margin = pd.read_csv('https://raw.githubusercontent.com/wesleybeckner/'\\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv') print(margin.shape, end='\\n\\n') display(margin.head()) (1668, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 We're going to recreate the same operations we employed in Session 2, Feature Engineering: # identify categorical columns cat_cols = margin.columns[:7] # create the encoder object enc = OneHotEncoder() # grab the columns we want to convert from strings X_cat = margin[cat_cols] # fit our encoder to this data enc.fit(X_cat) onehotlabels = enc.transform(X_cat).toarray() X_num = margin[['KG']] X_truf = np.concatenate((onehotlabels, X_num.values),axis=1) # grab our y data y_truf = margin['EBITDA/KG'].values Lastly, to create a classification task, we're going to identify high, med, and low value products: print('bad less than: {:.2f}'.format(margin[margin.columns[-1]].quantile(.25)), end='\\n\\n') print('low less than: {:.2f}'.format(margin[margin.columns[-1]].quantile(.5)), end='\\n\\n') print('med less than: {:.2f}'.format(margin[margin.columns[-1]].quantile(.75)), end='\\n\\n') pd.DataFrame(margin[margin.columns[-2]]).boxplot(showfliers=False) bad less than: 0.12 low less than: 0.22 med less than: 0.35 <matplotlib.axes._subplots.AxesSubplot at 0x7eff2a001b90> margin['profitability'] = margin[margin.columns[-1]].apply( lambda x: 'bad' if x <= margin[margin.columns[-1]].quantile(.25) else 'low' if x <= margin[margin.columns[-1]].quantile(.50) else 'med' if x <= margin[margin.columns[-1]].quantile(.75) else 'high') margin['profitability'].hist() <matplotlib.axes._subplots.AxesSubplot at 0x7eff29f68b90> class_profit = {'bad': 0, 'low': 1, 'med': 2, 'high': 3} y_truf_class = margin['profitability'].map(class_profit).values margin['profitability_encoding'] = y_truf_class margin.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG profitability profitability_encoding 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 high 3 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 med 2 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 low 1 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 med 2 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 high 3","title":"6.0.2 Load Dataset"},{"location":"S6_Bagging/#61-decision-trees","text":"back to top In essence, a decision tree is a series of binary questions. Let's begin this discussion by talking about how we make decision trees in sklearn.","title":"6.1 Decision Trees"},{"location":"S6_Bagging/#611-creating-a-decision-tree","text":"back to top from sklearn import tree X = [[0, 0], [1, 1]] y = [0, 1] clf = tree.DecisionTreeClassifier() clf = clf.fit(X, y) After fitting the model we can use the predict method to show the output for a sample clf.predict([[2., 2.]]) array([1]) Similar to what we saw with GMMs, we also have access to the probabilities of the outcomes: clf.predict_proba([[2., 2.]]) array([[0., 1.]]) Let's now go on to using visual strategies to interpreting trees.","title":"6.1.1 Creating a Decision Tree"},{"location":"S6_Bagging/#612-interpreting-a-decision-tree","text":"back to top Throughout today, we will discuss many ways to view both a single tree and a random forest of trees.","title":"6.1.2 Interpreting a Decision Tree"},{"location":"S6_Bagging/#6121-node-branch-diagram","text":"back to top We can visualize the decision tree: tree.plot_tree(clf) [Text(167.4, 163.07999999999998, 'X[1] <= 0.5\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'), Text(83.7, 54.360000000000014, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'), Text(251.10000000000002, 54.360000000000014, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]')] or, more prettily: import graphviz dot_data = tree.export_graphviz(clf, out_file=None) graph = graphviz.Source(dot_data) graph The gini label, also known as Gini impurity , is a measure of how often a sample passing through the node would be incorrectly labeled if it was randomly assigned a label based on the proportion of all labels passing through the node. So it is a measure of the progress of our tree. Let's take a more complex example from sklearn.datasets import make_classification as gen X, y = gen(random_state=42) Let's inspect our generated data: print(X.shape) print(y.shape) y[:5] # a binary classification (100, 20) (100,) array([0, 0, 1, 1, 0]) And now let's train our tree: clf = tree.DecisionTreeClassifier() clf = clf.fit(X, y) How do we interpret this graph? dot_data = tree.export_graphviz(clf, out_file=None) graph = graphviz.Source(dot_data) graph Can we confirm the observations in the tree by manually inspecting X and y? y[X[:,10] < .203] array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) We can confirm the gini score of the top left node by hand... scr = [] for j in range(1000): y_pred = [0 if random.random() > ( 3/52 ) else 1 for i in range(52)] y_true = [0 if random.random() > ( 3/52 ) else 1 for i in range(52)] scr.append(mean_squared_error(y_pred,y_true)) np.mean(scr) 0.10994230769230771 Let's take a look at this with our truffle dataset Vary the parameter max_depth what do you notice? Does the term greedy mean anything to you? clf = tree.DecisionTreeClassifier(max_depth=1) clf.fit(X_truf, y_truf_class) DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=1, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=None, splitter='best') And now lets look at the graph: dot_data = tree.export_graphviz(clf, out_file=None) graph = graphviz.Source(dot_data) graph What is X[4] ??? # It's those tasty sponge cake truffles! enc.get_feature_names()[4] 'x0_Sponge' This is one great aspect of decision trees, their interpretability . We will perform this analysis again, for now, let's proceed with simpler datasets while exploring the features of decision trees.","title":"6.1.2.1 Node &amp; Branch Diagram"},{"location":"S6_Bagging/#6121-decision-boundaries","text":"back to top Let's make some random blobs from sklearn.datasets import make_blobs as gen X, y = gen(random_state=42) plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis') <matplotlib.collections.PathCollection at 0x7eff29e6ea90> Let's call up our Classifier again, this time setting the max_depth to two clf = tree.DecisionTreeClassifier(max_depth=2, random_state=42) clf = clf.fit(X, y) # Parameters plot_step = 0.02 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) cs = plt.contourf(xx, yy, Z, cmap='viridis', alpha=0.2) plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', edgecolor='grey', alpha=0.9) <matplotlib.collections.PathCollection at 0x7eff2a0a4bd0> dot_data = tree.export_graphviz(clf, out_file=None) graph = graphviz.Source(dot_data) graph We can see from the output of this graph, that the tree attempts to create the class boundaries as far from the cluster centers as possible. What happens when these clusters overlap? X, y = gen(random_state=42, cluster_std=3) plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis') <matplotlib.collections.PathCollection at 0x7eff2a114150> Let's go ahead and write our plot into a function def plot_tree(X, clf): plot_step = 0.02 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) cs = plt.contourf(xx, yy, Z, cmap='viridis', alpha=0.2) plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', edgecolor='grey', alpha=0.9) return plt We see that the boundaries mislabel some points fig = plot_tree(X, clf)","title":"6.1.2.1 Decision Boundaries"},{"location":"S6_Bagging/#613-overfitting-a-decision-tree","text":"back to top Let's increase the max_depth clf = tree.DecisionTreeClassifier(max_depth=5, random_state=42) clf = clf.fit(X, y) plot_tree(X, clf) <module 'matplotlib.pyplot' from '/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py'> What we notice is that while the model accurately predicts the training data, we see some spurious labels, noteably the trailing purple bar that extends into the otherwise green region of the data. This is a well known fact about decision trees, that they tend to overfit their training data. In fact, this is a major motivation for why decision trees, a weak classifier, are conveniently packaged into ensembles. We combine the idea of bootstrapping, with decision trees, to come up with an overall better classifier.","title":"6.1.3 Overfitting a Decision Tree"},{"location":"S6_Bagging/#exercise-1-minimize-overfitting","text":"Repeat 4.1.3 with different max_depth settings, also read the docstring and play with any other hyperparameters available to you. What settings do you feel minimize overfitting? # Code Cell for 1","title":"\ud83c\udfcb\ufe0f Exercise 1: Minimize Overfitting"},{"location":"S6_Bagging/#62-random-forests-and-bagging","text":"back to top","title":"6.2 Random Forests and Bagging"},{"location":"S6_Bagging/#621-what-is-bagging","text":"back to top Bagging , or Bootstrap AGGregation is the process of creating subsets of your data and training separate models on them, and using the aggregate votes of the models to make a final prediction. Bootstrapping is a topic in and of itself that we will just touch on here. Without going through the statistical rigor of proof, bootstrapping, or sampling from your observations with replacement, simulates having drawn additional data from the true population. We use this method to create many new datasets that are then used to train separate learners in parallel. This overall approach is called Bagging . A Random Forest is an instance of bagging where the separate learners are decision trees.","title":"6.2.1 What is Bagging?"},{"location":"S6_Bagging/#622-random-forests-for-classification","text":"back to top from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier tree = DecisionTreeClassifier() bag = BaggingClassifier(tree, n_estimators=10, max_samples=0.8, random_state=1) bag.fit(X, y) plot_tree(X, bag) <module 'matplotlib.pyplot' from '/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py'> In the above, we have bootstrapped by providing each individual tree with 80% of the population data. In practice, Random Forests can achieve even better results by randomizing how the individual classifiers are constructed. In fact there are many unique methods of training individual trees and you can learn more about them here . For now, know that it is better in practice to implement the RandomForests method in sklearn rather than bag individual trees yourself. from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier(n_estimators=10, random_state=2) clf = clf.fit(X, y) plot_tree(X, clf) <module 'matplotlib.pyplot' from '/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py'>","title":"6.2.2 Random Forests for Classification"},{"location":"S6_Bagging/#6221-interpreting-a-random-forest","text":"back to top Let's revisit our truffle dataset again, this time with random forests # fit the model clf = RandomForestClassifier(n_estimators=10, min_samples_leaf=6) clf = clf.fit(X_truf, y_truf_class) We get a fairly high accuracy when our min_samples_leaf is low and an accuracy that leaves room for improvement when min_samples_leaf is high. This indicates to us the model may be prown to overfitting if we are not careful: accuracy_score(clf.predict(X_truf), y_truf_class) 0.6007194244604317 We need to do some housekeeping to get back the names and categories the features were pulled from: feat_dict = dict(zip([f'x{i}' for i in range(len(cat_cols))], cat_cols.str.replace(' ', '_'))) print(feat_dict) feats = [] # enc.get_feature_names unpacks like so: `x4_White` where # x<N> is an artifact of the original dimensions fed to enc() # I am unpacking the feature names with the original dataframe header for row in enc.get_feature_names(): feats.append('{} {}'.format(feat_dict[row.split('_')[0]], row.split('_')[1])) # feats will be used to visualize the feature importances feats[:5] {'x0': 'Base_Cake', 'x1': 'Truffle_Type', 'x2': 'Primary_Flavor', 'x3': 'Secondary_Flavor', 'x4': 'Color_Group', 'x5': 'Customer', 'x6': 'Date'} ['Base_Cake Butter', 'Base_Cake Cheese', 'Base_Cake Chiffon', 'Base_Cake Pound', 'Base_Cake Sponge'] The feature importances are stored in clf.feature_importances_ . These are calculated from the Mean Decrease in Impurity or MDI also called the Gini Importance . It is the sum of the number of nodes across all trees that include the feature, weighted by the number of samples passing through the node. One downside of estimating feature importance in this way is that it doesn't play well with highly cardinal features (features with many unique values such as mailing addresses, are highly cardinal features) # grab feature importances imp = clf.feature_importances_ # their std std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0) # create new dataframe feat = pd.DataFrame([feats + [\"KG\"], imp, std]).T feat.columns = ['name', 'importance', 'std'] feat = feat.sort_values('importance', ascending=False) feat = feat.reset_index(drop=True) feat.columns = ['feature', 'importance', 'std'] I'm going to use plotly to create this chart: How does feature importance change when we change the minimum leaf size from 2 to 6? px.bar(feat, x='feature', y='importance', error_y='std') if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"318eb430-e0f4-4b77-aa05-3c077a236bea\")) { Plotly.newPlot( '318eb430-e0f4-4b77-aa05-3c077a236bea', [{\"alignmentgroup\": \"True\", \"error_y\": {\"array\": [0.07555343650428877, 0.04063238643408586, 0.05961762914014548, 0.036511772297213245, 0.0290208966535553, 0.027395688661575626, 0.020452704347556663, 0.03393435310671577, 0.028642943310511235, 0.012532926656412787, 0.03556066576596446, 0.01583846311062793, 0.015769424865656922, 0.022456972121037738, 0.02357794745152038, 0.01442052676018209, 0.016310850227497925, 0.020135987095791785, 0.01966912273385237, 0.0172252292386212, 0.016188291852075806, 0.011232957255776955, 0.011752740590544949, 0.014215519364083693, 0.013231561004786502, 0.011128839190837566, 0.006571133069294302, 0.009489276856556898, 0.007960061906913968, 0.010480916770311037, 0.01077200787919219, 0.00994444700093636, 0.01698793108921301, 0.012224243018118919, 0.0074065694102534866, 0.01145262818011683, 0.014493721007728682, 0.008521135565720254, 0.012128874609229678, 0.00618660142472066, 0.008750592417062431, 0.01083766822715062, 0.006589032274060321, 0.005665682686641886, 0.007104402814719823, 0.009642691848896686, 0.005820366803373852, 0.004582819751796309, 0.006191175003169528, 0.010780497029991285, 0.009383117993507828, 0.01064557976533365, 0.007971400075341879, 0.007626220709434621, 0.004528508932574483, 0.007978753956889023, 0.00598655286524879, 0.004724575698947667, 0.006907133033972632, 0.012019731255860346, 0.0069698073631939425, 0.0042979530563582525, 0.00880028649397044, 0.006169496245711782, 0.011543630256147902, 0.011489894256293558, 0.004546766268214796, 0.0039653606347905285, 0.005142839080282742, 0.004941368568974321, 0.004792637324806003, 0.00421720688017499, 0.0035935515587245948, 0.008488082736005341, 0.006269385473890268, 0.006598567537142401, 0.003498034328041033, 0.003873584657620095, 0.0037805701429140014, 0.002515794877177775, 0.006673028932914227, 0.0023320341584585798, 0.00708673630649601, 0.004711100803629174, 0.003424765080306881, 0.0026921725725499285, 0.006302660607068695, 0.0032923995185687254, 0.0030791354423140693, 0.0030119835753018034, 0.0031442289102776937, 0.0020600908472405127, 0.002793480291008204, 0.00193979455345233, 0.0030857822778379788, 0.0035748079758318985, 0.0032705986017104887, 0.002272192880069807, 0.0022907382568384494, 0.0032671244214112233, 0.0019998154929131923, 0.0018820524312719476, 0.0019111629917143336, 0.0017978767817748138, 0.002340223707926282, 0.0016899808466311807, 0.0016614874356055674, 0.0015772015772983537, 0.0009475672660661238, 0.0011080745981432952, 0.0008970952531810846, 0.0005748636888217817, 0.0005470026863239088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"feature=%{x}<br>importance=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"Base_Cake Sponge\", \"Base_Cake Chiffon\", \"Base_Cake Pound\", \"KG\", \"Primary_Flavor Butter Toffee\", \"Base_Cake Tiramisu\", \"Secondary_Flavor Egg Nog\", \"Base_Cake Butter\", \"Base_Cake Cheese\", \"Customer Slugworth\", \"Color_Group Olive\", \"Customer Perk-a-Cola\", \"Color_Group White\", \"Secondary_Flavor Black Cherry\", \"Primary_Flavor Doughnut\", \"Customer Zebrabar\", \"Truffle_Type Candy Outer\", \"Primary_Flavor Orange Pineapple\\tP\", \"Primary_Flavor Kettle Corn\", \"Color_Group Amethyst\", \"Truffle_Type Chocolate Outer\", \"Secondary_Flavor Black Currant\", \"Color_Group Teal\", \"Secondary_Flavor Wild Cherry Cream\", \"Secondary_Flavor Cucumber\", \"Primary_Flavor Toasted Coconut\", \"Color_Group Opal\", \"Customer Fickelgruber\", \"Color_Group Black\", \"Primary_Flavor Lemon Custard\", \"Secondary_Flavor Tangerine\", \"Secondary_Flavor Lemon\", \"Primary_Flavor Dill Pickle\", \"Primary_Flavor Spice\", \"Customer Dandy's Candies\", \"Secondary_Flavor Whipped Cream\", \"Primary_Flavor Orange\", \"Primary_Flavor Ginger Lime\", \"Secondary_Flavor Papaya\", \"Primary_Flavor Irish Cream\", \"Secondary_Flavor Apple\", \"Primary_Flavor Plum\", \"Primary_Flavor Mango\", \"Secondary_Flavor Rock and Rye\", \"Color_Group Tiffany\", \"Secondary_Flavor Peppermint\", \"Primary_Flavor Grand Mariner\", \"Primary_Flavor Caramel Cream\", \"Primary_Flavor Cheesecake\", \"Truffle_Type Jelly Filled\", \"Primary_Flavor Sassafras\", \"Primary_Flavor Horchata\", \"Primary_Flavor Acai Berry\", \"Primary_Flavor Watermelon\", \"Date 12/2020\", \"Primary_Flavor Lemon\", \"Primary_Flavor Pecan\", \"Secondary_Flavor Kiwi\", \"Primary_Flavor Cream Soda\", \"Primary_Flavor Gingersnap\", \"Primary_Flavor Ginger Ale\", \"Secondary_Flavor Banana\", \"Primary_Flavor Pink Lemonade\", \"Secondary_Flavor Pear\", \"Primary_Flavor Amaretto\", \"Primary_Flavor Blueberry\", \"Primary_Flavor Bavarian Cream\", \"Primary_Flavor Black Licorice\", \"Primary_Flavor Cherry Cola\", \"Primary_Flavor Coffee\", \"Color_Group Burgundy\", \"Date 7/2020\", \"Date 11/2020\", \"Secondary_Flavor Rum\", \"Secondary_Flavor Apricot\", \"Primary_Flavor Vanilla\", \"Date 10/2020\", \"Date 8/2020\", \"Secondary_Flavor Vanilla\", \"Date 9/2020\", \"Color_Group Rose\", \"Date 4/2020\", \"Primary_Flavor Chocolate Mint\", \"Primary_Flavor Orange Brandy\", \"Primary_Flavor Sour\", \"Date 2/2020\", \"Color_Group Slate\", \"Date 3/2020\", \"Secondary_Flavor Mojito\", \"Date 5/2020\", \"Secondary_Flavor Mixed Berry\", \"Secondary_Flavor Fuzzy Navel\", \"Primary_Flavor Raspberry Ginger Ale\", \"Primary_Flavor Cherry Cream Spice\", \"Primary_Flavor Lemon Bar\", \"Primary_Flavor Wild Cherry Cream\", \"Secondary_Flavor Ginger Beer\", \"Primary_Flavor Apricot\", \"Secondary_Flavor Dill Pickle\", \"Secondary_Flavor Mango\", \"Primary_Flavor Creme de Menthe\", \"Date 1/2020\", \"Date 6/2020\", \"Secondary_Flavor Butter Rum\", \"Primary_Flavor Chocolate\", \"Primary_Flavor Butter Pecan\", \"Secondary_Flavor Passion Fruit\", \"Primary_Flavor Wintergreen\", \"Primary_Flavor Pear\", \"Primary_Flavor Fruit Punch\", \"Secondary_Flavor Toffee\", \"Primary_Flavor Margarita\", \"Color_Group Taupe\", \"Primary_Flavor Birch Beer\", \"Primary_Flavor Butter Milk\", \"Color_Group Citrine\", \"Primary_Flavor Coconut\", \"Secondary_Flavor Tutti Frutti\", \"Secondary_Flavor Hazelnut\"], \"xaxis\": \"x\", \"y\": [0.11070519650695783, 0.0639051536976566, 0.0548603760743442, 0.04690552959956567, 0.030875729545272637, 0.02964743757949146, 0.02903731620006769, 0.027549820016608686, 0.024187978949675365, 0.023614207046533706, 0.023004895620422038, 0.020589909133373552, 0.019963584487051135, 0.014793374515086494, 0.014393892077033574, 0.013853128581716494, 0.013244615846047074, 0.012690554084720291, 0.01259021885646476, 0.012404368567974087, 0.0113187384045495, 0.011071708910919128, 0.010741822613075441, 0.010696258810503114, 0.01036397151172899, 0.010153740726406372, 0.009873025937418815, 0.009624755083125741, 0.00939787547337474, 0.009324827381141128, 0.009223258704344116, 0.00920180252864671, 0.009030231263778518, 0.008748134672159761, 0.008650390754265581, 0.00857621547469525, 0.008182984461145851, 0.007935677327919664, 0.007710024862021607, 0.007655626597286043, 0.006909301064429637, 0.0068089928959846535, 0.006280078656786092, 0.0062527552424586595, 0.005937366089078937, 0.005569142846762498, 0.005395143076710668, 0.005349940638283773, 0.0053246203141319795, 0.005162744809445722, 0.00513549758800123, 0.005131023056676936, 0.00485863529267419, 0.004618572850759596, 0.004385632341887418, 0.004138155071736413, 0.004092217766380393, 0.004063384055128564, 0.004010104372789467, 0.004006577085286783, 0.00400548765617934, 0.00388998413390825, 0.0038868227387336947, 0.003858150268472716, 0.0038478767520493035, 0.0038428634866371617, 0.00373893601154158, 0.003709740677574182, 0.003654009077597494, 0.00348183888390368, 0.0034621232538648568, 0.0034131418308362077, 0.003344958987577941, 0.0032455102404200156, 0.003051238798750663, 0.0028849097281198836, 0.0028289898361587365, 0.0026498763977929355, 0.0026473717061639046, 0.002544288234247985, 0.002536787060152542, 0.0023816342255907124, 0.0023622454354986703, 0.002355487721202827, 0.0023118590899479387, 0.002239091551568516, 0.002100886869022899, 0.002075493235042351, 0.001917413173718334, 0.0018839487672445655, 0.001580612376283172, 0.0013055871972177259, 0.0012863009671007484, 0.0012487062252696925, 0.0012279317440148873, 0.0011916026586106328, 0.0011551536899490362, 0.0011197945637304875, 0.0011095070716949365, 0.0010890414738037415, 0.0010807649217126744, 0.0009599570923495142, 0.0009534468564891095, 0.0008997626619987524, 0.0007904775752950208, 0.0005633269488770604, 0.000553829145201856, 0.0005257338590994512, 0.0004349081329954505, 0.00036935819938109846, 0.0002990317510603616, 0.00019162122960726061, 0.00018233422877463634, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"yaxis\": \"y\"}], {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"feature\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"importance\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('318eb430-e0f4-4b77-aa05-3c077a236bea'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; We can then go and look at the different EBITDAs when selecting for each of these features: def ebitda_comp(feature=feat['feature']): if len(feature.split(' ')) > 1: group = feature.split(' ')[0].replace('_', ' ') sel = \" \".join(feature.split(' ')[1:]) pos = margin.loc[(margin[group] == sel)]['EBITDA/KG'].median() neg = margin.loc[~(margin[group] == sel)]['EBITDA/KG'].median() print(\"with: {:.2e}\".format(pos)) print(\"without: {:.2e}\".format(neg)) else: pass interact(ebitda_comp) interactive(children=(Dropdown(description='feature', options=('Base_Cake Sponge', 'Base_Cake Chiffon', 'Base_\u2026 <function __main__.ebitda_comp> This is an example of how the interpretability of the random forest ensemble approach can be highly valuable.","title":"6.2.2.1 Interpreting a Random Forest"},{"location":"S6_Bagging/#623-random-forests-for-regression","text":"back to top from sklearn.ensemble import RandomForestRegressor clf = RandomForestRegressor(n_estimators=10) Because our labels on our blob data were numerical, we can apply and view the estimator in the same way: clf = clf.fit(X, y) plot_tree(X, clf) <module 'matplotlib.pyplot' from '/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py'> I want to revisit a dataset we brought up in Session 2 on feature engineering: t = np.linspace(0,5,200) w = 5 h = 4 s = 4 * h / np.pi * (np.sin(w*t) + np.sin(3*w*t)/3 + np.sin(5*w*t)/5) F = np.fft.fft(s) freq = np.fft.fftfreq(t.shape[-1]) fig, ax = plt.subplots(1,2,figsize=(10,5)) ax[0].plot(t,s) ax[0].plot(t,np.sin(w*t), ls='--') ax[0].plot(t,np.sin(w*t*3)/3, ls='--') ax[0].plot(t,np.sin(w*t*5)/5, ls='--') ax[0].set_title('Time Domain') # tells us about the amplitude of the component at the # corresponding frequency magnitude = np.sqrt(F.real**2 + F.imag**2) ax[1].plot(freq, magnitude) ax[1].set_xlim(0,.15) ax[1].set_title('Frequency Domain') Text(0.5, 1.0, 'Frequency Domain') Let's see if a random forest regression model can capture the wave behavior of the time-series data clf = RandomForestRegressor(n_estimators=10) clf.fit(t.reshape(-1,1),s) RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) fig, ax = plt.subplots(1,1,figsize=(10,5)) ax.plot(t,s) ax.plot(t,clf.predict(t.reshape(-1,1))) [<matplotlib.lines.Line2D at 0x7eff29588690>] Nice! without specifying any perdiodicity, the random forest does a good job of embedding this periodicity in the final output.","title":"6.2.3 Random Forests for Regression"},{"location":"S6_Bagging/#extended-exercise-practice-with-random-forests","text":"With the wine dataset: predict: quality, quality group use train_test_split create a learning curve of train/test score vs model complexity for your random forest model(s) # Code Cell for Exercise 4.2.4 wine = pd.read_csv(\"https://raw.githubusercontent.com/wesleybeckner/\"\\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\") wine.dropna(inplace=True) wine['quality_label'] = wine['quality'].apply(lambda x: 'low' if x <=5 else 'med' if x <= 7 else 'high') wine['type_encoding'] = wine['type'].map({'red': 0, 'white': 1}) wine['quality_encoding'] = wine['quality_label'].map({'low':0, 'med': 1, 'high': 2}) wine.columns = wine.columns.str.replace(' ', '_') features = list(wine.columns[1:-1].values) features.remove('quality_label') features.remove('quality')","title":"\ud83c\udfcb\ufe0f Extended Exercise: Practice with Random Forests"},{"location":"S7_Boosting/","text":"Data Science Foundation, Session 7: Boosting Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're continuing on the topic of supervised learning an ensemble learning method called boosting. 7.1 Preparing Environment and Importing Data back to top 7.1.1 Import Packages back to top from sklearn import svm from sklearn.datasets import make_blobs from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier import matplotlib.pyplot as plt import numpy as np import plotly.express as px from ipywidgets import interact, FloatSlider, interactive 7.1.2 Load Dataset back to top For this session, we will use dummy datasets from sklearn. 7.2 Boosting back to top The last supervised learning algorithms we will cover, are the boosting learners. Similar to Bagging, Boosting algorithms leverage the idea of training on variations of the available data, only this time they do so in serial rather than parallel. What do I mean by this? It's a little nuanced, but the idea is straight forward. The first model trains on the dataset, it generates some error. The datapoints creating the greatest amount of error are emphasized in the second round of training, and so on, as the sequence of models proceeds, ever troublesome datapoints receive ever increasing influence. 7.2.1 AdaBoost Back to Top AdaBoost is the first boosting learner. It's weak learners the things that are stitched together in serial, are typically stumps or really shallow decision trees X, y = make_circles(random_state=42, noise=.01) clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3)) clf.fit(X,y) plot_boundaries(X, clf) X, y = make_blobs(random_state=42, centers=2, cluster_std=2.5) clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5)) clf.fit(X,y) plot_boundaries(X, clf) from sklearn.datasets import make_moons X, y = make_moons(random_state=42, noise=.05) clf = AdaBoostClassifier() clf.fit(X,y) plot_boundaries(X, clf) 7.2.1 Gradient Boosting Back to Top Gradient Boosting builds on the idea of AdaBoost . The term gradient implies that 2 or more derivatives are being taken somewhere. What this is referring to, is while AdaBoost is subject to a predefined loss function, Gradient Boosting can take in any arbitrary (as long as it is differentiable) loss function, to coordinate the training of its weak learners. X, y = make_circles(random_state=42, noise=.01) clf = GradientBoostingClassifier(loss='deviance') clf.fit(X,y) plot_boundaries(X, clf) X, y = make_blobs(random_state=42, centers=2, cluster_std=2.5) clf = GradientBoostingClassifier() clf.fit(X,y) plot_boundaries(X, clf) from sklearn.datasets import make_moons X, y = make_moons(random_state=42, noise=.05) clf.fit(X,y) plot_boundaries(X, clf) References back to top Generative vs Discriminative Models","title":"Boosting"},{"location":"S7_Boosting/#data-science-foundation-session-7-boosting","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're continuing on the topic of supervised learning an ensemble learning method called boosting.","title":"Data Science Foundation, Session 7: Boosting"},{"location":"S7_Boosting/#71-preparing-environment-and-importing-data","text":"back to top","title":"7.1 Preparing Environment and Importing Data"},{"location":"S7_Boosting/#711-import-packages","text":"back to top from sklearn import svm from sklearn.datasets import make_blobs from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier import matplotlib.pyplot as plt import numpy as np import plotly.express as px from ipywidgets import interact, FloatSlider, interactive","title":"7.1.1 Import Packages"},{"location":"S7_Boosting/#712-load-dataset","text":"back to top For this session, we will use dummy datasets from sklearn.","title":"7.1.2 Load Dataset"},{"location":"S7_Boosting/#72-boosting","text":"back to top The last supervised learning algorithms we will cover, are the boosting learners. Similar to Bagging, Boosting algorithms leverage the idea of training on variations of the available data, only this time they do so in serial rather than parallel. What do I mean by this? It's a little nuanced, but the idea is straight forward. The first model trains on the dataset, it generates some error. The datapoints creating the greatest amount of error are emphasized in the second round of training, and so on, as the sequence of models proceeds, ever troublesome datapoints receive ever increasing influence.","title":"7.2 Boosting"},{"location":"S7_Boosting/#721-adaboost","text":"Back to Top AdaBoost is the first boosting learner. It's weak learners the things that are stitched together in serial, are typically stumps or really shallow decision trees X, y = make_circles(random_state=42, noise=.01) clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3)) clf.fit(X,y) plot_boundaries(X, clf) X, y = make_blobs(random_state=42, centers=2, cluster_std=2.5) clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5)) clf.fit(X,y) plot_boundaries(X, clf) from sklearn.datasets import make_moons X, y = make_moons(random_state=42, noise=.05) clf = AdaBoostClassifier() clf.fit(X,y) plot_boundaries(X, clf)","title":"7.2.1 AdaBoost"},{"location":"S7_Boosting/#721-gradient-boosting","text":"Back to Top Gradient Boosting builds on the idea of AdaBoost . The term gradient implies that 2 or more derivatives are being taken somewhere. What this is referring to, is while AdaBoost is subject to a predefined loss function, Gradient Boosting can take in any arbitrary (as long as it is differentiable) loss function, to coordinate the training of its weak learners. X, y = make_circles(random_state=42, noise=.01) clf = GradientBoostingClassifier(loss='deviance') clf.fit(X,y) plot_boundaries(X, clf) X, y = make_blobs(random_state=42, centers=2, cluster_std=2.5) clf = GradientBoostingClassifier() clf.fit(X,y) plot_boundaries(X, clf) from sklearn.datasets import make_moons X, y = make_moons(random_state=42, noise=.05) clf.fit(X,y) plot_boundaries(X, clf)","title":"7.2.1 Gradient Boosting"},{"location":"S7_Boosting/#references","text":"back to top Generative vs Discriminative Models","title":"References"},{"location":"about/","text":"Data Science Foundations Cut through the deluge of data science material by focusing on the essentials. This course uses illustrations, code-based examples and case studies to demonstrate essential data science topics and the practical application of existing machine learning frameworks. By the end of the course, you will have trained and validated machine learning algorithms to make continuous-value as well as discrete-value predictions from data sources relevant in business and engineering. You will also be able to make statistically sound, data-driven decisions in business from sales and production data. The breakdown for this course is as follows: Data Topics Bias-variance tradeoff; regression: linear, logistic, and multivariate; regularization: L1 and L2; inferential statistics: moods median, t-tests, f-tests, ANOVA; descriptive statistics: mean, median, mode, kurtosis, skew; beyond regression coefficients: tree-based and resampling methods; unsupervised learning: clustering and dimensionality reduction Software Topics Unit Tests Sessions S1: Regression and Analysis S2: Inferential Statistics S3: Model Selection and Validation S4: Feature Engineering S5: Unsupervised Learning: Clustering and Dimensionality Reduction S6: Bagging: Decision Trees and Random Forests S7: Boosting: AdaBoost and XGBoost Exercises E1: Descriptive Statistics Data Hunt E2: Inferential Statistics Data Hunt E3: Feature Engineering E4: Supervised Learners E5: Writing Unit Tests Project P1: Statistical Analysis of TicTacToe Games P2: Heuristical TicTacToe Agents P3: 1-Step Look Ahead Agents P4: N-Step Look Ahead Agents Reading JVDP chapter 5","title":"About"},{"location":"about/#data-science-foundations","text":"Cut through the deluge of data science material by focusing on the essentials. This course uses illustrations, code-based examples and case studies to demonstrate essential data science topics and the practical application of existing machine learning frameworks. By the end of the course, you will have trained and validated machine learning algorithms to make continuous-value as well as discrete-value predictions from data sources relevant in business and engineering. You will also be able to make statistically sound, data-driven decisions in business from sales and production data. The breakdown for this course is as follows: Data Topics Bias-variance tradeoff; regression: linear, logistic, and multivariate; regularization: L1 and L2; inferential statistics: moods median, t-tests, f-tests, ANOVA; descriptive statistics: mean, median, mode, kurtosis, skew; beyond regression coefficients: tree-based and resampling methods; unsupervised learning: clustering and dimensionality reduction Software Topics Unit Tests Sessions S1: Regression and Analysis S2: Inferential Statistics S3: Model Selection and Validation S4: Feature Engineering S5: Unsupervised Learning: Clustering and Dimensionality Reduction S6: Bagging: Decision Trees and Random Forests S7: Boosting: AdaBoost and XGBoost Exercises E1: Descriptive Statistics Data Hunt E2: Inferential Statistics Data Hunt E3: Feature Engineering E4: Supervised Learners E5: Writing Unit Tests Project P1: Statistical Analysis of TicTacToe Games P2: Heuristical TicTacToe Agents P3: 1-Step Look Ahead Agents P4: N-Step Look Ahead Agents Reading JVDP chapter 5","title":"Data Science Foundations"},{"location":"exercises/E1_Descriptive_Statistics_Data_Hunt/","text":"Data Science Foundations, Lab 1: Data Hunt I Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's a data hunt. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from ipywidgets import interact df = pd.read_csv(\"https://raw.githubusercontent.com/wesleybeckner/\"\\ \"technology_explorers/main/assets/imdb_movies.csv\") # converting years to numbers for easy conditionals df['year'] = pd.to_numeric(df['year'], errors='coerce') df.shape /usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) (85855, 22) df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } imdb_title_id title original_title year date_published genre duration country language director writer production_company actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics 0 tt0000009 Miss Jerry Miss Jerry 1894.0 1894-10-09 Romance 45 USA None Alexander Black Alexander Black Alexander Black Photoplays Blanche Bayliss, William Courtenay, Chauncey D... The adventures of a female reporter in the 1890s. 5.9 154 NaN NaN NaN NaN 1.0 2.0 1 tt0000574 The Story of the Kelly Gang The Story of the Kelly Gang 1906.0 1906-12-26 Biography, Crime, Drama 70 Australia None Charles Tait Charles Tait J. and N. Tait Elizabeth Tait, John Tait, Norman Campbell, Be... True story of notorious Australian outlaw Ned ... 6.1 589 $ 2250 NaN NaN NaN 7.0 7.0 2 tt0001892 Den sorte dr\u00f8m Den sorte dr\u00f8m 1911.0 1911-08-19 Drama 53 Germany, Denmark NaN Urban Gad Urban Gad, Gebhard Sch\u00e4tzler-Perasini Fotorama Asta Nielsen, Valdemar Psilander, Gunnar Helse... Two men of high rank are both wooing the beaut... 5.8 188 NaN NaN NaN NaN 5.0 2.0 Q1 What american director has the highest mean avg_vote? director Daniel Keith, Snorri Sturluson 9.3 Anthony Bawn 9.3 Derek Ahonen 9.2 Raghav Peri 9.1 James Marlowe 8.8 ... Waleed Bedour 1.2 Fred Ashman 1.1 Aeneas Middleton 1.1 Steven A. Sandt 1.1 Francis Hamada 1.1 Name: avg_vote, Length: 12463, dtype: float64 Q2 What american director with more than 5 movies, has the highest mean avg_vote? director Quentin Tarantino 7.811111 Charles Chaplin 7.764286 David Fincher 7.625000 Billy Wilder 7.580952 Martin Scorsese 7.544444 ... Barry Mahon 2.728571 Dennis Devine 2.657143 Bill Zebub 2.483333 Mark Polonia 2.462500 Christopher Forbes 2.000000 Name: avg_vote, Length: 859, dtype: float64 Q3 What director has the largest variance in avg_vote? director Deniz Denizciler 4.030509 Rudi Lagemann 3.747666 Emilio Ruiz Barrachina 3.676955 Krishna Ghattamaneni 3.676955 Milos Avramovic 3.606245 ... \u00dcmit Degirmenci NaN \u00dcmit El\u00e7i NaN \u00dcmit K\u00f6reken NaN \u00deorsteinn Gunnar Bjarnason NaN \u00de\u00f3rhildur \u00deorleifsd\u00f3ttir NaN Name: avg_vote, Length: 34733, dtype: float64 Q4 What director with more than 10 movies has the largest variance in avg_vote? director Harry Baweja 1.869954 Shaji Kailas 1.854502 Zdenek Troska 1.775984 Adam Rifkin 1.711251 Ram Gopal Varma 1.687850 ... Ford Beebe 0.224343 Ray Nazarro 0.210311 Jean Gr\u00e9millon 0.196946 Louis Feuillade 0.156428 Tsutomu Shibayama 0.126121 Name: avg_vote, Length: 1135, dtype: float64 Q5 What american directors with more than 5 movies have the largest variance in avg_vote? director Martin Brest 2.033716 David Winters 1.926049 Adam Rifkin 1.711251 Gus Trikonis 1.661271 Jerry Jameson 1.646107 ... Edward Killy 0.155265 Willis Goldbeck 0.139443 Richard T. Heffron 0.136626 Bill Plympton 0.136626 Nate Watt 0.129099 Name: avg_vote, Length: 859, dtype: float64 Q6 Where does M. Night Shyamalan fall on this rank scale? (He's number 36/859) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } director avg_vote 36 M. Night Shyamalan 1.258791 Q7 How many movies were made each year in US from 2000-2020 year 2000.0 363 2001.0 386 2002.0 360 2003.0 339 2004.0 362 2005.0 453 2006.0 590 2007.0 574 2008.0 592 2009.0 656 2010.0 611 2011.0 652 2012.0 738 2013.0 820 2014.0 807 2015.0 800 2016.0 869 2017.0 905 2018.0 886 2019.0 700 2020.0 276 Name: title, dtype: int64 Q8 Visualize The Results of Q7! <matplotlib.axes._subplots.AxesSubplot at 0x7fea042dc890> Q9 For single country movies, how many movies were made each year in each country from 2000-2020, only include countries that made more than 1000 movies in that timeframe .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year title 0 Canada 2000.0 39 1 Canada 2001.0 51 2 Canada 2002.0 49 3 Canada 2003.0 38 4 Canada 2004.0 52 Q10 Visualize the results from Q9!","title":"Descriptive Statistics Data Hunt"},{"location":"exercises/E1_Descriptive_Statistics_Data_Hunt/#data-science-foundations-lab-1-data-hunt-i","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's a data hunt. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from ipywidgets import interact df = pd.read_csv(\"https://raw.githubusercontent.com/wesleybeckner/\"\\ \"technology_explorers/main/assets/imdb_movies.csv\") # converting years to numbers for easy conditionals df['year'] = pd.to_numeric(df['year'], errors='coerce') df.shape /usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) (85855, 22) df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } imdb_title_id title original_title year date_published genre duration country language director writer production_company actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics 0 tt0000009 Miss Jerry Miss Jerry 1894.0 1894-10-09 Romance 45 USA None Alexander Black Alexander Black Alexander Black Photoplays Blanche Bayliss, William Courtenay, Chauncey D... The adventures of a female reporter in the 1890s. 5.9 154 NaN NaN NaN NaN 1.0 2.0 1 tt0000574 The Story of the Kelly Gang The Story of the Kelly Gang 1906.0 1906-12-26 Biography, Crime, Drama 70 Australia None Charles Tait Charles Tait J. and N. Tait Elizabeth Tait, John Tait, Norman Campbell, Be... True story of notorious Australian outlaw Ned ... 6.1 589 $ 2250 NaN NaN NaN 7.0 7.0 2 tt0001892 Den sorte dr\u00f8m Den sorte dr\u00f8m 1911.0 1911-08-19 Drama 53 Germany, Denmark NaN Urban Gad Urban Gad, Gebhard Sch\u00e4tzler-Perasini Fotorama Asta Nielsen, Valdemar Psilander, Gunnar Helse... Two men of high rank are both wooing the beaut... 5.8 188 NaN NaN NaN NaN 5.0 2.0","title":"Data Science Foundations, Lab 1: Data Hunt I"},{"location":"exercises/E1_Descriptive_Statistics_Data_Hunt/#q1-what-american-director-has-the-highest-mean-avg_vote","text":"director Daniel Keith, Snorri Sturluson 9.3 Anthony Bawn 9.3 Derek Ahonen 9.2 Raghav Peri 9.1 James Marlowe 8.8 ... Waleed Bedour 1.2 Fred Ashman 1.1 Aeneas Middleton 1.1 Steven A. Sandt 1.1 Francis Hamada 1.1 Name: avg_vote, Length: 12463, dtype: float64","title":"Q1 What american director has the highest mean  avg_vote?"},{"location":"exercises/E1_Descriptive_Statistics_Data_Hunt/#q2-what-american-director-with-more-than-5-movies-has-the-highest-mean-avg_vote","text":"director Quentin Tarantino 7.811111 Charles Chaplin 7.764286 David Fincher 7.625000 Billy Wilder 7.580952 Martin Scorsese 7.544444 ... Barry Mahon 2.728571 Dennis Devine 2.657143 Bill Zebub 2.483333 Mark Polonia 2.462500 Christopher Forbes 2.000000 Name: avg_vote, Length: 859, dtype: float64","title":"Q2 What american director with more than 5 movies, has the highest mean avg_vote?"},{"location":"exercises/E1_Descriptive_Statistics_Data_Hunt/#q3-what-director-has-the-largest-variance-in-avg_vote","text":"director Deniz Denizciler 4.030509 Rudi Lagemann 3.747666 Emilio Ruiz Barrachina 3.676955 Krishna Ghattamaneni 3.676955 Milos Avramovic 3.606245 ... \u00dcmit Degirmenci NaN \u00dcmit El\u00e7i NaN \u00dcmit K\u00f6reken NaN \u00deorsteinn Gunnar Bjarnason NaN \u00de\u00f3rhildur \u00deorleifsd\u00f3ttir NaN Name: avg_vote, Length: 34733, dtype: float64","title":"Q3 What director has the largest variance in avg_vote?"},{"location":"exercises/E1_Descriptive_Statistics_Data_Hunt/#q4-what-director-with-more-than-10-movies-has-the-largest-variance-in-avg_vote","text":"director Harry Baweja 1.869954 Shaji Kailas 1.854502 Zdenek Troska 1.775984 Adam Rifkin 1.711251 Ram Gopal Varma 1.687850 ... Ford Beebe 0.224343 Ray Nazarro 0.210311 Jean Gr\u00e9millon 0.196946 Louis Feuillade 0.156428 Tsutomu Shibayama 0.126121 Name: avg_vote, Length: 1135, dtype: float64","title":"Q4 What director with more than 10 movies has the largest variance in avg_vote?"},{"location":"exercises/E1_Descriptive_Statistics_Data_Hunt/#q5-what-american-directors-with-more-than-5-movies-have-the-largest-variance-in-avg_vote","text":"director Martin Brest 2.033716 David Winters 1.926049 Adam Rifkin 1.711251 Gus Trikonis 1.661271 Jerry Jameson 1.646107 ... Edward Killy 0.155265 Willis Goldbeck 0.139443 Richard T. Heffron 0.136626 Bill Plympton 0.136626 Nate Watt 0.129099 Name: avg_vote, Length: 859, dtype: float64","title":"Q5 What american directors with more than 5 movies have the largest variance in avg_vote?"},{"location":"exercises/E1_Descriptive_Statistics_Data_Hunt/#q6-where-does-m-night-shyamalan-fall-on-this-rank-scale","text":"(He's number 36/859) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } director avg_vote 36 M. Night Shyamalan 1.258791","title":"Q6 Where does M. Night Shyamalan fall on this rank scale?"},{"location":"exercises/E1_Descriptive_Statistics_Data_Hunt/#q7-how-many-movies-were-made-each-year-in-us-from-2000-2020","text":"year 2000.0 363 2001.0 386 2002.0 360 2003.0 339 2004.0 362 2005.0 453 2006.0 590 2007.0 574 2008.0 592 2009.0 656 2010.0 611 2011.0 652 2012.0 738 2013.0 820 2014.0 807 2015.0 800 2016.0 869 2017.0 905 2018.0 886 2019.0 700 2020.0 276 Name: title, dtype: int64","title":"Q7 How many movies were made each year in US from 2000-2020"},{"location":"exercises/E1_Descriptive_Statistics_Data_Hunt/#q8-visualize-the-results-of-q7","text":"<matplotlib.axes._subplots.AxesSubplot at 0x7fea042dc890>","title":"Q8 Visualize The Results of Q7!"},{"location":"exercises/E1_Descriptive_Statistics_Data_Hunt/#q9-for-single-country-movies-how-many-movies-were-made-each-year-in-each-country-from-2000-2020-only-include-countries-that-made-more-than-1000-movies-in-that-timeframe","text":".dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year title 0 Canada 2000.0 39 1 Canada 2001.0 51 2 Canada 2002.0 49 3 Canada 2003.0 38 4 Canada 2004.0 52","title":"Q9 For single country movies, how many movies were made each year in each country from 2000-2020, only include countries that made more than 1000 movies in that timeframe"},{"location":"exercises/E1_Descriptive_Statistics_Data_Hunt/#q10-visualize-the-results-from-q9","text":"","title":"Q10 Visualize the results from Q9!"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/","text":"Data Science Foundations, Lab 2: Data Hunt II Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's the data hunt part TWO. Preparing Environment and Importing Data Import Packages !pip install -U plotly Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1) Collecting plotly Downloading plotly-5.1.0-py2.py3-none-any.whl (20.6 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.6 MB 1.3 MB/s \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0) Collecting tenacity>=6.2.0 Downloading tenacity-8.0.1-py3-none-any.whl (24 kB) Installing collected packages: tenacity, plotly Attempting uninstall: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-5.1.0 tenacity-8.0.1 # our standard libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns from ipywidgets import interact # our stats libraries import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy # our scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score Import and Clean Data df = pd.read_csv(\"https://raw.githubusercontent.com/wesleybeckner/\"\\ \"technology_fundamentals/main/assets/truffle_rates.csv\") df = df.loc[df['rate'] > 0] df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group rate 0 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.167097 1 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.153827 2 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.100299 3 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.333008 4 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.078108 df.shape (9210, 6) Exploratory Data Analysis Q1 Finding Influential Features Which of the five features (base_cake, truffle_type, primary_flavor, secondary_flavor, color_group) of the truffles is most influential on production rate? Back your answer with both a visualization of the distributions (boxplot, kernel denisty estimate, histogram, violin plot) and a statistical test (moods median, ANOVA, t-test) Be sure: everything is labeled (can you improve your labels with additional descriptive statistical information e.g. indicate mean, std, etc.) you meet the assumptions of your statistical test Q1.1 Visualization Use any number of visualizations. Here is an example to get you started: # Example: a KDE of the truffle_type and base_cake columns fig, ax = plt.subplots(2, 1, figsize=(12,12)) sns.kdeplot(x=df['rate'], hue=df['truffle_type'], fill=True, ax=ax[0]) sns.kdeplot(x=df['rate'], hue=df['base_cake'], fill=True, ax=ax[1]) <matplotlib.axes._subplots.AxesSubplot at 0x7f549eea03d0> /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 9 missing from current font. /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 9 missing from current font. Q1.2 Statistical Analysis What statistical tests can you perform to evaluate your hypothesis from the visualizations (maybe you think one particular feature is significant). Here's an ANOVA on the truffle_type column to get you started: model = ols('rate ~ C({})'.format('truffle_type'), data=df).fit() anova_table = sm.stats.anova_lm(model, typ=2) display(anova_table) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Is this P value significant? What is the null hypothesis? How do we check the assumptions of ANOVA? base_cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(base_cake) 331.373550 5.0 2349.684756 0.0 Residual 259.606073 9204.0 NaN NaN Shapiro: 0.9281061887741089 0.0 Bartlett: 619.3727153356931 1.3175663824168166e-131 truffle_type /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Shapiro: 0.9645588994026184 1.3704698981096711e-42 /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Bartlett: 533.0206680979852 1.8031528902362296e-116 primary_flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(primary_flavor) 159.105452 47.0 71.815842 0.0 Residual 431.874171 9162.0 NaN NaN /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Shapiro: 0.9738250970840454 6.485387538059916e-38 Bartlett: 1609.0029005171464 1.848613457353585e-306 secondary_flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(secondary_flavor) 115.773877 28.0 79.884192 0.0 Residual 475.205747 9181.0 NaN NaN Shapiro: 0.9717048406600952 4.3392384038527993e-39 Bartlett: 1224.4882890761903 3.5546073028894766e-240 color_group /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(color_group) 33.878491 11.0 50.849974 1.873235e-109 Residual 557.101132 9198.0 NaN NaN Shapiro: 0.9598756432533264 1.401298464324817e-44 Bartlett: 298.6432027161358 1.6917844519244488e-57 /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Q2 Finding Best and Worst Groups Q2.1 Compare Every Group to the Whole Of the primary flavors (feature), what 5 flavors (groups) would you recommend Truffletopia discontinue? Iterate through every level (i.e. pound, cheese, sponge cakes) of every category (i.e. base cake, primary flavor, secondary flavor) and use moods median testing to compare the group distribution to the grand median rate. (98, 10) After you've computed a moods median test on every group, filter any data above a significance level of 0.05 (76, 10) Return the groups with the lowest median performance (your table need not look exactly like the one I've created) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 primary_flavor Coconut 56.8675 4.66198e-14 0.310345 0.139998 0.0856284 100 2.64572e-29 [[12, 4593], [88, 4517]] 1 secondary_flavor Wild Cherry Cream 56.8675 4.66198e-14 0.310345 0.139998 0.0856284 100 2.64572e-29 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.5563 4.30253e-15 0.310345 0.129178 0.0928782 85 2.05798e-28 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.3203 7.84617e-13 0.310345 0.145727 0.0957584 91 1.11719e-28 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.5452 4.14269e-11 0.310345 0.148964 0.10588 70 2.59384e-20 [[7, 4598], [63, 4542]] 5 secondary_flavor Mixed Berry 164.099 1.43951e-37 0.310345 0.153713 0.115202 261 6.73636e-75 [[28, 4577], [233, 4372]] 6 secondary_flavor Peppermint 66.0235 4.45582e-16 0.310345 0.129107 0.12201 86 7.6449e-37 [[5, 4600], [81, 4524]] 7 base_cake Butter 696.649 1.60093e-153 0.310345 0.15951 0.136231 905 0 [[75, 4530], [830, 3775]] 8 secondary_flavor Rum 69.5192 7.56747e-17 0.310345 0.157568 0.139834 93 4.42643e-42 [[6, 4599], [87, 4518]] 9 secondary_flavor Cucumber 175.061 5.80604e-40 0.310345 0.170015 0.14097 288 4.33234e-79 [[33, 4572], [255, 4350]] 10 primary_flavor Gingersnap 131.114 2.33844e-30 0.310345 0.159268 0.143347 192 1.01371e-69 [[17, 4588], [175, 4430]] 11 primary_flavor Cherry Cream Spice 66.3302 3.81371e-16 0.310345 0.175751 0.146272 100 1.03408e-36 [[9, 4596], [91, 4514]] 12 primary_flavor Orange Brandy 97.0624 6.71776e-23 0.310345 0.185908 0.157804 186 1.86398e-49 [[26, 4579], [160, 4445]] 13 primary_flavor Irish Cream 87.5008 8.42448e-21 0.310345 0.184505 0.176935 151 6.30631e-52 [[18, 4587], [133, 4472]] 14 base_cake Chiffon 908.383 1.47733e-199 0.310345 0.208286 0.177773 1821 0 [[334, 4271], [1487, 3118]] 15 primary_flavor Ginger Lime 40.1257 2.38138e-10 0.310345 0.225157 0.181094 100 2.63308e-18 [[18, 4587], [82, 4523]] 16 primary_flavor Doughnut 98.4088 3.40338e-23 0.310345 0.234113 0.189888 300 1.03666e-40 [[65, 4540], [235, 4370]] 17 primary_flavor Butter Milk 28.3983 9.87498e-08 0.310345 0.237502 0.190708 100 1.96333e-20 [[23, 4582], [77, 4528]] 18 primary_flavor Pecan 40.8441 1.64868e-10 0.310345 0.197561 0.192372 89 2.86564e-25 [[14, 4591], [75, 4530]] 19 secondary_flavor Dill Pickle 69.8101 6.52964e-17 0.310345 0.228289 0.19916 241 6.39241e-33 [[56, 4549], [185, 4420]] We would want to cut the following primary flavors. Check to see that you get a similar answer. rip wild cherry cream. ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] Q2.2 Beyond Statistical Testing: Using Reasoning Let's look at the total profile of the products associated with the five worst primary flavors. Given the number of different products made with any of these flavors, would you alter your answer at all? # 1. filter df for only bottom five flavors # 2. groupby all columns besides rate # 3. describe the rate column. # by doing this we can evaluate just how much sampling variety we have for the # worst performing flavors. bottom_five = ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] df.loc[df['primary_flavor'].isin(bottom_five)].groupby(list(df.columns[:-1]))['rate'].describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max base_cake truffle_type primary_flavor secondary_flavor color_group Butter Jelly Filled Pink Lemonade Butter Rum Rose 85.0 0.129178 0.137326 0.000061 0.032887 0.092878 0.171350 0.860045 Chiffon Candy Outer Wild Cherry Cream Rock and Rye Olive 17.0 0.094287 0.059273 0.010464 0.053976 0.077098 0.120494 0.229933 Chocolate Outer Gingersnap Dill Pickle Burgundy 59.0 0.133272 0.080414 0.021099 0.069133 0.137972 0.172066 0.401387 Jelly Filled Chocolate Tutti Frutti Burgundy 91.0 0.145727 0.135230 0.000033 0.044847 0.095758 0.185891 0.586570 Pound Candy Outer Coconut Wild Cherry Cream Taupe 100.0 0.139998 0.147723 0.000705 0.036004 0.085628 0.187318 0.775210 Chocolate Outer Gingersnap Rock and Rye Black 67.0 0.156160 0.110666 0.002846 0.074615 0.139572 0.241114 0.551898 Jelly Filled Gingersnap Kiwi Taupe 66.0 0.185662 0.132272 0.000014 0.086377 0.166340 0.247397 0.593016 Wild Cherry Cream Mango Taupe 53.0 0.166502 0.160090 0.001412 0.056970 0.108918 0.207306 0.787224 Q2.3 The Jelly Filled Conundrum Your boss notices the Jelly filled truffles are being produced much faster than the candy outer truffles and suggests expanding into this product line. What is your response? Use the visualization tool below to help you think about this problem, then create any visualizations or analyses of your own. sunburst charts def sun(path=[['base_cake', 'truffle_type', 'primary_flavor', 'secondary_flavor', 'color_group'], ['truffle_type', 'base_cake', 'primary_flavor', 'secondary_flavor', 'color_group']]): fig = px.sunburst(df, path=path, color='rate', color_continuous_scale='viridis', ) fig.update_layout( margin=dict(l=20, r=20, t=20, b=20), height=650 ) fig.show() interact(sun) interactive(children=(Dropdown(description='path', options=(['base_cake', 'truffle_type', 'primary_flavor', 's\u2026 <function __main__.sun>","title":"Inferential Statisitcs Data Hunt"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/#data-science-foundations-lab-2-data-hunt-ii","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's the data hunt part TWO.","title":"Data Science Foundations, Lab 2: Data Hunt II"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/#preparing-environment-and-importing-data","text":"","title":"Preparing Environment and Importing Data"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/#import-packages","text":"!pip install -U plotly Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1) Collecting plotly Downloading plotly-5.1.0-py2.py3-none-any.whl (20.6 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.6 MB 1.3 MB/s \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0) Collecting tenacity>=6.2.0 Downloading tenacity-8.0.1-py3-none-any.whl (24 kB) Installing collected packages: tenacity, plotly Attempting uninstall: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-5.1.0 tenacity-8.0.1 # our standard libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns from ipywidgets import interact # our stats libraries import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy # our scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score","title":"Import Packages"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/#import-and-clean-data","text":"df = pd.read_csv(\"https://raw.githubusercontent.com/wesleybeckner/\"\\ \"technology_fundamentals/main/assets/truffle_rates.csv\") df = df.loc[df['rate'] > 0] df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group rate 0 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.167097 1 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.153827 2 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.100299 3 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.333008 4 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.078108 df.shape (9210, 6)","title":"Import and Clean Data"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/#exploratory-data-analysis","text":"","title":"Exploratory Data Analysis"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/#q1-finding-influential-features","text":"Which of the five features (base_cake, truffle_type, primary_flavor, secondary_flavor, color_group) of the truffles is most influential on production rate? Back your answer with both a visualization of the distributions (boxplot, kernel denisty estimate, histogram, violin plot) and a statistical test (moods median, ANOVA, t-test) Be sure: everything is labeled (can you improve your labels with additional descriptive statistical information e.g. indicate mean, std, etc.) you meet the assumptions of your statistical test","title":"Q1 Finding Influential Features"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/#q11-visualization","text":"Use any number of visualizations. Here is an example to get you started: # Example: a KDE of the truffle_type and base_cake columns fig, ax = plt.subplots(2, 1, figsize=(12,12)) sns.kdeplot(x=df['rate'], hue=df['truffle_type'], fill=True, ax=ax[0]) sns.kdeplot(x=df['rate'], hue=df['base_cake'], fill=True, ax=ax[1]) <matplotlib.axes._subplots.AxesSubplot at 0x7f549eea03d0> /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 9 missing from current font. /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 9 missing from current font.","title":"Q1.1 Visualization"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/#q12-statistical-analysis","text":"What statistical tests can you perform to evaluate your hypothesis from the visualizations (maybe you think one particular feature is significant). Here's an ANOVA on the truffle_type column to get you started: model = ols('rate ~ C({})'.format('truffle_type'), data=df).fit() anova_table = sm.stats.anova_lm(model, typ=2) display(anova_table) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Is this P value significant? What is the null hypothesis? How do we check the assumptions of ANOVA? base_cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(base_cake) 331.373550 5.0 2349.684756 0.0 Residual 259.606073 9204.0 NaN NaN Shapiro: 0.9281061887741089 0.0 Bartlett: 619.3727153356931 1.3175663824168166e-131 truffle_type /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Shapiro: 0.9645588994026184 1.3704698981096711e-42 /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Bartlett: 533.0206680979852 1.8031528902362296e-116 primary_flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(primary_flavor) 159.105452 47.0 71.815842 0.0 Residual 431.874171 9162.0 NaN NaN /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Shapiro: 0.9738250970840454 6.485387538059916e-38 Bartlett: 1609.0029005171464 1.848613457353585e-306 secondary_flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(secondary_flavor) 115.773877 28.0 79.884192 0.0 Residual 475.205747 9181.0 NaN NaN Shapiro: 0.9717048406600952 4.3392384038527993e-39 Bartlett: 1224.4882890761903 3.5546073028894766e-240 color_group /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(color_group) 33.878491 11.0 50.849974 1.873235e-109 Residual 557.101132 9198.0 NaN NaN Shapiro: 0.9598756432533264 1.401298464324817e-44 Bartlett: 298.6432027161358 1.6917844519244488e-57 /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000.","title":"Q1.2 Statistical Analysis"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/#q2-finding-best-and-worst-groups","text":"","title":"Q2 Finding Best and Worst Groups"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/#q21-compare-every-group-to-the-whole","text":"Of the primary flavors (feature), what 5 flavors (groups) would you recommend Truffletopia discontinue? Iterate through every level (i.e. pound, cheese, sponge cakes) of every category (i.e. base cake, primary flavor, secondary flavor) and use moods median testing to compare the group distribution to the grand median rate. (98, 10) After you've computed a moods median test on every group, filter any data above a significance level of 0.05 (76, 10) Return the groups with the lowest median performance (your table need not look exactly like the one I've created) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 primary_flavor Coconut 56.8675 4.66198e-14 0.310345 0.139998 0.0856284 100 2.64572e-29 [[12, 4593], [88, 4517]] 1 secondary_flavor Wild Cherry Cream 56.8675 4.66198e-14 0.310345 0.139998 0.0856284 100 2.64572e-29 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.5563 4.30253e-15 0.310345 0.129178 0.0928782 85 2.05798e-28 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.3203 7.84617e-13 0.310345 0.145727 0.0957584 91 1.11719e-28 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.5452 4.14269e-11 0.310345 0.148964 0.10588 70 2.59384e-20 [[7, 4598], [63, 4542]] 5 secondary_flavor Mixed Berry 164.099 1.43951e-37 0.310345 0.153713 0.115202 261 6.73636e-75 [[28, 4577], [233, 4372]] 6 secondary_flavor Peppermint 66.0235 4.45582e-16 0.310345 0.129107 0.12201 86 7.6449e-37 [[5, 4600], [81, 4524]] 7 base_cake Butter 696.649 1.60093e-153 0.310345 0.15951 0.136231 905 0 [[75, 4530], [830, 3775]] 8 secondary_flavor Rum 69.5192 7.56747e-17 0.310345 0.157568 0.139834 93 4.42643e-42 [[6, 4599], [87, 4518]] 9 secondary_flavor Cucumber 175.061 5.80604e-40 0.310345 0.170015 0.14097 288 4.33234e-79 [[33, 4572], [255, 4350]] 10 primary_flavor Gingersnap 131.114 2.33844e-30 0.310345 0.159268 0.143347 192 1.01371e-69 [[17, 4588], [175, 4430]] 11 primary_flavor Cherry Cream Spice 66.3302 3.81371e-16 0.310345 0.175751 0.146272 100 1.03408e-36 [[9, 4596], [91, 4514]] 12 primary_flavor Orange Brandy 97.0624 6.71776e-23 0.310345 0.185908 0.157804 186 1.86398e-49 [[26, 4579], [160, 4445]] 13 primary_flavor Irish Cream 87.5008 8.42448e-21 0.310345 0.184505 0.176935 151 6.30631e-52 [[18, 4587], [133, 4472]] 14 base_cake Chiffon 908.383 1.47733e-199 0.310345 0.208286 0.177773 1821 0 [[334, 4271], [1487, 3118]] 15 primary_flavor Ginger Lime 40.1257 2.38138e-10 0.310345 0.225157 0.181094 100 2.63308e-18 [[18, 4587], [82, 4523]] 16 primary_flavor Doughnut 98.4088 3.40338e-23 0.310345 0.234113 0.189888 300 1.03666e-40 [[65, 4540], [235, 4370]] 17 primary_flavor Butter Milk 28.3983 9.87498e-08 0.310345 0.237502 0.190708 100 1.96333e-20 [[23, 4582], [77, 4528]] 18 primary_flavor Pecan 40.8441 1.64868e-10 0.310345 0.197561 0.192372 89 2.86564e-25 [[14, 4591], [75, 4530]] 19 secondary_flavor Dill Pickle 69.8101 6.52964e-17 0.310345 0.228289 0.19916 241 6.39241e-33 [[56, 4549], [185, 4420]] We would want to cut the following primary flavors. Check to see that you get a similar answer. rip wild cherry cream. ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap']","title":"Q2.1 Compare Every Group to the Whole"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/#q22-beyond-statistical-testing-using-reasoning","text":"Let's look at the total profile of the products associated with the five worst primary flavors. Given the number of different products made with any of these flavors, would you alter your answer at all? # 1. filter df for only bottom five flavors # 2. groupby all columns besides rate # 3. describe the rate column. # by doing this we can evaluate just how much sampling variety we have for the # worst performing flavors. bottom_five = ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] df.loc[df['primary_flavor'].isin(bottom_five)].groupby(list(df.columns[:-1]))['rate'].describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max base_cake truffle_type primary_flavor secondary_flavor color_group Butter Jelly Filled Pink Lemonade Butter Rum Rose 85.0 0.129178 0.137326 0.000061 0.032887 0.092878 0.171350 0.860045 Chiffon Candy Outer Wild Cherry Cream Rock and Rye Olive 17.0 0.094287 0.059273 0.010464 0.053976 0.077098 0.120494 0.229933 Chocolate Outer Gingersnap Dill Pickle Burgundy 59.0 0.133272 0.080414 0.021099 0.069133 0.137972 0.172066 0.401387 Jelly Filled Chocolate Tutti Frutti Burgundy 91.0 0.145727 0.135230 0.000033 0.044847 0.095758 0.185891 0.586570 Pound Candy Outer Coconut Wild Cherry Cream Taupe 100.0 0.139998 0.147723 0.000705 0.036004 0.085628 0.187318 0.775210 Chocolate Outer Gingersnap Rock and Rye Black 67.0 0.156160 0.110666 0.002846 0.074615 0.139572 0.241114 0.551898 Jelly Filled Gingersnap Kiwi Taupe 66.0 0.185662 0.132272 0.000014 0.086377 0.166340 0.247397 0.593016 Wild Cherry Cream Mango Taupe 53.0 0.166502 0.160090 0.001412 0.056970 0.108918 0.207306 0.787224","title":"Q2.2 Beyond Statistical Testing: Using Reasoning"},{"location":"exercises/E2_Inferential_Statistics_Data_Hunt/#q23-the-jelly-filled-conundrum","text":"Your boss notices the Jelly filled truffles are being produced much faster than the candy outer truffles and suggests expanding into this product line. What is your response? Use the visualization tool below to help you think about this problem, then create any visualizations or analyses of your own. sunburst charts def sun(path=[['base_cake', 'truffle_type', 'primary_flavor', 'secondary_flavor', 'color_group'], ['truffle_type', 'base_cake', 'primary_flavor', 'secondary_flavor', 'color_group']]): fig = px.sunburst(df, path=path, color='rate', color_continuous_scale='viridis', ) fig.update_layout( margin=dict(l=20, r=20, t=20, b=20), height=650 ) fig.show() interact(sun) interactive(children=(Dropdown(description='path', options=(['base_cake', 'truffle_type', 'primary_flavor', 's\u2026 <function __main__.sun>","title":"Q2.3 The Jelly Filled Conundrum"},{"location":"exercises/E3_Feature_Engineering/","text":"Data Science Foundations, Lab 3: Practice with Feature Engineering and Pipelines Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines and feature engineering. We will use the wine dataset. L1 Q1: Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 # Code Cell for L1 Q1 L1 Q2: Use 3 different scaling methods on the input data and evaluate how they affect VIF, kurtosis, and skew # Code Cell for L1 Q2 L1 Q3: Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using the wine dataset: dependent variable: wine quality # Code Cell for L1 Q3","title":"Practice with Feature Engineering"},{"location":"exercises/E3_Feature_Engineering/#data-science-foundations-lab-3-practice-with-feature-engineering-and-pipelines","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines and feature engineering. We will use the wine dataset.","title":"Data Science Foundations, Lab 3: Practice with Feature Engineering and Pipelines"},{"location":"exercises/E3_Feature_Engineering/#l1-q1","text":"Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 # Code Cell for L1 Q1","title":"L1 Q1:"},{"location":"exercises/E3_Feature_Engineering/#l1-q2","text":"Use 3 different scaling methods on the input data and evaluate how they affect VIF, kurtosis, and skew # Code Cell for L1 Q2","title":"L1 Q2:"},{"location":"exercises/E3_Feature_Engineering/#l1-q3","text":"Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using the wine dataset: dependent variable: wine quality # Code Cell for L1 Q3","title":"L1 Q3:"},{"location":"exercises/E4_Supervised_Learners/","text":"Data Science Foundations, Lab 4: Practice with Supervised Learners Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines, feature engineering, and applying learning algorithms. Now that we have covered supervised learning methods, and we've covered Grid Search, we will use these tools to do a sophisticated, search of hyperparameter optimization. L3 Q1: Create train and test datasets for wine quality Create new train/test datasets that are normalized (but have the same indices as the original train/test sets for comparison) # Code Cell for L1 Q1 L3 Q2: Evaluate the performance of a Random Forest on classifying wine quality # Code Cell for L1 Q2 L3 Q3: Do a grid search to optimize your Random Forest model, use whatever hyperparameters you would like # Code Cell for L1 Q3","title":"Practice with Supervised Learners"},{"location":"exercises/E4_Supervised_Learners/#data-science-foundations-lab-4-practice-with-supervised-learners","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines, feature engineering, and applying learning algorithms. Now that we have covered supervised learning methods, and we've covered Grid Search, we will use these tools to do a sophisticated, search of hyperparameter optimization.","title":"Data Science Foundations, Lab 4: Practice with Supervised Learners"},{"location":"exercises/E4_Supervised_Learners/#l3-q1","text":"Create train and test datasets for wine quality Create new train/test datasets that are normalized (but have the same indices as the original train/test sets for comparison) # Code Cell for L1 Q1","title":"L3 Q1:"},{"location":"exercises/E4_Supervised_Learners/#l3-q2","text":"Evaluate the performance of a Random Forest on classifying wine quality # Code Cell for L1 Q2","title":"L3 Q2:"},{"location":"exercises/E4_Supervised_Learners/#l3-q3","text":"Do a grid search to optimize your Random Forest model, use whatever hyperparameters you would like # Code Cell for L1 Q3","title":"L3 Q3:"},{"location":"exercises/E5_Writing_Unit_Tests/","text":"Data Science Foundations, Lab 5: Writing Unit Tests Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com We will try our hand at writing unit tests Import Libraries # for numpy section import numpy as np np.random.seed(42) # for debugging section import random from contextlib import contextmanager import sys, os @contextmanager def suppress_stdout(): with open(os.devnull, \"w\") as devnull: old_stdout = sys.stdout sys.stdout = devnull try: yield finally: sys.stdout = old_stdout \ud83c\udfcb\ufe0f Exercise 1: Debug a Class Method Your friend is developing a new pokemon game. They are excited to release but are running into some trouble! class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None class Pokemon(): def __init__(self, name, weight, speed, type_): self.name = name self.weight = weight self.speed = speed self.type_ = type_ class FastBall(Pokeball): def __init__(self, contains=None, type_name=\"Fastball\"): Pokeball.__init__(self, contains, type_name) self.catch_rate = 0.6 def catch_fast(self, pokemon): if pokemon.speed > 100: if self.contains == None: self.contains = pokemon print(pokemon.name, \"has been captured\") else: print(\"Pokeball is not empty\") else: self.catch(pokemon) They're concerned that the object FastBall doesn't return the pokemon's name when executing print(fast.contains) when they know the pokeball contains a pokemon. Help them find the bug, then write the following unit tests: showing that the pokeball updates properly with the name of the pokemon after it makes a capture of a pokemon with a speed > 100 showing that the catch_rate of 0.6 is resulting in a 60% catch rate for pokemon with speeds < 100 # Your friend shows you this code fast = FastBall() mewtwo = Pokemon(name='Mewtwo', weight=18, speed=110, type_='Psychic') print(fast.contains) fast.catch_fast(mewtwo) # this is the line they are concerned about # why does this not return MewTwo? print(fast.contains) fast.catch_fast(mewtwo) None Mewtwo has been captured <__main__.Pokemon object at 0x7fd4bfe612d0> Pokeball is not empty Part 2 (Optional): Use a Test Runner Create the following files: pokemon.py test_pokemon.py paste the following into pokemon.py : import random import numpy as np class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None class Pokemon(): def __init__(self, name, weight, speed, type_): self.name = name self.weight = weight self.speed = speed self.type_ = type_ class FastBall(Pokeball): def __init__(self, contains=None, type_name=\"Fastball\"): Pokeball.__init__(self, contains, type_name) self.catch_rate = 0.6 def catch_fast(self, pokemon): if pokemon.speed > 100: if self.contains == None: self.contains = pokemon print(pokemon.name, \"has been captured\") else: print(\"Pokeball is not empty\") else: self.catch(pokemon) in test_pokemon.py paste any unit tests you've written along with the imports at the top of the file (be sure to import any other libraries you used in your unit tests as well) from pokemon import * import random import numpy as np ### YOUR UNIT TESTS HERE ### def test_<name_of_your_test>(): # .... assert <your assert statement> make sure pokemon.py and test_pokemon.py are in the same directory then run the command pytest from the command line. You should get a readout like the following ================================================= test session starts ================================================== platform linux -- Python 3.8.1, pytest-6.2.1, py-1.10.0, pluggy-0.13.1 rootdir: /mnt/c/Users/wesley/Documents/apps/temp_c3_l2 plugins: dash-1.20.0, anyio-2.2.0 collected 1 item test_pokemon.py . [100%] ================================================== 1 passed in 0.06s ===================================================","title":"Practice with Writing Unit Tests"},{"location":"exercises/E5_Writing_Unit_Tests/#data-science-foundations-lab-5-writing-unit-tests","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com We will try our hand at writing unit tests","title":"Data Science Foundations, Lab 5: Writing Unit Tests"},{"location":"exercises/E5_Writing_Unit_Tests/#import-libraries","text":"# for numpy section import numpy as np np.random.seed(42) # for debugging section import random from contextlib import contextmanager import sys, os @contextmanager def suppress_stdout(): with open(os.devnull, \"w\") as devnull: old_stdout = sys.stdout sys.stdout = devnull try: yield finally: sys.stdout = old_stdout","title":"Import Libraries"},{"location":"exercises/E5_Writing_Unit_Tests/#exercise-1-debug-a-class-method","text":"Your friend is developing a new pokemon game. They are excited to release but are running into some trouble! class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None class Pokemon(): def __init__(self, name, weight, speed, type_): self.name = name self.weight = weight self.speed = speed self.type_ = type_ class FastBall(Pokeball): def __init__(self, contains=None, type_name=\"Fastball\"): Pokeball.__init__(self, contains, type_name) self.catch_rate = 0.6 def catch_fast(self, pokemon): if pokemon.speed > 100: if self.contains == None: self.contains = pokemon print(pokemon.name, \"has been captured\") else: print(\"Pokeball is not empty\") else: self.catch(pokemon) They're concerned that the object FastBall doesn't return the pokemon's name when executing print(fast.contains) when they know the pokeball contains a pokemon. Help them find the bug, then write the following unit tests: showing that the pokeball updates properly with the name of the pokemon after it makes a capture of a pokemon with a speed > 100 showing that the catch_rate of 0.6 is resulting in a 60% catch rate for pokemon with speeds < 100 # Your friend shows you this code fast = FastBall() mewtwo = Pokemon(name='Mewtwo', weight=18, speed=110, type_='Psychic') print(fast.contains) fast.catch_fast(mewtwo) # this is the line they are concerned about # why does this not return MewTwo? print(fast.contains) fast.catch_fast(mewtwo) None Mewtwo has been captured <__main__.Pokemon object at 0x7fd4bfe612d0> Pokeball is not empty","title":"\ud83c\udfcb\ufe0f Exercise 1: Debug a Class Method"},{"location":"exercises/E5_Writing_Unit_Tests/#part-2-optional-use-a-test-runner","text":"Create the following files: pokemon.py test_pokemon.py paste the following into pokemon.py : import random import numpy as np class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None class Pokemon(): def __init__(self, name, weight, speed, type_): self.name = name self.weight = weight self.speed = speed self.type_ = type_ class FastBall(Pokeball): def __init__(self, contains=None, type_name=\"Fastball\"): Pokeball.__init__(self, contains, type_name) self.catch_rate = 0.6 def catch_fast(self, pokemon): if pokemon.speed > 100: if self.contains == None: self.contains = pokemon print(pokemon.name, \"has been captured\") else: print(\"Pokeball is not empty\") else: self.catch(pokemon) in test_pokemon.py paste any unit tests you've written along with the imports at the top of the file (be sure to import any other libraries you used in your unit tests as well) from pokemon import * import random import numpy as np ### YOUR UNIT TESTS HERE ### def test_<name_of_your_test>(): # .... assert <your assert statement> make sure pokemon.py and test_pokemon.py are in the same directory then run the command pytest from the command line. You should get a readout like the following ================================================= test session starts ================================================== platform linux -- Python 3.8.1, pytest-6.2.1, py-1.10.0, pluggy-0.13.1 rootdir: /mnt/c/Users/wesley/Documents/apps/temp_c3_l2 plugins: dash-1.20.0, anyio-2.2.0 collected 1 item test_pokemon.py . [100%] ================================================== 1 passed in 0.06s ===================================================","title":"Part 2 (Optional): Use a Test Runner"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/","text":"Data Science Foundations, Project Part 1: Statistical Analysis Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to perform statistical analysis on data generated from our tictactoe program! 1.0 Preparing Environment and Importing Data back to top 1.0.1 Import Packages back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe: # can preset winner and starting player def __init__(self, winner='', start_player=''): self.winner = winner self.start_player = start_player self.board = {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' ',} self.win_patterns = [[1,2,3], [4,5,6], [7,8,9], [1,4,7], [2,5,8], [3,6,9], [1,5,9], [7,5,3]] # the other functions are now passed self def visualize_board(self): print( \"|{}|{}|{}|\\n|{}|{}|{}|\\n|{}|{}|{}|\\n\".format(*self.board.values()) ) def check_winning(self): for pattern in self.win_patterns: values = [self.board[i] for i in pattern] if values == ['X', 'X', 'X']: self.winner = 'X' # we update the winner status return \"'X' Won!\" elif values == ['O', 'O', 'O']: self.winner = 'O' return \"'O' Won!\" return '' def check_stalemate(self): if (' ' not in self.board.values()) and (self.check_winning() == ''): self.winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine(TicTacToe): def __init__(self, setup='auto'): super().__init__() self.setup = setup def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) # we're going to allow the user to quit the game from the input line if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player)) move = int(move) if self.board[move] != ' ': continue else: break else: while True: move = random.randint(1,9) if self.board[move] != ' ': continue print('test') else: break self.board[move] = player_label # the winner varaible will now be check within the board object self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self 1.0.2 Load Dataset back to top data = {} for i in range(1000): game = GameEngine() game.setup_game() board = game.play_game() data['game {}'.format(i)] = {'board': board.board, 'winner': board.winner, 'starting player': board.start_player} \u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m |X|O| | | | |O| |X| | | |X|O| | | |O|O| |X| | | |X|O| | | |O|O| |X|X| | |X|O| | | |O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | | |X|O| | | |O| |X| |X| | |X|O| | | |O| |X| |X| |O|X|O| | |X|O| |X| |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| 'X' Won! |O|X|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X| | | | | |O| |X|O|O| |X|X| | | | |O| 'O' Won! |X|O|O| |X|X|O| | | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| | | |X| |O|X|O| |O| |X| | | |X| |O|X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| |X|O| | |X| | | | | |O| |X|O|O| |X| | | | | |O| |X|O|O| |X|X| | | | |O| 'O' Won! |X|O|O| |X|X|O| | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | |X| | | |O|O|X| | |O| | |X| | | |O|O|X| | |O|X| |X| | | |O|O|X| |O|O|X| |X| | | 'X' Won! |O|O|X| |O|O|X| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O|X| | | | |X| |X|O| | |O|X| | | |O|X| |X|O| | |O|X| | | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | | | |O|X|O| |X|O|X| | | | | |O|X|O| |X|O|X| | | |O| |O|X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | |X|X| | | |O|O| | |X|O| |X|X| | | |O|O| 'X' Won! | |X|O| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O| | | | |X| 'O' Won! |O|X|X| |O|O| | |O| |X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | | | |O| | |O|X| |O|X| | | |X|O| | |O|X| |O|X|O| | |X|O| | |O|X| |O|X|O| |X|X|O| | |O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| | |O| | | | |O| |X|O|X| |X|O| | 'O' Won! | |O|O| |X|O|X| |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | |X| | |O| | | | |O|X| |O|X| | |O| | | | |O|X| |O|X|X| |O| | | 'O' Won! |O|O|X| |O|X|X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | | | | |X| |X| |O|O| | |X| | | |X| |X| |O|O| | |X|O| | |X| |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| |X|O| | | |X|O| | | |O| |X|O| | | |X|O| | | |O| |X|O|X| | |X|O| | |O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X| | | | | |O|X| | |O| |X| | | |O| |O|X| | |O| |X| |X| |O| |O|X| | |O| |X| |X| |O| |O|X|O| |O| |X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| |O| | | |O|X|X| | | |O| |O| |X| |O|X|X| | | |O| |O| |X| |O|X|X| | |O|O| 'X' Won! |O| |X| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| |O| |X|O| | | |O|X| |X| |O| |X|O| | | |O|X| |X| |O| |X|O|O| 'X' Won! |X|O|X| |X| |O| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | |X| | | | |O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X| | | |X|O|O| |O|X|O| |X| | | 'X' Won! |X|O|O| |O|X|O| |X| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X| | | |O|X| | | |O|O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X|O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O|O| | |X| |X| |O|X| | |O|O| | |X|O|X| |O|X| | |O|O| | |X|O|X| |O|X|X| |O|O| | 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | |O| |X| |O| |X|O|X| | | |O| |X|O|O| |X|O|X| | | |O| |X|O|O| |X|O|X| | |X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X| | | | |O| |O|O| | 'X' Won! |X|X|X| | | |O| |O|O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | |O| | |O| |X| |X| | | |X|O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O|X| |O| |X| 'O' Won! |X|O|O| |X|O|X| |O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O|O|X| | | | | |X|X| | |O|O|X| | | | | |X|X| | |O|O|X| | | |O| 'X' Won! |X|X|X| |O|O|X| | | |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X|X| | | |O| |O| |O| | |X|X| | | |O| |O| |O| | |X|X| |X| |O| |O| |O| |O|X|X| |X| |O| |O| |O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O| |O| | | | | |O|X|X| |O| |O| | |X| | 'O' Won! |O|X|X| |O| |O| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | | |O| | |X|O|X| | |X| | | |O| | |X|O|X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|X| |X|O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O|O| | |O|X| | |X|X| | |O|O| | |O|X| |O|X|X| | |O|O| 'X' Won! | |O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| | | | | |O|X| | |X|X|O| |O| | | |O|X| | |X|X|O| 'X' Won! |O| |X| |O|X| | |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| |X| | | |O| | | 'O' Won! |X| |O| |X|O| | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X| | |O| |O| 'X' Won! |X|X|X| |O|X| | |O| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| |X| |X|O| | | |O| | |X| |X| |X|O|O| | |O| | 'X' Won! |X|X|X| |X|O|O| | |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | |O| | |X| | |X| |O| | |X|O| | |X| | 'O' Won! |X| |O| | |X|O| | |X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | | |X|X| |O|O|X| |O|O| | | |X|X| 'X' Won! |O|O|X| |O|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | |O|O| | 'X' Won! | | |O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | |O| | | |X|X| | | |O| | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | 'X' Won! |O|O|X| |O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X| | | |X|X| | |O| |O| |X| | | |X|X| | |O| |O| |X|O| | |X|X| | |O|X|O| |X|O| | |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | | |X|X| | |O|O| |O|X| | |O|X|X| | |O|O| |O|X| | |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| | |X|O| |O| | | | |X|O| |X|X|O| 'O' Won! |O| |O| | |X|O| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X|X|O| |O|X| | | | |O| |X|X|O| |O|X| | 'X' Won! | |X|O| |X|X|O| |O|X| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| |X|O| | |O|X| | | | |X| |X|O|O| |O|X| | | |X|X| |X|O|O| |O|X| | | |X|X| |X|O|O| |O|X|O| 'X' Won! |X|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| |X| |O| | |O| | | |O|X| |X| |O| | |O|X| | |O|X| |X| |O| | |O|X| |O|O|X| |X| |O| | |O|X| |O|O|X| |X|X|O| 'O' Won! |O|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | | |X| | |X|O|X| | |O| | | |X| | |X|O|X| |O|O| | | |X| | |X|O|X| |O|O|X| | |X| | |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| |X| | | |O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X|O|X| 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O|X| | | | | | |O| | |X|O|X| |O| | | | |O| | |X|O|X| |O| |X| 'O' Won! | |O| | |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X| | | |O|X| | | |X| |O|X| | |O|O|X| 'X' Won! |X| |X| |O|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | | | | |X|X| | |O|O|X| |O| | | |X|X| | 'X' Won! |O|O|X| |O|X| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| | | |O| |X| |X| |O| |X| | | |O| |X| |X| |O| |X|O| | 'X' Won! |O| |X| |X|X|O| |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | |X| | | |O|O| | | |X| | |X|O| |X|O|O| | | |X| | |X|O| |X|O|O| |O| |X| | |X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | |O| | |X| | |O|X| | | | |O| | |X| | |O|X|X| | |O|O| | |X| | |O|X|X| | |O|O| |X|X| | |O|X|X| | |O|O| |X|X|O| |O|X|X| 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| |O| | |O| | |X| |X| |X| |O| | |O| | |X|O|X| |X| |O| | |O| | 'X' Won! |X|O|X| |X| |O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O|X| |O| | | |X| | | |O|O|X| |O|X| | |X| | | |O|O|X| |O|X| | |X| |O| |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O| |O| | | |X| | |O|X| |O|X|O| | | |X| |O|O|X| |O|X|O| |X| |X| |O|O|X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O|X| | |X| |O| |O| | | |O|X| | |X| |O| |O| | | |O|X|X| |X| |O| |O| | | |O|X|X| |X|O|O| 'X' Won! |O| |X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| | | |O|O|X| |O| |X| |X|X| | |O|O|X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| | |O| | |O|O|X| | | |X| | |O| | 'X' Won! |O|O|X| | | |X| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O|O| |O| | | |X|X| | |X|O|O| |O| | | |X|X| | |X|O|O| |O|O| | |X|X| | |X|O|O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X|X| | |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X|X| | 'O' Won! |O|O|O| |X|X|O| |X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O| | |X|O| | | |X| | |X|O| | |X|O| | |O|X| | |X|O| |X|X|O| | |O|X| |O|X|O| |X|X|O| | |O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | |X| | | |X|O| | |O|X|O| |X| | | 'X' Won! |X|O|X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| | | |X| |X| |O|O| | |X| |O| |X| |X| 'X' Won! |O|O| | |X| |O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O|X| |O| | | | |X| | | |O|X| |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| |O|O|X| |O|O| | 'X' Won! | |X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X| | | | |X| | |X|O|O| |X| | | |O|X| | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| 'X' Won! |X|O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | |O| | |X| | | |O| |X| |X|O| | |X| | | |O| |X| |X|O| | |X|O| | |O| |X| |X|O|X| |X|O| | |O| |X| 'O' Won! |X|O|X| |X|O| | |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| |O| |O| |X| | | |X| |O| |O| |O| |X|X| | 'O' Won! |X| |O| |O| |O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| | |X| |O| | |X| | | |O| | |X| |O| |O|X| | | |O| | |X| |O| |O|X|X| |O|O| | |X| |O| |O|X|X| |O|O| | |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |X|X| | |O| | | |O|X|O| |X|X| | 'X' Won! |O|X| | |O|X|O| |X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O|O| | | |O|X| | |X|X| |O|O| | |O|O|X| | |X|X| |O|O| | 'X' Won! |O|O|X| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| | | | |X|O| |X| | | |O| | | | |X|O| |X| | | |O| |O| | |X|O| |X| | | |O|X|O| 'O' Won! | |X|O| |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O|O| |X|O|X| | | | | |X|O|O| 'O' Won! |X|O|X| | |O| | |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | |O|X| | |O|O| | | |X| | |O|X| | |O|O| | |X|X| 'O' Won! | |O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| | | |X|X|O| 'O' Won! |O| |X| |O|O| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| | |O| | | |O| | |X|X|O| |X|O| | | |O| | |X|X|O| |X|O|O| | |O| | 'X' Won! |X|X|O| |X|O|O| |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X|X| |O|X| | |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | |X| | | |X| |O| 'O' Won! |O| | | |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | |O| |X| | |O| | | |X| | |O| |X| |X|O| | | |X| | |O| |X| |X|O|O| | |X| | |O|X|X| |X|O|O| | |X| | |O|X|X| |X|O|O| |O|X| | It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X|O|O| | | | | | |X|X| |X|O|O| | | |O| | |X|X| |X|O|O| | |X|O| | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| 'O' Won! | |O| | | |O|X| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| |X| |X| | | |X| | |O|O| |X| |X| |O| |X| | |O|O| |X| |X| |O|X|X| 'O' Won! | |O|O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | | | |X|X| | 'O' Won! |O|O|O| | | | | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| |X| | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X| |X| | |X| | |O| |O| |X| |X| | |X|O| |O| |O| 'X' Won! |X|X|X| | |X|O| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | |O|X| 'X' Won! |X| | | | |X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| | | |O| |X| |O| |X| |X| |O| |O| |X| |O| |X| |X| 'X' Won! |O| |O| |X| |O| |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O| |O| | |X| | | |X| | |O| |O| | |X| | |O|X| | 'X' Won! |O|X|O| | |X| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | |X|X| | | | |O| |O| | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | |O| | |X|O| |X| |X| | | |O| | |X|O| |X| |X| | |O|O| | |X|O| |X| |X| |X|O|O| | |X|O| |X|O|X| |X|O|O| | |X|O| 'X' Won! |X|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| | | |X|X|O| 'O' Won! |O| |X| |O|O| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |X|X| | | |X| | |O|O| | |X|X| |O| |X| 'X' Won! |X|O|O| | |X|X| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| |X| | | | | |O| |X|O|O| |X| | | | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | | | |O| | |X| | |O|X| | | | |O| | |X|X| |O|X| | |O| |O| | |X|X| |O|X|X| |O| |O| | |X|X| 'O' Won! |O|X|X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X| | | | |X|X| 'O' Won! |O|O|O| |X| | | | |X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| |O| 'X' Won! |O| |X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X| |X| |X| |O| |O| 'X' Won! |O| |X| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X|O| | | |O| | | |X| |X|X|O| | | |O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| |X|O|O| | | |X| |X|X|O| |X|O|O| | |O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | |X| | | |X| |O| |O| | | |X| | | |X|O|O| |O| | | |X| | | |X|O|O| |O| |X| |X|O| | |X|O|O| |O| |X| |X|O|X| |X|O|O| |O| |X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | | |X| |X|O| | | | | | |O| |X| |X|O| | | | | | |O|X|X| |X|O| | | |O| | |O|X|X| |X|O| | | |O|X| |O|X|X| 'O' Won! |X|O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | | |X| |O| | | |O|O|X| | | |X| |O| | | |O|O|X| |X| |X| |O| |O| |O|O|X| |X| |X| 'X' Won! |O| |O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | |X|X| | |X|O| | |O| | | |X|X| | |X|O| | |O|O| | |X|X| |X|X|O| | |O|O| 'O' Won! | |X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | | |X| |O|X| | | | |O| | |X|X| |O|X| | | | |O| |O|X|X| |O|X| | | | |O| |O|X|X| |O|X|X| | | |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| |O|X| | | | | | | |O|X| |O|X| | | | | | |X|O|X| |O|X| | | | |O| |X|O|X| |O|X| | |X| |O| |X|O|X| |O|X|O| |X| |O| |X|O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| |O| | | | |X|O| |X|X|O| |O| | | |O|X|O| |X|X|O| |O| | | 'X' Won! |O|X|O| |X|X|O| |O|X| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| |O|X| | |X| |O| | | |O| |O|X|X| |X| |O| | | |O| |O|X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |X| | | | | |X|O| | | | |X| | |O| | |X|O| | | | |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O|X| |X|O|O| |O| |X| |X|O|X| 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | |X| | | |X| |O| |O| |O| |X| | | |X| |O| |O| |O| |X| | | |X| |O|X|O| |O|O|X| | | |X| |O|X|O| |O|O|X| |X| |X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | | | |O| |X| |X| |O| | | | |X|O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O|X| | |X|O| |X| |X| |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O|X|O| | | |X| | | | | |O|X|O| | | |X| |O| | | |O|X|O| | | |X| |O| |X| 'O' Won! |O|X|O| |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | |X|O| | | |O|X| | |X| | |X|O|O| | |O|X| | |X| | |X|O|O| |X|O|X| | |X| | |X|O|O| |X|O|X| | |X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O|X| | |O| |X| | | |O| |O|X| | |O| |X| | | |O| |O|X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | |X| | |O| | |O| |X| | |O|X| | |O| | |O| |X| |X|O|X| | |O| | |O| |X| |X|O|X| | |O|O| |O| |X| |X|O|X| |X|O|O| |O| |X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| |O| | | |X| | | |O| |X| |O| |X| |X| | | |O|O|X| |O| |X| |X| | | 'X' Won! |O|O|X| |O|X|X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | | |X| |X|O| | |O| |O| | | |X| |X|O| | |O| |O| |X| |X| 'O' Won! |X|O| | |O|O|O| |X| |X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O|X| | |O|X| | |X| | | |O|X|O| |O|X| | |X| |X| |O|X|O| 'O' Won! |O|X| | |X|O|X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O| |O| |X|X| | | | | | |O| |O| |X|X| | | |O| | |O| |O| |X|X| | |X|O| | |O| |O| |X|X| | |X|O|O| |O|X|O| |X|X| | |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| | | |O| |X| |X|O|O| |X| | | |O| |X| |X|O|O| |X| | | |O|O|X| 'X' Won! |X|O|O| |X|X| | |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | |O| | | | |X|X| | |O| | |O|X| | | |X|X| | |O| | |O|X|O| | |X|X| | |O|X| |O|X|O| | |X|X| |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O| |X|O| | | |X| | | |X|O| |X|O|O| | |X| | | |X|O| |X|O|O| |X|X| | 'O' Won! | |X|O| |X|O|O| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X|O| | |X| |O| | |O| | |X|O| | |X|X|O| | |O|O| |X|O| | |X|X|O| 'X' Won! |X|O|O| |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| |O| | | |X|O| | | |O|X| |O| | | |X|O|X| | |O|X| 'O' Won! |O|O| | |X|O|X| | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O|X| |O| | | |O| | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X| | |O| |O| |X|O|X| |O|X| | |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | | | |O|X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X| |O| |O|X|O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | |O| |X| |X| |X|O| | | | |O| |X| |X| |X|O| | | |O|O| 'X' Won! |X|X|X| |X|O| | | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X|X| | |X| | | |O| |O| |X|X| | |X|O| | |O| |O| |X|X| | |X|O|X| |O| |O| |X|X|O| |X|O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X|O| | | |O| | | |X|O| |X|O|X| | |O| | 'O' Won! | |X|O| |X|O|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | | |X|O| |O| | | |X| | | |X|X|O| |O| | | |X| |O| |X|X|O| |O| | | 'X' Won! |X| |O| |X|X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | |X|X| | | | |O| | | |O| |X|X| | | | |O| | |X|O| |X|X| | | | |O| |O|X|O| |X|X| | 'X' Won! | | |O| |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | | | |O|X|O| | | |X| | |X| | |O|X|O| |O| |X| | |X| | |O|X|O| |O| |X| |X|X| | |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X| | |O|X|O| | | |O| | |X|X| |O|X|O| | | |O| | |X|X| |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X| | |O|X| |O| |O| | | |X| | |O|X| |O| |O| | | |X| |X|O|X| 'O' Won! |O|O|O| | | |X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | | |X|O| |O| | | 'X' Won! | |X| | | |X|O| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | |X| | | |O|X|O| |X| | | |X| | | |O|X|O| |X| |O| |X|X| | |O|X|O| |X| |O| 'O' Won! |X|X|O| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| |X| | | | |X| | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O|X| | |X| | |X|O|O| |X|O|X| | |X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O|X| | | | | |O|O|X| | |O|X| 'X' Won! | | |X| |O|O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X|X| | | | | | |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | |X| |O| |O|O|X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| | |O| | | |O| | |X| |X| |X|O| | | |O| | |X| |X| |X|O| | |O|O| | 'X' Won! |X|X|X| |X|O| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | | | | |O|X|O| | |O|X| | | | | |O|X|O| | |O|X| |X| | | 'O' Won! |O|X|O| | |O|X| |X| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|O| | | | | |O|X|X| | |O|O| | |X| | 'O' Won! |O|X|X| | |O|O| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | | |O|X| |X|O| | | | |X| | |O|X| |X|O| | 'O' Won! | |O|X| | |O|X| |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | |O| |O| | | 'X' Won! |X|X|X| | | |O| |O| | | 1.1 Clean Data We will first need to organize the data into a parsable format. Q1 What is the object data and what does it contain? what are the keys of data? what are the keys of each game? # inspect data below by grabbing the first key in data # what are the three different keys within each game? data['game 0'] {'board': {1: 'X', 2: ' ', 3: 'O', 4: 'X', 5: 'X', 6: 'O', 7: 'O', 8: 'X', 9: 'O'}, 'starting player': 'X', 'winner': 'O'} Q2 Using those keys, iterate through every game in data and append the board, the winner, and the starting player to separate lists. Call these lists: boards, winners, and starters boards = [] winners = [] starters = [] for game in data: # YOUR CODE HERE Q3 Make a dataframe out of the list boards and call it df . Make a series out of the list winners . Make a series out of the list starters . Make a new column of df called \"Winner\" and set it equal to the pandas Series of the winners. Make a new column of df called \"Starter\" and set it equal to the pandas Series of the starters. # YOUR CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 Winner Starter 0 X O O O X O O 1 O O X O X X O X O O O 2 O X X X X O O X X 3 X X O O O X X O O Stalemate O 4 O O X O O X X X X O 1.2 Inferential Analysis We're going to use Bayes Rule or Bayesian Inference to make a probability of winning based on positions of the board. The formula is: $ P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)}$ Where $\\cap$ is the intersection of $A$ and $B$. The example we will use is the following: what is the probability of 'O' being the winner, given that they've played the center piece. $B$ = 'O' played the center piece $A$ = 'O' won the game So what is probability? We will define it in terms of frequencies. So if we are for instance asking what is the probability of player 'O' being in the center piece, it would be defined as: $ P(B) = \\frac{|O_c|} {|O_c| + |X_c| + |empty|}$ Where the pipes, | | , or cardinality represent the count of the indicated observation or set. In this case $O_c$ (O being in the center) and $X_c$ (X being in the center). Oc_Xc_empty = df[5].value_counts().sum() Oc_Xc_empty 1000 # example of assessing the probability of B, O playing the center piece player = 'O' Oc = (df[5] == player).value_counts() Oc_Xc_empty = df[5].value_counts().sum() Oc/Oc_Xc_empty False 0.577 True 0.423 Name: 5, dtype: float64 # we can also clean this up and replace the denominator with the whole # observation space (which is just the total number of games, df.shape[0]). # example of assesing probabiliy of A (df['Winner'] == 'O').value_counts()/df.shape[0] False 0.571 True 0.429 Name: Winner, dtype: float64 The $P(B|A) * P(A)$ is the intersection of $B$ and $A$. The intersection is defined as the two events occuring together. Continuing with the example, the probablity of 'O' playing the center piece AND 'O' being the winner is the number of times these observations occured together divided by the whole observation space : # in this view, the total times A and B occured together is 247 player = 'O' df.loc[(df['Winner'] == player) & (df[5] == player)].shape[0] 247 # the total observation space is 1000 (1000 games) df.shape[0] 1000 And so we get: $P(B|A) * P(A) = \\frac{247} {1000} = 0.247 $ In code: df.loc[(df['Winner'] == player) & (df[5] == player)].shape[0]/df.shape[0] 0.247 1.2.1 Behavioral Analysis of the Winner Q4 define the 3 different board piece types and label them middle , side , and corner . Middle should be an int and the other two should be lists. # define the 3 different board piece types # middle = # side = # corner = 1.2.1.1 What is the probability of winning after playing the middle piece? Q5 # A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'X' # define the intersection of A AND B, A_B # A_B = # define prob B # B = # return A_B over B (The prob B given A) A_B / B 0.5732758620689655 Q6 # A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'O' # define the intersection of A AND B, A_B # A_B = # define prob B # B = # return A_B over B (The prob B given A) A_B / B 0.5839243498817968 1.2.1.2 What is the probability of winning after playing a side piece? Q7 # A intersect B: O played side and O won / tot games # B: O played side / tot games player = 'O' A_B = df.loc[(df[side].T.apply(lambda x: player in x.values)) & (df['Winner'] == player)].shape[0] / df.shape[0] B = df.loc[(df[side].T.apply(lambda x: player in x.values))].shape[0] /\\ df.shape[0] A_B / B 0.4158609451385117 # A intersect B: X played side and X won / tot games # B: X played side / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.38845460012026456 1.2.1.3 What is the probability of winning after playing a corner piece? Q8 # A intersect B: O played corner and O won / tot games # B: O played corner / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.4779116465863454 Q9 # A intersect B: X played corner and X won / tot games # B: X played corner / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.47386964180857316 Are these results surprising to you? Why? This resource may be illustrative. 1.3 Improving the Analysis In this analysis, we only tracked what moves were made, not the order they were made in. It really limited our assessment! How might we change our recording of the games to track order of moves as well? Do we need to track all the moves or just the first and the winner?","title":"Statistical Analysis of TicTacToe"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#data-science-foundations-project-part-1-statistical-analysis","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to perform statistical analysis on data generated from our tictactoe program!","title":"Data Science Foundations, Project Part 1: Statistical Analysis"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#10-preparing-environment-and-importing-data","text":"back to top","title":"1.0 Preparing Environment and Importing Data"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#101-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe: # can preset winner and starting player def __init__(self, winner='', start_player=''): self.winner = winner self.start_player = start_player self.board = {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' ',} self.win_patterns = [[1,2,3], [4,5,6], [7,8,9], [1,4,7], [2,5,8], [3,6,9], [1,5,9], [7,5,3]] # the other functions are now passed self def visualize_board(self): print( \"|{}|{}|{}|\\n|{}|{}|{}|\\n|{}|{}|{}|\\n\".format(*self.board.values()) ) def check_winning(self): for pattern in self.win_patterns: values = [self.board[i] for i in pattern] if values == ['X', 'X', 'X']: self.winner = 'X' # we update the winner status return \"'X' Won!\" elif values == ['O', 'O', 'O']: self.winner = 'O' return \"'O' Won!\" return '' def check_stalemate(self): if (' ' not in self.board.values()) and (self.check_winning() == ''): self.winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine(TicTacToe): def __init__(self, setup='auto'): super().__init__() self.setup = setup def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) # we're going to allow the user to quit the game from the input line if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player)) move = int(move) if self.board[move] != ' ': continue else: break else: while True: move = random.randint(1,9) if self.board[move] != ' ': continue print('test') else: break self.board[move] = player_label # the winner varaible will now be check within the board object self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self","title":"1.0.1 Import Packages"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#102-load-dataset","text":"back to top data = {} for i in range(1000): game = GameEngine() game.setup_game() board = game.play_game() data['game {}'.format(i)] = {'board': board.board, 'winner': board.winner, 'starting player': board.start_player} \u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m |X|O| | | | |O| |X| | | |X|O| | | |O|O| |X| | | |X|O| | | |O|O| |X|X| | |X|O| | | |O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | | |X|O| | | |O| |X| |X| | |X|O| | | |O| |X| |X| |O|X|O| | |X|O| |X| |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| 'X' Won! |O|X|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X| | | | | |O| |X|O|O| |X|X| | | | |O| 'O' Won! |X|O|O| |X|X|O| | | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| | | |X| |O|X|O| |O| |X| | | |X| |O|X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| |X|O| | |X| | | | | |O| |X|O|O| |X| | | | | |O| |X|O|O| |X|X| | | | |O| 'O' Won! |X|O|O| |X|X|O| | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | |X| | | |O|O|X| | |O| | |X| | | |O|O|X| | |O|X| |X| | | |O|O|X| |O|O|X| |X| | | 'X' Won! |O|O|X| |O|O|X| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O|X| | | | |X| |X|O| | |O|X| | | |O|X| |X|O| | |O|X| | | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | | | |O|X|O| |X|O|X| | | | | |O|X|O| |X|O|X| | | |O| |O|X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | |X|X| | | |O|O| | |X|O| |X|X| | | |O|O| 'X' Won! | |X|O| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O| | | | |X| 'O' Won! |O|X|X| |O|O| | |O| |X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | | | |O| | |O|X| |O|X| | | |X|O| | |O|X| |O|X|O| | |X|O| | |O|X| |O|X|O| |X|X|O| | |O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| | |O| | | | |O| |X|O|X| |X|O| | 'O' Won! | |O|O| |X|O|X| |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | |X| | |O| | | | |O|X| |O|X| | |O| | | | |O|X| |O|X|X| |O| | | 'O' Won! |O|O|X| |O|X|X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | | | | |X| |X| |O|O| | |X| | | |X| |X| |O|O| | |X|O| | |X| |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| |X|O| | | |X|O| | | |O| |X|O| | | |X|O| | | |O| |X|O|X| | |X|O| | |O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X| | | | | |O|X| | |O| |X| | | |O| |O|X| | |O| |X| |X| |O| |O|X| | |O| |X| |X| |O| |O|X|O| |O| |X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| |O| | | |O|X|X| | | |O| |O| |X| |O|X|X| | | |O| |O| |X| |O|X|X| | |O|O| 'X' Won! |O| |X| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| |O| |X|O| | | |O|X| |X| |O| |X|O| | | |O|X| |X| |O| |X|O|O| 'X' Won! |X|O|X| |X| |O| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | |X| | | | |O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X| | | |X|O|O| |O|X|O| |X| | | 'X' Won! |X|O|O| |O|X|O| |X| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X| | | |O|X| | | |O|O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X|O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O|O| | |X| |X| |O|X| | |O|O| | |X|O|X| |O|X| | |O|O| | |X|O|X| |O|X|X| |O|O| | 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | |O| |X| |O| |X|O|X| | | |O| |X|O|O| |X|O|X| | | |O| |X|O|O| |X|O|X| | |X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X| | | | |O| |O|O| | 'X' Won! |X|X|X| | | |O| |O|O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | |O| | |O| |X| |X| | | |X|O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O|X| |O| |X| 'O' Won! |X|O|O| |X|O|X| |O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O|O|X| | | | | |X|X| | |O|O|X| | | | | |X|X| | |O|O|X| | | |O| 'X' Won! |X|X|X| |O|O|X| | | |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X|X| | | |O| |O| |O| | |X|X| | | |O| |O| |O| | |X|X| |X| |O| |O| |O| |O|X|X| |X| |O| |O| |O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O| |O| | | | | |O|X|X| |O| |O| | |X| | 'O' Won! |O|X|X| |O| |O| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | | |O| | |X|O|X| | |X| | | |O| | |X|O|X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|X| |X|O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O|O| | |O|X| | |X|X| | |O|O| | |O|X| |O|X|X| | |O|O| 'X' Won! | |O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| | | | | |O|X| | |X|X|O| |O| | | |O|X| | |X|X|O| 'X' Won! |O| |X| |O|X| | |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| |X| | | |O| | | 'O' Won! |X| |O| |X|O| | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X| | |O| |O| 'X' Won! |X|X|X| |O|X| | |O| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| |X| |X|O| | | |O| | |X| |X| |X|O|O| | |O| | 'X' Won! |X|X|X| |X|O|O| | |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | |O| | |X| | |X| |O| | |X|O| | |X| | 'O' Won! |X| |O| | |X|O| | |X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | | |X|X| |O|O|X| |O|O| | | |X|X| 'X' Won! |O|O|X| |O|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | |O|O| | 'X' Won! | | |O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | |O| | | |X|X| | | |O| | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | 'X' Won! |O|O|X| |O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X| | | |X|X| | |O| |O| |X| | | |X|X| | |O| |O| |X|O| | |X|X| | |O|X|O| |X|O| | |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | | |X|X| | |O|O| |O|X| | |O|X|X| | |O|O| |O|X| | |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| | |X|O| |O| | | | |X|O| |X|X|O| 'O' Won! |O| |O| | |X|O| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X|X|O| |O|X| | | | |O| |X|X|O| |O|X| | 'X' Won! | |X|O| |X|X|O| |O|X| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| |X|O| | |O|X| | | | |X| |X|O|O| |O|X| | | |X|X| |X|O|O| |O|X| | | |X|X| |X|O|O| |O|X|O| 'X' Won! |X|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| |X| |O| | |O| | | |O|X| |X| |O| | |O|X| | |O|X| |X| |O| | |O|X| |O|O|X| |X| |O| | |O|X| |O|O|X| |X|X|O| 'O' Won! |O|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | | |X| | |X|O|X| | |O| | | |X| | |X|O|X| |O|O| | | |X| | |X|O|X| |O|O|X| | |X| | |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| |X| | | |O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X|O|X| 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O|X| | | | | | |O| | |X|O|X| |O| | | | |O| | |X|O|X| |O| |X| 'O' Won! | |O| | |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X| | | |O|X| | | |X| |O|X| | |O|O|X| 'X' Won! |X| |X| |O|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | | | | |X|X| | |O|O|X| |O| | | |X|X| | 'X' Won! |O|O|X| |O|X| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| | | |O| |X| |X| |O| |X| | | |O| |X| |X| |O| |X|O| | 'X' Won! |O| |X| |X|X|O| |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | |X| | | |O|O| | | |X| | |X|O| |X|O|O| | | |X| | |X|O| |X|O|O| |O| |X| | |X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | |O| | |X| | |O|X| | | | |O| | |X| | |O|X|X| | |O|O| | |X| | |O|X|X| | |O|O| |X|X| | |O|X|X| | |O|O| |X|X|O| |O|X|X| 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| |O| | |O| | |X| |X| |X| |O| | |O| | |X|O|X| |X| |O| | |O| | 'X' Won! |X|O|X| |X| |O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O|X| |O| | | |X| | | |O|O|X| |O|X| | |X| | | |O|O|X| |O|X| | |X| |O| |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O| |O| | | |X| | |O|X| |O|X|O| | | |X| |O|O|X| |O|X|O| |X| |X| |O|O|X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O|X| | |X| |O| |O| | | |O|X| | |X| |O| |O| | | |O|X|X| |X| |O| |O| | | |O|X|X| |X|O|O| 'X' Won! |O| |X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| | | |O|O|X| |O| |X| |X|X| | |O|O|X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| | |O| | |O|O|X| | | |X| | |O| | 'X' Won! |O|O|X| | | |X| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O|O| |O| | | |X|X| | |X|O|O| |O| | | |X|X| | |X|O|O| |O|O| | |X|X| | |X|O|O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X|X| | |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X|X| | 'O' Won! |O|O|O| |X|X|O| |X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O| | |X|O| | | |X| | |X|O| | |X|O| | |O|X| | |X|O| |X|X|O| | |O|X| |O|X|O| |X|X|O| | |O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | |X| | | |X|O| | |O|X|O| |X| | | 'X' Won! |X|O|X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| | | |X| |X| |O|O| | |X| |O| |X| |X| 'X' Won! |O|O| | |X| |O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O|X| |O| | | | |X| | | |O|X| |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| |O|O|X| |O|O| | 'X' Won! | |X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X| | | | |X| | |X|O|O| |X| | | |O|X| | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| 'X' Won! |X|O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | |O| | |X| | | |O| |X| |X|O| | |X| | | |O| |X| |X|O| | |X|O| | |O| |X| |X|O|X| |X|O| | |O| |X| 'O' Won! |X|O|X| |X|O| | |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| |O| |O| |X| | | |X| |O| |O| |O| |X|X| | 'O' Won! |X| |O| |O| |O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| | |X| |O| | |X| | | |O| | |X| |O| |O|X| | | |O| | |X| |O| |O|X|X| |O|O| | |X| |O| |O|X|X| |O|O| | |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |X|X| | |O| | | |O|X|O| |X|X| | 'X' Won! |O|X| | |O|X|O| |X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O|O| | | |O|X| | |X|X| |O|O| | |O|O|X| | |X|X| |O|O| | 'X' Won! |O|O|X| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| | | | |X|O| |X| | | |O| | | | |X|O| |X| | | |O| |O| | |X|O| |X| | | |O|X|O| 'O' Won! | |X|O| |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O|O| |X|O|X| | | | | |X|O|O| 'O' Won! |X|O|X| | |O| | |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | |O|X| | |O|O| | | |X| | |O|X| | |O|O| | |X|X| 'O' Won! | |O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| | | |X|X|O| 'O' Won! |O| |X| |O|O| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| | |O| | | |O| | |X|X|O| |X|O| | | |O| | |X|X|O| |X|O|O| | |O| | 'X' Won! |X|X|O| |X|O|O| |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X|X| |O|X| | |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | |X| | | |X| |O| 'O' Won! |O| | | |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | |O| |X| | |O| | | |X| | |O| |X| |X|O| | | |X| | |O| |X| |X|O|O| | |X| | |O|X|X| |X|O|O| | |X| | |O|X|X| |X|O|O| |O|X| | It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X|O|O| | | | | | |X|X| |X|O|O| | | |O| | |X|X| |X|O|O| | |X|O| | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| 'O' Won! | |O| | | |O|X| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| |X| |X| | | |X| | |O|O| |X| |X| |O| |X| | |O|O| |X| |X| |O|X|X| 'O' Won! | |O|O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | | | |X|X| | 'O' Won! |O|O|O| | | | | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| |X| | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X| |X| | |X| | |O| |O| |X| |X| | |X|O| |O| |O| 'X' Won! |X|X|X| | |X|O| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | |O|X| 'X' Won! |X| | | | |X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| | | |O| |X| |O| |X| |X| |O| |O| |X| |O| |X| |X| 'X' Won! |O| |O| |X| |O| |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O| |O| | |X| | | |X| | |O| |O| | |X| | |O|X| | 'X' Won! |O|X|O| | |X| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | |X|X| | | | |O| |O| | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | |O| | |X|O| |X| |X| | | |O| | |X|O| |X| |X| | |O|O| | |X|O| |X| |X| |X|O|O| | |X|O| |X|O|X| |X|O|O| | |X|O| 'X' Won! |X|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| | | |X|X|O| 'O' Won! |O| |X| |O|O| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |X|X| | | |X| | |O|O| | |X|X| |O| |X| 'X' Won! |X|O|O| | |X|X| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| |X| | | | | |O| |X|O|O| |X| | | | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | | | |O| | |X| | |O|X| | | | |O| | |X|X| |O|X| | |O| |O| | |X|X| |O|X|X| |O| |O| | |X|X| 'O' Won! |O|X|X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X| | | | |X|X| 'O' Won! |O|O|O| |X| | | | |X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| |O| 'X' Won! |O| |X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X| |X| |X| |O| |O| 'X' Won! |O| |X| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X|O| | | |O| | | |X| |X|X|O| | | |O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| |X|O|O| | | |X| |X|X|O| |X|O|O| | |O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | |X| | | |X| |O| |O| | | |X| | | |X|O|O| |O| | | |X| | | |X|O|O| |O| |X| |X|O| | |X|O|O| |O| |X| |X|O|X| |X|O|O| |O| |X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | | |X| |X|O| | | | | | |O| |X| |X|O| | | | | | |O|X|X| |X|O| | | |O| | |O|X|X| |X|O| | | |O|X| |O|X|X| 'O' Won! |X|O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | | |X| |O| | | |O|O|X| | | |X| |O| | | |O|O|X| |X| |X| |O| |O| |O|O|X| |X| |X| 'X' Won! |O| |O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | |X|X| | |X|O| | |O| | | |X|X| | |X|O| | |O|O| | |X|X| |X|X|O| | |O|O| 'O' Won! | |X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | | |X| |O|X| | | | |O| | |X|X| |O|X| | | | |O| |O|X|X| |O|X| | | | |O| |O|X|X| |O|X|X| | | |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| |O|X| | | | | | | |O|X| |O|X| | | | | | |X|O|X| |O|X| | | | |O| |X|O|X| |O|X| | |X| |O| |X|O|X| |O|X|O| |X| |O| |X|O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| |O| | | | |X|O| |X|X|O| |O| | | |O|X|O| |X|X|O| |O| | | 'X' Won! |O|X|O| |X|X|O| |O|X| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| |O|X| | |X| |O| | | |O| |O|X|X| |X| |O| | | |O| |O|X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |X| | | | | |X|O| | | | |X| | |O| | |X|O| | | | |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O|X| |X|O|O| |O| |X| |X|O|X| 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | |X| | | |X| |O| |O| |O| |X| | | |X| |O| |O| |O| |X| | | |X| |O|X|O| |O|O|X| | | |X| |O|X|O| |O|O|X| |X| |X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | | | |O| |X| |X| |O| | | | |X|O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O|X| | |X|O| |X| |X| |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O|X|O| | | |X| | | | | |O|X|O| | | |X| |O| | | |O|X|O| | | |X| |O| |X| 'O' Won! |O|X|O| |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | |X|O| | | |O|X| | |X| | |X|O|O| | |O|X| | |X| | |X|O|O| |X|O|X| | |X| | |X|O|O| |X|O|X| | |X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O|X| | |O| |X| | | |O| |O|X| | |O| |X| | | |O| |O|X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | |X| | |O| | |O| |X| | |O|X| | |O| | |O| |X| |X|O|X| | |O| | |O| |X| |X|O|X| | |O|O| |O| |X| |X|O|X| |X|O|O| |O| |X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| |O| | | |X| | | |O| |X| |O| |X| |X| | | |O|O|X| |O| |X| |X| | | 'X' Won! |O|O|X| |O|X|X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | | |X| |X|O| | |O| |O| | | |X| |X|O| | |O| |O| |X| |X| 'O' Won! |X|O| | |O|O|O| |X| |X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O|X| | |O|X| | |X| | | |O|X|O| |O|X| | |X| |X| |O|X|O| 'O' Won! |O|X| | |X|O|X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O| |O| |X|X| | | | | | |O| |O| |X|X| | | |O| | |O| |O| |X|X| | |X|O| | |O| |O| |X|X| | |X|O|O| |O|X|O| |X|X| | |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| | | |O| |X| |X|O|O| |X| | | |O| |X| |X|O|O| |X| | | |O|O|X| 'X' Won! |X|O|O| |X|X| | |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | |O| | | | |X|X| | |O| | |O|X| | | |X|X| | |O| | |O|X|O| | |X|X| | |O|X| |O|X|O| | |X|X| |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O| |X|O| | | |X| | | |X|O| |X|O|O| | |X| | | |X|O| |X|O|O| |X|X| | 'O' Won! | |X|O| |X|O|O| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X|O| | |X| |O| | |O| | |X|O| | |X|X|O| | |O|O| |X|O| | |X|X|O| 'X' Won! |X|O|O| |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| |O| | | |X|O| | | |O|X| |O| | | |X|O|X| | |O|X| 'O' Won! |O|O| | |X|O|X| | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O|X| |O| | | |O| | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X| | |O| |O| |X|O|X| |O|X| | |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | | | |O|X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X| |O| |O|X|O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | |O| |X| |X| |X|O| | | | |O| |X| |X| |X|O| | | |O|O| 'X' Won! |X|X|X| |X|O| | | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X|X| | |X| | | |O| |O| |X|X| | |X|O| | |O| |O| |X|X| | |X|O|X| |O| |O| |X|X|O| |X|O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X|O| | | |O| | | |X|O| |X|O|X| | |O| | 'O' Won! | |X|O| |X|O|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | | |X|O| |O| | | |X| | | |X|X|O| |O| | | |X| |O| |X|X|O| |O| | | 'X' Won! |X| |O| |X|X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | |X|X| | | | |O| | | |O| |X|X| | | | |O| | |X|O| |X|X| | | | |O| |O|X|O| |X|X| | 'X' Won! | | |O| |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | | | |O|X|O| | | |X| | |X| | |O|X|O| |O| |X| | |X| | |O|X|O| |O| |X| |X|X| | |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X| | |O|X|O| | | |O| | |X|X| |O|X|O| | | |O| | |X|X| |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X| | |O|X| |O| |O| | | |X| | |O|X| |O| |O| | | |X| |X|O|X| 'O' Won! |O|O|O| | | |X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | | |X|O| |O| | | 'X' Won! | |X| | | |X|O| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | |X| | | |O|X|O| |X| | | |X| | | |O|X|O| |X| |O| |X|X| | |O|X|O| |X| |O| 'O' Won! |X|X|O| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| |X| | | | |X| | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O|X| | |X| | |X|O|O| |X|O|X| | |X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O|X| | | | | |O|O|X| | |O|X| 'X' Won! | | |X| |O|O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X|X| | | | | | |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | |X| |O| |O|O|X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| | |O| | | |O| | |X| |X| |X|O| | | |O| | |X| |X| |X|O| | |O|O| | 'X' Won! |X|X|X| |X|O| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | | | | |O|X|O| | |O|X| | | | | |O|X|O| | |O|X| |X| | | 'O' Won! |O|X|O| | |O|X| |X| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|O| | | | | |O|X|X| | |O|O| | |X| | 'O' Won! |O|X|X| | |O|O| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | | |O|X| |X|O| | | | |X| | |O|X| |X|O| | 'O' Won! | |O|X| | |O|X| |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | |O| |O| | | 'X' Won! |X|X|X| | | |O| |O| | |","title":"1.0.2 Load Dataset"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#11-clean-data","text":"We will first need to organize the data into a parsable format.","title":"1.1 Clean Data"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q1","text":"What is the object data and what does it contain? what are the keys of data? what are the keys of each game? # inspect data below by grabbing the first key in data # what are the three different keys within each game? data['game 0'] {'board': {1: 'X', 2: ' ', 3: 'O', 4: 'X', 5: 'X', 6: 'O', 7: 'O', 8: 'X', 9: 'O'}, 'starting player': 'X', 'winner': 'O'}","title":"Q1"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q2","text":"Using those keys, iterate through every game in data and append the board, the winner, and the starting player to separate lists. Call these lists: boards, winners, and starters boards = [] winners = [] starters = [] for game in data: # YOUR CODE HERE","title":"Q2"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q3","text":"Make a dataframe out of the list boards and call it df . Make a series out of the list winners . Make a series out of the list starters . Make a new column of df called \"Winner\" and set it equal to the pandas Series of the winners. Make a new column of df called \"Starter\" and set it equal to the pandas Series of the starters. # YOUR CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 Winner Starter 0 X O O O X O O 1 O O X O X X O X O O O 2 O X X X X O O X X 3 X X O O O X X O O Stalemate O 4 O O X O O X X X X O","title":"Q3"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#12-inferential-analysis","text":"We're going to use Bayes Rule or Bayesian Inference to make a probability of winning based on positions of the board. The formula is: $ P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)}$ Where $\\cap$ is the intersection of $A$ and $B$. The example we will use is the following: what is the probability of 'O' being the winner, given that they've played the center piece. $B$ = 'O' played the center piece $A$ = 'O' won the game So what is probability? We will define it in terms of frequencies. So if we are for instance asking what is the probability of player 'O' being in the center piece, it would be defined as: $ P(B) = \\frac{|O_c|} {|O_c| + |X_c| + |empty|}$ Where the pipes, | | , or cardinality represent the count of the indicated observation or set. In this case $O_c$ (O being in the center) and $X_c$ (X being in the center). Oc_Xc_empty = df[5].value_counts().sum() Oc_Xc_empty 1000 # example of assessing the probability of B, O playing the center piece player = 'O' Oc = (df[5] == player).value_counts() Oc_Xc_empty = df[5].value_counts().sum() Oc/Oc_Xc_empty False 0.577 True 0.423 Name: 5, dtype: float64 # we can also clean this up and replace the denominator with the whole # observation space (which is just the total number of games, df.shape[0]). # example of assesing probabiliy of A (df['Winner'] == 'O').value_counts()/df.shape[0] False 0.571 True 0.429 Name: Winner, dtype: float64 The $P(B|A) * P(A)$ is the intersection of $B$ and $A$. The intersection is defined as the two events occuring together. Continuing with the example, the probablity of 'O' playing the center piece AND 'O' being the winner is the number of times these observations occured together divided by the whole observation space : # in this view, the total times A and B occured together is 247 player = 'O' df.loc[(df['Winner'] == player) & (df[5] == player)].shape[0] 247 # the total observation space is 1000 (1000 games) df.shape[0] 1000 And so we get: $P(B|A) * P(A) = \\frac{247} {1000} = 0.247 $ In code: df.loc[(df['Winner'] == player) & (df[5] == player)].shape[0]/df.shape[0] 0.247","title":"1.2 Inferential Analysis"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#121-behavioral-analysis-of-the-winner","text":"","title":"1.2.1 Behavioral Analysis of the Winner"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q4","text":"define the 3 different board piece types and label them middle , side , and corner . Middle should be an int and the other two should be lists. # define the 3 different board piece types # middle = # side = # corner =","title":"Q4"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#1211-what-is-the-probability-of-winning-after-playing-the-middle-piece","text":"","title":"1.2.1.1 What is the probability of winning after playing the middle piece?"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q5","text":"# A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'X' # define the intersection of A AND B, A_B # A_B = # define prob B # B = # return A_B over B (The prob B given A) A_B / B 0.5732758620689655","title":"Q5"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q6","text":"# A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'O' # define the intersection of A AND B, A_B # A_B = # define prob B # B = # return A_B over B (The prob B given A) A_B / B 0.5839243498817968","title":"Q6"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#1212-what-is-the-probability-of-winning-after-playing-a-side-piece","text":"","title":"1.2.1.2 What is the probability of winning after playing a side piece?"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q7","text":"# A intersect B: O played side and O won / tot games # B: O played side / tot games player = 'O' A_B = df.loc[(df[side].T.apply(lambda x: player in x.values)) & (df['Winner'] == player)].shape[0] / df.shape[0] B = df.loc[(df[side].T.apply(lambda x: player in x.values))].shape[0] /\\ df.shape[0] A_B / B 0.4158609451385117 # A intersect B: X played side and X won / tot games # B: X played side / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.38845460012026456","title":"Q7"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#1213-what-is-the-probability-of-winning-after-playing-a-corner-piece","text":"","title":"1.2.1.3 What is the probability of winning after playing a corner piece?"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q8","text":"# A intersect B: O played corner and O won / tot games # B: O played corner / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.4779116465863454","title":"Q8"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q9","text":"# A intersect B: X played corner and X won / tot games # B: X played corner / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.47386964180857316 Are these results surprising to you? Why? This resource may be illustrative.","title":"Q9"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#13-improving-the-analysis","text":"In this analysis, we only tracked what moves were made, not the order they were made in. It really limited our assessment! How might we change our recording of the games to track order of moves as well? Do we need to track all the moves or just the first and the winner?","title":"1.3 Improving the Analysis"},{"location":"project/P2_Heuristical_TicTacToe_Agents/","text":"Technology Fundamentals Course 3, Project Part 4: Heuristical Agents (Symbolic AI) Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Teaching Assitants : Varsha Bang, Harsha Vardhan Contact : vbang@uw.edu, harshav@uw.edu We makin' some wack AI today 4.0 Preparing Environment and Importing Data back to top 4.0.1 Import Packages back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe: # can preset winner and starting player def __init__(self, winner='', start_player=''): self.winner = winner self.start_player = start_player self.board = {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' ',} self.win_patterns = [[1,2,3], [4,5,6], [7,8,9], [1,4,7], [2,5,8], [3,6,9], [1,5,9], [7,5,3]] # the other functions are now passed self def visualize_board(self): print( \"|{}|{}|{}|\\n|{}|{}|{}|\\n|{}|{}|{}|\\n\".format(*self.board.values()) ) def check_winning(self): for pattern in self.win_patterns: values = [self.board[i] for i in pattern] if values == ['X', 'X', 'X']: self.winner = 'X' # we update the winner status return \"'X' Won!\" elif values == ['O', 'O', 'O']: self.winner = 'O' return \"'O' Won!\" return '' def check_stalemate(self): if (' ' not in self.board.values()) and (self.check_winning() == ''): self.winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine(TicTacToe): def __init__(self, setup='auto'): super().__init__() self.setup = setup def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'human'}, 'second': {'label': 'O', 'type': 'human'}} if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) # we're going to allow the user to quit the game from the input line if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player)) move = int(move) if self.board[move] != ' ': continue else: break else: while True: move = random.randint(1,9) if self.board[move] != ' ': continue print('test') else: break self.board[move] = player_label # the winner varaible will now be check within the board object self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self 4.0.2 Load Dataset back to top 4.3 AI Heuristics Develop a better AI based on your analyses of game play so far. Q1 In our groups, let's discuss what rules we would like to hard code in. Harsha, Varsha and I will help you with the flow control to program these rules # we will define some variables to help us define the types of positions middle = 5 side = [2, 4, 6, 8] corner = [1, 3, 7, 9] # recall that our board is a dictionary tictactoe = TicTacToe() tictactoe.board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} # and we have a win_patterns object to help us with the algorithm tictactoe.win_patterns [[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 4, 7], [2, 5, 8], [3, 6, 9], [1, 5, 9], [7, 5, 3]] for example, if we want to check if the middle piece is available, and play it if it is. How do we do that? # set some key variables player = 'X' opponent = 'O' avail_moves = [i for i in tictactoe.board.keys() if tictactoe.board[i] == ' '] # a variable that will keep track if we've found a move we like or not move_found = False # <- some other moves we might want to make would go here -> # # and now for our middle piece play if move_found == False: # if no other move has been found yet if middle in avail_moves: # if middle is available move_found = True # then change our move_found status move = middle # update our move Our standard approach will be to always return a move by the agent . Whether the agent is heruistical or from some other ML framework we always want to return a move Repeate after me: ALWAYS RETURN A MOVE . Make sure you know what move is. Make sure you know what it is. And return it. Return a move. The purpose of the next lines of code we will write is to return a move. Make sure your code returns a move. Q2 Write down your algorithm steps in markdown. i.e. play a corner piece play to opposite corner from the opponent, etc. ....etc. Q3 Begin to codify your algorithm from Q3. Make sure that no matter what, you return a move # some starting variables for you player_label = 'X' opponent = 'O' avail_moves = [i for i in tictactoe.board.keys() if tictactoe.board[i] == ' '] # temp board will allow us to play hypothetical moves and see where they get us # in case you need it temp_board = tictactoe.board.copy() 4.4 Wrapping our Agent Now that we've created a conditional tree for our AI to make a decision, we need to integrate this within the gaming framework we've made so far. How should we do this? Let's define this thought pattern or tree as an agent. Recall our play_game function within GameEngine def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) # we're going to allow the user to quit the game from the input line if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player)) move = int(move) if self.board[move] != ' ': continue else: break ######################################################################## ##################### WE WANT TO CHANGE THESE LINES #################### ######################################################################## else: while True: move = random.randint(1,9) if self.board[move] != ' ': continue print('test') else: break self.board[move] = player_label # the winner varaible will now be check within the board object self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self 4.4.1 Redefining the Random Agent In particular, we want to change lines 30-37 to take our gaming agent in as a parameter to make decisions. Let's try this. In setup_game we want to have the option to set the AI type/level. In play_game we want to make a call to that AI to make the move. For instance, our random AI will go from: while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break to: def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move class GameEngine(TicTacToe): def __init__(self, setup='auto'): super().__init__() self.setup = setup ############################################################################## ########## our fresh off the assembly line tictactoe playing robot ########### ############################################################################## def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'human'}, 'second': {'label': 'O', 'type': 'human'}} if players != 2: ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int(input(\"select AI level (1, 2)\")) if level == 1: self.ai_level = 1 elif level == 2: self.ai_level = 2 else: print(\"Unknown AI level entered, this will cause problems\") if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self.ai_level = 1 def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player)) move = int(move) if self.board[move] != ' ': continue else: break else: if self.ai_level == 1: move = self.random_ai() ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self.ai_level == 2: pass self.board[move] = player_label self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self Let's test that our random ai works now in this format random.seed(12) game = GameEngine(setup='auto') game.setup_game() game.play_game() | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| <__main__.GameEngine at 0x7fadbea428d0> Let's try it with a user player: random.seed(12) game = GameEngine(setup='user') game.setup_game() game.play_game() How many Players? (type 0, 1, or 2)2 | | | | | | | | | | | | X, what's your move?q quiting the game <__main__.GameEngine at 0x7fadbea25e90> Q4 Now let's fold in our specialized AI agent. Add your code under the heurstic_ai function. Note that the player_label is passed as an input parameter now class GameEngine(TicTacToe): def __init__(self, setup='auto'): super().__init__() self.setup = setup ############################################################################## ################### YOUR BADASS HEURISTIC AGENT GOES HERE #################### ############################################################################## def heuristic_ai(self, player_label): # SOME HELPER VARIABLES IF YOU NEED THEM opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() ################## YOUR CODE GOES HERE, RETURN THAT MOVE! ################## while True: # DELETE LINES 20 - 25, USED FOR TESTING PURPOSES ONLY move = random.randint(1,9) if self.board[move] != ' ': continue else: break ############################################################################ return move def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'human'}, 'second': {'label': 'O', 'type': 'human'}} if players != 2: ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int(input(\"select AI level (1, 2)\")) if level == 1: self.ai_level = 1 elif level == 2: self.ai_level = 2 else: print(\"Unknown AI level entered, this will cause problems\") if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self.ai_level = 1 def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player)) move = int(move) if self.board[move] != ' ': continue else: break else: if self.ai_level == 1: move = self.random_ai() ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self.ai_level == 2: move = self.heuristic_ai(player_label) self.board[move] = player_label self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self Q5 And we'll test that it works! random.seed(12) game = GameEngine(setup='user') game.setup_game() game.play_game() How many Players? (type 0, 1, or 2)1 select AI level (1, 2)2 who will go first? (X, (AI), or O (Player))O | | | | | | | | | | | | O, what's your move?5 | | | | | |O| | | | | | | | | | | |O| | | |X| | O, what's your move?9 | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| O, what's your move?1 'O' Won! |O| | | | |O|X| | |X|O| <__main__.GameEngine at 0x7fadbe93f610> Q6 Test the autorun feature! game = GameEngine(setup='auto') game.setup_game() game.play_game() | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| 'O' Won! |O|X| | |O| | | |O| |X| <__main__.GameEngine at 0x7fadbe8cc050>","title":"Heuristical TicTacToe Agents"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#technology-fundamentals-course-3-project-part-4-heuristical-agents-symbolic-ai","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Teaching Assitants : Varsha Bang, Harsha Vardhan Contact : vbang@uw.edu, harshav@uw.edu We makin' some wack AI today","title":"Technology Fundamentals Course 3, Project Part 4: Heuristical Agents (Symbolic AI)"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#40-preparing-environment-and-importing-data","text":"back to top","title":"4.0 Preparing Environment and Importing Data"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#401-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe: # can preset winner and starting player def __init__(self, winner='', start_player=''): self.winner = winner self.start_player = start_player self.board = {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' ',} self.win_patterns = [[1,2,3], [4,5,6], [7,8,9], [1,4,7], [2,5,8], [3,6,9], [1,5,9], [7,5,3]] # the other functions are now passed self def visualize_board(self): print( \"|{}|{}|{}|\\n|{}|{}|{}|\\n|{}|{}|{}|\\n\".format(*self.board.values()) ) def check_winning(self): for pattern in self.win_patterns: values = [self.board[i] for i in pattern] if values == ['X', 'X', 'X']: self.winner = 'X' # we update the winner status return \"'X' Won!\" elif values == ['O', 'O', 'O']: self.winner = 'O' return \"'O' Won!\" return '' def check_stalemate(self): if (' ' not in self.board.values()) and (self.check_winning() == ''): self.winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine(TicTacToe): def __init__(self, setup='auto'): super().__init__() self.setup = setup def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'human'}, 'second': {'label': 'O', 'type': 'human'}} if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) # we're going to allow the user to quit the game from the input line if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player)) move = int(move) if self.board[move] != ' ': continue else: break else: while True: move = random.randint(1,9) if self.board[move] != ' ': continue print('test') else: break self.board[move] = player_label # the winner varaible will now be check within the board object self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self","title":"4.0.1 Import Packages"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#402-load-dataset","text":"back to top","title":"4.0.2 Load Dataset"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#43-ai-heuristics","text":"Develop a better AI based on your analyses of game play so far.","title":"4.3 AI Heuristics"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q1","text":"In our groups, let's discuss what rules we would like to hard code in. Harsha, Varsha and I will help you with the flow control to program these rules # we will define some variables to help us define the types of positions middle = 5 side = [2, 4, 6, 8] corner = [1, 3, 7, 9] # recall that our board is a dictionary tictactoe = TicTacToe() tictactoe.board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} # and we have a win_patterns object to help us with the algorithm tictactoe.win_patterns [[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 4, 7], [2, 5, 8], [3, 6, 9], [1, 5, 9], [7, 5, 3]] for example, if we want to check if the middle piece is available, and play it if it is. How do we do that? # set some key variables player = 'X' opponent = 'O' avail_moves = [i for i in tictactoe.board.keys() if tictactoe.board[i] == ' '] # a variable that will keep track if we've found a move we like or not move_found = False # <- some other moves we might want to make would go here -> # # and now for our middle piece play if move_found == False: # if no other move has been found yet if middle in avail_moves: # if middle is available move_found = True # then change our move_found status move = middle # update our move Our standard approach will be to always return a move by the agent . Whether the agent is heruistical or from some other ML framework we always want to return a move Repeate after me: ALWAYS RETURN A MOVE . Make sure you know what move is. Make sure you know what it is. And return it. Return a move. The purpose of the next lines of code we will write is to return a move. Make sure your code returns a move.","title":"Q1"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q2","text":"Write down your algorithm steps in markdown. i.e. play a corner piece play to opposite corner from the opponent, etc. ....etc.","title":"Q2"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q3","text":"Begin to codify your algorithm from Q3. Make sure that no matter what, you return a move # some starting variables for you player_label = 'X' opponent = 'O' avail_moves = [i for i in tictactoe.board.keys() if tictactoe.board[i] == ' '] # temp board will allow us to play hypothetical moves and see where they get us # in case you need it temp_board = tictactoe.board.copy()","title":"Q3"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#44-wrapping-our-agent","text":"Now that we've created a conditional tree for our AI to make a decision, we need to integrate this within the gaming framework we've made so far. How should we do this? Let's define this thought pattern or tree as an agent. Recall our play_game function within GameEngine def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) # we're going to allow the user to quit the game from the input line if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player)) move = int(move) if self.board[move] != ' ': continue else: break ######################################################################## ##################### WE WANT TO CHANGE THESE LINES #################### ######################################################################## else: while True: move = random.randint(1,9) if self.board[move] != ' ': continue print('test') else: break self.board[move] = player_label # the winner varaible will now be check within the board object self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self","title":"4.4 Wrapping our Agent"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#441-redefining-the-random-agent","text":"In particular, we want to change lines 30-37 to take our gaming agent in as a parameter to make decisions. Let's try this. In setup_game we want to have the option to set the AI type/level. In play_game we want to make a call to that AI to make the move. For instance, our random AI will go from: while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break to: def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move class GameEngine(TicTacToe): def __init__(self, setup='auto'): super().__init__() self.setup = setup ############################################################################## ########## our fresh off the assembly line tictactoe playing robot ########### ############################################################################## def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'human'}, 'second': {'label': 'O', 'type': 'human'}} if players != 2: ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int(input(\"select AI level (1, 2)\")) if level == 1: self.ai_level = 1 elif level == 2: self.ai_level = 2 else: print(\"Unknown AI level entered, this will cause problems\") if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self.ai_level = 1 def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player)) move = int(move) if self.board[move] != ' ': continue else: break else: if self.ai_level == 1: move = self.random_ai() ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self.ai_level == 2: pass self.board[move] = player_label self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self Let's test that our random ai works now in this format random.seed(12) game = GameEngine(setup='auto') game.setup_game() game.play_game() | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| <__main__.GameEngine at 0x7fadbea428d0> Let's try it with a user player: random.seed(12) game = GameEngine(setup='user') game.setup_game() game.play_game() How many Players? (type 0, 1, or 2)2 | | | | | | | | | | | | X, what's your move?q quiting the game <__main__.GameEngine at 0x7fadbea25e90>","title":"4.4.1 Redefining the Random Agent"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q4","text":"Now let's fold in our specialized AI agent. Add your code under the heurstic_ai function. Note that the player_label is passed as an input parameter now class GameEngine(TicTacToe): def __init__(self, setup='auto'): super().__init__() self.setup = setup ############################################################################## ################### YOUR BADASS HEURISTIC AGENT GOES HERE #################### ############################################################################## def heuristic_ai(self, player_label): # SOME HELPER VARIABLES IF YOU NEED THEM opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() ################## YOUR CODE GOES HERE, RETURN THAT MOVE! ################## while True: # DELETE LINES 20 - 25, USED FOR TESTING PURPOSES ONLY move = random.randint(1,9) if self.board[move] != ' ': continue else: break ############################################################################ return move def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'human'}, 'second': {'label': 'O', 'type': 'human'}} if players != 2: ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int(input(\"select AI level (1, 2)\")) if level == 1: self.ai_level = 1 elif level == 2: self.ai_level = 2 else: print(\"Unknown AI level entered, this will cause problems\") if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self.ai_level = 1 def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player)) move = int(move) if self.board[move] != ' ': continue else: break else: if self.ai_level == 1: move = self.random_ai() ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self.ai_level == 2: move = self.heuristic_ai(player_label) self.board[move] = player_label self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self","title":"Q4"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q5","text":"And we'll test that it works! random.seed(12) game = GameEngine(setup='user') game.setup_game() game.play_game() How many Players? (type 0, 1, or 2)1 select AI level (1, 2)2 who will go first? (X, (AI), or O (Player))O | | | | | | | | | | | | O, what's your move?5 | | | | | |O| | | | | | | | | | | |O| | | |X| | O, what's your move?9 | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| O, what's your move?1 'O' Won! |O| | | | |O|X| | |X|O| <__main__.GameEngine at 0x7fadbe93f610>","title":"Q5"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q6","text":"Test the autorun feature! game = GameEngine(setup='auto') game.setup_game() game.play_game() | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| 'O' Won! |O|X| | |O| | | |O| |X| <__main__.GameEngine at 0x7fadbe8cc050>","title":"Q6"},{"location":"project/P3_1_Step_Look_Ahead_Agents/","text":"Data Science Foundations, Project Part 3: 1-Step Look Ahead Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we're working on a more advanced AI structure: 1-step lookahead. 4.0 Preparing Environment and Importing Data back to top 4.0.1 Import Packages back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe: # can preset winner and starting player def __init__(self, winner='', start_player=''): self.winner = winner self.start_player = start_player self.board = {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' ',} self.win_patterns = [[1,2,3], [4,5,6], [7,8,9], [1,4,7], [2,5,8], [3,6,9], [1,5,9], [7,5,3]] # the other functions are now passed self def visualize_board(self): print( \"|{}|{}|{}|\\n|{}|{}|{}|\\n|{}|{}|{}|\\n\".format(*self.board.values()) ) def check_winning(self): for pattern in self.win_patterns: values = [self.board[i] for i in pattern] if values == ['X', 'X', 'X']: self.winner = 'X' # we update the winner status return \"'X' Won!\" elif values == ['O', 'O', 'O']: self.winner = 'O' return \"'O' Won!\" return '' def check_stalemate(self): if (' ' not in self.board.values()) and (self.check_winning() == ''): self.winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine(TicTacToe): def __init__(self, setup='auto'): super().__init__() self.setup = setup def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() middle = 5 corner = [1,3,7,9] side = [2,4,6,8] # first check for a winning move move_found = False for move in avail_moves: temp_board[move] = player_label for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [player_label, player_label, player_label]: move_found = True break if move_found: break else: temp_board[move] = ' ' # check if the opponent has a winning move if move_found == False: for move in avail_moves: temp_board[move] = opponent for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [opponent, opponent, opponent]: move_found = True break if move_found: break else: temp_board[move] = ' ' # check if middle avail if move_found == False: if middle in avail_moves: move_found = True move = middle # check corners if move_found == False: move_corner = [val for val in avail_moves if val in corner] if len(move_corner) > 0: move = random.choice(move_corner) move_found = True # check side if move_found == False: move_side = [val for val in avail_moves if val in side] if len(move_side) > 0: move = random.choice(move_side) move_found = True return move def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} if players != 2: ########## # Allow the user to set the ai level ########## level = int(input(\"select AI level (1, 2)\")) if level == 1: self.ai_level = 1 elif level == 2: self.ai_level = 2 else: print(\"Unknown AI level entered, this will cause problems\") if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} ########## # and automatically set the ai level otherwise ########## self.ai_level = 2 def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) # we're going to allow the user to quit the game from the input line if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player_label)) move = int(move) if self.board[move] != ' ': continue else: break else: ########## # Our level 1 ai agent (random) ########## if self.ai_level == 1: move = self.random_ai() ########## # Our level 2 ai agent (heuristic) ########## elif self.ai_level == 2: move = self.heuristic_ai(player_label) self.board[move] = player_label # the winner varaible will now be check within the board object self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self 4.0.2 Load Dataset back to top 4.1 Rethinking gameplay To implement the broader strategies used in game theory and machine learning, we need to rebroadcast our approach to creating our AI agent. In the heurstical agent model, we thought in terms of checking for specific move types, defined by what kind of advantage they give us during game play, i.e. see if a winning move is available, a blocking move, if the center place is free, etc. Rather than thinking with this look and check mindset that is centered around specific strategies and our own prior knowledge about the game (we know that a center piece is statistically likely to give us a higher chance of winning) we will evaluate every available move to the AI, and rate them quantitatively. switching from ordinal to interval Notice the datatype change when we move from giving simple preferences of moves to actual scores of moves. Catalog this in your mind for future reference when considering datatypes! 4.1.1 One-Step Look Ahead For now, when we rate our boards, we will only look 1-step ahead in gameplay. Hence the name we give this AI strategy, 1-step lookahead The beginning portion of our code will look about the same as the heuristic AI model. Recall: def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() but now, instead of searching progressively through our preferred move-types (winning, middle, etc.) . We are going to give every available move (1, 3, 7, etc.) a score. Our score regimen will look like the following: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move Q1 Rewrite avail_moves define avail_moves as a dictionary of available moves with scores for each move as empty strings. We will update this dictionary with numerical scores in the next step # we're going to steal the parameter names to # prototype our new function self = TicTacToe() player_label = 'X' opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] # instead of a list, we want avail_moves to now be a dictionary that will # contain the move and its score # avail_moves = # YOUR CODE HERE temp_board = self.board.copy() Q2 Score each move in avail_moves Now let's fold this into our new one_step_ai function. Remember: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## # avail_moves = # temp_board = self.board.copy() # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves.keys(): temp_board[move] = player_label for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [player_label, player_label, player_label]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score temp_board[move] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## ############################################################################## ################### All remaining moves receive a score of 1 ############################################################################## return avail_moves one_step_ai(self, 'X') {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} Q3 Test one_step_ai That's great, but how do we check that our code will work when a winning move is available, or a losing move is just around the corner? let's create a unit test for these! # just defining a new game self = TicTacToe() player_label = 'X' # seeding the board with some X's self.board[1] = 'X' self.board[2] = 'X' self.board {1: 'X', 2: 'X', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} Now test the winning move. Your code should return 100 at move 3 and 1 everywhere else one_step_ai(self, player_label) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} We can test the losing move by reversing the players player_label = 'O' one_step_ai(self, player_label) {3: 10, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} great! Let's keep these shorthand codes in mind when we go to write actual unit tests with the one_step_ai function embedded in the GameEngine module. We're not done yet, recall that our other ai agents returned the actual selected move, not a dictionary of the moves with scores. We need to create a move from this dictionary and return it. Here's what the general procedure will look like: Grab the maximum score (after assigning scores to all of avail_moves Select all moves that have this maximum score Return a random selection of the moves with the max score and then in code format: avail_moves = one_step_ai(self, player_label) # 1. grab the maximum score max_score = max(avail_moves.values()) # 2. select all moves that have this maximum score valid = [] for key, value in avail_moves.items(): if value == max_score: valid.append(key) # 3. return a random selection of the moves with the max score move = random.choice(valid) move 3 4.2 Putting it all together Q4 Finish one_step_ai to return a move Let's see if we can rewrite our game engine to take new AI models in as a passable parameter. This way our base module will be much cleaner, and allow us to continue to write new functions for the base engine as long as they pass along the same variables. # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai(board, win_patterns, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] temp_board = board.copy() # define avail_moves # avail_moves = # your code for giving scores for avail_moves # first grab max score # then select all moves that have this max score # return a random selection of the moves with the max score return move 4.2.1 Allow GameEngine to take an ai agent as a passable parameter Let's rewrite our GameEngine to take an ai agent as a passable parameter under user_ai . The default value will be None Additional user_ai criteria will be that user_ai receives board , win_patterns and player_label and returns move . class GameEngine(TicTacToe): def __init__(self, setup='auto', user_ai=None): super().__init__() self.setup = setup self.user_ai = user_ai def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() middle = 5 corner = [1,3,7,9] side = [2,4,6,8] # first check for a winning move move_found = False for move in avail_moves: temp_board[move] = player_label for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [player_label, player_label, player_label]: move_found = True break if move_found: break else: temp_board[move] = ' ' # check if the opponent has a winning move if move_found == False: for move in avail_moves: temp_board[move] = opponent for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [opponent, opponent, opponent]: move_found = True break if move_found: break else: temp_board[move] = ' ' # check if middle avail if move_found == False: if middle in avail_moves: move_found = True move = middle # check corners if move_found == False: move_corner = [val for val in avail_moves if val in corner] if len(move_corner) > 0: move = random.choice(move_corner) move_found = True # check side if move_found == False: move_side = [val for val in avail_moves if val in side] if len(move_side) > 0: move = random.choice(move_side) move_found = True return move def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} if players != 2: ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self.user_ai == None: level = int(input(\"select AI level (1, 2)\")) if level == 1: self.ai_level = 1 elif level == 2: self.ai_level = 2 else: print(\"Unknown AI level entered, this will cause problems\") else: self.ai_level = 3 if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} ########## # and automatically set the ai level otherwise ########## if self.user_ai == None: self.ai_level = 2 else: self.ai_level = 3 def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) # we're going to allow the user to quit the game from the input line if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player_label)) move = int(move) if self.board[move] != ' ': continue else: break else: ########## # Our level 1 ai agent (random) ########## if self.ai_level == 1: move = self.random_ai() ########## # Our level 2 ai agent (heuristic) ########## elif self.ai_level == 2: move = self.heuristic_ai(player_label) ########## # Our user-defined AI agent ########## elif self.ai_level == 3: move = self.user_ai(self.board, self.win_patterns, player_label) self.board[move] = player_label # the winner varaible will now be check within the board object self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self Test the auto and user functions game = GameEngine(setup='user', user_ai=one_step_ai) game.setup_game() How many Players? (type 0, 1, or 2)1 who will go first? (X, (AI), or O (Player))X game.play_game() | | | | | | | | | | | | | | | | | | | | | |X| | O, what's your move?5 | | | | | |O| | | |X| | | | |X| | |O| | | |X| | O, what's your move?4 | | |X| |O|O| | | |X| | | | |X| |O|O|X| | |X| | O, what's your move?1 |O| |X| |O|O|X| | |X| | 'X' Won! |O| |X| |O|O|X| | |X|X| <__main__.GameEngine at 0x7f1f812e7d50> 4.3 Write Unit Tests for the New Code def test_user_ai(): random.seed(42) game = GameEngine(setup='auto', user_ai=one_step_ai) game.setup_game() outcome = game.play_game() assert outcome.winner == 'X', 'X should have won!' test_user_ai() | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| | |X|O| |O| | | 'X' Won! |X|O|X| | |X|O| |O| |X|","title":"1-Step Look Ahead Agents"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#data-science-foundations-project-part-3-1-step-look-ahead","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we're working on a more advanced AI structure: 1-step lookahead.","title":"Data Science Foundations, Project Part 3: 1-Step Look Ahead"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#40-preparing-environment-and-importing-data","text":"back to top","title":"4.0 Preparing Environment and Importing Data"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#401-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe: # can preset winner and starting player def __init__(self, winner='', start_player=''): self.winner = winner self.start_player = start_player self.board = {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' ',} self.win_patterns = [[1,2,3], [4,5,6], [7,8,9], [1,4,7], [2,5,8], [3,6,9], [1,5,9], [7,5,3]] # the other functions are now passed self def visualize_board(self): print( \"|{}|{}|{}|\\n|{}|{}|{}|\\n|{}|{}|{}|\\n\".format(*self.board.values()) ) def check_winning(self): for pattern in self.win_patterns: values = [self.board[i] for i in pattern] if values == ['X', 'X', 'X']: self.winner = 'X' # we update the winner status return \"'X' Won!\" elif values == ['O', 'O', 'O']: self.winner = 'O' return \"'O' Won!\" return '' def check_stalemate(self): if (' ' not in self.board.values()) and (self.check_winning() == ''): self.winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine(TicTacToe): def __init__(self, setup='auto'): super().__init__() self.setup = setup def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() middle = 5 corner = [1,3,7,9] side = [2,4,6,8] # first check for a winning move move_found = False for move in avail_moves: temp_board[move] = player_label for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [player_label, player_label, player_label]: move_found = True break if move_found: break else: temp_board[move] = ' ' # check if the opponent has a winning move if move_found == False: for move in avail_moves: temp_board[move] = opponent for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [opponent, opponent, opponent]: move_found = True break if move_found: break else: temp_board[move] = ' ' # check if middle avail if move_found == False: if middle in avail_moves: move_found = True move = middle # check corners if move_found == False: move_corner = [val for val in avail_moves if val in corner] if len(move_corner) > 0: move = random.choice(move_corner) move_found = True # check side if move_found == False: move_side = [val for val in avail_moves if val in side] if len(move_side) > 0: move = random.choice(move_side) move_found = True return move def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} if players != 2: ########## # Allow the user to set the ai level ########## level = int(input(\"select AI level (1, 2)\")) if level == 1: self.ai_level = 1 elif level == 2: self.ai_level = 2 else: print(\"Unknown AI level entered, this will cause problems\") if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} ########## # and automatically set the ai level otherwise ########## self.ai_level = 2 def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) # we're going to allow the user to quit the game from the input line if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player_label)) move = int(move) if self.board[move] != ' ': continue else: break else: ########## # Our level 1 ai agent (random) ########## if self.ai_level == 1: move = self.random_ai() ########## # Our level 2 ai agent (heuristic) ########## elif self.ai_level == 2: move = self.heuristic_ai(player_label) self.board[move] = player_label # the winner varaible will now be check within the board object self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self","title":"4.0.1 Import Packages"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#402-load-dataset","text":"back to top","title":"4.0.2 Load Dataset"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#41-rethinking-gameplay","text":"To implement the broader strategies used in game theory and machine learning, we need to rebroadcast our approach to creating our AI agent. In the heurstical agent model, we thought in terms of checking for specific move types, defined by what kind of advantage they give us during game play, i.e. see if a winning move is available, a blocking move, if the center place is free, etc. Rather than thinking with this look and check mindset that is centered around specific strategies and our own prior knowledge about the game (we know that a center piece is statistically likely to give us a higher chance of winning) we will evaluate every available move to the AI, and rate them quantitatively. switching from ordinal to interval Notice the datatype change when we move from giving simple preferences of moves to actual scores of moves. Catalog this in your mind for future reference when considering datatypes!","title":"4.1 Rethinking gameplay"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#411-one-step-look-ahead","text":"For now, when we rate our boards, we will only look 1-step ahead in gameplay. Hence the name we give this AI strategy, 1-step lookahead The beginning portion of our code will look about the same as the heuristic AI model. Recall: def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() but now, instead of searching progressively through our preferred move-types (winning, middle, etc.) . We are going to give every available move (1, 3, 7, etc.) a score. Our score regimen will look like the following: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move","title":"4.1.1 One-Step Look Ahead"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#q1-rewrite-avail_moves","text":"define avail_moves as a dictionary of available moves with scores for each move as empty strings. We will update this dictionary with numerical scores in the next step # we're going to steal the parameter names to # prototype our new function self = TicTacToe() player_label = 'X' opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] # instead of a list, we want avail_moves to now be a dictionary that will # contain the move and its score # avail_moves = # YOUR CODE HERE temp_board = self.board.copy()","title":"Q1 Rewrite avail_moves"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#q2-score-each-move-in-avail_moves","text":"Now let's fold this into our new one_step_ai function. Remember: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## # avail_moves = # temp_board = self.board.copy() # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves.keys(): temp_board[move] = player_label for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [player_label, player_label, player_label]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score temp_board[move] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## ############################################################################## ################### All remaining moves receive a score of 1 ############################################################################## return avail_moves one_step_ai(self, 'X') {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}","title":"Q2 Score each move in avail_moves"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#q3-test-one_step_ai","text":"That's great, but how do we check that our code will work when a winning move is available, or a losing move is just around the corner? let's create a unit test for these! # just defining a new game self = TicTacToe() player_label = 'X' # seeding the board with some X's self.board[1] = 'X' self.board[2] = 'X' self.board {1: 'X', 2: 'X', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} Now test the winning move. Your code should return 100 at move 3 and 1 everywhere else one_step_ai(self, player_label) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} We can test the losing move by reversing the players player_label = 'O' one_step_ai(self, player_label) {3: 10, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} great! Let's keep these shorthand codes in mind when we go to write actual unit tests with the one_step_ai function embedded in the GameEngine module. We're not done yet, recall that our other ai agents returned the actual selected move, not a dictionary of the moves with scores. We need to create a move from this dictionary and return it. Here's what the general procedure will look like: Grab the maximum score (after assigning scores to all of avail_moves Select all moves that have this maximum score Return a random selection of the moves with the max score and then in code format: avail_moves = one_step_ai(self, player_label) # 1. grab the maximum score max_score = max(avail_moves.values()) # 2. select all moves that have this maximum score valid = [] for key, value in avail_moves.items(): if value == max_score: valid.append(key) # 3. return a random selection of the moves with the max score move = random.choice(valid) move 3","title":"Q3 Test one_step_ai"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#42-putting-it-all-together","text":"","title":"4.2 Putting it all together"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#q4-finish-one_step_ai-to-return-a-move","text":"Let's see if we can rewrite our game engine to take new AI models in as a passable parameter. This way our base module will be much cleaner, and allow us to continue to write new functions for the base engine as long as they pass along the same variables. # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai(board, win_patterns, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] temp_board = board.copy() # define avail_moves # avail_moves = # your code for giving scores for avail_moves # first grab max score # then select all moves that have this max score # return a random selection of the moves with the max score return move","title":"Q4 Finish one_step_ai to return a move"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#421-allow-gameengine-to-take-an-ai-agent-as-a-passable-parameter","text":"Let's rewrite our GameEngine to take an ai agent as a passable parameter under user_ai . The default value will be None Additional user_ai criteria will be that user_ai receives board , win_patterns and player_label and returns move . class GameEngine(TicTacToe): def __init__(self, setup='auto', user_ai=None): super().__init__() self.setup = setup self.user_ai = user_ai def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() middle = 5 corner = [1,3,7,9] side = [2,4,6,8] # first check for a winning move move_found = False for move in avail_moves: temp_board[move] = player_label for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [player_label, player_label, player_label]: move_found = True break if move_found: break else: temp_board[move] = ' ' # check if the opponent has a winning move if move_found == False: for move in avail_moves: temp_board[move] = opponent for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [opponent, opponent, opponent]: move_found = True break if move_found: break else: temp_board[move] = ' ' # check if middle avail if move_found == False: if middle in avail_moves: move_found = True move = middle # check corners if move_found == False: move_corner = [val for val in avail_moves if val in corner] if len(move_corner) > 0: move = random.choice(move_corner) move_found = True # check side if move_found == False: move_side = [val for val in avail_moves if val in side] if len(move_side) > 0: move = random.choice(move_side) move_found = True return move def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} if players != 2: ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self.user_ai == None: level = int(input(\"select AI level (1, 2)\")) if level == 1: self.ai_level = 1 elif level == 2: self.ai_level = 2 else: print(\"Unknown AI level entered, this will cause problems\") else: self.ai_level = 3 if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} ########## # and automatically set the ai level otherwise ########## if self.user_ai == None: self.ai_level = 2 else: self.ai_level = 3 def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) # we're going to allow the user to quit the game from the input line if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player_label)) move = int(move) if self.board[move] != ' ': continue else: break else: ########## # Our level 1 ai agent (random) ########## if self.ai_level == 1: move = self.random_ai() ########## # Our level 2 ai agent (heuristic) ########## elif self.ai_level == 2: move = self.heuristic_ai(player_label) ########## # Our user-defined AI agent ########## elif self.ai_level == 3: move = self.user_ai(self.board, self.win_patterns, player_label) self.board[move] = player_label # the winner varaible will now be check within the board object self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self Test the auto and user functions game = GameEngine(setup='user', user_ai=one_step_ai) game.setup_game() How many Players? (type 0, 1, or 2)1 who will go first? (X, (AI), or O (Player))X game.play_game() | | | | | | | | | | | | | | | | | | | | | |X| | O, what's your move?5 | | | | | |O| | | |X| | | | |X| | |O| | | |X| | O, what's your move?4 | | |X| |O|O| | | |X| | | | |X| |O|O|X| | |X| | O, what's your move?1 |O| |X| |O|O|X| | |X| | 'X' Won! |O| |X| |O|O|X| | |X|X| <__main__.GameEngine at 0x7f1f812e7d50>","title":"4.2.1 Allow GameEngine to take an ai agent as a passable parameter"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#43-write-unit-tests-for-the-new-code","text":"def test_user_ai(): random.seed(42) game = GameEngine(setup='auto', user_ai=one_step_ai) game.setup_game() outcome = game.play_game() assert outcome.winner == 'X', 'X should have won!' test_user_ai() | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| | |X|O| |O| | | 'X' Won! |X|O|X| | |X|O| |O| |X|","title":"4.3 Write Unit Tests for the New Code"},{"location":"project/P4_N_Step_Look_Ahead_Agents/","text":"Data Science Foundations, Project Part 4: N-Step Look Ahead Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com For this lesson, we will be adding N-step Look Ahead algorithm to our ensemble of AI agents! 4.0 Preparing Environment and Importing Data back to top 4.0.1 Import Packages back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt def one_step_ai(board, win_patterns, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = {i: 1 for i in board.keys() if board[i] == ' '} temp_board = board.copy() ######################################## # we're going to change the following lines, instead of caring # whether we've found the best move, we want to update the move # with a score ######################################## # check if the opponent has a winning move first, we will overwrite # the score for this move if it is also a winning move for the current # player for move in avail_moves.keys(): temp_board[move] = opponent for pattern in win_patterns: values = [temp_board[i] for i in pattern] if values == [opponent, opponent, opponent]: avail_moves[move] = 10 temp_board[move] = ' ' for move in avail_moves.keys(): temp_board[move] = player_label for pattern in win_patterns: values = [temp_board[i] for i in pattern] if values == [player_label, player_label, player_label]: avail_moves[move] = 100 temp_board[move] = ' ' # first grab max score max_score = max(avail_moves.values()) # then select all moves that have this max score valid = [] for key, value in avail_moves.items(): if value == max_score: valid.append(key) # return a random selection of the moves with the max score move = random.choice(valid) return move class TicTacToe: # can preset winner and starting player def __init__(self, winner='', start_player=''): self.winner = winner self.start_player = start_player self.board = {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' ',} self.win_patterns = [[1,2,3], [4,5,6], [7,8,9], [1,4,7], [2,5,8], [3,6,9], [1,5,9], [7,5,3]] # the other functions are now passed self def visualize_board(self): print( \"|{}|{}|{}|\\n|{}|{}|{}|\\n|{}|{}|{}|\\n\".format(*self.board.values()) ) def check_winning(self): for pattern in self.win_patterns: values = [self.board[i] for i in pattern] if values == ['X', 'X', 'X']: self.winner = 'X' # we update the winner status return \"'X' Won!\" elif values == ['O', 'O', 'O']: self.winner = 'O' return \"'O' Won!\" return '' def check_stalemate(self): if (' ' not in self.board.values()) and (self.check_winning() == ''): self.winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine(TicTacToe): def __init__(self, setup='auto', user_ai=None): super().__init__() self.setup = setup self.user_ai = user_ai def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() middle = 5 corner = [1,3,7,9] side = [2,4,6,8] # first check for a winning move move_found = False for move in avail_moves: temp_board[move] = player_label for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [player_label, player_label, player_label]: move_found = True break if move_found: break else: temp_board[move] = ' ' # check if the opponent has a winning move if move_found == False: for move in avail_moves: temp_board[move] = opponent for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [opponent, opponent, opponent]: move_found = True break if move_found: break else: temp_board[move] = ' ' # check if middle avail if move_found == False: if middle in avail_moves: move_found = True move = middle # check corners if move_found == False: move_corner = [val for val in avail_moves if val in corner] if len(move_corner) > 0: move = random.choice(move_corner) move_found = True # check side if move_found == False: move_side = [val for val in avail_moves if val in side] if len(move_side) > 0: move = random.choice(move_side) move_found = True return move def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} if players != 2: ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self.user_ai == None: level = int(input(\"select AI level (1, 2)\")) if level == 1: self.ai_level = 1 elif level == 2: self.ai_level = 2 else: print(\"Unknown AI level entered, this will cause problems\") else: self.ai_level = 3 if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} ########## # and automatically set the ai level otherwise ########## if self.user_ai == None: self.ai_level = 2 else: self.ai_level = 3 def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) # we're going to allow the user to quit the game from the input line if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player_label)) move = int(move) if self.board[move] != ' ': continue else: break else: ########## # Our level 1 ai agent (random) ########## if self.ai_level == 1: move = self.random_ai() ########## # Our level 2 ai agent (heuristic) ########## elif self.ai_level == 2: move = self.heuristic_ai(player_label) ########## # Our user-defined AI agent ########## elif self.ai_level == 3: move = self.user_ai(self.board, self.win_patterns, player_label) self.board[move] = player_label # the winner varaible will now be check within the board object self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self 4.1 N-Step Look Ahead and Minimax In the previous tic tac toe module, our AI only looked 1 step ahead, and we can probably see how this has disadvantages. When we play strategy games ourselves, we often do better by looking a number of steps into the future. One new idea that this requires, is how we will anticipate our opponents move. This gets us into game theory . We're not going to borrow a whole lot from here, just the following: we will assume our opponent will work to minimize our score This switching from us wanting to maximize our score to the opponent wanting to minimize our score is called the minimax algorithm. As well look ahead into the future possibility of moves, we will use minimax to set our hypothetical behavior as well as our opponents You can investigate the pseudocode for minmax on wiki. # we're going to pull out and reformat some of our helper functions in the # TicTacToe class win_patterns = [[1,2,3], [4,5,6], [7,8,9], [1,4,7], [2,5,8], [3,6,9], [1,5,9], [7,5,3]] def check_winning(board, win_patterns): for pattern in win_patterns: values = [board[i] for i in pattern] if values == ['X', 'X', 'X'] or values == ['O', 'O', 'O']: return True return False def check_stalemate(board, win_patterns): if (' ' not in board.values()) and (check_winning(board, win_patterns) == ''): return True return False def minimax(depth, board, maximizing_player, player_label, verbiose=False): # infer the opponent opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] # set the available moves avail_moves = [i for i in board.keys() if board[i] == ' '] # check if the depth is 0, or stalemate/winner has been reached # if so this is the basecase and we want to return get_score() terminal_move = is_terminal_node(board, avail_moves) if terminal_move or depth == 0: score = get_score(board, player_label, win_patterns) if verbiose: print('{} score: {}. depth: {}'.format(board, score, depth)) return score ### in the following we want to search through every possible board at the ### current level (the possible moves for the current player, given that the ### player is either the one whose turn it is or the imagined opponent) # call minimax where it is the current players turn and so we want to # maximize the score if maximizing_player: score = -np.Inf for move in avail_moves: new_board = board.copy() new_board[move] = player_label score = max(score, minimax(depth-1, new_board, False, player_label, verbiose)) if verbiose: print('{} max. score: {}. depth: {}'.format(board, score, depth)) return score # call minimax where it is the opponent players turn and so we want to # minimize the score elif not maximizing_player: score = np.Inf for move in avail_moves: new_board = board.copy() new_board[move] = opponent score = min(score, minimax(depth-1, new_board, True, player_label, verbiose)) if verbiose: print('{} min. score: {}. depth: {}'.format(board, score, depth)) return score def is_terminal_node(board, avail_moves): if check_winning(board, win_patterns): return True elif check_stalemate(board, win_patterns): return True else: return False def get_score(board, player_label, win_patterns): # this will look somewhat similar to our 1-step lookahead algorithm opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] score = 0 for pattern in win_patterns: values = [board[i] for i in pattern] # if the opponent wins, the score is -100 if values == [opponent, opponent, opponent]: score = -100 elif values == [player_label, player_label, player_label]: score = 100 return score board = TicTacToe().board board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} minimax(depth=1, board=board, maximizing_player=True, player_label='O') 0 Finally, we need a couple wrapper functions to handle this. The first is a handler for the top level of the game tree (we want to see the minmax result for every possible move at the current place in the game) verbiose = True verbiose True def get_minimax(depth, board, player_label, verbiose=False): score = minimax(depth-1, board, False, player_label, verbiose=verbiose) return score def n_step_ai_temp(board, win_patterns, player_label, n_steps, verbiose=False): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = {i: 1 for i in board.keys() if board[i] == ' '} for move in avail_moves.keys(): temp_board = board.copy() temp_board[move] = player_label score = get_minimax(n_steps, temp_board, player_label, verbiose=verbiose) avail_moves[move] = score return avail_moves Let's test our n_step_ai while we're still returning the dictionary of available moves. Does this make sense? board = TicTacToe().board board[1] = 'X' board[5] = 'O' board[2] = 'X' # with this setup we should see that a good move will be to play 3 # be sure to vary the number of lookahead steps n_step_ai_temp(board=board, win_patterns=win_patterns, player_label='X', n_steps=2) {3: 100, 4: 0, 6: 0, 7: 0, 8: 0, 9: 0} Looks like it's making sense. Let's also try when the opponent looks like they've got a winning move to be made board = TicTacToe().board board[1] = 'X' board[5] = 'O' board[2] = 'X' board[4] = 'O' # with this setup we should see that a good move will be to play 6 or 3 # be sure to vary the number of lookahead steps n_step_ai_temp(board=board, win_patterns=win_patterns, player_label='X', n_steps=3) Let's look at the following as well. This should be an interesting game situation to you. It is \"X's\" move. What do you notice? Does this outcome make sense? X cannot win this game board = TicTacToe().board board[1] = 'O' board[5] = 'O' board[2] = 'X' board[8] = 'X' # be sure to vary the number of lookahead steps n_step_ai_temp(board=board, win_patterns=win_patterns, player_label='X', n_steps=4, verbiose=False) # as a side note, sometimes it helps to interpret the outcome if you discount # the results from looking further down the game tree. (maybe O will make a mistake) # how would you encode this in your n-step look ahead algorithm? {3: -100, 4: -100, 6: -100, 7: -100, 9: -100} 4.2 Packaging for GameEngine Nice. Let's finish packaging our n_steps_ai so we can feed it to our game engine. def n_step_ai(board, win_patterns, player_label, n_steps=3): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = {i: 1 for i in board.keys() if board[i] == ' '} for move in avail_moves.keys(): temp_board = board.copy() temp_board[move] = player_label score = get_minimax(n_steps, temp_board, player_label) avail_moves[move] = score ########################################## ### The rest of our ai agent harness is the same ########################################## # first grab max score max_score = max(avail_moves.values()) # then select all moves that have this max score valid = [] for key, value in avail_moves.items(): if value == max_score: valid.append(key) # return a random selection of the moves with the max score move = random.choice(valid) return move game = GameEngine(setup='user', user_ai=n_step_ai) game.setup_game() How many Players? (type 0, 1, or 2)1 who will go first? (X, (AI), or O (Player))X game.play_game() | | | | | | | | | | | | | | | | |X| | | | | | | O, what's your move?5 | | | | |X|O| | | | | | | | | | |X|O| | | |X| | O, what's your move?2 | |O| | |X|O| | | |X| | | |O| | |X|O| | |X|X| | O, what's your move?1 |O|O| | |X|O| | |X|X| | 'X' Won! |O|O| | |X|O| | |X|X|X| <__main__.GameEngine at 0x7fe529a0d850> Let's investigate the behavior of our AI and double check that it makes sense board = game.board board[9] = ' ' board {1: 'O', 2: 'O', 3: ' ', 4: 'X', 5: 'O', 6: ' ', 7: 'X', 8: 'X', 9: ' '} game.visualize_board() |O|O| | |X|O| | |X|X| | n_step_ai_temp(board=board, win_patterns=win_patterns, player_label='X', n_steps=3) {3: -100, 6: -100, 9: 100} 4.3 Writing Tests def test_n_step_ai(): random.seed(42) game = GameEngine(setup='auto', user_ai=n_step_ai) game.setup_game() game.play_game() # check that the winner is X assert game.winner == 'X', \"Winner should be X!\" # check that the ai level is set to 3 which means our engine is properly # accessing the user defined ai assert game.ai_level == 3, \"The engine is not using the user defined AI!\" test_n_step_ai() | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | 'X' Won! |X|O|X| |O|X|O| | | |X|","title":"N-Step Look Ahead Agents"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#data-science-foundations-project-part-4-n-step-look-ahead","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com For this lesson, we will be adding N-step Look Ahead algorithm to our ensemble of AI agents!","title":"Data Science Foundations, Project Part 4: N-Step Look Ahead"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#40-preparing-environment-and-importing-data","text":"back to top","title":"4.0 Preparing Environment and Importing Data"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#401-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt def one_step_ai(board, win_patterns, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = {i: 1 for i in board.keys() if board[i] == ' '} temp_board = board.copy() ######################################## # we're going to change the following lines, instead of caring # whether we've found the best move, we want to update the move # with a score ######################################## # check if the opponent has a winning move first, we will overwrite # the score for this move if it is also a winning move for the current # player for move in avail_moves.keys(): temp_board[move] = opponent for pattern in win_patterns: values = [temp_board[i] for i in pattern] if values == [opponent, opponent, opponent]: avail_moves[move] = 10 temp_board[move] = ' ' for move in avail_moves.keys(): temp_board[move] = player_label for pattern in win_patterns: values = [temp_board[i] for i in pattern] if values == [player_label, player_label, player_label]: avail_moves[move] = 100 temp_board[move] = ' ' # first grab max score max_score = max(avail_moves.values()) # then select all moves that have this max score valid = [] for key, value in avail_moves.items(): if value == max_score: valid.append(key) # return a random selection of the moves with the max score move = random.choice(valid) return move class TicTacToe: # can preset winner and starting player def __init__(self, winner='', start_player=''): self.winner = winner self.start_player = start_player self.board = {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' ',} self.win_patterns = [[1,2,3], [4,5,6], [7,8,9], [1,4,7], [2,5,8], [3,6,9], [1,5,9], [7,5,3]] # the other functions are now passed self def visualize_board(self): print( \"|{}|{}|{}|\\n|{}|{}|{}|\\n|{}|{}|{}|\\n\".format(*self.board.values()) ) def check_winning(self): for pattern in self.win_patterns: values = [self.board[i] for i in pattern] if values == ['X', 'X', 'X']: self.winner = 'X' # we update the winner status return \"'X' Won!\" elif values == ['O', 'O', 'O']: self.winner = 'O' return \"'O' Won!\" return '' def check_stalemate(self): if (' ' not in self.board.values()) and (self.check_winning() == ''): self.winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine(TicTacToe): def __init__(self, setup='auto', user_ai=None): super().__init__() self.setup = setup self.user_ai = user_ai def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() middle = 5 corner = [1,3,7,9] side = [2,4,6,8] # first check for a winning move move_found = False for move in avail_moves: temp_board[move] = player_label for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [player_label, player_label, player_label]: move_found = True break if move_found: break else: temp_board[move] = ' ' # check if the opponent has a winning move if move_found == False: for move in avail_moves: temp_board[move] = opponent for pattern in self.win_patterns: values = [temp_board[i] for i in pattern] if values == [opponent, opponent, opponent]: move_found = True break if move_found: break else: temp_board[move] = ' ' # check if middle avail if move_found == False: if middle in avail_moves: move_found = True move = middle # check corners if move_found == False: move_corner = [val for val in avail_moves if val in corner] if len(move_corner) > 0: move = random.choice(move_corner) move_found = True # check side if move_found == False: move_side = [val for val in avail_moves if val in side] if len(move_side) > 0: move = random.choice(move_side) move_found = True return move def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move def setup_game(self): if self.setup == 'user': players = int(input(\"How many Players? (type 0, 1, or 2)\")) self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'human'}} if players != 2: ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self.user_ai == None: level = int(input(\"select AI level (1, 2)\")) if level == 1: self.ai_level = 1 elif level == 2: self.ai_level = 2 else: print(\"Unknown AI level entered, this will cause problems\") else: self.ai_level = 3 if players == 1: first = input(\"who will go first? (X, (AI), or O (Player))\") if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'human'}} elif players == 0: first = random.choice(['X', 'O']) if first == 'O': self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} elif self.setup == 'auto': first = random.choice(['X', 'O']) if first == 'O': self.start_player = 'O' self.player_meta = {'second': {'label': 'X', 'type': 'ai'}, 'first': {'label': 'O', 'type': 'ai'}} else: self.start_player = 'X' self.player_meta = {'first': {'label': 'X', 'type': 'ai'}, 'second': {'label': 'O', 'type': 'ai'}} ########## # and automatically set the ai level otherwise ########## if self.user_ai == None: self.ai_level = 2 else: self.ai_level = 3 def play_game(self): while True: for player in ['first', 'second']: self.visualize_board() player_label = self.player_meta[player]['label'] player_type = self.player_meta[player]['type'] if player_type == 'human': move = input(\"{}, what's your move?\".format(player_label)) # we're going to allow the user to quit the game from the input line if move in ['q', 'quit']: self.winner = 'F' print('quiting the game') break move = int(move) if self.board[move] != ' ': while True: move = input(\"{}, that position is already taken! \"\\ \"What's your move?\".format(player_label)) move = int(move) if self.board[move] != ' ': continue else: break else: ########## # Our level 1 ai agent (random) ########## if self.ai_level == 1: move = self.random_ai() ########## # Our level 2 ai agent (heuristic) ########## elif self.ai_level == 2: move = self.heuristic_ai(player_label) ########## # Our user-defined AI agent ########## elif self.ai_level == 3: move = self.user_ai(self.board, self.win_patterns, player_label) self.board[move] = player_label # the winner varaible will now be check within the board object self.check_winning() self.check_stalemate() if self.winner == '': continue elif self.winner == 'Stalemate': print(self.check_stalemate()) self.visualize_board() break else: print(self.check_winning()) self.visualize_board() break if self.winner != '': return self","title":"4.0.1 Import Packages"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#41-n-step-look-ahead-and-minimax","text":"In the previous tic tac toe module, our AI only looked 1 step ahead, and we can probably see how this has disadvantages. When we play strategy games ourselves, we often do better by looking a number of steps into the future. One new idea that this requires, is how we will anticipate our opponents move. This gets us into game theory . We're not going to borrow a whole lot from here, just the following: we will assume our opponent will work to minimize our score This switching from us wanting to maximize our score to the opponent wanting to minimize our score is called the minimax algorithm. As well look ahead into the future possibility of moves, we will use minimax to set our hypothetical behavior as well as our opponents You can investigate the pseudocode for minmax on wiki. # we're going to pull out and reformat some of our helper functions in the # TicTacToe class win_patterns = [[1,2,3], [4,5,6], [7,8,9], [1,4,7], [2,5,8], [3,6,9], [1,5,9], [7,5,3]] def check_winning(board, win_patterns): for pattern in win_patterns: values = [board[i] for i in pattern] if values == ['X', 'X', 'X'] or values == ['O', 'O', 'O']: return True return False def check_stalemate(board, win_patterns): if (' ' not in board.values()) and (check_winning(board, win_patterns) == ''): return True return False def minimax(depth, board, maximizing_player, player_label, verbiose=False): # infer the opponent opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] # set the available moves avail_moves = [i for i in board.keys() if board[i] == ' '] # check if the depth is 0, or stalemate/winner has been reached # if so this is the basecase and we want to return get_score() terminal_move = is_terminal_node(board, avail_moves) if terminal_move or depth == 0: score = get_score(board, player_label, win_patterns) if verbiose: print('{} score: {}. depth: {}'.format(board, score, depth)) return score ### in the following we want to search through every possible board at the ### current level (the possible moves for the current player, given that the ### player is either the one whose turn it is or the imagined opponent) # call minimax where it is the current players turn and so we want to # maximize the score if maximizing_player: score = -np.Inf for move in avail_moves: new_board = board.copy() new_board[move] = player_label score = max(score, minimax(depth-1, new_board, False, player_label, verbiose)) if verbiose: print('{} max. score: {}. depth: {}'.format(board, score, depth)) return score # call minimax where it is the opponent players turn and so we want to # minimize the score elif not maximizing_player: score = np.Inf for move in avail_moves: new_board = board.copy() new_board[move] = opponent score = min(score, minimax(depth-1, new_board, True, player_label, verbiose)) if verbiose: print('{} min. score: {}. depth: {}'.format(board, score, depth)) return score def is_terminal_node(board, avail_moves): if check_winning(board, win_patterns): return True elif check_stalemate(board, win_patterns): return True else: return False def get_score(board, player_label, win_patterns): # this will look somewhat similar to our 1-step lookahead algorithm opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] score = 0 for pattern in win_patterns: values = [board[i] for i in pattern] # if the opponent wins, the score is -100 if values == [opponent, opponent, opponent]: score = -100 elif values == [player_label, player_label, player_label]: score = 100 return score board = TicTacToe().board board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} minimax(depth=1, board=board, maximizing_player=True, player_label='O') 0 Finally, we need a couple wrapper functions to handle this. The first is a handler for the top level of the game tree (we want to see the minmax result for every possible move at the current place in the game) verbiose = True verbiose True def get_minimax(depth, board, player_label, verbiose=False): score = minimax(depth-1, board, False, player_label, verbiose=verbiose) return score def n_step_ai_temp(board, win_patterns, player_label, n_steps, verbiose=False): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = {i: 1 for i in board.keys() if board[i] == ' '} for move in avail_moves.keys(): temp_board = board.copy() temp_board[move] = player_label score = get_minimax(n_steps, temp_board, player_label, verbiose=verbiose) avail_moves[move] = score return avail_moves Let's test our n_step_ai while we're still returning the dictionary of available moves. Does this make sense? board = TicTacToe().board board[1] = 'X' board[5] = 'O' board[2] = 'X' # with this setup we should see that a good move will be to play 3 # be sure to vary the number of lookahead steps n_step_ai_temp(board=board, win_patterns=win_patterns, player_label='X', n_steps=2) {3: 100, 4: 0, 6: 0, 7: 0, 8: 0, 9: 0} Looks like it's making sense. Let's also try when the opponent looks like they've got a winning move to be made board = TicTacToe().board board[1] = 'X' board[5] = 'O' board[2] = 'X' board[4] = 'O' # with this setup we should see that a good move will be to play 6 or 3 # be sure to vary the number of lookahead steps n_step_ai_temp(board=board, win_patterns=win_patterns, player_label='X', n_steps=3) Let's look at the following as well. This should be an interesting game situation to you. It is \"X's\" move. What do you notice? Does this outcome make sense? X cannot win this game board = TicTacToe().board board[1] = 'O' board[5] = 'O' board[2] = 'X' board[8] = 'X' # be sure to vary the number of lookahead steps n_step_ai_temp(board=board, win_patterns=win_patterns, player_label='X', n_steps=4, verbiose=False) # as a side note, sometimes it helps to interpret the outcome if you discount # the results from looking further down the game tree. (maybe O will make a mistake) # how would you encode this in your n-step look ahead algorithm? {3: -100, 4: -100, 6: -100, 7: -100, 9: -100}","title":"4.1 N-Step Look Ahead and Minimax"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#42-packaging-for-gameengine","text":"Nice. Let's finish packaging our n_steps_ai so we can feed it to our game engine. def n_step_ai(board, win_patterns, player_label, n_steps=3): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = {i: 1 for i in board.keys() if board[i] == ' '} for move in avail_moves.keys(): temp_board = board.copy() temp_board[move] = player_label score = get_minimax(n_steps, temp_board, player_label) avail_moves[move] = score ########################################## ### The rest of our ai agent harness is the same ########################################## # first grab max score max_score = max(avail_moves.values()) # then select all moves that have this max score valid = [] for key, value in avail_moves.items(): if value == max_score: valid.append(key) # return a random selection of the moves with the max score move = random.choice(valid) return move game = GameEngine(setup='user', user_ai=n_step_ai) game.setup_game() How many Players? (type 0, 1, or 2)1 who will go first? (X, (AI), or O (Player))X game.play_game() | | | | | | | | | | | | | | | | |X| | | | | | | O, what's your move?5 | | | | |X|O| | | | | | | | | | |X|O| | | |X| | O, what's your move?2 | |O| | |X|O| | | |X| | | |O| | |X|O| | |X|X| | O, what's your move?1 |O|O| | |X|O| | |X|X| | 'X' Won! |O|O| | |X|O| | |X|X|X| <__main__.GameEngine at 0x7fe529a0d850> Let's investigate the behavior of our AI and double check that it makes sense board = game.board board[9] = ' ' board {1: 'O', 2: 'O', 3: ' ', 4: 'X', 5: 'O', 6: ' ', 7: 'X', 8: 'X', 9: ' '} game.visualize_board() |O|O| | |X|O| | |X|X| | n_step_ai_temp(board=board, win_patterns=win_patterns, player_label='X', n_steps=3) {3: -100, 6: -100, 9: 100}","title":"4.2 Packaging for GameEngine"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#43-writing-tests","text":"def test_n_step_ai(): random.seed(42) game = GameEngine(setup='auto', user_ai=n_step_ai) game.setup_game() game.play_game() # check that the winner is X assert game.winner == 'X', \"Winner should be X!\" # check that the ai level is set to 3 which means our engine is properly # accessing the user defined ai assert game.ai_level == 3, \"The engine is not using the user defined AI!\" test_n_step_ai() | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | 'X' Won! |X|O|X| |O|X|O| | | |X|","title":"4.3 Writing Tests"}]}