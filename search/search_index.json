{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science Foundations \u00b6 Welcome to Data Science Foundations, my name is Wesley Visit the github repo to access all materials and code for this class This is part II of a multi-part lesson path. Checkout Python Foundations for part I and General Applications of Neural Networks for part III Access the solutions if you get stuck The recommended schedule for this material: Day (2.5 hrs/day) Modules 1 Session 1: Regression and Analysis Lab 1: Descriptive Statistics Data Hunt 2 Session 2: Inferential Statistics Lab 2: Inferential Statistics Data Hunt 3 Session 3: Model Selection and Validation Project Part 1: Statistical Analysis of Tic-Tac-Toe 4 Session 4: Feature Engineering Lab 3: Practice with Feature Engineering 5 Session 5: Unsupervised Learning Project Part 2: Heuristical Tic-Tac-Toe Agents 6 Session 6: Bagging Lab 4: Practice with Supervised Learners 7 Session 7: Boosting Lab 5: Practice with Writing Unit Tests Project Part 3: 1-Step Look Ahead Agents 8 Project Part 4: N-Step Look Ahead Agents Happy learning \ud83e\uddd1\u200d\ud83c\udfeb","title":"About"},{"location":"#data-science-foundations","text":"Welcome to Data Science Foundations, my name is Wesley Visit the github repo to access all materials and code for this class This is part II of a multi-part lesson path. Checkout Python Foundations for part I and General Applications of Neural Networks for part III Access the solutions if you get stuck The recommended schedule for this material: Day (2.5 hrs/day) Modules 1 Session 1: Regression and Analysis Lab 1: Descriptive Statistics Data Hunt 2 Session 2: Inferential Statistics Lab 2: Inferential Statistics Data Hunt 3 Session 3: Model Selection and Validation Project Part 1: Statistical Analysis of Tic-Tac-Toe 4 Session 4: Feature Engineering Lab 3: Practice with Feature Engineering 5 Session 5: Unsupervised Learning Project Part 2: Heuristical Tic-Tac-Toe Agents 6 Session 6: Bagging Lab 4: Practice with Supervised Learners 7 Session 7: Boosting Lab 5: Practice with Writing Unit Tests Project Part 3: 1-Step Look Ahead Agents 8 Project Part 4: N-Step Look Ahead Agents Happy learning \ud83e\uddd1\u200d\ud83c\udfeb","title":"Data Science Foundations"},{"location":"S1_Regression_and_Analysis/","text":"Data Science Foundations Session 1: Regression and Analysis \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at fitting data to a curve using regression . We will also look at using regression to make predictions for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error. 1.0 Preparing Environment and Importing Data \u00b6 back to top 1.0.1 Import Packages \u00b6 back to top # Import pandas, pyplot, ipywidgets import pandas as pd from matplotlib import pyplot as plt from ipywidgets import interact # Import Scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score # for enrichment topics import seaborn as sns import numpy as np import scipy 1.0.2 Load Dataset \u00b6 back to top For our discussion on regression and descriptive statistics today we will use a well known dataset of different wines and their quality ratings df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) df . shape (6497, 13) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 1.1 What is regression? \u00b6 It is the process of finding a relationship between dependent and independent variables to find trends in data. This abstract definition means that you have one variable (the dependent variable) which depends on one or more variables (the independent variables). One of the reasons for which we want to regress data is to understand whether there is a trend between two variables. Housing Prices Example We can imagine this scenario with housing prices. Envision a mixed dataset of continuous and discrete independent variables. Some features could be continuous, floating point values like location ranking and housing condition. Others could be discrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a regression model. 1.2 Linear regression fitting with scikit-learn \u00b6 \ud83c\udfcb\ufe0f Exercise 1: rudimentary EDA \u00b6 What does the data look like? Recall how to visualize data in a pandas dataframe for every column calculate the: skew: df.skew() kurtosis: df.kurtosis() pearsons correlation with the dependent variable: df.corr() number of missing entries df.isnull() and organize this into a new dataframe note: pearsons is just one type of correlation, another correlation available to us is spearman which differs from pearsons in that it depends on ranked values rather than their direct quantities, you can read more here df . isnull () . sum () type 0 fixed acidity 10 volatile acidity 8 citric acid 3 residual sugar 2 chlorides 2 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 9 sulphates 4 alcohol 0 quality 0 dtype: int64 # Cell for Exercise 1 # part A # using df.<method> define the following four variables with the results from # skew(), kurtosis(), corr() (while selecting for quality), and isnull() # for isnull() you'll notice the return is a dataframe of booleans. we would # like to simply know the number of null values for each column. change the # return of isnull() using the sum() method along the columns # skew = # kurt = # pear = # null = # part B # on line 13, put these results in a list using square brackets and call # pd.DataFrame on the list to make your new DataFrame! store it under the # variable name dff # part C # take the transpose of this DataFrame using dff.T. reassign dff to this copy # part D # set the column names to 'skew', 'kurtosis', 'pearsons _quality', and # 'null count' using dff.columns # Now return dff to the output to view your hand work # dff # uncomment this line I have gone ahead and repeated this exercise with the red vs white wine types: red = df . loc [ df [ 'type' ] == 'red' ] wht = df . loc [ df [ 'type' ] == 'white' ] def get_summary ( df ): skew = df . skew () kurt = df . kurtosis () pear = df . corr ()[ 'quality' ] null = df . isnull () . sum () med = df . median () men = df . mean () dff = pd . DataFrame ([ skew , kurt , pear , null , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'pearsons _quality' , 'null count' , 'median' , 'mean' ] return dff dffr = get_summary ( red ) dffw = get_summary ( wht ) desc = pd . concat ([ dffr , dffw ], keys = [ 'red' , 'white' ]) /tmp/ipykernel_277/2387423026.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. skew = df.skew() /tmp/ipykernel_277/2387423026.py:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. kurt = df.kurtosis() /tmp/ipykernel_277/2387423026.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. med = df.median() /tmp/ipykernel_277/2387423026.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. men = df.mean() desc .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count median mean red fixed acidity 0.982192 1.132624 0.123834 2.0 7.90000 8.322104 volatile acidity 0.672862 1.226846 -0.390858 1.0 0.52000 0.527738 citric acid 0.317891 -0.788476 0.226917 1.0 0.26000 0.271145 residual sugar 4.540655 28.617595 0.013732 0.0 2.20000 2.538806 chlorides 5.680347 41.715787 -0.128907 0.0 0.07900 0.087467 free sulfur dioxide 1.250567 2.023562 -0.050656 0.0 14.00000 15.874922 total sulfur dioxide 1.515531 3.809824 -0.185100 0.0 38.00000 46.467792 density 0.071288 0.934079 -0.174919 0.0 0.99675 0.996747 pH 0.194803 0.814690 -0.057094 2.0 3.31000 3.310864 sulphates 2.429115 11.712632 0.251685 2.0 0.62000 0.658078 alcohol 0.860829 0.200029 0.476166 0.0 10.20000 10.422983 quality 0.217802 0.296708 1.000000 0.0 6.00000 5.636023 type NaN NaN NaN 0.0 NaN NaN white fixed acidity 0.647981 2.176560 -0.114032 8.0 6.80000 6.855532 volatile acidity 1.578595 5.095526 -0.194976 7.0 0.26000 0.278252 citric acid 1.284217 6.182036 -0.009194 2.0 0.32000 0.334250 residual sugar 1.076601 3.469536 -0.097373 2.0 5.20000 6.393250 chlorides 5.023412 37.560847 -0.210181 2.0 0.04300 0.045778 free sulfur dioxide 1.406745 11.466342 0.008158 0.0 34.00000 35.308085 total sulfur dioxide 0.390710 0.571853 -0.174737 0.0 134.00000 138.360657 density 0.977773 9.793807 -0.307123 0.0 0.99374 0.994027 pH 0.458402 0.532552 0.098858 7.0 3.18000 3.188203 sulphates 0.977361 1.589847 0.053690 2.0 0.47000 0.489835 alcohol 0.487342 -0.698425 0.435575 0.0 10.40000 10.514267 quality 0.155796 0.216526 1.000000 0.0 6.00000 5.877909 type NaN NaN NaN 0.0 NaN NaN def my_fig ( metric = desc . columns ): fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) pd . DataFrame ( desc [ metric ]) . unstack ()[ metric ] . T . plot ( kind = 'barh' , ax = ax ) # interact(my_fig) \ud83d\ude4b Question 1: Discussion Around EDA Plot \u00b6 What do we think of this plot? metric = mean , the cholrides values metric = kurtosis , residual sugar metric = pearsons _quality , magnitudes and directions How to improve the plot, what other plots would we like to see? For instance, what if we were really curious about the high kurtosis for chlorides content? What more would we like to glean about the distribution of chloride content? # we can use df.describe() to take a look at the quantile values and min/max df [ 'chlorides' ] . describe () count 6495.000000 mean 0.056042 std 0.035036 min 0.009000 25% 0.038000 50% 0.047000 75% 0.065000 max 0.611000 Name: chlorides, dtype: float64 # and see how these values appear in a KDE fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) df [ 'chlorides' ] . plot ( kind = 'kde' , ax = ax ) ax . set_xlim ( 0 , .61 ) (0.0, 0.61) # lastly we may want to look at the raw values themselves. We can sort them # too view outliers df [ 'chlorides' ] . sort_values ( ascending = False )[: 50 ] 5156 0.611 5049 0.610 5004 0.467 4979 0.464 5590 0.422 6268 0.415 6270 0.415 5652 0.415 6217 0.414 5949 0.414 5349 0.413 6158 0.403 4981 0.401 5628 0.387 6063 0.369 4915 0.368 5067 0.360 5179 0.358 484 0.346 5189 0.343 4917 0.341 5124 0.337 4940 0.332 1217 0.301 687 0.290 4473 0.271 5079 0.270 6272 0.267 5138 0.263 1865 0.255 5466 0.250 1034 0.244 5674 0.243 5675 0.241 683 0.240 1638 0.239 5045 0.236 6456 0.235 6468 0.230 5465 0.226 5464 0.226 5564 0.222 2186 0.217 5996 0.216 6333 0.214 5206 0.214 6332 0.214 5205 0.213 4497 0.212 1835 0.211 Name: chlorides, dtype: float64 1.2.2 Visualizing the data set - motivating regression analysis \u00b6 In order to demonstrate simple linear regression with this dataset we will look at two particular features: fixed acidity and density . We can create a scatter plot of fixed acidity vs density for the red wine in the dataset using df.plot() and see that there appears to be a general trend between the two features: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) df . loc [ df [ 'type' ] == 'red' ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> Now the question is: How do we quantify this trend? 1.2.3 Estimating the regression coefficients \u00b6 It looks like density increases with fixed acidity following a line, maybe something like y(x)= m \\cdot x + b \\;\\;\\;\\;\\;\\;\\;\\; \\sf{eq. 1} with \\( y=\\sf density \\), \\(x=\\sf fixed \\space acidity\\), and \\(m\\) the slope and \\(b\\) the intercept. To solve the problem, we need to find the values of \\(b\\) and \\(m\\) in equation 1 to best fit the data. This is called linear regression . In linear regression our goal is to minimize the error between computed values of positions \\(y^{\\sf calc}(x_i)\\equiv y^{\\sf calc}_i\\) and known values \\(y^{\\sf exact}(x_i)\\equiv y^{\\sf exact}_i\\). In Ordinary Least Squares the error term we try to minimize is called the residual sum of squares . We find \\(b\\) and \\(m\\) which lead to lowest value of \\epsilon (m,b) =SS_{\\sf res}=\\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - m\\cdot x_i - b \\right)^2\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sf{eq. 2} To find out more see e.g. https://en.wikipedia.org/wiki/Simple_linear_regression Using some calculus, we can solve for \\(b\\) and \\(m\\) analytically: m = \\frac{\\sum_{i=1}^n{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}} b = \\bar{y} - m\\bar{x} Note that for multivariate regression, the coefficients can be solved analytically as well \ud83d\ude0e \ud83d\ude4b Question 2: linear regression loss function \u00b6 Do we always want m and b to be large positive numbers so as to minimize eq. 2? Luckily scikit-learn contains many functions related to regression including linear regression . The function we will use is called LinearRegression() . # Create linear regression object model = linear_model . LinearRegression () # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red [ 'density' ] . values . reshape ( - 1 , 1 ) y = red [ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) print ( red [ 'density' ] . values . shape , red [ 'fixed acidity' ] . values . shape ) print ( x . shape , y . shape ) (1599,) (1599,) (1599, 1) (1599, 1) What happens when we try to fit the data as is? # Fit to the data # model.fit(x, y) \ud83c\udfcb\ufe0f Exercise 2: drop Null Values (and practice pandas operations) \u00b6 Let's look back at our dataset description dataframe above, what do we notice, what contains null values? There are several strategies for dealing with null values. For now let's take the simplest case, and drop rows in our dataframe that contain null # Cell for Exercise 2 # For this templated exercise you are going to complete everything in one line # of code, but we are going to break it up into steps. So for each part (A, B, # etc.) paste your answer from the previous part to begin (your opertaions will # read from left to right) # step A # select the 'density' and 'fixed acidity' columns of red. make sure the return # is a dataframe # step B # now use the dropna() method on axis 0 (the rows) to drop any null values # step B # select column 'density' # step C # select the values # step D # reshape the result with an empty second dimension using .reshape() and store # the result under variable x # repeat the same process with 'fixed acidity' and variable y Now that we have our x and y arrays we can fit using ScikitLearn x = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values . reshape ( - 1 , 1 ) y = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) \ud83d\ude4b Question 3: why do we drop null values across both columns? \u00b6 Notice in the above cell how we selected both density and fixed acidity before calling dropna ? Why did we do that? Why didn't we just select density in the x variable case and fixed acidity in the y variable case? # Fit to the data model . fit ( x , y ) # Extract the values of interest m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # Print the slope m and intercept b print ( 'Scikit learn - Slope: ' , m , 'Intercept: ' , b ) Scikit learn - Slope: 616.01314280661 Intercept: -605.6880086750523 \ud83c\udfcb\ufe0f Exercise 3: calculating y_pred \u00b6 Estimate the values of \\(y\\) by using your fitted parameters. Hint: Use your model.coef_ and model.intercept_ parameters to estimate y_pred following equation 1 # define y_pred in terms of m, x, and b # y_pred = # uncomment the following lines! # fig, ax = plt.subplots(1,1, figsize=(10,10)) # ax.plot(x, y_pred, ls='', marker='*') # ax.plot(x, y, ls='', marker='.') We can also return predictions directly with the model object using the predict() method note: it is great to get in the habit of utilizing model outputs this way, as the API will be similar across all scikit-learn models (and sometimes models in other libraries as well!) # Another way to get this is using the model.predict function y_pred = model . predict ( x ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x , y_pred , ls = '' , marker = '*' ) ax . plot ( x , y , ls = '' , marker = '.' ) ax . set_ylabel ( 'acidity' ) ax . set_xlabel ( 'density' ) Text(0.5, 0, 'density') 1.3 Error and topics of model fitting (assessing model accuracy) \u00b6 1.3.1 Measuring the quality of fit \u00b6 1.3.1.1 Mean Squared Error \u00b6 The plot in Section 1.2.3 looks good, but numerically what is our error? What is the mean value of $\\epsilon$, i.e. the Mean Squared Error (MSE) ? {\\sf MSE}=\\epsilon_{\\sf ave} = \\frac{\\sum_{i=1}^{N_{\\sf times}}\\left(y^{\\sf exact}_i - m\\cdot t_i - b \\right)^2}{N_{\\sf times}}\\;\\;\\;\\;\\;\\sf eq. 3 # The mean squared error print ( 'Mean squared error: %.2f ' % mean_squared_error ( y , y_pred )) Mean squared error: 1.68 1.3.1.2 R-square \u00b6 Another way to measure error is the regression score, \\(R^2\\). \\(R^2\\) is generally defined as the ratio of the total sum of squares \\(SS_{\\sf tot}\\) to the residual sum of squares \\(SS_{\\sf res}\\): SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\; \\sf eq. 4 SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2\\;\\;\\;\\;\\; \\sf eq. 5 R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}} \\;\\;\\;\\;\\;\\; \\sf eq. 6 In eq. 4, \\(\\bar{y}=\\sum_i y^{\\sf exact}_i/N\\) is the average value of y for \\(N\\) points. The best value of \\(R^2\\) is 1 but it can also take a negative value if the error is large. See all the different regression metrics here . \ud83d\ude4b Question 4: lets understand \\(R^2\\) \u00b6 Do we need a large value of \\(SS_{\\sf tot}\\) to minimize \\(R^2\\) - is this something which we have the power to control? # Print the coefficient of determination - 1 is perfect prediction print ( 'Coefficient of determination: %.2f ' % r2_score ( y , y_pred )) Coefficient of determination: 0.45 \ud83c\udf52 1.3.1.3 Enrichment : Residual Standard Error \u00b6 the residual standard error (RSE) is an estimate of the standard deviation of the irreducible error (described more in Model Selection and Validation ) RSE = \\sqrt{\\frac{1}{n-2}RSS} In effect, the RSE tells us that even if we were to find the true values of \\(b\\) and \\(m\\) in our linear model, this is by how much our estimates of \\(y\\) based on \\(x\\) would be off. \ud83c\udf52 1.3.2 Enrichment : Assessing the accuracy of the coefficient estimates \u00b6 1.3.2.1 Standard errors \u00b6 The difference between our sample and the population data is often a question in statistical learning (covered more in session Model Selection and Validation ). As an estimate on how deviant our sample mean may be from the true population mean, we can calculate he standard error of the sample mean: SE(\\hat{\\mu})^2 = \\frac{\\sigma^2}{N} where \\(\\sigma\\) is the standard deviation of each of the realizations of \\(y_i\\) with \\(Y\\). We can similarly calculate standard errors for our estimation coefficients: SE(\\hat{b})^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}}] SE(\\hat{m})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}} In practice, \\(\\sigma\\) is estimated as the residual standard error : RSE = \\sqrt{RSS/(n-2)} We might observe: \\(SE(\\hat{b})^2\\) would equal \\(SE(\\hat{\\mu})^2\\) if \\(\\bar{x}\\) were 0 (and \\(\\hat{b}\\) would equal \\(\\bar{y}\\)) and the error associated with \\(m\\) and \\(b\\) would be smaller with the more spread we have in \\(x_i\\) (intuitively a spread in \\(x_i\\) gives us more leverage) 1.3.2.2 Confidence intervals \u00b6 These standard errors can be used to compute confidence intervals . A 95% confidence interval is defined as a range a values that with a 95% probability contain the true value of the predicted parameter. \\hat{b} \\space \u00b1 \\space 2 SE(\\hat{b}) and \\hat{m} \\space \u00b1 \\space 2 SE(\\hat{m}) # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) n = y . shape [ 0 ] print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) b: -6.06e+02 m: 6.16e+02 n: 1597 x_bar: 9.97e-01 RSE: 1.67e-02 SE_b: 2.21e-01 SE_m: 2.22e-01 The confidence interval around the regression line takes on a similar form SE(\\hat{y})^2 = (\\frac{\\sum_{i=1}^n{(y_i - \\hat{y})^2}} {n-2}) (\\frac{1}{n} + \\frac{(x-\\bar{x})^2} {\\sum_{i=1}^n{(x_i - \\bar{x})^2}}) and then \\(\\hat{y}\\) can be described as \\hat{y}_h \u00b1 t_{\\alpha/2,n-2} SE(\\hat{y}) where \\(t\\) is the calculated critical t-value at the corresponding confidence interval and degrees of freedom (we can obtain this from scipy) def calc_SE_y ( x , y , m , b ): y_hat = m * x + b x_bar = np . mean ( x ) n = x . shape [ 0 ] return np . sqrt (( np . sum (( y - y_hat ) ** 2 )) / ( n - 2 )) \\ * np . sqrt (( 1 / n ) + (( x - x_bar ) ** 2 / ( np . sum (( x - x_bar ) ** 2 )))) x_line = np . linspace ( min ( x ), max ( x ), 100 ) y_line = m * x_line + b fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x_line , y_line , ls = '--' ) ax . plot ( x , y , ls = '' , marker = '.' ) SE_y = calc_SE_y ( x , y , m , b ) y_line = m * x + b ct = scipy . stats . t . ppf ( q = ( 0.975 ), df = ( n - 2 )) ### upper CI y_up = y_line + ct * SE_y ax . plot ( x , y_up , ls = '-.' , marker = '' , color = 'tab:green' , lw = .1 ) ### lower CI y_down = y_line - ct * SE_y ax . plot ( x , y_down , ls = '-.' , marker = '' , color = 'tab:green' , lw = .1 ) ax . set_ylabel ( 'acidity' ) ax . set_xlabel ( 'density' ) Text(0.5, 0, 'density') 1.3.2.3 Hypothesis testing \u00b6 standard errors can also be used to perform hypothesis tests on the coefficients. The most common hypothesis test involves testing the null hypothesis of \\(H_0:\\) There is no relationship between X and Y versus the alternative hypothesis \\(H_\\alpha:\\) There is some relationship between X and Y This is equivalent to H_0: m = 0 and H_\\alpha: m \u2260 0 since if \\(m = 0\\) there is no relationship between X and Y. We need to assess if our \\(m\\) is sufficiently far from 0. Intuitively, this would also require us to consider the error \\(SE(\\hat{m})\\). In practice we compute a t-statistic t = \\frac{\\hat{m}-0}{SE(\\hat{m})} which measures the number of standard deviations \\(m\\) is from 0. Associated with this statistic, we determine a p-value (a \"probability\"-value) that tells us the probability of observing any value equal to \\(|t|\\) or larger, assuming the null hypothesis of \\(m=0\\). Hence, if we determine a small p-value, we can conclude that it is unlikely that there is no relationship between X and Y. t = m / SE_m print ( t ) 2777.3933859895524 scipy . stats . t . sf ( x = t , df = n - 2 ) 0.0 We see that with this data it is very unlikely that there is no relationship between X and Y! 1.3.3 Corollaries with classification models \u00b6 For classification tasks, we typically assess accuracy vs MSE or R-square, since we are dealing with categorical rather than numerical predictions. What is accuracy? It is defined as the ratio of True assignments to all assignments. For a binary positive/negative classification task this can be written as the following: Acc = \\frac{T_p + T_n}{F_p + F_n + T_p + T_n} Where \\(T\\) is True, \\(F\\) is false, \\(p\\) is positive, \\(n\\) is negative Just as a quick example, we can perform this type of task on our wine dataset by predicting on quality, which is a discrete 3-9 quality score: y_train = df [ 'type' ] . values . reshape ( - 1 , 1 ) x_train = df [ 'quality' ] . values . reshape ( - 1 , 1 ) # train a logistic regression model on the training set from sklearn.linear_model import LogisticRegression # instantiate model logreg = LogisticRegression () # fit model logreg . fit ( x_train , y_train ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) LogisticRegression() # make class predictions for the testing set y_pred_class = logreg . predict ( x_train ) # calculate accuracy from sklearn import metrics print ( metrics . accuracy_score ( y_train , y_pred_class )) 0.7538864091118977 \ud83d\ude4b Question 5: What is variance vs total sum of squares vs standard deviation? \u00b6 1.3.4 Beyond a single input feature \u00b6 ( also: quick appreciative beat for folding in domain area expertise into our models and features ) The acidity of the wine (the dependent variable v) could depend on: potassium from the soil (increases alkalinity) unripe grapes (increases acidity) grapes grown in colder climates or reduced sunshine create less sugar (increases acidity) preprocessing such as adding tartaric acid to the grape juice before fermentation (increases acidity) malolactic fermentation (reduces acidity) + others So in our lab today we will look at folding in additional variables in our dataset into the model 1.4 Multivariate regression \u00b6 Let's now turn our attention to wine quality. The value we aim to predict or evaluate is the quality of each wine in our dataset. This is our dependent variable. We will look at how this is related to the 12 other independent variables, also known as input features . We're going to do this with only the red wine data red . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 4898 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 4899 red 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5 4900 red 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5 4901 red 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6 4902 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 1.4.1 Linear regression with all input fields \u00b6 For this example, notice we have a categorical data variable in the 'type' column. We will ignore this for now, and only work with our red wines. In the future we will discuss how to deal with categorical variable such as this in a mathematical representation. # this is a list of all our features or independent variables features = list ( red . columns [ 1 :]) # we're going to remove our target or dependent variable, density from this # list features . remove ( 'density' ) # now we define X and y according to these lists of names X = red . dropna ( axis = 0 )[ features ] . values y = red . dropna ( axis = 0 )[ 'density' ] . values # we will talk about scaling/centering our data at a later time X = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) red . isnull () . sum ( axis = 0 ) # we are getting rid of some nasty nulls! type 0 fixed acidity 2 volatile acidity 1 citric acid 1 residual sugar 0 chlorides 0 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 2 sulphates 2 alcohol 0 quality 0 dtype: int64 # Create linear regression object - note that we are using all the input features model = linear_model . LinearRegression () model . fit ( X , y ) y_hat = model . predict ( X ) Let's see what the coefficients look like ... print ( \"Fit coefficients: \\n \" , model . coef_ , \" \\n Number of coefficients:\" , len ( model . coef_ )) Fit coefficients: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] Number of coefficients: 11 We have 11 !!! That's because we are regressing respect to all 11 independent variables !!! So now, y_{\\sf calc}= m_1x_1 +\\, m_2x_2 \\,+ \\,m_3x_3 \\,+\\,... \\,+ \\,b =\\sum_{i=1}^{13}m_i x_i + b\\;\\;\\;\\;\\; \\sf eq. 7 print ( \"We have 11 slopes / weights: \\n\\n \" , model . coef_ ) print ( \" \\n And one intercept: \" , model . intercept_ ) We have 11 slopes / weights: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] And one intercept: 0.9967517451349656 # This size should match the number of columns in X if len ( X [ 0 ]) == len ( model . coef_ ): print ( \"All good! The number of coefficients matches the number of input features.\" ) else : print ( \"Hmm .. something strange is going on.\" ) All good! The number of coefficients matches the number of input features. \ud83c\udfcb\ufe0f Exercise 4: evaluate the error \u00b6 Let's evaluate the error by computing the MSE and \\(R^2\\) metrics (see eq. 3 and 6). # The mean squared error # part A # calculate the MSE using mean_squared_error() # mse = # part B # calculate the R square using r2_score() # r2 = # print('Mean squared error: {:.2f}'.format(mse)) # print('Coefficient of determination: {:.2f}'.format(r2)) \ud83c\udfcb\ufe0f Exercise 5: make a plot of y actual vs y predicted \u00b6 We can also look at how well the computed values match the true values graphically by generating a scatterplot. # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style # plt.plot() plt . title ( \"Linear regression - computed values on entire data set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () \ud83c\udf52 1.4.2 Enrichment : Splitting into train and test sets \u00b6 note: more of this topic is covered in Model Selection and Validation To see whether we can predict, we will carry out our regression only on a part, 80%, of the full data set. This part is called the training data. We will then test the trained model to predict the rest of the data, 20% - the test data. The function which fits won't see the test data until it has to predict it. We start by splitting out data using scikit-learn's train_test_split() function: X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 42 ) Now we check the size of y_train and y_test , the sum should be the size of y! If this works then we move on and carry out regression but we only use the training data! if len ( y_test ) + len ( y_train ) == len ( y ): print ( 'All good, ready to to go and regress! \\n ' ) # Carry out linear regression print ( 'Running linear regression algorithm on the training set \\n ' ) model = linear_model . LinearRegression () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_pred_test = model . predict ( X_test ) All good, ready to to go and regress! Running linear regression algorithm on the training set Fit coefficients and intercept: [ 1.62385613e-03 1.10578142e-04 7.75216492e-07 5.87755741e-04 7.65190323e-05 -1.03490059e-04 8.87357873e-05 7.79083342e-04 2.23534769e-04 -9.99858829e-04 5.85256438e-06] 0.9967531628434799 Now we can plot our predicted values to see how accurate we are in predicting. We will generate a scatterplot and computing the MSE and \\(R^2\\) metrics of error. sns . scatterplot ( x = y_pred_test , y = y_test , color = \"mediumvioletred\" , s = 50 ) plt . title ( \"Linear regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2e ' % mean_squared_error ( y_test , y_pred_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_pred_test )) Mean squared error: 5.45e-07 Coefficient of determination: 0.87 \ud83c\udf52 1.4.3 Enrichment : Some important questions \u00b6 is at least one of the predictors useful in estimating y? We'll see related topics in Inferential Statistics Do we need all the predictors or only some of them? We'll see related topics in Feature Engineering How do we assess goodness of fit? How accurate is any given prediction? 1.4.3.1 Is there a relationship between X and y? \u00b6 This question is similar to the hypothesis test we had as an enrichment topic in simple linear regression. To be formulaic: H_0 : \\beta_1 = \\beta_2 = ... = \\beta_p = 0 H_\\alpha : at \\space least \\space one \\space \\beta_j \\space is \\space non-zero To answer this question we perform an F-statistic F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)} where \\(p\\) is the number of predictors. With this F-statistic, we are expecting that if there is no relationship between the predictors and target variable the ratio would converge to 1, since both numerator and denominator would evaluate to simply the variance of the data. On the other hand, if there is a relationship the numerator (the variance captured by the model over the number of predictors) should evaluate to something greater than the variance of the data. Now you might ask, why can't single out every predictor and perform the hypothesis testing we saw before?! Well, by sheer chance, we may incorrectly throw out the null hypothesis. Take an extreme example, if we have 100 predictors, with a confidence level of 0.05 we would by chance alone accept 5 of those 100 predictors, even if they have no relationship to Y. This is where the F-statistic comes in handy (it is worth noting however, that even the F-statistic can't handle \\(p\\) > \\(n\\) for which there is no solution!) y_bar = np . mean ( y ) y_hat = model . predict ( X ) RSS = np . sum (( y - y_hat ) ** 2 ) TSS = np . sum (( y - y_bar ) ** 2 ) p = X . shape [ 1 ] n = X . shape [ 0 ] F = ( TSS - RSS ) / p / ( RSS / ( n - p - 1 )) print ( f \" { F : .2f } \" ) 767.86 # http://pytolearn.csd.auth.gr/d1-hyptest/11/f-distro.html # dfn = degrees of freedom in the numerator # dfd = degrees of freedom in the denominator scipy . stats . f . sf ( x = t , dfn = p , dfd = ( n - p - 1 )) 0.0 with this result we can be fairly certain 1 or more of the prdictors is related to the target variable! 1.4.2.2 What are the important variables? \u00b6 the process of determining which of the variables are associated with y is known as variable selection . One approach, is to iteratively select every combination of variables, perform the regression, and use some metric of goodness of fit to determine which subset of p is the best. However this is \\(2^p\\) combinations to try, which quickly blows up. Instead we typically perform one of three approaches Forward selection start with an intercept model. Add a single variable, keep the one that results in the lowest RSS. Repeat. Can always be used no matter how large p is. This is also a greedy approach. Backward selection start with all predictors in the model. Remove the predictor with the largest p-value. Stop when all p-values are below some threshold. Can't be used if p > n. Mixed selection a combination of 1 and 2. start with an intercept model. continue as in 1 accept now, if any predictor p-value rises above some threshold, remove that predictor. This approach counteracts some of the greediness of approach 1. 1.4.2.3 How do we assess goodness of fit? \u00b6 Two of the most common measures of goodness of fit are \\(RSE\\) and \\(R^2\\). As it turns out, however, \\(R^2\\) will always increase with more predictors . The \\(RSE\\) defined for multivariate models accounts for the additional p: RSE = \\sqrt{\\frac{1}{n-p-1} RSS} and thus can be used in tandem with \\(R^2\\) to assess whether the additional parameter is worth it or not. Lastly, it is worth noting that visual approaches can lend a hand where simple test values can not (think Anscombe's quartet). 1.4.2.4 How accurate is any given prediction? \u00b6 We used confidence intervals to quantify the uncertainty in the estimate for \\(f(X)\\). To assess the uncertainty around a particular prediction we use prediction intervals . A prediction interval will always be wider than a confidence interval because it incorporates both the uncertainty in the regression estimate ( reducible error or sampling error) and uncertainty in how much an individual point will deviate from the solution surface (the irreducible error ). Put another way, with the confidence interval we expect the average response of y to fall within the calculated interval 95% of the time, however we sample the population data. Whereas we expect an individual response, y, to fall within the calculated prediction interval 95% of the time, however we sample the population data. 1.5 Major assumptions of linear regression \u00b6 There are two major assumptions of linear regression: that the predictors are additive, and that the relationships between X and y are linear. We can combat these assumptions, however, through feature engineering, a topic we discuss later on but briefly we will mention here. In the additive case, predictors that are suspected to have some interaction affect can be combined to produce a new feature, i.e. \\(X_{new} = X_2*X_2\\). On the linearity case, we can again engineer a new feature, i.e. \\(X_{new} = X_1^2\\). \ud83c\udf52 1.6 Enrichment : Potential problems with linear regression \u00b6 Non-linearity of the response-predictor relationships. plot the residuals (observe there is no pattern) Correlation of error terms. standard errors are predicated on no correlation of error terms. Extreme example: we double our data, n is now twice as large, and our confidence intervals narrower by a factor of \\(\\sqrt(2)\\)! a frequent problem in time series data plot residuals as a function of time (observe there is no pattern) Non-constant variance of error terms (heteroscedasticity). standard errors, hypothesis tests, and confidence intervals rely on this assumption a funnel shape in the residuals indicates heteroscedasticity can transform, \\(\\log{y}\\) Outliers. unusual values in response variable check residual plot potentially remove datapoint High-leverage points. unusual values in predictor variable will bias the least squares line problem can be hard to observe in multilinear regression (when a combination of predictors accounts for the leverage) Collinearity. refers to when two or more variables are closely related to one another reduces the accuracy of esimates for coefficients increases standard error of estimates for coefficients. it then reduces the t-statistic and may result in type-2 error, false negative. This means it reduces the power of the hypothesis test, the probability of correctly detecting a non-zero coefficient. look at the correlation matrix of variables for multicolinearity compute the variance inflation factor (VIF) \\(1/(1-R^2)\\) where the regression is performed against the indicated predictor across all other predictors remove features with a VIF above 5-10 We will discuss many of these in the session on Feature Engineering \ud83c\udf52 1.7 Enrichment : Other regression algorithms \u00b6 There are many other regression algorithms the two we want to highlight here are Ridge, LASSO, and Elastic Net. They differ by an added term to the loss function. Let's review. Eq. 2 expanded to multivariate form yields: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 for Ridge regression, we add a regularization term known as L2 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}\\beta_{j}^2 for LASSO (Least Absolute Shrinkage and Selection Operator) we add L1 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}|\\beta_{j}| The key difference here is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. Elastic Net is a combination of these two regularization methods. model = linear_model . Ridge () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_hat_test = model . predict ( X_test ) Fit coefficients and intercept: [ 1.61930554e-03 1.11227142e-04 2.64709094e-06 5.87271456e-04 7.58510569e-05 -1.02851782e-04 8.76686650e-05 7.75641517e-04 2.23315063e-04 -9.98653815e-04 5.26839010e-06] 0.9967531358810221 sns . scatterplot ( x = y_hat_test , y = y_test , color = \"lightseagreen\" , s = 50 ) plt . title ( \"Ridge regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2f ' % mean_squared_error ( y_test , y_hat_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_hat_test )) Mean squared error: 0.00 Coefficient of determination: 0.87 \ud83c\udfcb\ufe0f Exercise 6: Tune Hyperparameter for Ridge Regression \u00b6 Use the docstring to peak into the hyperparameters for Ridge Regression. What is the optimal value of lambda? Plot the \\(\\beta\\) values vs \\(\\lambda\\) from the results of your analysis # cell for exercise 3 out_lambdas = [] out_coefs = [] out_scores = [] for i in range ( 10 ): lambdas = [] coefs = [] scores = [] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) for lamb in range ( 1 , int ( 5e3 ), 20 ): model = linear_model . Ridge ( alpha = lamb ) model . fit ( X_train , y_train ) lambdas . append ( lamb ) coefs . append ( model . coef_ ) scores . append ( r2_score ( y_test , model . predict ( X_test ))) # print('MSE: %.4f' % mean_squared_error(y_test, model.predict(X_test))) # print('R2: %.4f' % r2_score(y_test, model.predict(X_test))) out_lambdas . append ( lambdas ) out_coefs . append ( coefs ) out_scores . append ( scores ) coef_means = np . array ( out_coefs ) . mean ( axis = 0 ) coef_stds = np . array ( out_coefs ) . std ( axis = 0 ) results_means = pd . DataFrame ( coef_means , columns = features ) results_stds = pd . DataFrame ( coef_stds , columns = features ) results_means [ 'lambda' ] = [ i for i in lambdas ] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) for feat in features : ax . errorbar ([ i for i in lambdas ], results_means [ feat ], yerr = results_stds [ feat ], label = feat ) # results.plot('lambda', 'scores', ax=ax[1]) ax . legend () <matplotlib.legend.Legend at 0x7fdd72270fd0> results = pd . DataFrame ( coefs , columns = features ) results [ 'lambda' ] = [ i for i in lambdas ] results [ 'scores' ] = scores fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) for feat in features : results . plot ( 'lambda' , feat , ax = ax [ 0 ]) results . plot ( 'lambda' , 'scores' , ax = ax [ 1 ]) <AxesSubplot:xlabel='lambda'> \ud83c\udf52 1.8 Enrichment : Additional Regression Exercises \u00b6 Problem 1) Number and choice of input features \u00b6 Load the red wine dataset and evaluate how the linear regression predictions changes as you change the number and choice of input features . The total number of columns in X is 11 and each column represents a specific input feature. Estimate the MSE print ( X_train . shape ) (1274, 11) If you want to use the first 5 features you could proceed as following: X_train_five = X_train [:, 0 : 5 ] X_test_five = X_test [:, 0 : 5 ] Check that the new variables have the shape your expect print ( X_train_five . shape ) print ( X_test_five . shape ) (1274, 5) (319, 5) Now you can use these to train your linear regression model and repeat for different numbers or sets of input features! Note that you do not need to change the output feature! It's size is independent from the number of input features, yet recall that its length is the same as the number of values per input feature. Questions to think about while you work on this problem - How many input feature variables does one need? Is there a maximum or minimum number? - Could one input feature variable be better than the rest? - What if values are missing for one of the input feature variables - is it still worth using it? - Can you use L1 or L2 to determine these optimum features more quickly? Problem 2) Type of regression algorithm \u00b6 Try using other types of linear regression methods on the wine dataset: the LASSO model and the Elastic net model which are described by the sklearn.linear_model.ElasticNet() sklearn.linear_model.Lasso() scikit-learn functions. For more detail see ElasticNet and Lasso . Questions to think about while you work on this problem - How does the error change with each model? - Which model seems to perform best? - How can you optimize the hyperparameter, \\(\\lambda\\) - Does one model do better than the other at determining which input features are more important? - How about non linear regression / what if the data does not follow a line? - How do the bias and variance change for each model from sklearn.linear_model import ElasticNet from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.linear_model import LinearRegression for model in [ ElasticNet , Lasso , Ridge , LinearRegression ]: model = model () model . fit ( X_train , y_train ) print ( str ( model )) print ( 'Mean squared error: %.ef' % mean_squared_error ( y_test , model . predict ( X_test ))) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , model . predict ( X_test ))) print () ElasticNet() Mean squared error: 4e-06f Coefficient of determination: -0.00 Lasso() Mean squared error: 4e-06f Coefficient of determination: -0.00 Ridge() Mean squared error: 7e-07f Coefficient of determination: 0.83 LinearRegression() Mean squared error: 7e-07f Coefficient of determination: 0.83 References \u00b6 Linear Regression To find out more see simple linear regression scikit-learn Scikit-learn Linear regression in scikit-learn Metrics of error The Boston dataset Pearson correlation To find out more see pearson Irreducible error, bias and variance Great Coursera videos here and here","title":"Regression and Analysis"},{"location":"S1_Regression_and_Analysis/#data-science-foundations-session-1-regression-and-analysis","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at fitting data to a curve using regression . We will also look at using regression to make predictions for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error.","title":"Data Science Foundations  Session 1: Regression and Analysis"},{"location":"S1_Regression_and_Analysis/#10-preparing-environment-and-importing-data","text":"back to top","title":"1.0 Preparing Environment and Importing Data"},{"location":"S1_Regression_and_Analysis/#101-import-packages","text":"back to top # Import pandas, pyplot, ipywidgets import pandas as pd from matplotlib import pyplot as plt from ipywidgets import interact # Import Scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score # for enrichment topics import seaborn as sns import numpy as np import scipy","title":"1.0.1 Import Packages"},{"location":"S1_Regression_and_Analysis/#102-load-dataset","text":"back to top For our discussion on regression and descriptive statistics today we will use a well known dataset of different wines and their quality ratings df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) df . shape (6497, 13) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6","title":"1.0.2 Load Dataset"},{"location":"S1_Regression_and_Analysis/#11-what-is-regression","text":"It is the process of finding a relationship between dependent and independent variables to find trends in data. This abstract definition means that you have one variable (the dependent variable) which depends on one or more variables (the independent variables). One of the reasons for which we want to regress data is to understand whether there is a trend between two variables. Housing Prices Example We can imagine this scenario with housing prices. Envision a mixed dataset of continuous and discrete independent variables. Some features could be continuous, floating point values like location ranking and housing condition. Others could be discrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a regression model.","title":"1.1 What is regression?"},{"location":"S1_Regression_and_Analysis/#12-linear-regression-fitting-with-scikit-learn","text":"","title":"1.2  Linear regression fitting with scikit-learn"},{"location":"S1_Regression_and_Analysis/#exercise-1-rudimentary-eda","text":"What does the data look like? Recall how to visualize data in a pandas dataframe for every column calculate the: skew: df.skew() kurtosis: df.kurtosis() pearsons correlation with the dependent variable: df.corr() number of missing entries df.isnull() and organize this into a new dataframe note: pearsons is just one type of correlation, another correlation available to us is spearman which differs from pearsons in that it depends on ranked values rather than their direct quantities, you can read more here df . isnull () . sum () type 0 fixed acidity 10 volatile acidity 8 citric acid 3 residual sugar 2 chlorides 2 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 9 sulphates 4 alcohol 0 quality 0 dtype: int64 # Cell for Exercise 1 # part A # using df.<method> define the following four variables with the results from # skew(), kurtosis(), corr() (while selecting for quality), and isnull() # for isnull() you'll notice the return is a dataframe of booleans. we would # like to simply know the number of null values for each column. change the # return of isnull() using the sum() method along the columns # skew = # kurt = # pear = # null = # part B # on line 13, put these results in a list using square brackets and call # pd.DataFrame on the list to make your new DataFrame! store it under the # variable name dff # part C # take the transpose of this DataFrame using dff.T. reassign dff to this copy # part D # set the column names to 'skew', 'kurtosis', 'pearsons _quality', and # 'null count' using dff.columns # Now return dff to the output to view your hand work # dff # uncomment this line I have gone ahead and repeated this exercise with the red vs white wine types: red = df . loc [ df [ 'type' ] == 'red' ] wht = df . loc [ df [ 'type' ] == 'white' ] def get_summary ( df ): skew = df . skew () kurt = df . kurtosis () pear = df . corr ()[ 'quality' ] null = df . isnull () . sum () med = df . median () men = df . mean () dff = pd . DataFrame ([ skew , kurt , pear , null , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'pearsons _quality' , 'null count' , 'median' , 'mean' ] return dff dffr = get_summary ( red ) dffw = get_summary ( wht ) desc = pd . concat ([ dffr , dffw ], keys = [ 'red' , 'white' ]) /tmp/ipykernel_277/2387423026.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. skew = df.skew() /tmp/ipykernel_277/2387423026.py:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. kurt = df.kurtosis() /tmp/ipykernel_277/2387423026.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. med = df.median() /tmp/ipykernel_277/2387423026.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. men = df.mean() desc .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count median mean red fixed acidity 0.982192 1.132624 0.123834 2.0 7.90000 8.322104 volatile acidity 0.672862 1.226846 -0.390858 1.0 0.52000 0.527738 citric acid 0.317891 -0.788476 0.226917 1.0 0.26000 0.271145 residual sugar 4.540655 28.617595 0.013732 0.0 2.20000 2.538806 chlorides 5.680347 41.715787 -0.128907 0.0 0.07900 0.087467 free sulfur dioxide 1.250567 2.023562 -0.050656 0.0 14.00000 15.874922 total sulfur dioxide 1.515531 3.809824 -0.185100 0.0 38.00000 46.467792 density 0.071288 0.934079 -0.174919 0.0 0.99675 0.996747 pH 0.194803 0.814690 -0.057094 2.0 3.31000 3.310864 sulphates 2.429115 11.712632 0.251685 2.0 0.62000 0.658078 alcohol 0.860829 0.200029 0.476166 0.0 10.20000 10.422983 quality 0.217802 0.296708 1.000000 0.0 6.00000 5.636023 type NaN NaN NaN 0.0 NaN NaN white fixed acidity 0.647981 2.176560 -0.114032 8.0 6.80000 6.855532 volatile acidity 1.578595 5.095526 -0.194976 7.0 0.26000 0.278252 citric acid 1.284217 6.182036 -0.009194 2.0 0.32000 0.334250 residual sugar 1.076601 3.469536 -0.097373 2.0 5.20000 6.393250 chlorides 5.023412 37.560847 -0.210181 2.0 0.04300 0.045778 free sulfur dioxide 1.406745 11.466342 0.008158 0.0 34.00000 35.308085 total sulfur dioxide 0.390710 0.571853 -0.174737 0.0 134.00000 138.360657 density 0.977773 9.793807 -0.307123 0.0 0.99374 0.994027 pH 0.458402 0.532552 0.098858 7.0 3.18000 3.188203 sulphates 0.977361 1.589847 0.053690 2.0 0.47000 0.489835 alcohol 0.487342 -0.698425 0.435575 0.0 10.40000 10.514267 quality 0.155796 0.216526 1.000000 0.0 6.00000 5.877909 type NaN NaN NaN 0.0 NaN NaN def my_fig ( metric = desc . columns ): fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) pd . DataFrame ( desc [ metric ]) . unstack ()[ metric ] . T . plot ( kind = 'barh' , ax = ax ) # interact(my_fig)","title":"\ud83c\udfcb\ufe0f Exercise 1: rudimentary EDA"},{"location":"S1_Regression_and_Analysis/#question-1-discussion-around-eda-plot","text":"What do we think of this plot? metric = mean , the cholrides values metric = kurtosis , residual sugar metric = pearsons _quality , magnitudes and directions How to improve the plot, what other plots would we like to see? For instance, what if we were really curious about the high kurtosis for chlorides content? What more would we like to glean about the distribution of chloride content? # we can use df.describe() to take a look at the quantile values and min/max df [ 'chlorides' ] . describe () count 6495.000000 mean 0.056042 std 0.035036 min 0.009000 25% 0.038000 50% 0.047000 75% 0.065000 max 0.611000 Name: chlorides, dtype: float64 # and see how these values appear in a KDE fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) df [ 'chlorides' ] . plot ( kind = 'kde' , ax = ax ) ax . set_xlim ( 0 , .61 ) (0.0, 0.61) # lastly we may want to look at the raw values themselves. We can sort them # too view outliers df [ 'chlorides' ] . sort_values ( ascending = False )[: 50 ] 5156 0.611 5049 0.610 5004 0.467 4979 0.464 5590 0.422 6268 0.415 6270 0.415 5652 0.415 6217 0.414 5949 0.414 5349 0.413 6158 0.403 4981 0.401 5628 0.387 6063 0.369 4915 0.368 5067 0.360 5179 0.358 484 0.346 5189 0.343 4917 0.341 5124 0.337 4940 0.332 1217 0.301 687 0.290 4473 0.271 5079 0.270 6272 0.267 5138 0.263 1865 0.255 5466 0.250 1034 0.244 5674 0.243 5675 0.241 683 0.240 1638 0.239 5045 0.236 6456 0.235 6468 0.230 5465 0.226 5464 0.226 5564 0.222 2186 0.217 5996 0.216 6333 0.214 5206 0.214 6332 0.214 5205 0.213 4497 0.212 1835 0.211 Name: chlorides, dtype: float64","title":"\ud83d\ude4b Question 1: Discussion Around EDA Plot"},{"location":"S1_Regression_and_Analysis/#122-visualizing-the-data-set-motivating-regression-analysis","text":"In order to demonstrate simple linear regression with this dataset we will look at two particular features: fixed acidity and density . We can create a scatter plot of fixed acidity vs density for the red wine in the dataset using df.plot() and see that there appears to be a general trend between the two features: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) df . loc [ df [ 'type' ] == 'red' ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> Now the question is: How do we quantify this trend?","title":"1.2.2 Visualizing the data set - motivating regression analysis"},{"location":"S1_Regression_and_Analysis/#123-estimating-the-regression-coefficients","text":"It looks like density increases with fixed acidity following a line, maybe something like y(x)= m \\cdot x + b \\;\\;\\;\\;\\;\\;\\;\\; \\sf{eq. 1} with \\( y=\\sf density \\), \\(x=\\sf fixed \\space acidity\\), and \\(m\\) the slope and \\(b\\) the intercept. To solve the problem, we need to find the values of \\(b\\) and \\(m\\) in equation 1 to best fit the data. This is called linear regression . In linear regression our goal is to minimize the error between computed values of positions \\(y^{\\sf calc}(x_i)\\equiv y^{\\sf calc}_i\\) and known values \\(y^{\\sf exact}(x_i)\\equiv y^{\\sf exact}_i\\). In Ordinary Least Squares the error term we try to minimize is called the residual sum of squares . We find \\(b\\) and \\(m\\) which lead to lowest value of \\epsilon (m,b) =SS_{\\sf res}=\\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - m\\cdot x_i - b \\right)^2\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sf{eq. 2} To find out more see e.g. https://en.wikipedia.org/wiki/Simple_linear_regression Using some calculus, we can solve for \\(b\\) and \\(m\\) analytically: m = \\frac{\\sum_{i=1}^n{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}} b = \\bar{y} - m\\bar{x} Note that for multivariate regression, the coefficients can be solved analytically as well \ud83d\ude0e","title":"1.2.3 Estimating the regression coefficients"},{"location":"S1_Regression_and_Analysis/#question-2-linear-regression-loss-function","text":"Do we always want m and b to be large positive numbers so as to minimize eq. 2? Luckily scikit-learn contains many functions related to regression including linear regression . The function we will use is called LinearRegression() . # Create linear regression object model = linear_model . LinearRegression () # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red [ 'density' ] . values . reshape ( - 1 , 1 ) y = red [ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) print ( red [ 'density' ] . values . shape , red [ 'fixed acidity' ] . values . shape ) print ( x . shape , y . shape ) (1599,) (1599,) (1599, 1) (1599, 1) What happens when we try to fit the data as is? # Fit to the data # model.fit(x, y)","title":"\ud83d\ude4b Question 2: linear regression loss function"},{"location":"S1_Regression_and_Analysis/#exercise-2-drop-null-values-and-practice-pandas-operations","text":"Let's look back at our dataset description dataframe above, what do we notice, what contains null values? There are several strategies for dealing with null values. For now let's take the simplest case, and drop rows in our dataframe that contain null # Cell for Exercise 2 # For this templated exercise you are going to complete everything in one line # of code, but we are going to break it up into steps. So for each part (A, B, # etc.) paste your answer from the previous part to begin (your opertaions will # read from left to right) # step A # select the 'density' and 'fixed acidity' columns of red. make sure the return # is a dataframe # step B # now use the dropna() method on axis 0 (the rows) to drop any null values # step B # select column 'density' # step C # select the values # step D # reshape the result with an empty second dimension using .reshape() and store # the result under variable x # repeat the same process with 'fixed acidity' and variable y Now that we have our x and y arrays we can fit using ScikitLearn x = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values . reshape ( - 1 , 1 ) y = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'fixed acidity' ] . values . reshape ( - 1 , 1 )","title":"\ud83c\udfcb\ufe0f Exercise 2: drop Null Values (and practice pandas operations)"},{"location":"S1_Regression_and_Analysis/#question-3-why-do-we-drop-null-values-across-both-columns","text":"Notice in the above cell how we selected both density and fixed acidity before calling dropna ? Why did we do that? Why didn't we just select density in the x variable case and fixed acidity in the y variable case? # Fit to the data model . fit ( x , y ) # Extract the values of interest m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # Print the slope m and intercept b print ( 'Scikit learn - Slope: ' , m , 'Intercept: ' , b ) Scikit learn - Slope: 616.01314280661 Intercept: -605.6880086750523","title":"\ud83d\ude4b Question 3: why do we drop null values across both columns?"},{"location":"S1_Regression_and_Analysis/#exercise-3-calculating-y_pred","text":"Estimate the values of \\(y\\) by using your fitted parameters. Hint: Use your model.coef_ and model.intercept_ parameters to estimate y_pred following equation 1 # define y_pred in terms of m, x, and b # y_pred = # uncomment the following lines! # fig, ax = plt.subplots(1,1, figsize=(10,10)) # ax.plot(x, y_pred, ls='', marker='*') # ax.plot(x, y, ls='', marker='.') We can also return predictions directly with the model object using the predict() method note: it is great to get in the habit of utilizing model outputs this way, as the API will be similar across all scikit-learn models (and sometimes models in other libraries as well!) # Another way to get this is using the model.predict function y_pred = model . predict ( x ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x , y_pred , ls = '' , marker = '*' ) ax . plot ( x , y , ls = '' , marker = '.' ) ax . set_ylabel ( 'acidity' ) ax . set_xlabel ( 'density' ) Text(0.5, 0, 'density')","title":"\ud83c\udfcb\ufe0f Exercise 3: calculating y_pred"},{"location":"S1_Regression_and_Analysis/#13-error-and-topics-of-model-fitting-assessing-model-accuracy","text":"","title":"1.3 Error and topics of model fitting (assessing model accuracy)"},{"location":"S1_Regression_and_Analysis/#131-measuring-the-quality-of-fit","text":"","title":"1.3.1 Measuring the quality of fit"},{"location":"S1_Regression_and_Analysis/#1311-mean-squared-error","text":"The plot in Section 1.2.3 looks good, but numerically what is our error? What is the mean value of $\\epsilon$, i.e. the Mean Squared Error (MSE) ? {\\sf MSE}=\\epsilon_{\\sf ave} = \\frac{\\sum_{i=1}^{N_{\\sf times}}\\left(y^{\\sf exact}_i - m\\cdot t_i - b \\right)^2}{N_{\\sf times}}\\;\\;\\;\\;\\;\\sf eq. 3 # The mean squared error print ( 'Mean squared error: %.2f ' % mean_squared_error ( y , y_pred )) Mean squared error: 1.68","title":"1.3.1.1 Mean Squared Error"},{"location":"S1_Regression_and_Analysis/#1312-r-square","text":"Another way to measure error is the regression score, \\(R^2\\). \\(R^2\\) is generally defined as the ratio of the total sum of squares \\(SS_{\\sf tot}\\) to the residual sum of squares \\(SS_{\\sf res}\\): SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\; \\sf eq. 4 SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2\\;\\;\\;\\;\\; \\sf eq. 5 R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}} \\;\\;\\;\\;\\;\\; \\sf eq. 6 In eq. 4, \\(\\bar{y}=\\sum_i y^{\\sf exact}_i/N\\) is the average value of y for \\(N\\) points. The best value of \\(R^2\\) is 1 but it can also take a negative value if the error is large. See all the different regression metrics here .","title":"1.3.1.2 R-square"},{"location":"S1_Regression_and_Analysis/#question-4-lets-understand-r2","text":"Do we need a large value of \\(SS_{\\sf tot}\\) to minimize \\(R^2\\) - is this something which we have the power to control? # Print the coefficient of determination - 1 is perfect prediction print ( 'Coefficient of determination: %.2f ' % r2_score ( y , y_pred )) Coefficient of determination: 0.45","title":"\ud83d\ude4b Question 4: lets understand \\(R^2\\)"},{"location":"S1_Regression_and_Analysis/#1313-enrichment-residual-standard-error","text":"the residual standard error (RSE) is an estimate of the standard deviation of the irreducible error (described more in Model Selection and Validation ) RSE = \\sqrt{\\frac{1}{n-2}RSS} In effect, the RSE tells us that even if we were to find the true values of \\(b\\) and \\(m\\) in our linear model, this is by how much our estimates of \\(y\\) based on \\(x\\) would be off.","title":"\ud83c\udf52 1.3.1.3 Enrichment: Residual Standard Error"},{"location":"S1_Regression_and_Analysis/#132-enrichment-assessing-the-accuracy-of-the-coefficient-estimates","text":"","title":"\ud83c\udf52 1.3.2 Enrichment: Assessing the accuracy of the coefficient estimates"},{"location":"S1_Regression_and_Analysis/#1321-standard-errors","text":"The difference between our sample and the population data is often a question in statistical learning (covered more in session Model Selection and Validation ). As an estimate on how deviant our sample mean may be from the true population mean, we can calculate he standard error of the sample mean: SE(\\hat{\\mu})^2 = \\frac{\\sigma^2}{N} where \\(\\sigma\\) is the standard deviation of each of the realizations of \\(y_i\\) with \\(Y\\). We can similarly calculate standard errors for our estimation coefficients: SE(\\hat{b})^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}}] SE(\\hat{m})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}} In practice, \\(\\sigma\\) is estimated as the residual standard error : RSE = \\sqrt{RSS/(n-2)} We might observe: \\(SE(\\hat{b})^2\\) would equal \\(SE(\\hat{\\mu})^2\\) if \\(\\bar{x}\\) were 0 (and \\(\\hat{b}\\) would equal \\(\\bar{y}\\)) and the error associated with \\(m\\) and \\(b\\) would be smaller with the more spread we have in \\(x_i\\) (intuitively a spread in \\(x_i\\) gives us more leverage)","title":"1.3.2.1 Standard errors"},{"location":"S1_Regression_and_Analysis/#1322-confidence-intervals","text":"These standard errors can be used to compute confidence intervals . A 95% confidence interval is defined as a range a values that with a 95% probability contain the true value of the predicted parameter. \\hat{b} \\space \u00b1 \\space 2 SE(\\hat{b}) and \\hat{m} \\space \u00b1 \\space 2 SE(\\hat{m}) # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) n = y . shape [ 0 ] print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) b: -6.06e+02 m: 6.16e+02 n: 1597 x_bar: 9.97e-01 RSE: 1.67e-02 SE_b: 2.21e-01 SE_m: 2.22e-01 The confidence interval around the regression line takes on a similar form SE(\\hat{y})^2 = (\\frac{\\sum_{i=1}^n{(y_i - \\hat{y})^2}} {n-2}) (\\frac{1}{n} + \\frac{(x-\\bar{x})^2} {\\sum_{i=1}^n{(x_i - \\bar{x})^2}}) and then \\(\\hat{y}\\) can be described as \\hat{y}_h \u00b1 t_{\\alpha/2,n-2} SE(\\hat{y}) where \\(t\\) is the calculated critical t-value at the corresponding confidence interval and degrees of freedom (we can obtain this from scipy) def calc_SE_y ( x , y , m , b ): y_hat = m * x + b x_bar = np . mean ( x ) n = x . shape [ 0 ] return np . sqrt (( np . sum (( y - y_hat ) ** 2 )) / ( n - 2 )) \\ * np . sqrt (( 1 / n ) + (( x - x_bar ) ** 2 / ( np . sum (( x - x_bar ) ** 2 )))) x_line = np . linspace ( min ( x ), max ( x ), 100 ) y_line = m * x_line + b fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x_line , y_line , ls = '--' ) ax . plot ( x , y , ls = '' , marker = '.' ) SE_y = calc_SE_y ( x , y , m , b ) y_line = m * x + b ct = scipy . stats . t . ppf ( q = ( 0.975 ), df = ( n - 2 )) ### upper CI y_up = y_line + ct * SE_y ax . plot ( x , y_up , ls = '-.' , marker = '' , color = 'tab:green' , lw = .1 ) ### lower CI y_down = y_line - ct * SE_y ax . plot ( x , y_down , ls = '-.' , marker = '' , color = 'tab:green' , lw = .1 ) ax . set_ylabel ( 'acidity' ) ax . set_xlabel ( 'density' ) Text(0.5, 0, 'density')","title":"1.3.2.2 Confidence intervals"},{"location":"S1_Regression_and_Analysis/#1323-hypothesis-testing","text":"standard errors can also be used to perform hypothesis tests on the coefficients. The most common hypothesis test involves testing the null hypothesis of \\(H_0:\\) There is no relationship between X and Y versus the alternative hypothesis \\(H_\\alpha:\\) There is some relationship between X and Y This is equivalent to H_0: m = 0 and H_\\alpha: m \u2260 0 since if \\(m = 0\\) there is no relationship between X and Y. We need to assess if our \\(m\\) is sufficiently far from 0. Intuitively, this would also require us to consider the error \\(SE(\\hat{m})\\). In practice we compute a t-statistic t = \\frac{\\hat{m}-0}{SE(\\hat{m})} which measures the number of standard deviations \\(m\\) is from 0. Associated with this statistic, we determine a p-value (a \"probability\"-value) that tells us the probability of observing any value equal to \\(|t|\\) or larger, assuming the null hypothesis of \\(m=0\\). Hence, if we determine a small p-value, we can conclude that it is unlikely that there is no relationship between X and Y. t = m / SE_m print ( t ) 2777.3933859895524 scipy . stats . t . sf ( x = t , df = n - 2 ) 0.0 We see that with this data it is very unlikely that there is no relationship between X and Y!","title":"1.3.2.3 Hypothesis testing"},{"location":"S1_Regression_and_Analysis/#133-corollaries-with-classification-models","text":"For classification tasks, we typically assess accuracy vs MSE or R-square, since we are dealing with categorical rather than numerical predictions. What is accuracy? It is defined as the ratio of True assignments to all assignments. For a binary positive/negative classification task this can be written as the following: Acc = \\frac{T_p + T_n}{F_p + F_n + T_p + T_n} Where \\(T\\) is True, \\(F\\) is false, \\(p\\) is positive, \\(n\\) is negative Just as a quick example, we can perform this type of task on our wine dataset by predicting on quality, which is a discrete 3-9 quality score: y_train = df [ 'type' ] . values . reshape ( - 1 , 1 ) x_train = df [ 'quality' ] . values . reshape ( - 1 , 1 ) # train a logistic regression model on the training set from sklearn.linear_model import LogisticRegression # instantiate model logreg = LogisticRegression () # fit model logreg . fit ( x_train , y_train ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) LogisticRegression() # make class predictions for the testing set y_pred_class = logreg . predict ( x_train ) # calculate accuracy from sklearn import metrics print ( metrics . accuracy_score ( y_train , y_pred_class )) 0.7538864091118977","title":"1.3.3 Corollaries with classification models"},{"location":"S1_Regression_and_Analysis/#question-5-what-is-variance-vs-total-sum-of-squares-vs-standard-deviation","text":"","title":"\ud83d\ude4b Question 5: What is variance vs total sum of squares vs standard deviation?"},{"location":"S1_Regression_and_Analysis/#134-beyond-a-single-input-feature","text":"( also: quick appreciative beat for folding in domain area expertise into our models and features ) The acidity of the wine (the dependent variable v) could depend on: potassium from the soil (increases alkalinity) unripe grapes (increases acidity) grapes grown in colder climates or reduced sunshine create less sugar (increases acidity) preprocessing such as adding tartaric acid to the grape juice before fermentation (increases acidity) malolactic fermentation (reduces acidity) + others So in our lab today we will look at folding in additional variables in our dataset into the model","title":"1.3.4 Beyond a single input feature"},{"location":"S1_Regression_and_Analysis/#14-multivariate-regression","text":"Let's now turn our attention to wine quality. The value we aim to predict or evaluate is the quality of each wine in our dataset. This is our dependent variable. We will look at how this is related to the 12 other independent variables, also known as input features . We're going to do this with only the red wine data red . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 4898 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 4899 red 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5 4900 red 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5 4901 red 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6 4902 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5","title":"1.4 Multivariate regression"},{"location":"S1_Regression_and_Analysis/#141-linear-regression-with-all-input-fields","text":"For this example, notice we have a categorical data variable in the 'type' column. We will ignore this for now, and only work with our red wines. In the future we will discuss how to deal with categorical variable such as this in a mathematical representation. # this is a list of all our features or independent variables features = list ( red . columns [ 1 :]) # we're going to remove our target or dependent variable, density from this # list features . remove ( 'density' ) # now we define X and y according to these lists of names X = red . dropna ( axis = 0 )[ features ] . values y = red . dropna ( axis = 0 )[ 'density' ] . values # we will talk about scaling/centering our data at a later time X = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) red . isnull () . sum ( axis = 0 ) # we are getting rid of some nasty nulls! type 0 fixed acidity 2 volatile acidity 1 citric acid 1 residual sugar 0 chlorides 0 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 2 sulphates 2 alcohol 0 quality 0 dtype: int64 # Create linear regression object - note that we are using all the input features model = linear_model . LinearRegression () model . fit ( X , y ) y_hat = model . predict ( X ) Let's see what the coefficients look like ... print ( \"Fit coefficients: \\n \" , model . coef_ , \" \\n Number of coefficients:\" , len ( model . coef_ )) Fit coefficients: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] Number of coefficients: 11 We have 11 !!! That's because we are regressing respect to all 11 independent variables !!! So now, y_{\\sf calc}= m_1x_1 +\\, m_2x_2 \\,+ \\,m_3x_3 \\,+\\,... \\,+ \\,b =\\sum_{i=1}^{13}m_i x_i + b\\;\\;\\;\\;\\; \\sf eq. 7 print ( \"We have 11 slopes / weights: \\n\\n \" , model . coef_ ) print ( \" \\n And one intercept: \" , model . intercept_ ) We have 11 slopes / weights: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] And one intercept: 0.9967517451349656 # This size should match the number of columns in X if len ( X [ 0 ]) == len ( model . coef_ ): print ( \"All good! The number of coefficients matches the number of input features.\" ) else : print ( \"Hmm .. something strange is going on.\" ) All good! The number of coefficients matches the number of input features.","title":"1.4.1 Linear regression with all input fields"},{"location":"S1_Regression_and_Analysis/#exercise-4-evaluate-the-error","text":"Let's evaluate the error by computing the MSE and \\(R^2\\) metrics (see eq. 3 and 6). # The mean squared error # part A # calculate the MSE using mean_squared_error() # mse = # part B # calculate the R square using r2_score() # r2 = # print('Mean squared error: {:.2f}'.format(mse)) # print('Coefficient of determination: {:.2f}'.format(r2))","title":"\ud83c\udfcb\ufe0f Exercise 4: evaluate the error"},{"location":"S1_Regression_and_Analysis/#exercise-5-make-a-plot-of-y-actual-vs-y-predicted","text":"We can also look at how well the computed values match the true values graphically by generating a scatterplot. # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style # plt.plot() plt . title ( \"Linear regression - computed values on entire data set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show ()","title":"\ud83c\udfcb\ufe0f Exercise 5: make a plot of y actual vs y predicted"},{"location":"S1_Regression_and_Analysis/#142-enrichment-splitting-into-train-and-test-sets","text":"note: more of this topic is covered in Model Selection and Validation To see whether we can predict, we will carry out our regression only on a part, 80%, of the full data set. This part is called the training data. We will then test the trained model to predict the rest of the data, 20% - the test data. The function which fits won't see the test data until it has to predict it. We start by splitting out data using scikit-learn's train_test_split() function: X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 42 ) Now we check the size of y_train and y_test , the sum should be the size of y! If this works then we move on and carry out regression but we only use the training data! if len ( y_test ) + len ( y_train ) == len ( y ): print ( 'All good, ready to to go and regress! \\n ' ) # Carry out linear regression print ( 'Running linear regression algorithm on the training set \\n ' ) model = linear_model . LinearRegression () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_pred_test = model . predict ( X_test ) All good, ready to to go and regress! Running linear regression algorithm on the training set Fit coefficients and intercept: [ 1.62385613e-03 1.10578142e-04 7.75216492e-07 5.87755741e-04 7.65190323e-05 -1.03490059e-04 8.87357873e-05 7.79083342e-04 2.23534769e-04 -9.99858829e-04 5.85256438e-06] 0.9967531628434799 Now we can plot our predicted values to see how accurate we are in predicting. We will generate a scatterplot and computing the MSE and \\(R^2\\) metrics of error. sns . scatterplot ( x = y_pred_test , y = y_test , color = \"mediumvioletred\" , s = 50 ) plt . title ( \"Linear regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2e ' % mean_squared_error ( y_test , y_pred_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_pred_test )) Mean squared error: 5.45e-07 Coefficient of determination: 0.87","title":"\ud83c\udf52 1.4.2 Enrichment: Splitting into train and test sets"},{"location":"S1_Regression_and_Analysis/#143-enrichment-some-important-questions","text":"is at least one of the predictors useful in estimating y? We'll see related topics in Inferential Statistics Do we need all the predictors or only some of them? We'll see related topics in Feature Engineering How do we assess goodness of fit? How accurate is any given prediction?","title":"\ud83c\udf52 1.4.3 Enrichment: Some important questions"},{"location":"S1_Regression_and_Analysis/#1431-is-there-a-relationship-between-x-and-y","text":"This question is similar to the hypothesis test we had as an enrichment topic in simple linear regression. To be formulaic: H_0 : \\beta_1 = \\beta_2 = ... = \\beta_p = 0 H_\\alpha : at \\space least \\space one \\space \\beta_j \\space is \\space non-zero To answer this question we perform an F-statistic F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)} where \\(p\\) is the number of predictors. With this F-statistic, we are expecting that if there is no relationship between the predictors and target variable the ratio would converge to 1, since both numerator and denominator would evaluate to simply the variance of the data. On the other hand, if there is a relationship the numerator (the variance captured by the model over the number of predictors) should evaluate to something greater than the variance of the data. Now you might ask, why can't single out every predictor and perform the hypothesis testing we saw before?! Well, by sheer chance, we may incorrectly throw out the null hypothesis. Take an extreme example, if we have 100 predictors, with a confidence level of 0.05 we would by chance alone accept 5 of those 100 predictors, even if they have no relationship to Y. This is where the F-statistic comes in handy (it is worth noting however, that even the F-statistic can't handle \\(p\\) > \\(n\\) for which there is no solution!) y_bar = np . mean ( y ) y_hat = model . predict ( X ) RSS = np . sum (( y - y_hat ) ** 2 ) TSS = np . sum (( y - y_bar ) ** 2 ) p = X . shape [ 1 ] n = X . shape [ 0 ] F = ( TSS - RSS ) / p / ( RSS / ( n - p - 1 )) print ( f \" { F : .2f } \" ) 767.86 # http://pytolearn.csd.auth.gr/d1-hyptest/11/f-distro.html # dfn = degrees of freedom in the numerator # dfd = degrees of freedom in the denominator scipy . stats . f . sf ( x = t , dfn = p , dfd = ( n - p - 1 )) 0.0 with this result we can be fairly certain 1 or more of the prdictors is related to the target variable!","title":"1.4.3.1 Is there a relationship between X and y?"},{"location":"S1_Regression_and_Analysis/#1422-what-are-the-important-variables","text":"the process of determining which of the variables are associated with y is known as variable selection . One approach, is to iteratively select every combination of variables, perform the regression, and use some metric of goodness of fit to determine which subset of p is the best. However this is \\(2^p\\) combinations to try, which quickly blows up. Instead we typically perform one of three approaches Forward selection start with an intercept model. Add a single variable, keep the one that results in the lowest RSS. Repeat. Can always be used no matter how large p is. This is also a greedy approach. Backward selection start with all predictors in the model. Remove the predictor with the largest p-value. Stop when all p-values are below some threshold. Can't be used if p > n. Mixed selection a combination of 1 and 2. start with an intercept model. continue as in 1 accept now, if any predictor p-value rises above some threshold, remove that predictor. This approach counteracts some of the greediness of approach 1.","title":"1.4.2.2 What are the important variables?"},{"location":"S1_Regression_and_Analysis/#1423-how-do-we-assess-goodness-of-fit","text":"Two of the most common measures of goodness of fit are \\(RSE\\) and \\(R^2\\). As it turns out, however, \\(R^2\\) will always increase with more predictors . The \\(RSE\\) defined for multivariate models accounts for the additional p: RSE = \\sqrt{\\frac{1}{n-p-1} RSS} and thus can be used in tandem with \\(R^2\\) to assess whether the additional parameter is worth it or not. Lastly, it is worth noting that visual approaches can lend a hand where simple test values can not (think Anscombe's quartet).","title":"1.4.2.3 How do we assess goodness of fit?"},{"location":"S1_Regression_and_Analysis/#1424-how-accurate-is-any-given-prediction","text":"We used confidence intervals to quantify the uncertainty in the estimate for \\(f(X)\\). To assess the uncertainty around a particular prediction we use prediction intervals . A prediction interval will always be wider than a confidence interval because it incorporates both the uncertainty in the regression estimate ( reducible error or sampling error) and uncertainty in how much an individual point will deviate from the solution surface (the irreducible error ). Put another way, with the confidence interval we expect the average response of y to fall within the calculated interval 95% of the time, however we sample the population data. Whereas we expect an individual response, y, to fall within the calculated prediction interval 95% of the time, however we sample the population data.","title":"1.4.2.4 How accurate is any given prediction?"},{"location":"S1_Regression_and_Analysis/#15-major-assumptions-of-linear-regression","text":"There are two major assumptions of linear regression: that the predictors are additive, and that the relationships between X and y are linear. We can combat these assumptions, however, through feature engineering, a topic we discuss later on but briefly we will mention here. In the additive case, predictors that are suspected to have some interaction affect can be combined to produce a new feature, i.e. \\(X_{new} = X_2*X_2\\). On the linearity case, we can again engineer a new feature, i.e. \\(X_{new} = X_1^2\\).","title":"1.5 Major assumptions of linear regression"},{"location":"S1_Regression_and_Analysis/#16-enrichment-potential-problems-with-linear-regression","text":"Non-linearity of the response-predictor relationships. plot the residuals (observe there is no pattern) Correlation of error terms. standard errors are predicated on no correlation of error terms. Extreme example: we double our data, n is now twice as large, and our confidence intervals narrower by a factor of \\(\\sqrt(2)\\)! a frequent problem in time series data plot residuals as a function of time (observe there is no pattern) Non-constant variance of error terms (heteroscedasticity). standard errors, hypothesis tests, and confidence intervals rely on this assumption a funnel shape in the residuals indicates heteroscedasticity can transform, \\(\\log{y}\\) Outliers. unusual values in response variable check residual plot potentially remove datapoint High-leverage points. unusual values in predictor variable will bias the least squares line problem can be hard to observe in multilinear regression (when a combination of predictors accounts for the leverage) Collinearity. refers to when two or more variables are closely related to one another reduces the accuracy of esimates for coefficients increases standard error of estimates for coefficients. it then reduces the t-statistic and may result in type-2 error, false negative. This means it reduces the power of the hypothesis test, the probability of correctly detecting a non-zero coefficient. look at the correlation matrix of variables for multicolinearity compute the variance inflation factor (VIF) \\(1/(1-R^2)\\) where the regression is performed against the indicated predictor across all other predictors remove features with a VIF above 5-10 We will discuss many of these in the session on Feature Engineering","title":"\ud83c\udf52 1.6 Enrichment: Potential problems with linear regression"},{"location":"S1_Regression_and_Analysis/#17-enrichment-other-regression-algorithms","text":"There are many other regression algorithms the two we want to highlight here are Ridge, LASSO, and Elastic Net. They differ by an added term to the loss function. Let's review. Eq. 2 expanded to multivariate form yields: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 for Ridge regression, we add a regularization term known as L2 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}\\beta_{j}^2 for LASSO (Least Absolute Shrinkage and Selection Operator) we add L1 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}|\\beta_{j}| The key difference here is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. Elastic Net is a combination of these two regularization methods. model = linear_model . Ridge () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_hat_test = model . predict ( X_test ) Fit coefficients and intercept: [ 1.61930554e-03 1.11227142e-04 2.64709094e-06 5.87271456e-04 7.58510569e-05 -1.02851782e-04 8.76686650e-05 7.75641517e-04 2.23315063e-04 -9.98653815e-04 5.26839010e-06] 0.9967531358810221 sns . scatterplot ( x = y_hat_test , y = y_test , color = \"lightseagreen\" , s = 50 ) plt . title ( \"Ridge regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2f ' % mean_squared_error ( y_test , y_hat_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_hat_test )) Mean squared error: 0.00 Coefficient of determination: 0.87","title":"\ud83c\udf52 1.7 Enrichment: Other regression algorithms"},{"location":"S1_Regression_and_Analysis/#exercise-6-tune-hyperparameter-for-ridge-regression","text":"Use the docstring to peak into the hyperparameters for Ridge Regression. What is the optimal value of lambda? Plot the \\(\\beta\\) values vs \\(\\lambda\\) from the results of your analysis # cell for exercise 3 out_lambdas = [] out_coefs = [] out_scores = [] for i in range ( 10 ): lambdas = [] coefs = [] scores = [] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) for lamb in range ( 1 , int ( 5e3 ), 20 ): model = linear_model . Ridge ( alpha = lamb ) model . fit ( X_train , y_train ) lambdas . append ( lamb ) coefs . append ( model . coef_ ) scores . append ( r2_score ( y_test , model . predict ( X_test ))) # print('MSE: %.4f' % mean_squared_error(y_test, model.predict(X_test))) # print('R2: %.4f' % r2_score(y_test, model.predict(X_test))) out_lambdas . append ( lambdas ) out_coefs . append ( coefs ) out_scores . append ( scores ) coef_means = np . array ( out_coefs ) . mean ( axis = 0 ) coef_stds = np . array ( out_coefs ) . std ( axis = 0 ) results_means = pd . DataFrame ( coef_means , columns = features ) results_stds = pd . DataFrame ( coef_stds , columns = features ) results_means [ 'lambda' ] = [ i for i in lambdas ] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) for feat in features : ax . errorbar ([ i for i in lambdas ], results_means [ feat ], yerr = results_stds [ feat ], label = feat ) # results.plot('lambda', 'scores', ax=ax[1]) ax . legend () <matplotlib.legend.Legend at 0x7fdd72270fd0> results = pd . DataFrame ( coefs , columns = features ) results [ 'lambda' ] = [ i for i in lambdas ] results [ 'scores' ] = scores fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) for feat in features : results . plot ( 'lambda' , feat , ax = ax [ 0 ]) results . plot ( 'lambda' , 'scores' , ax = ax [ 1 ]) <AxesSubplot:xlabel='lambda'>","title":"\ud83c\udfcb\ufe0f Exercise 6: Tune Hyperparameter for Ridge Regression"},{"location":"S1_Regression_and_Analysis/#18-enrichment-additional-regression-exercises","text":"","title":"\ud83c\udf52 1.8 Enrichment: Additional Regression Exercises"},{"location":"S1_Regression_and_Analysis/#problem-1-number-and-choice-of-input-features","text":"Load the red wine dataset and evaluate how the linear regression predictions changes as you change the number and choice of input features . The total number of columns in X is 11 and each column represents a specific input feature. Estimate the MSE print ( X_train . shape ) (1274, 11) If you want to use the first 5 features you could proceed as following: X_train_five = X_train [:, 0 : 5 ] X_test_five = X_test [:, 0 : 5 ] Check that the new variables have the shape your expect print ( X_train_five . shape ) print ( X_test_five . shape ) (1274, 5) (319, 5) Now you can use these to train your linear regression model and repeat for different numbers or sets of input features! Note that you do not need to change the output feature! It's size is independent from the number of input features, yet recall that its length is the same as the number of values per input feature. Questions to think about while you work on this problem - How many input feature variables does one need? Is there a maximum or minimum number? - Could one input feature variable be better than the rest? - What if values are missing for one of the input feature variables - is it still worth using it? - Can you use L1 or L2 to determine these optimum features more quickly?","title":"Problem 1) Number and choice of input features"},{"location":"S1_Regression_and_Analysis/#problem-2-type-of-regression-algorithm","text":"Try using other types of linear regression methods on the wine dataset: the LASSO model and the Elastic net model which are described by the sklearn.linear_model.ElasticNet() sklearn.linear_model.Lasso() scikit-learn functions. For more detail see ElasticNet and Lasso . Questions to think about while you work on this problem - How does the error change with each model? - Which model seems to perform best? - How can you optimize the hyperparameter, \\(\\lambda\\) - Does one model do better than the other at determining which input features are more important? - How about non linear regression / what if the data does not follow a line? - How do the bias and variance change for each model from sklearn.linear_model import ElasticNet from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.linear_model import LinearRegression for model in [ ElasticNet , Lasso , Ridge , LinearRegression ]: model = model () model . fit ( X_train , y_train ) print ( str ( model )) print ( 'Mean squared error: %.ef' % mean_squared_error ( y_test , model . predict ( X_test ))) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , model . predict ( X_test ))) print () ElasticNet() Mean squared error: 4e-06f Coefficient of determination: -0.00 Lasso() Mean squared error: 4e-06f Coefficient of determination: -0.00 Ridge() Mean squared error: 7e-07f Coefficient of determination: 0.83 LinearRegression() Mean squared error: 7e-07f Coefficient of determination: 0.83","title":"Problem 2) Type of regression algorithm"},{"location":"S1_Regression_and_Analysis/#references","text":"Linear Regression To find out more see simple linear regression scikit-learn Scikit-learn Linear regression in scikit-learn Metrics of error The Boston dataset Pearson correlation To find out more see pearson Irreducible error, bias and variance Great Coursera videos here and here","title":"References"},{"location":"S2_Inferential_Statistics/","text":"Data Science Foundations Session 2: Inferential Statistics \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com img src Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (\u201cinferences\u201d) from that data. With inferential statistics, you take data from samples and make generalizations about a population statshowto In this session we will explore inferential statistics. 2.0 Preparing Environment and Importing Data \u00b6 back to top 2.0.1 Import Packages \u00b6 back to top # The modules we've seen before import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns # our stats modules import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy 2.0.2 Load Dataset \u00b6 back to top For this session, we will use dummy datasets from sklearn. df = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... ... ... ... ... ... 1663 Tiramisu Chocolate Outer Doughnut Pear Amethyst Fickelgruber 12/2020 38128.802589 0.420111 1664 Tiramisu Chocolate Outer Doughnut Pear Burgundy Zebrabar 12/2020 108.642857 0.248659 1665 Tiramisu Chocolate Outer Doughnut Pear Teal Zebrabar 12/2020 3517.933333 0.378501 1666 Tiramisu Chocolate Outer Doughnut Rock and Rye Amethyst Slugworth 12/2020 10146.898432 0.213149 1667 Tiramisu Chocolate Outer Doughnut Rock and Rye Burgundy Zebrabar 12/2020 1271.904762 0.431813 1668 rows \u00d7 9 columns descriptors = df . columns [: - 2 ] for col in df . columns [: - 2 ]: print ( col ) print ( df [ col ] . unique ()) print () Base Cake ['Butter' 'Cheese' 'Chiffon' 'Pound' 'Sponge' 'Tiramisu'] Truffle Type ['Candy Outer' 'Chocolate Outer' 'Jelly Filled'] Primary Flavor ['Butter Pecan' 'Ginger Lime' 'Margarita' 'Pear' 'Pink Lemonade' 'Raspberry Ginger Ale' 'Sassafras' 'Spice' 'Wild Cherry Cream' 'Cream Soda' 'Horchata' 'Kettle Corn' 'Lemon Bar' 'Orange Pineapple\\tP' 'Plum' 'Orange' 'Butter Toffee' 'Lemon' 'Acai Berry' 'Apricot' 'Birch Beer' 'Cherry Cream Spice' 'Creme de Menthe' 'Fruit Punch' 'Ginger Ale' 'Grand Mariner' 'Orange Brandy' 'Pecan' 'Toasted Coconut' 'Watermelon' 'Wintergreen' 'Vanilla' 'Bavarian Cream' 'Black Licorice' 'Caramel Cream' 'Cheesecake' 'Cherry Cola' 'Coffee' 'Irish Cream' 'Lemon Custard' 'Mango' 'Sour' 'Amaretto' 'Blueberry' 'Butter Milk' 'Chocolate Mint' 'Coconut' 'Dill Pickle' 'Gingersnap' 'Chocolate' 'Doughnut'] Secondary Flavor ['Toffee' 'Banana' 'Rum' 'Tutti Frutti' 'Vanilla' 'Mixed Berry' 'Whipped Cream' 'Apricot' 'Passion Fruit' 'Peppermint' 'Dill Pickle' 'Black Cherry' 'Wild Cherry Cream' 'Papaya' 'Mango' 'Cucumber' 'Egg Nog' 'Pear' 'Rock and Rye' 'Tangerine' 'Apple' 'Black Currant' 'Kiwi' 'Lemon' 'Hazelnut' 'Butter Rum' 'Fuzzy Navel' 'Mojito' 'Ginger Beer'] Color Group ['Taupe' 'Amethyst' 'Burgundy' 'White' 'Black' 'Opal' 'Citrine' 'Rose' 'Slate' 'Teal' 'Tiffany' 'Olive'] Customer ['Slugworth' 'Perk-a-Cola' 'Fickelgruber' 'Zebrabar' \"Dandy's Candies\"] Date ['1/2020' '2/2020' '3/2020' '4/2020' '5/2020' '6/2020' '7/2020' '8/2020' '9/2020' '10/2020' '11/2020' '12/2020'] 2.1 Navigating the Many Forms of Hypothesis Testing \u00b6 2.1.1 A Refresher on Data Types \u00b6 Before we dive in, lets remind ourselves of the different data types. The acronym is NOIR: Nominal, Ordinal, Invterval, Ratio. And as we proceed along the acronym, more information is encapsulated in the data type. For this notebook the important distinction is between discrete (nominal/ordinal) and continuous (interval/ratio) data types. Depending on whether we are discrete or continuous, in either the target or predictor variable, we will proceed with a given set of hypothesis tests. 2.1.2 The Roadmap \u00b6 Taking the taxonomy of datatypes and considering each of the predictor and outcome variables, we can concieve a roadmap source: scribbr The above diagram is imperfect (e.g. ANOVA is a comparison of variances, not of means) and non-comprehensive (e.g. where does moods median fit in this?). It will have to do until I make my own. There are many modalities of hypothesis testing. We will not cover them all. The goal here, is to cover enough that you can leave this notebook with a basic idea of how the different hypothesis tests relate to one another as well as their basic ingredients. What you will find is that among all these tests, a sample statistic is produced. The sample statistic itself is comprised typically of some expected value (for instance a mean) divided by a standard error of measure (for instance a standard deviation). In fact, we saw this in the previous notebook when discussing the confidence intervals around linear regression coefficients (and yes linear regression IS a form of hypothesis testing \ud83d\ude09) In this notebook, we will touch on each of the following: Non-parametric tests Moods Median (Comparison of Medians) Kruskal-Wallis (Comparison of Medians, compare to ANOVA ) Mann Whitney (Rank order test, compare to T-test ) Comparison of means T-test Independent Equal Variances (students T-test) Unequal Variances (Welch's T-test) Dependent Comparison of variances Analysis of Variance (ANOVA) One Way ANOVA Two Way ANOVA MANOVA Factorial ANOVA When do I use each of these? We will talk about this as we proceed through the examples. Visit this Minitab Post for additional reading. Finally, before we dive in and in the shadow of my making this hypothesis test decision tree a giant landmark on our journey, I invite you to read an excerpt from Chapter 1 of Statistical Rethinking (McElreath 2016) . In this excerpt, McElreath tells a cautionary tale of using such roadmaps. Sorry McElreath. And if videos are more your bag, there you go. 2.2 What is Mood's Median? \u00b6 You can use Chi-Square to test for a goodness of fit (whether a sample of data represents a distribution) or whether two variables are related (using a contingency table, which we will create below!) A special case of Pearon's Chi-Squared Test: We create a table that counts the observations above and below the global median for two or more groups . We then perform a chi-squared test of significance on this contingency table Null hypothesis: the Medians are all equal The chi-square test statistic: x^2 = \\sum{\\frac{(O-E)^2}{E}} Where \\(O\\) is the observed frequency and \\(E\\) is the expected frequency. Let's take an example , say we have three shifts with the following production rates: np . random . seed ( 7 ) shift_one = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_two = [ round ( i ) for i in np . random . normal ( 21 , 3 , 10 )] print ( shift_one ) print ( shift_two ) [21, 15, 16, 17, 14, 16, 16, 11, 19, 18] [19, 20, 23, 20, 20, 17, 23, 21, 22, 16] stat , p , m , table = scipy . stats . median_test ( shift_one , shift_two , correction = False ) what is median_test returning? print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 7.20 p-value of the test: 0.007 the grand median: 18.5 Let's evaluate that test statistic ourselves by taking a look at the contingency table: table array([[2, 8], [8, 2]]) This is easier to make sense of if we order the shift times shift_one . sort () shift_one [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] When we look at shift one, we see that 8 values are at or below the grand median. shift_two . sort () shift_two [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] For shift two, only two are at or below the grand median. Since the sample sizes are the same, the expected value for both groups is the same, 5 above and 5 below the grand median. The chi-square is then: X^2 = \\frac{(2-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(2-5)^2}{5} ( 2 - 5 ) ** 2 / 5 + ( 8 - 5 ) ** 2 / 5 + ( 8 - 5 ) ** 2 / 5 + ( 2 - 5 ) ** 2 / 5 7.2 Our p-value, or the probability of observing the null-hypothsis, is under 0.05 (at 0.007). We can conclude that these shift performances were drawn under seperate distributions. For comparison, let's do this analysis again with shifts of equal performances np . random . seed ( 3 ) shift_three = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_four = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] stat , p , m , table = scipy . stats . median_test ( shift_three , shift_four , correction = False ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000 the grand median: 15.5 and the shift raw values: shift_three . sort () shift_four . sort () print ( shift_three ) print ( shift_four ) [10, 14, 15, 15, 15, 16, 16, 16, 17, 21] [11, 12, 13, 14, 15, 16, 19, 19, 19, 21] table array([[5, 5], [5, 5]]) 2.2.1 When to Use Mood's? \u00b6 Mood's Median Test is highly flexible but has the following assumptions: Considers only one categorical factor Response variable is continuous (our shift rates) Data does not need to be normally distributed But the distributions are similarly shaped Sample sizes can be unequal and small (less than 20 observations) Other considerations: Not as powerful as Kruskal-Wallis Test but still useful for small sample sizes or when there are outliers \ud83c\udfcb\ufe0f Exercise 1: Use Mood's Median Test \u00b6 Part A Perform moods median test on Base Cake (Categorical Variable) and EBITDA/KG (Continuous Variable) in Truffle data \u00b6 We're also going to get some practice with pandas groupby. # what is returned by this groupby? gp = df . groupby ( 'Base Cake' ) How do we find out? We could iterate through it: # seems to be a tuple of some sort for i in gp : print ( i ) break ('Butter', Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns]) # the first object appears to be the group print ( i [ 0 ]) # the second object appears to be the df belonging to that group print ( i [ 1 ]) Butter Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns] going back to our diagram from our earlier pandas session. It looks like whenever we split in the groupby method, we create separate dataframes as well as their group label: Ok, so we know gp is separate dataframes. How do we turn them into arrays to then pass to median_test ? # complete this for loop for i , j in gp : pass # turn 'EBITDA/KG' of j into an array using the .values attribute # print this to the screen After you've completed the previous step, turn this into a list comprehension and pass the result to a variable called margins # complete the code below # margins = [# YOUR LIST COMPREHENSION HERE] Remember the list unpacking we did for the tic tac toe project? We're going to do the same thing here. Unpack the margins list for median_test and run the cell below! # complete the following line # stat, p, m, table = scipy.stats.median_test(<UNPACK MARGINS HERE>, correction=False) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.2e} \" . format ( p )) print ( \"the grand median: {:.2e} \" . format ( m )) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.00e+00 the grand median: 1.55e+01 Part B View the distributions of the data using matplotlib and seaborn \u00b6 What a fantastic statistical result we found! Can we affirm our result with some visualizations? I hope so! Create a boxplot below using pandas. In your call to df.boxplot() the by parameter should be set to Base Cake and the column parameter should be set to EBITDA/KG # YOUR BOXPLOT HERE For comparison, I've shown the boxplot below using seaborn! fig , ax = plt . subplots ( figsize = ( 10 , 7 )) ax = sns . boxplot ( x = 'Base Cake' , y = 'EBITDA/KG' , data = df , color = '#A0cbe8' ) Part C Perform Moods Median on all the other groups \u00b6 # Recall the other descriptors we have descriptors Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') for desc in descriptors : # YOUR CODE FORM MARGINS BELOW # margins = [<YOUR LIST COMPREHENSION>] # UNPACK MARGINS INTO MEDIAN_TEST # stat, p, m, table = scipy.stats.median_test(<YOUR UNPACKING METHOD>, correction=False) print ( desc ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:e} \" . format ( p )) print ( \"the grand median: {} \" . format ( m ), end = ' \\n\\n ' ) Base Cake The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Truffle Type The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Primary Flavor The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Secondary Flavor The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Color Group The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Customer The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Date The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Part D Many boxplots \u00b6 And finally, we will confirm these visually. Complete the Boxplot for each group: for desc in descriptors : fig , ax = plt . subplots ( figsize = ( 10 , 5 )) # sns.boxplot(x=<YOUR X VARIABLE HERE>, y='EBITDA/KG', data=df, color='#A0cbe8', ax=ax) 2.3 What is a T-test? \u00b6 There are 1-sample and 2-sample T-tests (note: we would use a 1-sample T-test just to determine if the sample mean is equal to a hypothesized population mean) Within 2-sample T-tests we have independent and dependent T-tests (uncorrelated or correlated samples) For independent, two-sample T-tests: Equal variance (or pooled) T-test scipy.stats.ttest_ind(equal_var=True) also called Student's T-test Unequal variance T-test scipy.stats.ttest_ind(equal_var=False) also called Welch's T-test For dependent T-tests: Paired (or correlated) T-test scipy.stats.ttest_rel ex : patient symptoms before and after treatment A full discussion on T-tests is outside the scope of this session, but we can refer to wikipedia for more information, including formulas on how each statistic is computed: student's T-test 2.3.1 Demonstration of T-tests \u00b6 back to top We'll assume our shifts are of equal variance and proceed with the appropriate independent two-sample T-test... print ( shift_one ) print ( shift_two ) [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] To calculate the T-test, we follow a slightly different statistical formula: T=\\frac{\\mu_1 - \\mu_2}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} where \\(\\mu\\) are the means of the two groups, \\(n\\) are the sample sizes and \\(s\\) is the pooled standard deviation, also known as the cummulative variance (depending on if you square it or not): s= \\sqrt{\\frac{(n_1-1)\\sigma_1^2 + (n_2-1)\\sigma_2^2}{n_1 + n_2 - 2}} where \\(\\sigma\\) are the standard deviations. What you'll notice here is we are combining the two variances, we can only do this if we assume the variances are somewhat equal, this is known as the equal variances t-test. mean_shift_one = np . mean ( shift_one ) mean_shift_two = np . mean ( shift_two ) print ( mean_shift_one , mean_shift_two ) 16.3 20.1 com_var = (( np . sum ([( i - mean_shift_one ) ** 2 for i in shift_one ]) + np . sum ([( i - mean_shift_two ) ** 2 for i in shift_two ])) / ( len ( shift_one ) + len ( shift_two ) - 2 )) print ( com_var ) 6.5 T = ( np . abs ( mean_shift_one - mean_shift_two ) / ( np . sqrt ( com_var / len ( shift_one ) + com_var / len ( shift_two )))) T 3.3328204733667115 We see that this hand-computed result matches that of the scipy module: scipy . stats . ttest_ind ( shift_two , shift_one , equal_var = True ) Ttest_indResult(statistic=3.3328204733667115, pvalue=0.0037029158660758575) 2.4 What is ANOVA? \u00b6 2.4.1 But First... What are F-statistics and the F-test? \u00b6 The F-statistic is simply a ratio of two variances, or the ratio of mean squares mean squares is the estimate of population variance that accounts for the degrees of freedom to compute that estimate. We will explore this in the context of ANOVA 2.4.2 What is Analysis of Variance? \u00b6 ANOVA uses the F-test to determine whether the variability between group means is larger than the variability within the groups. If that statistic is large enough, you can conclude that the means of the groups are not equal. The caveat is that ANOVA tells us whether there is a difference in means but it does not tell us where the difference is. To find where the difference is between the groups, we have to conduct post-hoc tests. There are two main types: One-way (one factor) and Two-way (two factor) where factor is an independent variable Ind A Ind B Dep X H 10 X I 12 Y I 11 Y H 20 ANOVA Hypotheses \u00b6 Null hypothesis : group means are equal Alternative hypothesis : at least one group mean is different from the other groups ANOVA Assumptions \u00b6 Residuals (experimental error) are normally distributed (test with Shapiro-Wilk) Homogeneity of variances (variances are equal between groups) (test with Bartlett's) Observations are sampled independently from each other Note: ANOVA assumptions can be checked using test statistics (e.g. Shapiro-Wilk, Bartlett\u2019s, Levene\u2019s test) and the visual approaches such as residual plots (e.g. QQ-plots) and histograms. Steps for ANOVA \u00b6 Check sample sizes: equal observations must be in each group Calculate Sum of Square between groups and within groups (\\(SS_B, SS_E\\)) Calculate Mean Square between groups and within groups (\\(MS_B, MS_E\\)) Calculate F value (\\(MS_B/MS_E\\)) This might be easier to see in a table: Source of Variation degree of freedom (Df) Sum of squares (SS) Mean square (MS) F value Between Groups Df_b = P-1 SS_B MS_B = SS_B / Df_B MS_B / MS_E Within Groups Df_E = P(N-1) SS_E MS_E = SS_E / Df_E total Df_T = PN-1 SS_T Where: SS_B = \\sum_{i}^{P}{(\\bar{y}_i-\\bar{y})^2} SS_E = \\sum_{ik}^{PN}{(\\bar{y}_{ik}-\\bar{y}_i)^2} SS_T = SS_B + SS_E Let's go back to our shift data to take an example: shifts = pd . DataFrame ([ shift_one , shift_two , shift_three , shift_four ]) . T shifts . columns = [ 'A' , 'B' , 'C' , 'D' ] shifts . boxplot () <AxesSubplot:> 2.4.2.1 SNS Boxplot \u00b6 this is another great way to view boxplot data. Notice how sns also shows us the raw data alongside the box and whiskers using a swarmplot . shift_melt = pd . melt ( shifts . reset_index (), id_vars = [ 'index' ], value_vars = [ 'A' , 'B' , 'C' , 'D' ]) shift_melt . columns = [ 'index' , 'shift' , 'rate' ] ax = sns . boxplot ( x = 'shift' , y = 'rate' , data = shift_melt , color = '#A0cbe8' ) ax = sns . swarmplot ( x = \"shift\" , y = \"rate\" , data = shift_melt , color = '#79706e' ) Anyway back to ANOVA... fvalue , pvalue = stats . f_oneway ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( fvalue , pvalue ) 5.599173553719008 0.0029473487978665873 We can get this in the format of the table we saw above: # get ANOVA table import statsmodels.api as sm from statsmodels.formula.api import ols # Ordinary Least Squares (OLS) model model = ols ( 'rate ~ C(shift)' , data = shift_melt ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(shift) 135.5 3.0 5.599174 0.002947 Residual 290.4 36.0 NaN NaN The Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9750654697418213 0.5121709108352661 We can use Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. w , pvalue = stats . bartlett ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( w , pvalue ) 1.3763632854696672 0.711084540821183 2.4.2.2 ANOVA Interpretation \u00b6 The p value form ANOVA analysis is significant ( p < 0.05) and we can conclude there are significant difference between the shifts. But we do not know which shift(s) are different. For this we need to perform a post hoc test. There are a multitude of these that are beyond the scope of this discussion ( Tukey-kramer is one such test) \ud83c\udf52 2.5 Enrichment: Evaluate statistical significance of product margin: a snake in the garden \u00b6 2.5.1 Mood's Median on product descriptors \u00b6 The first issue we run into with moods is... what? Since Mood's is nonparametric, we can easily become overconfident in our results. Let's take an example, continuing with the Truffle Type column. Recall that there are 3 unique Truffle Types: df [ 'Truffle Type' ] . unique () array(['Candy Outer', 'Chocolate Outer', 'Jelly Filled'], dtype=object) We can loop through each group and compute the: Moods test (comparison of medians) Welch's T-test (unequal variances, comparison of means) Shapiro-Wilk test for normality col = 'Truffle Type' moodsdf = pd . DataFrame () confidence_level = 0.01 welch_rej = mood_rej = shapiro_rej = False for truff in df [ col ] . unique (): # for each group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) if p < confidence_level : mood_rej = True median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) print ( \" {} : N= {} \" . format ( truff , size )) print ( \"Moods Median Test\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( stat , p ), end = \" \\n \" ) print ( f \" \\t reject: { mood_rej } \" ) print ( \"Welch's T-Test\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( * scipy . stats . ttest_ind ( group , pop , equal_var = False ))) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue if welchp < confidence_level : welch_rej = True print ( f \" \\t reject: { welch_rej } \" ) print ( \"Shapiro-Wilk\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( * stats . shapiro ( group ))) if stats . shapiro ( group ) . pvalue < confidence_level : shapiro_rej = True print ( f \" \\t reject: { shapiro_rej } \" ) print ( end = \" \\n\\n \" ) moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ col , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] Candy Outer: N=288 Moods Median Test statistic=1.52, pvalue=2.18e-01 reject: False Welch's T-Test statistic=-2.76, pvalue=5.91e-03 reject: True Shapiro-Wilk statistic=0.96, pvalue=2.74e-07 reject: True Chocolate Outer: N=1356 Moods Median Test statistic=6.63, pvalue=1.00e-02 reject: False Welch's T-Test statistic=4.41, pvalue=1.19e-05 reject: True Shapiro-Wilk statistic=0.96, pvalue=6.30e-19 reject: True Jelly Filled: N=24 Moods Median Test statistic=18.64, pvalue=1.58e-05 reject: True Welch's T-Test statistic=-8.41, pvalue=7.93e-09 reject: True Shapiro-Wilk statistic=0.87, pvalue=6.56e-03 reject: True \ud83d\ude4b\u200d\u2640\ufe0f Question 1: Moods Results on Truffle Type \u00b6 What can we say about these results? Recall that: Shapiro-Wilk Null Hypothesis: the distribution is normal Welch's T-test: requires that the distributions be normal Moods test: does not require normality in distributions main conclusions: all the groups are non-normal and therefore welch's test is invalid. We saw that the Welch's test had much lower p-values than the Moods median test. This is good news! It means that our Moods test, while allowing for non-normality, is much more conservative in its test-statistic, and therefore was unable to reject the null hypothesis in the cases of the chocolate outer and candy outer groups We can go ahead and repeat this analysis for all of our product categories: df . columns [: 5 ] Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group'], dtype='object') moodsdf = pd . DataFrame () for col in df . columns [: 5 ]: for truff in df [ col ] . unique (): group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] print ( moodsdf . shape ) (101, 10) moodsdf = moodsdf . loc [( moodsdf [ 'p_value' ] < confidence_level )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( moodsdf . shape ) (57, 10) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Secondary Flavor Wild Cherry Cream 6.798913 0.009121 0.216049 -0.122491 -0.15791 12 0.00001 [[1, 833], [11, 823]] 1 Primary Flavor Lemon Bar 6.798913 0.009121 0.216049 -0.122491 -0.15791 12 0.00001 [[1, 833], [11, 823]] 2 Primary Flavor Orange Pineapple\\tP 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 3 Secondary Flavor Papaya 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 4 Primary Flavor Cherry Cream Spice 10.156401 0.001438 0.216049 0.018702 0.009701 12 0.000001 [[0, 834], [12, 822]] 5 Secondary Flavor Cucumber 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 6 Truffle Type Jelly Filled 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 7 Primary Flavor Orange 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 8 Primary Flavor Toasted Coconut 15.261253 0.000094 0.216049 0.037002 0.028392 24 0.0 [[2, 832], [22, 812]] 9 Primary Flavor Wild Cherry Cream 6.798913 0.009121 0.216049 0.047647 0.028695 12 0.00038 [[1, 833], [11, 823]] 10 Secondary Flavor Apricot 15.261253 0.000094 0.216049 0.060312 0.037422 24 0.0 [[2, 832], [22, 812]] 11 Primary Flavor Kettle Corn 29.062065 0.0 0.216049 0.055452 0.045891 60 0.0 [[9, 825], [51, 783]] 12 Primary Flavor Acai Berry 18.643248 0.000016 0.216049 0.036505 0.049466 24 0.0 [[1, 833], [23, 811]] 13 Primary Flavor Pink Lemonade 10.156401 0.001438 0.216049 0.039862 0.056349 12 0.000011 [[0, 834], [12, 822]] 14 Secondary Flavor Black Cherry 58.900366 0.0 0.216049 0.055975 0.062898 96 0.0 [[11, 823], [85, 749]] 15 Primary Flavor Watermelon 15.261253 0.000094 0.216049 0.04405 0.067896 24 0.0 [[2, 832], [22, 812]] 16 Primary Flavor Sassafras 6.798913 0.009121 0.216049 0.072978 0.074112 12 0.000059 [[1, 833], [11, 823]] 17 Primary Flavor Plum 34.851608 0.0 0.216049 0.084963 0.079993 72 0.0 [[11, 823], [61, 773]] 18 Secondary Flavor Dill Pickle 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 19 Primary Flavor Horchata 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 20 Primary Flavor Lemon Custard 12.217457 0.000473 0.216049 0.079389 0.087969 24 0.000006 [[3, 831], [21, 813]] 21 Primary Flavor Fruit Punch 10.156401 0.001438 0.216049 0.078935 0.090326 12 0.000076 [[0, 834], [12, 822]] 22 Base Cake Chiffon 117.046226 0.0 0.216049 0.127851 0.125775 288 0.0 [[60, 774], [228, 606]] 23 Base Cake Butter 134.36727 0.0 0.216049 0.142082 0.139756 456 0.0 [[122, 712], [334, 500]] 24 Secondary Flavor Banana 10.805348 0.001012 0.216049 0.163442 0.15537 60 0.0 [[17, 817], [43, 791]] 25 Primary Flavor Cream Soda 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 26 Secondary Flavor Peppermint 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 27 Primary Flavor Grand Mariner 10.581767 0.001142 0.216049 0.197463 0.165529 72 0.000829 [[22, 812], [50, 784]] 28 Color Group Amethyst 20.488275 0.000006 0.216049 0.195681 0.167321 300 0.0 [[114, 720], [186, 648]] 29 Color Group Burgundy 10.999677 0.000911 0.216049 0.193048 0.171465 120 0.000406 [[42, 792], [78, 756]] 30 Color Group White 35.76526 0.0 0.216049 0.19 0.177264 432 0.0 [[162, 672], [270, 564]] 31 Primary Flavor Ginger Lime 7.835047 0.005124 0.216049 0.21435 0.183543 84 0.001323 [[29, 805], [55, 779]] 32 Primary Flavor Mango 11.262488 0.000791 0.216049 0.28803 0.245049 132 0.009688 [[85, 749], [47, 787]] 33 Color Group Opal 11.587164 0.000664 0.216049 0.317878 0.259304 324 0.0 [[190, 644], [134, 700]] 34 Secondary Flavor Apple 27.283292 0.0 0.216049 0.326167 0.293876 36 0.001176 [[34, 800], [2, 832]] 35 Secondary Flavor Tangerine 32.626389 0.0 0.216049 0.342314 0.319273 48 0.000113 [[44, 790], [4, 830]] 36 Secondary Flavor Black Currant 34.778391 0.0 0.216049 0.357916 0.332449 36 0.0 [[36, 798], [0, 834]] 37 Secondary Flavor Pear 16.614303 0.000046 0.216049 0.373034 0.33831 60 0.000031 [[46, 788], [14, 820]] 38 Primary Flavor Vanilla 34.778391 0.0 0.216049 0.378053 0.341626 36 0.000001 [[36, 798], [0, 834]] 39 Color Group Citrine 10.156401 0.001438 0.216049 0.390728 0.342512 12 0.001925 [[12, 822], [0, 834]] 40 Color Group Teal 13.539679 0.000234 0.216049 0.323955 0.3446 96 0.00121 [[66, 768], [30, 804]] 41 Base Cake Tiramisu 52.360619 0.0 0.216049 0.388267 0.362102 144 0.0 [[114, 720], [30, 804]] 42 Primary Flavor Doughnut 74.935256 0.0 0.216049 0.439721 0.379361 108 0.0 [[98, 736], [10, 824]] 43 Secondary Flavor Ginger Beer 22.363443 0.000002 0.216049 0.444895 0.382283 24 0.000481 [[24, 810], [0, 834]] 44 Color Group Rose 18.643248 0.000016 0.216049 0.42301 0.407061 24 0.000062 [[23, 811], [1, 833]] 45 Base Cake Cheese 66.804744 0.0 0.216049 0.450934 0.435638 84 0.0 [[79, 755], [5, 829]] 46 Primary Flavor Butter Toffee 60.181468 0.0 0.216049 0.50366 0.456343 60 0.0 [[60, 774], [0, 834]] 47 Color Group Slate 10.156401 0.001438 0.216049 0.540214 0.483138 12 0.000017 [[12, 822], [0, 834]] 48 Primary Flavor Gingersnap 22.363443 0.000002 0.216049 0.643218 0.623627 24 0.0 [[24, 810], [0, 834]] 49 Primary Flavor Dill Pickle 22.363443 0.000002 0.216049 0.642239 0.655779 24 0.0 [[24, 810], [0, 834]] 50 Color Group Olive 44.967537 0.0 0.216049 0.637627 0.670186 60 0.0 [[56, 778], [4, 830]] 51 Primary Flavor Butter Milk 10.156401 0.001438 0.216049 0.699284 0.688601 12 0.0 [[12, 822], [0, 834]] 52 Base Cake Sponge 127.156266 0.0 0.216049 0.698996 0.699355 120 0.0 [[120, 714], [0, 834]] 53 Primary Flavor Chocolate Mint 10.156401 0.001438 0.216049 0.685546 0.699666 12 0.0 [[12, 822], [0, 834]] 54 Primary Flavor Coconut 10.156401 0.001438 0.216049 0.732777 0.717641 12 0.0 [[12, 822], [0, 834]] 55 Primary Flavor Blueberry 22.363443 0.000002 0.216049 0.759643 0.72536 24 0.0 [[24, 810], [0, 834]] 56 Primary Flavor Amaretto 10.156401 0.001438 0.216049 0.782156 0.764845 12 0.0 [[12, 822], [0, 834]] 2.5.2 Broad Analysis of Categories: ANOVA \u00b6 Recall our \"melted\" shift data. It will be useful to think of getting our Truffle data in this format: shift_melt . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index shift rate 0 0 A 15 1 1 A 15 2 2 A 15 3 3 A 16 4 4 A 17 df . columns = df . columns . str . replace ( ' ' , '_' ) df . columns = df . columns . str . replace ( '/' , '_' ) # get ANOVA table # Ordinary Least Squares (OLS) model model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Recall the Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9576056599617004 1.2598073820281984e-21 And the Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. gb = df . groupby ( 'Truffle_Type' )[ 'EBITDA_KG' ] gb <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fafac7cfd10> w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( w , pvalue ) 109.93252546442552 1.344173733366234e-24 Wow it looks like our data is not drawn from a normal distribution! Let's check this for other categories... We can wrap these in a for loop: for col in df . columns [: 5 ]: print ( col ) model = ols ( 'EBITDA_KG ~ C( {} )' . format ( col ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) w , pvalue = stats . shapiro ( model . resid ) print ( \"Shapiro: \" , w , pvalue ) gb = df . groupby ( col )[ 'EBITDA_KG' ] w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( \"Bartlett: \" , w , pvalue ) print () Base_Cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Base_Cake) 39.918103 5.0 314.869955 1.889884e-237 Residual 42.140500 1662.0 NaN NaN Shapiro: 0.9634131193161011 4.1681337029688696e-20 Bartlett: 69.83288886114195 1.1102218566053728e-13 Truffle_Type .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Shapiro: 0.9576056599617004 1.2598073820281984e-21 Bartlett: 109.93252546442552 1.344173733366234e-24 Primary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Primary_Flavor) 50.270639 50.0 51.143649 1.153434e-292 Residual 31.787964 1617.0 NaN NaN Shapiro: 0.948470413684845 9.90281706784179e-24 Bartlett: 210.15130419114894 1.5872504991231547e-21 Secondary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Secondary_Flavor) 15.088382 28.0 13.188089 1.929302e-54 Residual 66.970220 1639.0 NaN NaN Shapiro: 0.9548103213310242 2.649492974953278e-22 Bartlett: 420.6274502894803 1.23730070350945e-71 Color_Group .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Color_Group) 16.079685 11.0 36.689347 6.544980e-71 Residual 65.978918 1656.0 NaN NaN Shapiro: 0.969061017036438 1.8926407335144587e-18 Bartlett: 136.55525281340468 8.164787784033709e-24 2.5.3 Visual Analysis of Residuals: QQ-Plots \u00b6 This can be distressing and is often why we want visual methods to see what is going on with our data! model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () #create instance of influence influence = model . get_influence () #obtain standardized residuals standardized_residuals = influence . resid_studentized_internal # res.anova_std_residuals are standardized residuals obtained from ANOVA (check above) sm . qqplot ( standardized_residuals , line = '45' ) plt . xlabel ( \"Theoretical Quantiles\" ) plt . ylabel ( \"Standardized Residuals\" ) plt . show () # histogram plt . hist ( model . resid , bins = 'auto' , histtype = 'bar' , ec = 'k' ) plt . xlabel ( \"Residuals\" ) plt . ylabel ( 'Frequency' ) plt . show () We see that a lot of our data is swayed by extremely high and low values, so what can we conclude? You need the right test statistic for the right job, in this case, we are littered with unequal variance in our groupings so we use the moods median and welch (unequal variance t-test) to make conclusions about our data References \u00b6 Renesh Bedre ANOVA Minitab ANOVA Analytics Vidhya ANOVA Renesh Bedre Hypothesis Testing Real Statistics Turkey-kramer Mutual Information","title":"Inferential Statistics"},{"location":"S2_Inferential_Statistics/#data-science-foundations-session-2-inferential-statistics","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com img src Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (\u201cinferences\u201d) from that data. With inferential statistics, you take data from samples and make generalizations about a population statshowto In this session we will explore inferential statistics.","title":"Data Science Foundations  Session 2: Inferential Statistics"},{"location":"S2_Inferential_Statistics/#20-preparing-environment-and-importing-data","text":"back to top","title":"2.0 Preparing Environment and Importing Data"},{"location":"S2_Inferential_Statistics/#201-import-packages","text":"back to top # The modules we've seen before import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns # our stats modules import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy","title":"2.0.1 Import Packages"},{"location":"S2_Inferential_Statistics/#202-load-dataset","text":"back to top For this session, we will use dummy datasets from sklearn. df = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... ... ... ... ... ... 1663 Tiramisu Chocolate Outer Doughnut Pear Amethyst Fickelgruber 12/2020 38128.802589 0.420111 1664 Tiramisu Chocolate Outer Doughnut Pear Burgundy Zebrabar 12/2020 108.642857 0.248659 1665 Tiramisu Chocolate Outer Doughnut Pear Teal Zebrabar 12/2020 3517.933333 0.378501 1666 Tiramisu Chocolate Outer Doughnut Rock and Rye Amethyst Slugworth 12/2020 10146.898432 0.213149 1667 Tiramisu Chocolate Outer Doughnut Rock and Rye Burgundy Zebrabar 12/2020 1271.904762 0.431813 1668 rows \u00d7 9 columns descriptors = df . columns [: - 2 ] for col in df . columns [: - 2 ]: print ( col ) print ( df [ col ] . unique ()) print () Base Cake ['Butter' 'Cheese' 'Chiffon' 'Pound' 'Sponge' 'Tiramisu'] Truffle Type ['Candy Outer' 'Chocolate Outer' 'Jelly Filled'] Primary Flavor ['Butter Pecan' 'Ginger Lime' 'Margarita' 'Pear' 'Pink Lemonade' 'Raspberry Ginger Ale' 'Sassafras' 'Spice' 'Wild Cherry Cream' 'Cream Soda' 'Horchata' 'Kettle Corn' 'Lemon Bar' 'Orange Pineapple\\tP' 'Plum' 'Orange' 'Butter Toffee' 'Lemon' 'Acai Berry' 'Apricot' 'Birch Beer' 'Cherry Cream Spice' 'Creme de Menthe' 'Fruit Punch' 'Ginger Ale' 'Grand Mariner' 'Orange Brandy' 'Pecan' 'Toasted Coconut' 'Watermelon' 'Wintergreen' 'Vanilla' 'Bavarian Cream' 'Black Licorice' 'Caramel Cream' 'Cheesecake' 'Cherry Cola' 'Coffee' 'Irish Cream' 'Lemon Custard' 'Mango' 'Sour' 'Amaretto' 'Blueberry' 'Butter Milk' 'Chocolate Mint' 'Coconut' 'Dill Pickle' 'Gingersnap' 'Chocolate' 'Doughnut'] Secondary Flavor ['Toffee' 'Banana' 'Rum' 'Tutti Frutti' 'Vanilla' 'Mixed Berry' 'Whipped Cream' 'Apricot' 'Passion Fruit' 'Peppermint' 'Dill Pickle' 'Black Cherry' 'Wild Cherry Cream' 'Papaya' 'Mango' 'Cucumber' 'Egg Nog' 'Pear' 'Rock and Rye' 'Tangerine' 'Apple' 'Black Currant' 'Kiwi' 'Lemon' 'Hazelnut' 'Butter Rum' 'Fuzzy Navel' 'Mojito' 'Ginger Beer'] Color Group ['Taupe' 'Amethyst' 'Burgundy' 'White' 'Black' 'Opal' 'Citrine' 'Rose' 'Slate' 'Teal' 'Tiffany' 'Olive'] Customer ['Slugworth' 'Perk-a-Cola' 'Fickelgruber' 'Zebrabar' \"Dandy's Candies\"] Date ['1/2020' '2/2020' '3/2020' '4/2020' '5/2020' '6/2020' '7/2020' '8/2020' '9/2020' '10/2020' '11/2020' '12/2020']","title":"2.0.2 Load Dataset"},{"location":"S2_Inferential_Statistics/#21-navigating-the-many-forms-of-hypothesis-testing","text":"","title":"2.1 Navigating the Many Forms of Hypothesis Testing"},{"location":"S2_Inferential_Statistics/#211-a-refresher-on-data-types","text":"Before we dive in, lets remind ourselves of the different data types. The acronym is NOIR: Nominal, Ordinal, Invterval, Ratio. And as we proceed along the acronym, more information is encapsulated in the data type. For this notebook the important distinction is between discrete (nominal/ordinal) and continuous (interval/ratio) data types. Depending on whether we are discrete or continuous, in either the target or predictor variable, we will proceed with a given set of hypothesis tests.","title":"2.1.1 A Refresher on Data Types"},{"location":"S2_Inferential_Statistics/#212-the-roadmap","text":"Taking the taxonomy of datatypes and considering each of the predictor and outcome variables, we can concieve a roadmap source: scribbr The above diagram is imperfect (e.g. ANOVA is a comparison of variances, not of means) and non-comprehensive (e.g. where does moods median fit in this?). It will have to do until I make my own. There are many modalities of hypothesis testing. We will not cover them all. The goal here, is to cover enough that you can leave this notebook with a basic idea of how the different hypothesis tests relate to one another as well as their basic ingredients. What you will find is that among all these tests, a sample statistic is produced. The sample statistic itself is comprised typically of some expected value (for instance a mean) divided by a standard error of measure (for instance a standard deviation). In fact, we saw this in the previous notebook when discussing the confidence intervals around linear regression coefficients (and yes linear regression IS a form of hypothesis testing \ud83d\ude09) In this notebook, we will touch on each of the following: Non-parametric tests Moods Median (Comparison of Medians) Kruskal-Wallis (Comparison of Medians, compare to ANOVA ) Mann Whitney (Rank order test, compare to T-test ) Comparison of means T-test Independent Equal Variances (students T-test) Unequal Variances (Welch's T-test) Dependent Comparison of variances Analysis of Variance (ANOVA) One Way ANOVA Two Way ANOVA MANOVA Factorial ANOVA When do I use each of these? We will talk about this as we proceed through the examples. Visit this Minitab Post for additional reading. Finally, before we dive in and in the shadow of my making this hypothesis test decision tree a giant landmark on our journey, I invite you to read an excerpt from Chapter 1 of Statistical Rethinking (McElreath 2016) . In this excerpt, McElreath tells a cautionary tale of using such roadmaps. Sorry McElreath. And if videos are more your bag, there you go.","title":"2.1.2 The Roadmap"},{"location":"S2_Inferential_Statistics/#22-what-is-moods-median","text":"You can use Chi-Square to test for a goodness of fit (whether a sample of data represents a distribution) or whether two variables are related (using a contingency table, which we will create below!) A special case of Pearon's Chi-Squared Test: We create a table that counts the observations above and below the global median for two or more groups . We then perform a chi-squared test of significance on this contingency table Null hypothesis: the Medians are all equal The chi-square test statistic: x^2 = \\sum{\\frac{(O-E)^2}{E}} Where \\(O\\) is the observed frequency and \\(E\\) is the expected frequency. Let's take an example , say we have three shifts with the following production rates: np . random . seed ( 7 ) shift_one = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_two = [ round ( i ) for i in np . random . normal ( 21 , 3 , 10 )] print ( shift_one ) print ( shift_two ) [21, 15, 16, 17, 14, 16, 16, 11, 19, 18] [19, 20, 23, 20, 20, 17, 23, 21, 22, 16] stat , p , m , table = scipy . stats . median_test ( shift_one , shift_two , correction = False ) what is median_test returning? print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 7.20 p-value of the test: 0.007 the grand median: 18.5 Let's evaluate that test statistic ourselves by taking a look at the contingency table: table array([[2, 8], [8, 2]]) This is easier to make sense of if we order the shift times shift_one . sort () shift_one [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] When we look at shift one, we see that 8 values are at or below the grand median. shift_two . sort () shift_two [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] For shift two, only two are at or below the grand median. Since the sample sizes are the same, the expected value for both groups is the same, 5 above and 5 below the grand median. The chi-square is then: X^2 = \\frac{(2-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(2-5)^2}{5} ( 2 - 5 ) ** 2 / 5 + ( 8 - 5 ) ** 2 / 5 + ( 8 - 5 ) ** 2 / 5 + ( 2 - 5 ) ** 2 / 5 7.2 Our p-value, or the probability of observing the null-hypothsis, is under 0.05 (at 0.007). We can conclude that these shift performances were drawn under seperate distributions. For comparison, let's do this analysis again with shifts of equal performances np . random . seed ( 3 ) shift_three = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_four = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] stat , p , m , table = scipy . stats . median_test ( shift_three , shift_four , correction = False ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000 the grand median: 15.5 and the shift raw values: shift_three . sort () shift_four . sort () print ( shift_three ) print ( shift_four ) [10, 14, 15, 15, 15, 16, 16, 16, 17, 21] [11, 12, 13, 14, 15, 16, 19, 19, 19, 21] table array([[5, 5], [5, 5]])","title":"2.2 What is Mood's Median?"},{"location":"S2_Inferential_Statistics/#221-when-to-use-moods","text":"Mood's Median Test is highly flexible but has the following assumptions: Considers only one categorical factor Response variable is continuous (our shift rates) Data does not need to be normally distributed But the distributions are similarly shaped Sample sizes can be unequal and small (less than 20 observations) Other considerations: Not as powerful as Kruskal-Wallis Test but still useful for small sample sizes or when there are outliers","title":"2.2.1 When to Use Mood's?"},{"location":"S2_Inferential_Statistics/#exercise-1-use-moods-median-test","text":"","title":"\ud83c\udfcb\ufe0f Exercise 1: Use Mood's Median Test"},{"location":"S2_Inferential_Statistics/#part-a-perform-moods-median-test-on-base-cake-categorical-variable-and-ebitdakg-continuous-variable-in-truffle-data","text":"We're also going to get some practice with pandas groupby. # what is returned by this groupby? gp = df . groupby ( 'Base Cake' ) How do we find out? We could iterate through it: # seems to be a tuple of some sort for i in gp : print ( i ) break ('Butter', Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns]) # the first object appears to be the group print ( i [ 0 ]) # the second object appears to be the df belonging to that group print ( i [ 1 ]) Butter Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns] going back to our diagram from our earlier pandas session. It looks like whenever we split in the groupby method, we create separate dataframes as well as their group label: Ok, so we know gp is separate dataframes. How do we turn them into arrays to then pass to median_test ? # complete this for loop for i , j in gp : pass # turn 'EBITDA/KG' of j into an array using the .values attribute # print this to the screen After you've completed the previous step, turn this into a list comprehension and pass the result to a variable called margins # complete the code below # margins = [# YOUR LIST COMPREHENSION HERE] Remember the list unpacking we did for the tic tac toe project? We're going to do the same thing here. Unpack the margins list for median_test and run the cell below! # complete the following line # stat, p, m, table = scipy.stats.median_test(<UNPACK MARGINS HERE>, correction=False) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.2e} \" . format ( p )) print ( \"the grand median: {:.2e} \" . format ( m )) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.00e+00 the grand median: 1.55e+01","title":"Part A Perform moods median test on Base Cake (Categorical Variable) and EBITDA/KG (Continuous Variable) in Truffle data"},{"location":"S2_Inferential_Statistics/#part-b-view-the-distributions-of-the-data-using-matplotlib-and-seaborn","text":"What a fantastic statistical result we found! Can we affirm our result with some visualizations? I hope so! Create a boxplot below using pandas. In your call to df.boxplot() the by parameter should be set to Base Cake and the column parameter should be set to EBITDA/KG # YOUR BOXPLOT HERE For comparison, I've shown the boxplot below using seaborn! fig , ax = plt . subplots ( figsize = ( 10 , 7 )) ax = sns . boxplot ( x = 'Base Cake' , y = 'EBITDA/KG' , data = df , color = '#A0cbe8' )","title":"Part B View the distributions of the data using matplotlib and seaborn"},{"location":"S2_Inferential_Statistics/#part-c-perform-moods-median-on-all-the-other-groups","text":"# Recall the other descriptors we have descriptors Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') for desc in descriptors : # YOUR CODE FORM MARGINS BELOW # margins = [<YOUR LIST COMPREHENSION>] # UNPACK MARGINS INTO MEDIAN_TEST # stat, p, m, table = scipy.stats.median_test(<YOUR UNPACKING METHOD>, correction=False) print ( desc ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:e} \" . format ( p )) print ( \"the grand median: {} \" . format ( m ), end = ' \\n\\n ' ) Base Cake The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Truffle Type The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Primary Flavor The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Secondary Flavor The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Color Group The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Customer The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Date The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5","title":"Part C Perform Moods Median on all the other groups"},{"location":"S2_Inferential_Statistics/#part-d-many-boxplots","text":"And finally, we will confirm these visually. Complete the Boxplot for each group: for desc in descriptors : fig , ax = plt . subplots ( figsize = ( 10 , 5 )) # sns.boxplot(x=<YOUR X VARIABLE HERE>, y='EBITDA/KG', data=df, color='#A0cbe8', ax=ax)","title":"Part D Many boxplots"},{"location":"S2_Inferential_Statistics/#23-what-is-a-t-test","text":"There are 1-sample and 2-sample T-tests (note: we would use a 1-sample T-test just to determine if the sample mean is equal to a hypothesized population mean) Within 2-sample T-tests we have independent and dependent T-tests (uncorrelated or correlated samples) For independent, two-sample T-tests: Equal variance (or pooled) T-test scipy.stats.ttest_ind(equal_var=True) also called Student's T-test Unequal variance T-test scipy.stats.ttest_ind(equal_var=False) also called Welch's T-test For dependent T-tests: Paired (or correlated) T-test scipy.stats.ttest_rel ex : patient symptoms before and after treatment A full discussion on T-tests is outside the scope of this session, but we can refer to wikipedia for more information, including formulas on how each statistic is computed: student's T-test","title":"2.3 What is a T-test?"},{"location":"S2_Inferential_Statistics/#231-demonstration-of-t-tests","text":"back to top We'll assume our shifts are of equal variance and proceed with the appropriate independent two-sample T-test... print ( shift_one ) print ( shift_two ) [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] To calculate the T-test, we follow a slightly different statistical formula: T=\\frac{\\mu_1 - \\mu_2}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} where \\(\\mu\\) are the means of the two groups, \\(n\\) are the sample sizes and \\(s\\) is the pooled standard deviation, also known as the cummulative variance (depending on if you square it or not): s= \\sqrt{\\frac{(n_1-1)\\sigma_1^2 + (n_2-1)\\sigma_2^2}{n_1 + n_2 - 2}} where \\(\\sigma\\) are the standard deviations. What you'll notice here is we are combining the two variances, we can only do this if we assume the variances are somewhat equal, this is known as the equal variances t-test. mean_shift_one = np . mean ( shift_one ) mean_shift_two = np . mean ( shift_two ) print ( mean_shift_one , mean_shift_two ) 16.3 20.1 com_var = (( np . sum ([( i - mean_shift_one ) ** 2 for i in shift_one ]) + np . sum ([( i - mean_shift_two ) ** 2 for i in shift_two ])) / ( len ( shift_one ) + len ( shift_two ) - 2 )) print ( com_var ) 6.5 T = ( np . abs ( mean_shift_one - mean_shift_two ) / ( np . sqrt ( com_var / len ( shift_one ) + com_var / len ( shift_two )))) T 3.3328204733667115 We see that this hand-computed result matches that of the scipy module: scipy . stats . ttest_ind ( shift_two , shift_one , equal_var = True ) Ttest_indResult(statistic=3.3328204733667115, pvalue=0.0037029158660758575)","title":"2.3.1 Demonstration of T-tests"},{"location":"S2_Inferential_Statistics/#24-what-is-anova","text":"","title":"2.4 What is ANOVA?"},{"location":"S2_Inferential_Statistics/#241-but-first-what-are-f-statistics-and-the-f-test","text":"The F-statistic is simply a ratio of two variances, or the ratio of mean squares mean squares is the estimate of population variance that accounts for the degrees of freedom to compute that estimate. We will explore this in the context of ANOVA","title":"2.4.1 But First... What are F-statistics and the F-test?"},{"location":"S2_Inferential_Statistics/#242-what-is-analysis-of-variance","text":"ANOVA uses the F-test to determine whether the variability between group means is larger than the variability within the groups. If that statistic is large enough, you can conclude that the means of the groups are not equal. The caveat is that ANOVA tells us whether there is a difference in means but it does not tell us where the difference is. To find where the difference is between the groups, we have to conduct post-hoc tests. There are two main types: One-way (one factor) and Two-way (two factor) where factor is an independent variable Ind A Ind B Dep X H 10 X I 12 Y I 11 Y H 20","title":"2.4.2 What is Analysis of Variance?"},{"location":"S2_Inferential_Statistics/#anova-hypotheses","text":"Null hypothesis : group means are equal Alternative hypothesis : at least one group mean is different from the other groups","title":"ANOVA Hypotheses"},{"location":"S2_Inferential_Statistics/#anova-assumptions","text":"Residuals (experimental error) are normally distributed (test with Shapiro-Wilk) Homogeneity of variances (variances are equal between groups) (test with Bartlett's) Observations are sampled independently from each other Note: ANOVA assumptions can be checked using test statistics (e.g. Shapiro-Wilk, Bartlett\u2019s, Levene\u2019s test) and the visual approaches such as residual plots (e.g. QQ-plots) and histograms.","title":"ANOVA Assumptions"},{"location":"S2_Inferential_Statistics/#steps-for-anova","text":"Check sample sizes: equal observations must be in each group Calculate Sum of Square between groups and within groups (\\(SS_B, SS_E\\)) Calculate Mean Square between groups and within groups (\\(MS_B, MS_E\\)) Calculate F value (\\(MS_B/MS_E\\)) This might be easier to see in a table: Source of Variation degree of freedom (Df) Sum of squares (SS) Mean square (MS) F value Between Groups Df_b = P-1 SS_B MS_B = SS_B / Df_B MS_B / MS_E Within Groups Df_E = P(N-1) SS_E MS_E = SS_E / Df_E total Df_T = PN-1 SS_T Where: SS_B = \\sum_{i}^{P}{(\\bar{y}_i-\\bar{y})^2} SS_E = \\sum_{ik}^{PN}{(\\bar{y}_{ik}-\\bar{y}_i)^2} SS_T = SS_B + SS_E Let's go back to our shift data to take an example: shifts = pd . DataFrame ([ shift_one , shift_two , shift_three , shift_four ]) . T shifts . columns = [ 'A' , 'B' , 'C' , 'D' ] shifts . boxplot () <AxesSubplot:>","title":"Steps for ANOVA"},{"location":"S2_Inferential_Statistics/#2421-sns-boxplot","text":"this is another great way to view boxplot data. Notice how sns also shows us the raw data alongside the box and whiskers using a swarmplot . shift_melt = pd . melt ( shifts . reset_index (), id_vars = [ 'index' ], value_vars = [ 'A' , 'B' , 'C' , 'D' ]) shift_melt . columns = [ 'index' , 'shift' , 'rate' ] ax = sns . boxplot ( x = 'shift' , y = 'rate' , data = shift_melt , color = '#A0cbe8' ) ax = sns . swarmplot ( x = \"shift\" , y = \"rate\" , data = shift_melt , color = '#79706e' ) Anyway back to ANOVA... fvalue , pvalue = stats . f_oneway ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( fvalue , pvalue ) 5.599173553719008 0.0029473487978665873 We can get this in the format of the table we saw above: # get ANOVA table import statsmodels.api as sm from statsmodels.formula.api import ols # Ordinary Least Squares (OLS) model model = ols ( 'rate ~ C(shift)' , data = shift_melt ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(shift) 135.5 3.0 5.599174 0.002947 Residual 290.4 36.0 NaN NaN The Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9750654697418213 0.5121709108352661 We can use Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. w , pvalue = stats . bartlett ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( w , pvalue ) 1.3763632854696672 0.711084540821183","title":"2.4.2.1 SNS Boxplot"},{"location":"S2_Inferential_Statistics/#2422-anova-interpretation","text":"The p value form ANOVA analysis is significant ( p < 0.05) and we can conclude there are significant difference between the shifts. But we do not know which shift(s) are different. For this we need to perform a post hoc test. There are a multitude of these that are beyond the scope of this discussion ( Tukey-kramer is one such test)","title":"2.4.2.2 ANOVA Interpretation"},{"location":"S2_Inferential_Statistics/#25-enrichment-evaluate-statistical-significance-of-product-margin-a-snake-in-the-garden","text":"","title":"\ud83c\udf52 2.5 Enrichment: Evaluate statistical significance of product margin: a snake in the garden"},{"location":"S2_Inferential_Statistics/#251-moods-median-on-product-descriptors","text":"The first issue we run into with moods is... what? Since Mood's is nonparametric, we can easily become overconfident in our results. Let's take an example, continuing with the Truffle Type column. Recall that there are 3 unique Truffle Types: df [ 'Truffle Type' ] . unique () array(['Candy Outer', 'Chocolate Outer', 'Jelly Filled'], dtype=object) We can loop through each group and compute the: Moods test (comparison of medians) Welch's T-test (unequal variances, comparison of means) Shapiro-Wilk test for normality col = 'Truffle Type' moodsdf = pd . DataFrame () confidence_level = 0.01 welch_rej = mood_rej = shapiro_rej = False for truff in df [ col ] . unique (): # for each group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) if p < confidence_level : mood_rej = True median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) print ( \" {} : N= {} \" . format ( truff , size )) print ( \"Moods Median Test\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( stat , p ), end = \" \\n \" ) print ( f \" \\t reject: { mood_rej } \" ) print ( \"Welch's T-Test\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( * scipy . stats . ttest_ind ( group , pop , equal_var = False ))) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue if welchp < confidence_level : welch_rej = True print ( f \" \\t reject: { welch_rej } \" ) print ( \"Shapiro-Wilk\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( * stats . shapiro ( group ))) if stats . shapiro ( group ) . pvalue < confidence_level : shapiro_rej = True print ( f \" \\t reject: { shapiro_rej } \" ) print ( end = \" \\n\\n \" ) moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ col , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] Candy Outer: N=288 Moods Median Test statistic=1.52, pvalue=2.18e-01 reject: False Welch's T-Test statistic=-2.76, pvalue=5.91e-03 reject: True Shapiro-Wilk statistic=0.96, pvalue=2.74e-07 reject: True Chocolate Outer: N=1356 Moods Median Test statistic=6.63, pvalue=1.00e-02 reject: False Welch's T-Test statistic=4.41, pvalue=1.19e-05 reject: True Shapiro-Wilk statistic=0.96, pvalue=6.30e-19 reject: True Jelly Filled: N=24 Moods Median Test statistic=18.64, pvalue=1.58e-05 reject: True Welch's T-Test statistic=-8.41, pvalue=7.93e-09 reject: True Shapiro-Wilk statistic=0.87, pvalue=6.56e-03 reject: True","title":"2.5.1 Mood's Median on product descriptors"},{"location":"S2_Inferential_Statistics/#question-1-moods-results-on-truffle-type","text":"What can we say about these results? Recall that: Shapiro-Wilk Null Hypothesis: the distribution is normal Welch's T-test: requires that the distributions be normal Moods test: does not require normality in distributions main conclusions: all the groups are non-normal and therefore welch's test is invalid. We saw that the Welch's test had much lower p-values than the Moods median test. This is good news! It means that our Moods test, while allowing for non-normality, is much more conservative in its test-statistic, and therefore was unable to reject the null hypothesis in the cases of the chocolate outer and candy outer groups We can go ahead and repeat this analysis for all of our product categories: df . columns [: 5 ] Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group'], dtype='object') moodsdf = pd . DataFrame () for col in df . columns [: 5 ]: for truff in df [ col ] . unique (): group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] print ( moodsdf . shape ) (101, 10) moodsdf = moodsdf . loc [( moodsdf [ 'p_value' ] < confidence_level )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( moodsdf . shape ) (57, 10) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Secondary Flavor Wild Cherry Cream 6.798913 0.009121 0.216049 -0.122491 -0.15791 12 0.00001 [[1, 833], [11, 823]] 1 Primary Flavor Lemon Bar 6.798913 0.009121 0.216049 -0.122491 -0.15791 12 0.00001 [[1, 833], [11, 823]] 2 Primary Flavor Orange Pineapple\\tP 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 3 Secondary Flavor Papaya 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 4 Primary Flavor Cherry Cream Spice 10.156401 0.001438 0.216049 0.018702 0.009701 12 0.000001 [[0, 834], [12, 822]] 5 Secondary Flavor Cucumber 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 6 Truffle Type Jelly Filled 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 7 Primary Flavor Orange 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 8 Primary Flavor Toasted Coconut 15.261253 0.000094 0.216049 0.037002 0.028392 24 0.0 [[2, 832], [22, 812]] 9 Primary Flavor Wild Cherry Cream 6.798913 0.009121 0.216049 0.047647 0.028695 12 0.00038 [[1, 833], [11, 823]] 10 Secondary Flavor Apricot 15.261253 0.000094 0.216049 0.060312 0.037422 24 0.0 [[2, 832], [22, 812]] 11 Primary Flavor Kettle Corn 29.062065 0.0 0.216049 0.055452 0.045891 60 0.0 [[9, 825], [51, 783]] 12 Primary Flavor Acai Berry 18.643248 0.000016 0.216049 0.036505 0.049466 24 0.0 [[1, 833], [23, 811]] 13 Primary Flavor Pink Lemonade 10.156401 0.001438 0.216049 0.039862 0.056349 12 0.000011 [[0, 834], [12, 822]] 14 Secondary Flavor Black Cherry 58.900366 0.0 0.216049 0.055975 0.062898 96 0.0 [[11, 823], [85, 749]] 15 Primary Flavor Watermelon 15.261253 0.000094 0.216049 0.04405 0.067896 24 0.0 [[2, 832], [22, 812]] 16 Primary Flavor Sassafras 6.798913 0.009121 0.216049 0.072978 0.074112 12 0.000059 [[1, 833], [11, 823]] 17 Primary Flavor Plum 34.851608 0.0 0.216049 0.084963 0.079993 72 0.0 [[11, 823], [61, 773]] 18 Secondary Flavor Dill Pickle 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 19 Primary Flavor Horchata 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 20 Primary Flavor Lemon Custard 12.217457 0.000473 0.216049 0.079389 0.087969 24 0.000006 [[3, 831], [21, 813]] 21 Primary Flavor Fruit Punch 10.156401 0.001438 0.216049 0.078935 0.090326 12 0.000076 [[0, 834], [12, 822]] 22 Base Cake Chiffon 117.046226 0.0 0.216049 0.127851 0.125775 288 0.0 [[60, 774], [228, 606]] 23 Base Cake Butter 134.36727 0.0 0.216049 0.142082 0.139756 456 0.0 [[122, 712], [334, 500]] 24 Secondary Flavor Banana 10.805348 0.001012 0.216049 0.163442 0.15537 60 0.0 [[17, 817], [43, 791]] 25 Primary Flavor Cream Soda 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 26 Secondary Flavor Peppermint 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 27 Primary Flavor Grand Mariner 10.581767 0.001142 0.216049 0.197463 0.165529 72 0.000829 [[22, 812], [50, 784]] 28 Color Group Amethyst 20.488275 0.000006 0.216049 0.195681 0.167321 300 0.0 [[114, 720], [186, 648]] 29 Color Group Burgundy 10.999677 0.000911 0.216049 0.193048 0.171465 120 0.000406 [[42, 792], [78, 756]] 30 Color Group White 35.76526 0.0 0.216049 0.19 0.177264 432 0.0 [[162, 672], [270, 564]] 31 Primary Flavor Ginger Lime 7.835047 0.005124 0.216049 0.21435 0.183543 84 0.001323 [[29, 805], [55, 779]] 32 Primary Flavor Mango 11.262488 0.000791 0.216049 0.28803 0.245049 132 0.009688 [[85, 749], [47, 787]] 33 Color Group Opal 11.587164 0.000664 0.216049 0.317878 0.259304 324 0.0 [[190, 644], [134, 700]] 34 Secondary Flavor Apple 27.283292 0.0 0.216049 0.326167 0.293876 36 0.001176 [[34, 800], [2, 832]] 35 Secondary Flavor Tangerine 32.626389 0.0 0.216049 0.342314 0.319273 48 0.000113 [[44, 790], [4, 830]] 36 Secondary Flavor Black Currant 34.778391 0.0 0.216049 0.357916 0.332449 36 0.0 [[36, 798], [0, 834]] 37 Secondary Flavor Pear 16.614303 0.000046 0.216049 0.373034 0.33831 60 0.000031 [[46, 788], [14, 820]] 38 Primary Flavor Vanilla 34.778391 0.0 0.216049 0.378053 0.341626 36 0.000001 [[36, 798], [0, 834]] 39 Color Group Citrine 10.156401 0.001438 0.216049 0.390728 0.342512 12 0.001925 [[12, 822], [0, 834]] 40 Color Group Teal 13.539679 0.000234 0.216049 0.323955 0.3446 96 0.00121 [[66, 768], [30, 804]] 41 Base Cake Tiramisu 52.360619 0.0 0.216049 0.388267 0.362102 144 0.0 [[114, 720], [30, 804]] 42 Primary Flavor Doughnut 74.935256 0.0 0.216049 0.439721 0.379361 108 0.0 [[98, 736], [10, 824]] 43 Secondary Flavor Ginger Beer 22.363443 0.000002 0.216049 0.444895 0.382283 24 0.000481 [[24, 810], [0, 834]] 44 Color Group Rose 18.643248 0.000016 0.216049 0.42301 0.407061 24 0.000062 [[23, 811], [1, 833]] 45 Base Cake Cheese 66.804744 0.0 0.216049 0.450934 0.435638 84 0.0 [[79, 755], [5, 829]] 46 Primary Flavor Butter Toffee 60.181468 0.0 0.216049 0.50366 0.456343 60 0.0 [[60, 774], [0, 834]] 47 Color Group Slate 10.156401 0.001438 0.216049 0.540214 0.483138 12 0.000017 [[12, 822], [0, 834]] 48 Primary Flavor Gingersnap 22.363443 0.000002 0.216049 0.643218 0.623627 24 0.0 [[24, 810], [0, 834]] 49 Primary Flavor Dill Pickle 22.363443 0.000002 0.216049 0.642239 0.655779 24 0.0 [[24, 810], [0, 834]] 50 Color Group Olive 44.967537 0.0 0.216049 0.637627 0.670186 60 0.0 [[56, 778], [4, 830]] 51 Primary Flavor Butter Milk 10.156401 0.001438 0.216049 0.699284 0.688601 12 0.0 [[12, 822], [0, 834]] 52 Base Cake Sponge 127.156266 0.0 0.216049 0.698996 0.699355 120 0.0 [[120, 714], [0, 834]] 53 Primary Flavor Chocolate Mint 10.156401 0.001438 0.216049 0.685546 0.699666 12 0.0 [[12, 822], [0, 834]] 54 Primary Flavor Coconut 10.156401 0.001438 0.216049 0.732777 0.717641 12 0.0 [[12, 822], [0, 834]] 55 Primary Flavor Blueberry 22.363443 0.000002 0.216049 0.759643 0.72536 24 0.0 [[24, 810], [0, 834]] 56 Primary Flavor Amaretto 10.156401 0.001438 0.216049 0.782156 0.764845 12 0.0 [[12, 822], [0, 834]]","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: Moods Results on Truffle Type"},{"location":"S2_Inferential_Statistics/#252-broad-analysis-of-categories-anova","text":"Recall our \"melted\" shift data. It will be useful to think of getting our Truffle data in this format: shift_melt . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index shift rate 0 0 A 15 1 1 A 15 2 2 A 15 3 3 A 16 4 4 A 17 df . columns = df . columns . str . replace ( ' ' , '_' ) df . columns = df . columns . str . replace ( '/' , '_' ) # get ANOVA table # Ordinary Least Squares (OLS) model model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Recall the Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9576056599617004 1.2598073820281984e-21 And the Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. gb = df . groupby ( 'Truffle_Type' )[ 'EBITDA_KG' ] gb <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fafac7cfd10> w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( w , pvalue ) 109.93252546442552 1.344173733366234e-24 Wow it looks like our data is not drawn from a normal distribution! Let's check this for other categories... We can wrap these in a for loop: for col in df . columns [: 5 ]: print ( col ) model = ols ( 'EBITDA_KG ~ C( {} )' . format ( col ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) w , pvalue = stats . shapiro ( model . resid ) print ( \"Shapiro: \" , w , pvalue ) gb = df . groupby ( col )[ 'EBITDA_KG' ] w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( \"Bartlett: \" , w , pvalue ) print () Base_Cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Base_Cake) 39.918103 5.0 314.869955 1.889884e-237 Residual 42.140500 1662.0 NaN NaN Shapiro: 0.9634131193161011 4.1681337029688696e-20 Bartlett: 69.83288886114195 1.1102218566053728e-13 Truffle_Type .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Shapiro: 0.9576056599617004 1.2598073820281984e-21 Bartlett: 109.93252546442552 1.344173733366234e-24 Primary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Primary_Flavor) 50.270639 50.0 51.143649 1.153434e-292 Residual 31.787964 1617.0 NaN NaN Shapiro: 0.948470413684845 9.90281706784179e-24 Bartlett: 210.15130419114894 1.5872504991231547e-21 Secondary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Secondary_Flavor) 15.088382 28.0 13.188089 1.929302e-54 Residual 66.970220 1639.0 NaN NaN Shapiro: 0.9548103213310242 2.649492974953278e-22 Bartlett: 420.6274502894803 1.23730070350945e-71 Color_Group .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Color_Group) 16.079685 11.0 36.689347 6.544980e-71 Residual 65.978918 1656.0 NaN NaN Shapiro: 0.969061017036438 1.8926407335144587e-18 Bartlett: 136.55525281340468 8.164787784033709e-24","title":"2.5.2 Broad Analysis of Categories: ANOVA"},{"location":"S2_Inferential_Statistics/#253-visual-analysis-of-residuals-qq-plots","text":"This can be distressing and is often why we want visual methods to see what is going on with our data! model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () #create instance of influence influence = model . get_influence () #obtain standardized residuals standardized_residuals = influence . resid_studentized_internal # res.anova_std_residuals are standardized residuals obtained from ANOVA (check above) sm . qqplot ( standardized_residuals , line = '45' ) plt . xlabel ( \"Theoretical Quantiles\" ) plt . ylabel ( \"Standardized Residuals\" ) plt . show () # histogram plt . hist ( model . resid , bins = 'auto' , histtype = 'bar' , ec = 'k' ) plt . xlabel ( \"Residuals\" ) plt . ylabel ( 'Frequency' ) plt . show () We see that a lot of our data is swayed by extremely high and low values, so what can we conclude? You need the right test statistic for the right job, in this case, we are littered with unequal variance in our groupings so we use the moods median and welch (unequal variance t-test) to make conclusions about our data","title":"2.5.3 Visual Analysis of Residuals: QQ-Plots"},{"location":"S2_Inferential_Statistics/#references","text":"Renesh Bedre ANOVA Minitab ANOVA Analytics Vidhya ANOVA Renesh Bedre Hypothesis Testing Real Statistics Turkey-kramer Mutual Information","title":"References"},{"location":"S3_Model_Selection_and_Validation/","text":"Data Science Foundations Session 3: Model Selection and Validation \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com At the end of session 1, we saw the basic recipe for creating a supervised machine learning model: Environment setup and importing data Rudimentary exploratory data analysis Feature engineering (these were created for us) Choosing and training a model: choose model choose hyperparameters fit using (training) data predict using (validation) data In session 1, I chose our model and hyperparameters preemptively. How did I do that? In the real world, you won't necessarily have the best intution about how to make these choices. In today's session, we will algorithmize the way we approach choosing and training a model Note: I will import libraries at the beginning of this notebook, as is good practice, but will reimport them as they are used to remind ourselves where each method came from! 3.0 Preparing Environment and Importing Data \u00b6 back to top 3.0.1 Import Packages \u00b6 back to top # Pandas library for the pandas dataframes import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import seaborn as sns import random import scipy.stats as stats from patsy import dmatrices from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import LabelEncoder , StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn import metrics from sklearn.metrics import r2_score , mean_squared_error from sklearn.datasets import load_iris 3.0.2 Load Dataset \u00b6 back to top In session 1 we cursorily discussed why we may need strategies for validating our model. Here we'll discuss it more in depth. I'm going to take a simple example. In the following, I have a dataset that contains some data about flowers. It's a very famous dataset used in the ML world iris = load_iris () X = iris . data y = iris . target print ( X . shape ) print ( y . shape ) (150, 4) (150,) let's go ahead and load our wine dataset as well... wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) wine . dropna ( inplace = True ) wine [ 'quality_label' ] = wine [ 'quality' ] . apply ( lambda x : 'low' if x <= 5 else 'med' if x <= 7 else 'high' ) class_tp = { 'red' : 0 , 'white' : 1 } y_tp = wine [ 'type' ] . map ( class_tp ) wine [ 'type_encoding' ] = y_tp class_ql = { 'low' : 0 , 'med' : 1 , 'high' : 2 } y_ql = wine [ 'quality_label' ] . map ( class_ql ) wine [ 'quality_encoding' ] = y_ql wine . drop ([ 'type' , 'quality_label' , 'quality' ], axis = 1 , inplace = True ) wine . columns = wine . columns . str . replace ( ' ' , '_' ) wine . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide density pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1 3.1 Model Validation \u00b6 back to top doing it the wrong way While we're here, I'm going to introduce a VERY SIMPLE supervised learning method called K-Nearest Neighbors. 3.1.0 K-Nearest Neighbors \u00b6 back to top K-Nearest Neighbors is perhaps the simplest algorithm of them all. It is essentially a lookup table: We select the hyperparameter K, and when assigning a new value a data label, assign it according to, the majority label in the vicinity of the new datapoint. The vicinity being determined by K, the number of nearest neighbors we are going to assess. knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X , y ) KNeighborsRegressor(n_neighbors=1) knn . score ( X , y ) 1.0 Wow! we achieved a model with a perfect score! But is this really how we would expect the model to perform on data it had never seen before? Probably not. How do we actually check the performance of our model? 3.1.1 Holdout Sets \u00b6 back to top The way we account for unseen data, in practice, is to leave a portion of the dataset out for testing. This way, we can estimate how our model will perform on entirely new data it may come across in application. train_size = 0.6 X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = train_size , random_state = 42 ) knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X_train , y_train ) print ( knn . score ( X_test , y_test )) 0.9753593429158111 We see that we get a more reasonable value for our performance! 3.1.2 Data Leakage and Cross-Validation \u00b6 back to top An even more rigorous method to leaving out a single test set, is to perform cross validation. Imagine a situation where we are trying to estimate the best value of K in our KNN algorithm. If we continually train our model with new values of K on our training set, and test with our testing set, \"knowledge\" of our test set values with leak into our model, as we choose the best value for K based on how it performs on our test set (even though we did not train on this test set). We call this phenomenon data leakage . CV or Cross Validation overcomes this by only evaluating our parameters with our training set. img src In this scheme, we don't evaluate our model on the test set until the very end. Rather, we estimate our hyperparameter performances by slicing the training set into cross folds img src from sklearn.model_selection import cross_val_score scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) scores array([0.91666667, 0.81725888, 0.85714286, 1. , 0.91 ]) print ( \" %0.2f accuracy with a standard deviation of %0.3f \" % ( scores . mean (), scores . std ())) 0.90 accuracy with a standard deviation of 0.062 More information on the cross_val_score method in sklearn can be found here An additional topic on cross validation is the extreme leave-one-out validation, you can read more about that here Taking these building blocks, if we wanted to for example optimize the number of nearest neighbors without incurring data leakage. We might take the following approach: for k in range ( 1 , 10 ): knn = KNeighborsRegressor ( n_neighbors = k ) scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) print ( \"k= %0.0f , %0.3f accuracy with a standard deviation of %0.3f \" % ( k , scores . mean (), scores . std ())) k=1, 0.900 accuracy with a standard deviation of 0.062 k=2, 0.925 accuracy with a standard deviation of 0.071 k=3, 0.916 accuracy with a standard deviation of 0.077 k=4, 0.932 accuracy with a standard deviation of 0.046 k=5, 0.928 accuracy with a standard deviation of 0.045 k=6, 0.924 accuracy with a standard deviation of 0.047 k=7, 0.929 accuracy with a standard deviation of 0.041 k=8, 0.924 accuracy with a standard deviation of 0.047 k=9, 0.923 accuracy with a standard deviation of 0.042 k = 4 # select best hyperparameters knn = KNeighborsRegressor ( n_neighbors = k ) knn . fit ( X_train , y_train ) # final train print ( knn . score ( X_test , y_test )) 0.9845995893223819 3.1.3 Bias-Variance Tradeoff \u00b6 back to top This next concept will be most easily understood if we go ahead an make up some data ourselves, I'm going to do that now. # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) Text(0, 0.5, 'Y') Let's fit to just a portion of this data random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] plt . plot ( X_train , y_train , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f03e3ce5730>] Now let's take two extreme scenarios: fitting to these datapoints a linear line and a high order polynomial. Keeping in mind the larger dataset (the population) as well as the (irreducible) error we introduced in our data generating function will really illustrate our point! # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) # plot linear model ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) # plot polynomial model ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has high bias because we are forcing the functional form without much consideration to the underlying data \u2014 we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has low variance with respect to the underlying data. On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has low bias because we don't introduce many constraints on the final form of the model. it is high variance because depending on the underlying training data, the final outcome of the model can change quite drastically! In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further: what happens when we retrain these models on different samples of the data population and let's use this to better understand what we mean by bias and variance what happens when we tie this back in with the irreducible error we introduced to the data generator? and let's use this to better understand irreducible error # we're going to perform this task again, but this time for 5 rounds, training # on different samples of the population data random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) for samples in range ( 5 ): X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) As we can see, depending on what data we train our model on, the high bias model changes relatively slightly, while the high variance model changes a whole awful lot! The high variance model is prone to something we call overfitting . It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the population : # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance models are particuarly prone to the phenomenon of over fitting and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this. x = np . arange ( 20 , 100 ) y = [ func ( t , err = 0 ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f03e399ab50>] random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') This time, our high variance model really gets it ! And this is because the data we trained on actually is a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the bias variance tradeoff \ud83c\udfcb\ufe0f Exercise 1: Quantitatively Define Performance \u00b6 Up until now, we've explored this idea of bias variance tradeoff from a qualitative standpoint. As an exercise, continue with this idea, this time calculating the mean squared error (MSE) and R-square between the model and UNSEEN (non-training data) population data. error metrics r2_score mean_squared_error Do this for a 9 th order polynomial and repeat for population data with low, med, and high degrees of error and small and large training fractions (for 6 total comparisons). Complete the chart below. error training fraction MSE R2 # Code Cell for Exercise 1 from sklearn.preprocessing import PolynomialFeatures random . seed ( 42 ) # function to generate data def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) ################################################################################ ########## CHANGE ERR TO CHANGE THE AMOUNT OF NOISE IN YOUR DATA ############### ################################################################################ # err = <YOUR ERR> # change the error (.1 - 0.9) y_actual = [ func ( t , err ) for t in x ] ################################################################################ ############# CHANGE TRAIN_SIZE TO SAMPLE THE DATA FOR TRAINING ################ ################################################################################ # train_size=<YOUR NUMBER>, # change the training size x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = train_size , random_state = 42 ) # solving our training data with a 9-degree polynomial coefs = np . polyfit ( x_train , y_train , 9 ) # generate y data with 9-degree polynomial model and X_seq y_pred = np . polyval ( coefs , x_test ) ################################################################################ ############################## CALCULATE MSE AND R2 ############################ ################################################################################ mse = mean_squared_error ( y_test , y_pred ) r2 = r2_score ( y_test , y_pred ) # look at results print ( \"mean square error: {:.2f} \" . format ( mse )) print ( \"r2: {:.2f} \" . format ( r2 )) mean square error: 0.08 r2: 0.63 3.1.4 Learning Curves \u00b6 back to top To move from qualitative to quantitative understanding of bias-variance tradeoff we need to introduce some metric for model performance. A good one to use here is R-square, a measure of the degree to which predictions match actual values. We can import a tool from sklearn to calculate this for us. from sklearn.metrics import r2_score 3.1.4.1 Considering Model Complexity \u00b6 back to top In a learning curve, we will typically plot the training and testing scores together, to give a sense of when we have either too much bias or too much variance in our model. I'm going to go ahead and recreate the original data distribution we introduced in 1.1.3 # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f03e387c7f0>] Now let's itteratively introduce more complexity into our model random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score = r2_score ( np . polyval ( coefs , X_train ), y_train ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score , ls = '' , marker = '.' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f03e37859a0> As we see from both plots, the score on the training data increases with added model complexity. Giving us the expected perfect fit when the order is the same as the number of data points! This is part I of our learning curve. Part II consists of plotting the training data score with the testing data score. Something else I'm going to do, is define the training portion of the data as a fraction of the overall dataset . This is a typical practice in machine learning. To keep the comparisons the same as up until now, I will keep this training fraction low at .2 random . seed ( 42 ) # defining my training fraction training_frac = .2 # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) train_indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in train_indices ] test_indices = [ i for i in range ( len ( x )) if i not in train_indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) ax [ 0 ] . plot ( X_test , y_test , ls = '' , marker = '.' , color = 'grey' , alpha = 0.5 ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score1 )) ax [ 1 ] . plot ( complexity , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend ([ 'Train $R^2$' , 'Test $R^2$' ]) ax [ 0 ] . legend () <matplotlib.legend.Legend at 0x7f03e36458e0> As we can see, The 2 nd order polynomial achieves the greatest best test set data \\(R^2\\), while the highest order polynomial achieves the best training set data \\(R^2\\). This learning curve is explanative of what we see generally, namely a divergence after some degree of complexity between training and test set performances. In this case, we would resolve to choose the 2 nd order polynomial as the best model for our data. img src 3.1.4.2 Considering Training Set Size \u00b6 back to top The last piece of the puzzle we require, to fully cover learning curves, is the effect of training data size on the model. Let's explore. random . seed ( 42 ) # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .9 , 50 ): # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax . plot ( training_frac , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax . plot ( training_frac , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax . set_title ( \"9th-order Polynomial Score with Increasing Training Set Size\" ) ax . legend ([ 'Train' , 'Test' ]) ax . set_xlabel ( 'Training Fraction' ) ax . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$') What we see here is a trend that happens generally, as our amount of training data increases, our models handle more complexity. This is because the more data we have, the more we are able to dicipher noise from signal. Our model will ignore noise as it does not generalize well across the other datapoints. Instead, it will will fit to relationships that appear across the entire training dataset. img src \ud83c\udfcb\ufe0f Exercise 2: Visualization \u00b6 Starting with the code below, make a side-by-side plot of a 3 rd degree polynomial and a 9 th degree polynomial. On the x axis slowly increase the training set size, on the y axis plot the scores for the training and test sets. # Code Cell for Exercise 2 random . seed ( 42 ) # create the figure and axes fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .95 , 50 ): # create test and training data x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = training_frac , random_state = 42 ) # solving our training data with a n-degree polynomial coefs = np . polyfit ( x_train , y_train , 9 ) # recording the scores for the training and test sets score1 = r2_score ( np . polyval ( coefs , x_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , x_test ), y_test ) ax . plot ( training_frac , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax . plot ( training_frac , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax . set_title ( \"9th-order Polynomial Score\" ) ax . legend ([ 'Train' , 'Test' ]) ax . set_xlabel ( 'Training Fraction' ) ax . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$') \ud83d\ude4b\u200d\u2640\ufe0f Question 1: In what regions of the plots are we overfitting? \u00b6 Where in these plots is overfitting occuring? Why is it different for each polynomial? 3.2 Model Validation in Practice \u00b6 back to top We will now turn our attention to practical implementation. In practice, there are a wide number of variables (called hyperparameters) to consider when choosing a model. Scikit learn has a useful method called Grid Search that will iterate through every possible combination of a range of hyperparameter settings you provide as input. Before we get started with grid search, we'll need to switch over from our numpy polynomial fit method to one in sklearn. Here, the caveat is our actual model will solve for the coefficients infront of the polynomials. We will engineer the polynomial features ourselves. This is an example of feature engineering which we will revisit in depth in the next session. from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) 3.2.1 Grid Search \u00b6 back to top from sklearn.model_selection import GridSearchCV param_grid = { 'polynomialfeatures__degree' : np . arange ( 10 ), 'linearregression__fit_intercept' : [ True , False ]} grid = GridSearchCV ( PolynomialRegression (), param_grid , cv = 7 ) # create test and training data random . seed ( 42 ) X_train = random . sample ( list ( x ), int ( int ( len ( x )) * .8 )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] grid . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) GridSearchCV(cv=7, estimator=Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()), ('linearregression', LinearRegression())]), param_grid={'linearregression__fit_intercept': [True, False], 'polynomialfeatures__degree': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}) grid . best_params_ {'linearregression__fit_intercept': True, 'polynomialfeatures__degree': 3} to grab the best model from the CV/search outcome. we use grid.best_estimator model = grid . best_estimator_ # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 5 )) ax . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax . plot ( X_train , y_train , ls = '' , marker = '.' ) ax . set_ylim ( min ( y ), max ( y )) ax . set_title ( \"Best Grid Search CV Model\" ) Text(0.5, 1.0, 'Best Grid Search CV Model') \ud83c\udfcb\ufe0f Exercise 3: Grid Search \u00b6 There are two objectives for this exercise: Complete the object param_grid by reading the parameters below in StandardScaler . We would like to include in our grid search whether or not to scale our input features so that they are centered at 0 ( with_mean ) and have unit variance ( with_std ) After you have completed the grid search, create a side-by-side plot of actual versus predicted values for training data (left plot) and testing data (right plot) Include in your plot title the R2 for testing and R2 for training df = wine . copy () y = df . pop ( 'density' ) X = df display ( X . head ()) display ( y . head ()) print ( X . shape ) print ( y . shape ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) def NormalizedRegression ( ** kwargs ): return make_pipeline ( StandardScaler (), LinearRegression ( ** kwargs )) ###################################################################################################### ####################### YOUR EXTRA PARAMETERS GO IN THE DICTIONARY BELOW ############################# ###################################################################################################### param_grid = { 'linearregression__fit_intercept' : [ True , False ]} grid = GridSearchCV ( NormalizedRegression (), param_grid , cv = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 0 1.0010 1 0.9940 2 0.9951 3 0.9956 4 0.9956 Name: density, dtype: float64 (6463, 12) (6463,) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) {'linearregression__fit_intercept': True} model = grid . best_estimator_ ###################################################################################################### ##################################### ADJUST THE PLOT LOGIC BELOW #################################### ###################################################################################################### fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Best Grid Search CV Model\" ) Text(0.5, 1.0, 'Best Grid Search CV Model') References \u00b6 back to top Model Validation \u00b6 cross_val_score leave-one-out","title":"Model Selection and Validation"},{"location":"S3_Model_Selection_and_Validation/#data-science-foundations-session-3-model-selection-and-validation","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com At the end of session 1, we saw the basic recipe for creating a supervised machine learning model: Environment setup and importing data Rudimentary exploratory data analysis Feature engineering (these were created for us) Choosing and training a model: choose model choose hyperparameters fit using (training) data predict using (validation) data In session 1, I chose our model and hyperparameters preemptively. How did I do that? In the real world, you won't necessarily have the best intution about how to make these choices. In today's session, we will algorithmize the way we approach choosing and training a model Note: I will import libraries at the beginning of this notebook, as is good practice, but will reimport them as they are used to remind ourselves where each method came from!","title":"Data Science Foundations  Session 3: Model Selection and Validation"},{"location":"S3_Model_Selection_and_Validation/#30-preparing-environment-and-importing-data","text":"back to top","title":"3.0 Preparing Environment and Importing Data"},{"location":"S3_Model_Selection_and_Validation/#301-import-packages","text":"back to top # Pandas library for the pandas dataframes import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import seaborn as sns import random import scipy.stats as stats from patsy import dmatrices from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import LabelEncoder , StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn import metrics from sklearn.metrics import r2_score , mean_squared_error from sklearn.datasets import load_iris","title":"3.0.1 Import Packages"},{"location":"S3_Model_Selection_and_Validation/#302-load-dataset","text":"back to top In session 1 we cursorily discussed why we may need strategies for validating our model. Here we'll discuss it more in depth. I'm going to take a simple example. In the following, I have a dataset that contains some data about flowers. It's a very famous dataset used in the ML world iris = load_iris () X = iris . data y = iris . target print ( X . shape ) print ( y . shape ) (150, 4) (150,) let's go ahead and load our wine dataset as well... wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) wine . dropna ( inplace = True ) wine [ 'quality_label' ] = wine [ 'quality' ] . apply ( lambda x : 'low' if x <= 5 else 'med' if x <= 7 else 'high' ) class_tp = { 'red' : 0 , 'white' : 1 } y_tp = wine [ 'type' ] . map ( class_tp ) wine [ 'type_encoding' ] = y_tp class_ql = { 'low' : 0 , 'med' : 1 , 'high' : 2 } y_ql = wine [ 'quality_label' ] . map ( class_ql ) wine [ 'quality_encoding' ] = y_ql wine . drop ([ 'type' , 'quality_label' , 'quality' ], axis = 1 , inplace = True ) wine . columns = wine . columns . str . replace ( ' ' , '_' ) wine . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide density pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1","title":"3.0.2 Load Dataset"},{"location":"S3_Model_Selection_and_Validation/#31-model-validation","text":"back to top doing it the wrong way While we're here, I'm going to introduce a VERY SIMPLE supervised learning method called K-Nearest Neighbors.","title":"3.1 Model Validation"},{"location":"S3_Model_Selection_and_Validation/#310-k-nearest-neighbors","text":"back to top K-Nearest Neighbors is perhaps the simplest algorithm of them all. It is essentially a lookup table: We select the hyperparameter K, and when assigning a new value a data label, assign it according to, the majority label in the vicinity of the new datapoint. The vicinity being determined by K, the number of nearest neighbors we are going to assess. knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X , y ) KNeighborsRegressor(n_neighbors=1) knn . score ( X , y ) 1.0 Wow! we achieved a model with a perfect score! But is this really how we would expect the model to perform on data it had never seen before? Probably not. How do we actually check the performance of our model?","title":"3.1.0 K-Nearest Neighbors"},{"location":"S3_Model_Selection_and_Validation/#311-holdout-sets","text":"back to top The way we account for unseen data, in practice, is to leave a portion of the dataset out for testing. This way, we can estimate how our model will perform on entirely new data it may come across in application. train_size = 0.6 X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = train_size , random_state = 42 ) knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X_train , y_train ) print ( knn . score ( X_test , y_test )) 0.9753593429158111 We see that we get a more reasonable value for our performance!","title":"3.1.1 Holdout Sets"},{"location":"S3_Model_Selection_and_Validation/#312-data-leakage-and-cross-validation","text":"back to top An even more rigorous method to leaving out a single test set, is to perform cross validation. Imagine a situation where we are trying to estimate the best value of K in our KNN algorithm. If we continually train our model with new values of K on our training set, and test with our testing set, \"knowledge\" of our test set values with leak into our model, as we choose the best value for K based on how it performs on our test set (even though we did not train on this test set). We call this phenomenon data leakage . CV or Cross Validation overcomes this by only evaluating our parameters with our training set. img src In this scheme, we don't evaluate our model on the test set until the very end. Rather, we estimate our hyperparameter performances by slicing the training set into cross folds img src from sklearn.model_selection import cross_val_score scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) scores array([0.91666667, 0.81725888, 0.85714286, 1. , 0.91 ]) print ( \" %0.2f accuracy with a standard deviation of %0.3f \" % ( scores . mean (), scores . std ())) 0.90 accuracy with a standard deviation of 0.062 More information on the cross_val_score method in sklearn can be found here An additional topic on cross validation is the extreme leave-one-out validation, you can read more about that here Taking these building blocks, if we wanted to for example optimize the number of nearest neighbors without incurring data leakage. We might take the following approach: for k in range ( 1 , 10 ): knn = KNeighborsRegressor ( n_neighbors = k ) scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) print ( \"k= %0.0f , %0.3f accuracy with a standard deviation of %0.3f \" % ( k , scores . mean (), scores . std ())) k=1, 0.900 accuracy with a standard deviation of 0.062 k=2, 0.925 accuracy with a standard deviation of 0.071 k=3, 0.916 accuracy with a standard deviation of 0.077 k=4, 0.932 accuracy with a standard deviation of 0.046 k=5, 0.928 accuracy with a standard deviation of 0.045 k=6, 0.924 accuracy with a standard deviation of 0.047 k=7, 0.929 accuracy with a standard deviation of 0.041 k=8, 0.924 accuracy with a standard deviation of 0.047 k=9, 0.923 accuracy with a standard deviation of 0.042 k = 4 # select best hyperparameters knn = KNeighborsRegressor ( n_neighbors = k ) knn . fit ( X_train , y_train ) # final train print ( knn . score ( X_test , y_test )) 0.9845995893223819","title":"3.1.2 Data Leakage and Cross-Validation"},{"location":"S3_Model_Selection_and_Validation/#313-bias-variance-tradeoff","text":"back to top This next concept will be most easily understood if we go ahead an make up some data ourselves, I'm going to do that now. # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) Text(0, 0.5, 'Y') Let's fit to just a portion of this data random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] plt . plot ( X_train , y_train , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f03e3ce5730>] Now let's take two extreme scenarios: fitting to these datapoints a linear line and a high order polynomial. Keeping in mind the larger dataset (the population) as well as the (irreducible) error we introduced in our data generating function will really illustrate our point! # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) # plot linear model ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) # plot polynomial model ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has high bias because we are forcing the functional form without much consideration to the underlying data \u2014 we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has low variance with respect to the underlying data. On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has low bias because we don't introduce many constraints on the final form of the model. it is high variance because depending on the underlying training data, the final outcome of the model can change quite drastically! In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further: what happens when we retrain these models on different samples of the data population and let's use this to better understand what we mean by bias and variance what happens when we tie this back in with the irreducible error we introduced to the data generator? and let's use this to better understand irreducible error # we're going to perform this task again, but this time for 5 rounds, training # on different samples of the population data random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) for samples in range ( 5 ): X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) As we can see, depending on what data we train our model on, the high bias model changes relatively slightly, while the high variance model changes a whole awful lot! The high variance model is prone to something we call overfitting . It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the population : # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance models are particuarly prone to the phenomenon of over fitting and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this. x = np . arange ( 20 , 100 ) y = [ func ( t , err = 0 ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f03e399ab50>] random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') This time, our high variance model really gets it ! And this is because the data we trained on actually is a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the bias variance tradeoff","title":"3.1.3 Bias-Variance Tradeoff"},{"location":"S3_Model_Selection_and_Validation/#exercise-1-quantitatively-define-performance","text":"Up until now, we've explored this idea of bias variance tradeoff from a qualitative standpoint. As an exercise, continue with this idea, this time calculating the mean squared error (MSE) and R-square between the model and UNSEEN (non-training data) population data. error metrics r2_score mean_squared_error Do this for a 9 th order polynomial and repeat for population data with low, med, and high degrees of error and small and large training fractions (for 6 total comparisons). Complete the chart below. error training fraction MSE R2 # Code Cell for Exercise 1 from sklearn.preprocessing import PolynomialFeatures random . seed ( 42 ) # function to generate data def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) ################################################################################ ########## CHANGE ERR TO CHANGE THE AMOUNT OF NOISE IN YOUR DATA ############### ################################################################################ # err = <YOUR ERR> # change the error (.1 - 0.9) y_actual = [ func ( t , err ) for t in x ] ################################################################################ ############# CHANGE TRAIN_SIZE TO SAMPLE THE DATA FOR TRAINING ################ ################################################################################ # train_size=<YOUR NUMBER>, # change the training size x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = train_size , random_state = 42 ) # solving our training data with a 9-degree polynomial coefs = np . polyfit ( x_train , y_train , 9 ) # generate y data with 9-degree polynomial model and X_seq y_pred = np . polyval ( coefs , x_test ) ################################################################################ ############################## CALCULATE MSE AND R2 ############################ ################################################################################ mse = mean_squared_error ( y_test , y_pred ) r2 = r2_score ( y_test , y_pred ) # look at results print ( \"mean square error: {:.2f} \" . format ( mse )) print ( \"r2: {:.2f} \" . format ( r2 )) mean square error: 0.08 r2: 0.63","title":"\ud83c\udfcb\ufe0f Exercise 1: Quantitatively Define Performance"},{"location":"S3_Model_Selection_and_Validation/#314-learning-curves","text":"back to top To move from qualitative to quantitative understanding of bias-variance tradeoff we need to introduce some metric for model performance. A good one to use here is R-square, a measure of the degree to which predictions match actual values. We can import a tool from sklearn to calculate this for us. from sklearn.metrics import r2_score","title":"3.1.4 Learning Curves"},{"location":"S3_Model_Selection_and_Validation/#3141-considering-model-complexity","text":"back to top In a learning curve, we will typically plot the training and testing scores together, to give a sense of when we have either too much bias or too much variance in our model. I'm going to go ahead and recreate the original data distribution we introduced in 1.1.3 # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f03e387c7f0>] Now let's itteratively introduce more complexity into our model random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score = r2_score ( np . polyval ( coefs , X_train ), y_train ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score , ls = '' , marker = '.' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f03e37859a0> As we see from both plots, the score on the training data increases with added model complexity. Giving us the expected perfect fit when the order is the same as the number of data points! This is part I of our learning curve. Part II consists of plotting the training data score with the testing data score. Something else I'm going to do, is define the training portion of the data as a fraction of the overall dataset . This is a typical practice in machine learning. To keep the comparisons the same as up until now, I will keep this training fraction low at .2 random . seed ( 42 ) # defining my training fraction training_frac = .2 # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) train_indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in train_indices ] test_indices = [ i for i in range ( len ( x )) if i not in train_indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) ax [ 0 ] . plot ( X_test , y_test , ls = '' , marker = '.' , color = 'grey' , alpha = 0.5 ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score1 )) ax [ 1 ] . plot ( complexity , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend ([ 'Train $R^2$' , 'Test $R^2$' ]) ax [ 0 ] . legend () <matplotlib.legend.Legend at 0x7f03e36458e0> As we can see, The 2 nd order polynomial achieves the greatest best test set data \\(R^2\\), while the highest order polynomial achieves the best training set data \\(R^2\\). This learning curve is explanative of what we see generally, namely a divergence after some degree of complexity between training and test set performances. In this case, we would resolve to choose the 2 nd order polynomial as the best model for our data. img src","title":"3.1.4.1 Considering Model Complexity"},{"location":"S3_Model_Selection_and_Validation/#3142-considering-training-set-size","text":"back to top The last piece of the puzzle we require, to fully cover learning curves, is the effect of training data size on the model. Let's explore. random . seed ( 42 ) # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .9 , 50 ): # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax . plot ( training_frac , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax . plot ( training_frac , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax . set_title ( \"9th-order Polynomial Score with Increasing Training Set Size\" ) ax . legend ([ 'Train' , 'Test' ]) ax . set_xlabel ( 'Training Fraction' ) ax . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$') What we see here is a trend that happens generally, as our amount of training data increases, our models handle more complexity. This is because the more data we have, the more we are able to dicipher noise from signal. Our model will ignore noise as it does not generalize well across the other datapoints. Instead, it will will fit to relationships that appear across the entire training dataset. img src","title":"3.1.4.2 Considering Training Set Size"},{"location":"S3_Model_Selection_and_Validation/#exercise-2-visualization","text":"Starting with the code below, make a side-by-side plot of a 3 rd degree polynomial and a 9 th degree polynomial. On the x axis slowly increase the training set size, on the y axis plot the scores for the training and test sets. # Code Cell for Exercise 2 random . seed ( 42 ) # create the figure and axes fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .95 , 50 ): # create test and training data x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = training_frac , random_state = 42 ) # solving our training data with a n-degree polynomial coefs = np . polyfit ( x_train , y_train , 9 ) # recording the scores for the training and test sets score1 = r2_score ( np . polyval ( coefs , x_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , x_test ), y_test ) ax . plot ( training_frac , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax . plot ( training_frac , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax . set_title ( \"9th-order Polynomial Score\" ) ax . legend ([ 'Train' , 'Test' ]) ax . set_xlabel ( 'Training Fraction' ) ax . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$')","title":"\ud83c\udfcb\ufe0f Exercise 2: Visualization"},{"location":"S3_Model_Selection_and_Validation/#question-1-in-what-regions-of-the-plots-are-we-overfitting","text":"Where in these plots is overfitting occuring? Why is it different for each polynomial?","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: In what regions of the plots are we overfitting?"},{"location":"S3_Model_Selection_and_Validation/#32-model-validation-in-practice","text":"back to top We will now turn our attention to practical implementation. In practice, there are a wide number of variables (called hyperparameters) to consider when choosing a model. Scikit learn has a useful method called Grid Search that will iterate through every possible combination of a range of hyperparameter settings you provide as input. Before we get started with grid search, we'll need to switch over from our numpy polynomial fit method to one in sklearn. Here, the caveat is our actual model will solve for the coefficients infront of the polynomials. We will engineer the polynomial features ourselves. This is an example of feature engineering which we will revisit in depth in the next session. from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs ))","title":"3.2 Model Validation in Practice"},{"location":"S3_Model_Selection_and_Validation/#321-grid-search","text":"back to top from sklearn.model_selection import GridSearchCV param_grid = { 'polynomialfeatures__degree' : np . arange ( 10 ), 'linearregression__fit_intercept' : [ True , False ]} grid = GridSearchCV ( PolynomialRegression (), param_grid , cv = 7 ) # create test and training data random . seed ( 42 ) X_train = random . sample ( list ( x ), int ( int ( len ( x )) * .8 )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] grid . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) GridSearchCV(cv=7, estimator=Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()), ('linearregression', LinearRegression())]), param_grid={'linearregression__fit_intercept': [True, False], 'polynomialfeatures__degree': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}) grid . best_params_ {'linearregression__fit_intercept': True, 'polynomialfeatures__degree': 3} to grab the best model from the CV/search outcome. we use grid.best_estimator model = grid . best_estimator_ # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 5 )) ax . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax . plot ( X_train , y_train , ls = '' , marker = '.' ) ax . set_ylim ( min ( y ), max ( y )) ax . set_title ( \"Best Grid Search CV Model\" ) Text(0.5, 1.0, 'Best Grid Search CV Model')","title":"3.2.1 Grid Search"},{"location":"S3_Model_Selection_and_Validation/#exercise-3-grid-search","text":"There are two objectives for this exercise: Complete the object param_grid by reading the parameters below in StandardScaler . We would like to include in our grid search whether or not to scale our input features so that they are centered at 0 ( with_mean ) and have unit variance ( with_std ) After you have completed the grid search, create a side-by-side plot of actual versus predicted values for training data (left plot) and testing data (right plot) Include in your plot title the R2 for testing and R2 for training df = wine . copy () y = df . pop ( 'density' ) X = df display ( X . head ()) display ( y . head ()) print ( X . shape ) print ( y . shape ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) def NormalizedRegression ( ** kwargs ): return make_pipeline ( StandardScaler (), LinearRegression ( ** kwargs )) ###################################################################################################### ####################### YOUR EXTRA PARAMETERS GO IN THE DICTIONARY BELOW ############################# ###################################################################################################### param_grid = { 'linearregression__fit_intercept' : [ True , False ]} grid = GridSearchCV ( NormalizedRegression (), param_grid , cv = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 0 1.0010 1 0.9940 2 0.9951 3 0.9956 4 0.9956 Name: density, dtype: float64 (6463, 12) (6463,) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) {'linearregression__fit_intercept': True} model = grid . best_estimator_ ###################################################################################################### ##################################### ADJUST THE PLOT LOGIC BELOW #################################### ###################################################################################################### fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Best Grid Search CV Model\" ) Text(0.5, 1.0, 'Best Grid Search CV Model')","title":"\ud83c\udfcb\ufe0f Exercise 3: Grid Search"},{"location":"S3_Model_Selection_and_Validation/#references","text":"back to top","title":"References"},{"location":"S3_Model_Selection_and_Validation/#model-validation","text":"cross_val_score leave-one-out","title":"Model Validation"},{"location":"S4_Feature_Engineering/","text":"Data Science Foundations Session 4: Feature Engineering \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we talked about model pipelines and conveniently began with a suitable set of input data. In the real world, this is hardly ever the case! What is constant is this: at the end of the day, our models need numbers. Not only this, but a suitable set of numbers. What does that mean? The answer to that question is the subject of our session today. 4.0 Preparing Environment and Importing Data \u00b6 back to top 4.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats as stats from scipy.stats import gamma from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor import seaborn as sns ; sns . set () from sklearn.datasets import load_iris from sklearn.metrics import mean_squared_error , r2_score 4.0.2 Load Dataset \u00b6 back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) orders = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_orders.csv' ) time_cols = [ i for i in orders . columns if '/' in i ] margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 dfcat = margin . columns [: - 2 ] dfcat Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') orders . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer 1/2020 2/2020 3/2020 4/2020 5/2020 6/2020 7/2020 8/2020 9/2020 10/2020 11/2020 12/2020 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 4.1 Categorical Features \u00b6 back to top At the end of the day, our algorithms operate on numerical values. How do you get from a series of string values to numerical values? margin [ 'Customer' ] . unique () array(['Slugworth', 'Perk-a-Cola', 'Fickelgruber', 'Zebrabar', \"Dandy's Candies\"], dtype=object) A naive way to do it would be to assign a number to every entry 'Slugworth' = 1 'Perk-a-Cola' = 2 'Dandy's Candies' = 3 but we would inadvertently end up with some weird mathematical relationships between these variables, e.g. Dandy's Candies - Perk-a-Cola = Slugworth (3 - 2 = 1). A work around for this is to think multi-dimensionally we express our categorical values as vectors in a hyperspace where they cannot be expressed in terms of one another, i.e. they are orthogonal 'Slugworth' = [1,0,0] 'Perk-a-Cola' = [0,1,0] 'Dandy's Candies' = [0,0,1] such a scheme, in machine learning vernacular, is termed one-hot encoding. 4.1.1 One-Hot Encoding \u00b6 back to top sklearn has a couple useful libraries for one-hot encoding. let's start with the OneHotEncoder class in its preprocessing library from sklearn.preprocessing import OneHotEncoder # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ 'Customer' ] . values . reshape ( - 1 , 1 ) # fit our encoder to this data enc . fit ( X_cat ) OneHotEncoder() After fitting our encoder, we can then use this object to create our training array. # as a reference here's our original data display ( X_cat [: 10 ]) print ( X_cat . shape , end = ' \\n\\n ' ) onehotlabels = enc . transform ( X_cat ) . toarray () print ( onehotlabels . shape , end = ' \\n\\n ' ) # And here is our new data onehotlabels [: 10 ] array([['Slugworth'], ['Slugworth'], ['Perk-a-Cola'], ['Fickelgruber'], ['Fickelgruber'], ['Fickelgruber'], ['Slugworth'], ['Zebrabar'], ['Slugworth'], ['Zebrabar']], dtype=object) (1668, 1) (1668, 5) array([[0., 0., 0., 1., 0.], [0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]]) We have our customer information one-hot encoded, we need to do this for all our variables and concatenate them with our regular numerical variables in our original dataframe. # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ dfcat ] . values # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [ \"KG\" ] print ( X_num . shape ) X = np . concatenate (( onehotlabels , X_num . values . reshape ( - 1 , 1 )), axis = 1 ) X . shape (1668,) (1668, 119) And now we grab our EBITDA (margin) data for prediction y = margin [ \"EBITDA/KG\" ] \ud83c\udfcb\ufe0f Exercise 1: Create a simple linear model \u00b6 Using the X and y sets, use train_test_split and LinearRegression to make a baseline model based on what we've learned so far. Assess your model performance visually by plottying y_test vs y_test_pred # Cell for Exercise 1 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression [<matplotlib.lines.Line2D at 0x7fa4b01f90d0>] \ud83d\ude4b Question 1: \u00b6 How can we assess the relative feature importance of the features in our model? We could be tempted to inspect the coefficients ( linear.coef_ ) of our model to evaluate the relative feature importance, but in order to do this our features need to be scaled (so that the relative coefficient sizes are meaningful). What other issues might there be (think categorical vs continuous variables). 4.2 Derived Features \u00b6 back to top Can we recall an example of where we've seen this previously? That's right earlier on in our session on model selection and validation we derived some polynomial features to create our polynomial model using the linear regression class in sklearn. We actually see this a lot in engineering, where we will describe log relationships or some other transformation of the original variable. Actually let me see if I can find an example in my handy BSL... concentration profiles in continous stirred tank vs plug flow reactors. Notice the y-axis is log scale. Thanks Bird, Stewart, Lightfoot! Can we think of other examples where we would like to derive features from our input data? 4.2.1 Creating Polynomials \u00b6 back to top Let's revisit our example from the previous session, right before we introduced Grid Search in sklearn # from Model Selection and Validation, 1.2.1 from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) in the above, we use sklearn's convenient tool, make_pipeline to join together the preprocessing tool PolynomialFeatures and the basic model LinearRegression . Let's take a look at what PolynomialFeatures does to some simple data x = np . arange ( 1 , 11 ) y = x ** 3 print ( x ) print ( y ) [ 1 2 3 4 5 6 7 8 9 10] [ 1 8 27 64 125 216 343 512 729 1000] features = PolynomialFeatures ( degree = 3 ) X2 = features . fit_transform ( x . reshape ( - 1 , 1 )) we see our new feature set contains our original features, plus new features up to the nth-degree polynomial we set when creating the features object from PolynomialFeatures print ( X2 ) [[ 1. 1. 1. 1.] [ 1. 2. 4. 8.] [ 1. 3. 9. 27.] [ 1. 4. 16. 64.] [ 1. 5. 25. 125.] [ 1. 6. 36. 216.] [ 1. 7. 49. 343.] [ 1. 8. 64. 512.] [ 1. 9. 81. 729.] [ 1. 10. 100. 1000.]] model = LinearRegression () . fit ( X2 , y ) yhat = model . predict ( X2 ) plt . scatter ( x , y ) plt . plot ( x , yhat ); 4.2.2 Dealing with Time Series \u00b6 back to top Often, we will be dealing with time series data, whether its data generated by machinery, reactors, or sales and customers. In the following we discuss some simple practices for dealing with time series data. \ud83c\udf52 4.2.2.1 Enrichment : Fast Fourier Transform \u00b6 Special thanks to Brian Gerwe for his contribution to this section \ud83d\udc68\u200d\ud83c\udf73 back to top Sometimes we'll want to create a more sophisticated transformation of our input data. As engineers, this can often have to do with some empirical knowledge we understand about our process. When working with equipment and machinery, we will often want to convert a signal from the time to frequency domain. Let's cover how we can do that with numpy! img src What I've drawn here in the following is called a square-wave signal t = np . linspace ( 0 , 1 , 501 ) # FFT should be given an integer number of cycles so we leave out last sample t = t [: - 1 ] f = 5 # linear frequency in Hz w = f * 2 * np . pi # radial frequency h = 4 # height of square wave amp = 4 * h / np . pi s = amp * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) # here is the call to numpy FFT F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ], d = t [ 1 ]) # reorder frequency spectrum and frequency bins with 0 Hz at the center F = np . fft . fftshift ( F ) freq = np . fft . fftshift ( freq ) # scale frequency spectrum to correct amplitude F = F / t . size # amplitudes amps = [ max ( np . sin ( w * t )), max ( np . sin ( w * t * 3 ) / 3 ), max ( np . sin ( w * t * 5 ) / 5 )] fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) ax [ 0 ] . set_xlim ( 0 , 1 ) ax [ 0 ] . set_xlabel ( 'Time (s)' ) # tells us about the amplitude of the component at the # corresponding frequency. Multiplied by two because the # signal power is split between (-) and (+) frequency branches # of FFT, but we're only visualizing the (+) branch magnitude = 2 * np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , 30 ) ax [ 1 ] . set_xlabel ( 'Frequency (Hz)' ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain') 4.2.2.2 Rolling Windows \u00b6 back to top to see an example of this dataset in action visit this link One powerful technique for dealing with time series data, is to create a rolling window of features based on the historical data. The proper window size can usually be determined by trial and error, or constraints around access to the data itself. In the above gif, we have a window size of 7. What that means is for whatever time step units we are in (that could be minutes, days, months, etc.) we will have 7 of them included in a single instance or observation. This instance or observation is then interpreted by our model and used to assess the target value, typically the quantity in the very next time step after the window (the green bar in the gif). Let's take an example with the orders data tidy_orders = orders . melt ( id_vars = orders . columns [: 6 ], var_name = 'Date' , value_name = 'KG' ) display ( tidy_orders . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 In the next exercise, we are going to attempt to predict an order amount, based on the previous order history. We will scrub all categorical labels and only use historical amounts to inform our models. In effect the data that the model will see will look like the following: fig , ax = plt . subplots ( 3 , 2 , figsize = ( 10 , 20 )) indices = np . argwhere ( ax ) color_dict = { 0 : 'tab:blue' , 1 : 'tab:green' , 2 : 'tab:orange' , 3 : 'tab:red' , 4 : 'tab:pink' , 5 : 'tab:brown' } for index , customer in enumerate ( tidy_orders . Customer . unique ()): orders . loc [ orders . Customer == customer ] . iloc [:, 6 :] . reset_index () . T . plot ( c = color_dict [ index ], legend = False , ax = ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]]) ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]] . set_title ( customer ) What we may notice is that there is very little noise or drift in our order history, but there is certainly some periodicity. The question is can we use a linear model to predict the next order amount based on these history orders? def process_data ( Xy , time_cols = 12 , window = 3 , remove_null = False ): \"\"\" This function splits your time series data into the proper windows Parameters ---------- Xy: array The input data. If there are non-time series columns, assumes they are on the left and time columns are on the right. time_cols: int The number of time columns, default 12 window: int The time window size, default 3 Returns ------- X_: array The independent variables, includes time and non-time series columns with the new window y_: array The dependent variable, selected from the time columns at the end of the window labels: The time series labels, can be used in subsequent plot \"\"\" # separate the non-time series columns X_cat = Xy [:,: - time_cols ] # select the columns to apply the sweeping window X = Xy [:, - time_cols :] X_ = [] y = [] for i in range ( X . shape [ 1 ] - window ): # after attaching the current window to the non-time series # columns, add it to a growing list X_ . append ( np . concatenate (( X_cat , X [:, i : i + window ]), axis = 1 )) # add the next time delta after the window to the list of y # values y . append ( X [:, i + window ]) # X_ is 3D: [number of replicates from sweeping window, # length of input data, # size of new feature with categories and time] # we want to reshape X_ so that the replicates due to the sweeping window is # a part of the same dimension as the instances of the input data X_ = np . array ( X_ ) . reshape ( X . shape [ 0 ] * np . array ( X_ ) . shape [ 0 ], window + X_cat . shape [ 1 ]) y = np . array ( y ) . reshape ( X . shape [ 0 ] * np . array ( y ) . shape [ 0 ],) if remove_null : # remove training data where the target is 0 (may be unfair advantage) X_ = X_ [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] y = y [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] # create labels that show the previous month values used to train the model labels = [] for row in X_ : labels . append ( \"X: {} \" . format ( np . array2string ( row [ - window :] . astype ( float ) . round ()))) return X_ , y , labels \ud83c\udfcb\ufe0f Exercise 2: Optimize Rolling Window Size for Customer Forecasts \u00b6 For this exercise, you will use the process_data function below to help you optimize the window size for predicting the order quantity in any given month. You will train a LinearRegression model. create a model using a window size of 3 and predict the order quantity for the month immediately following the window create a model for window sizes 1-11 and report the \\(R^2\\) for each model # Cell for Exercise 2 kg_month_data = orders . values [:, 6 :] # use kg_month_data and the function process_data to create your X, y arrays # then use train_test_split to create train and test portions # USE y_test and y_pred for your actual and true test data # change only window parameter in process_data() print ( \"window R2\" ) for window in range ( 1 , 12 ): ###################### ### YOUR CODE HERE ### ###################### pass window R2 1, 0.97 2, 0.88 3, 0.80 4, 0.96 5, 0.90 6, 0.86 7, 0.61 8, 0.89 9, 0.76 10, 0.74 11, 0.87 #### RUN AFTER EXERCISE 2.2.2.2.1 #### fig = px . scatter ( x = y_test , y = y_pred , labels = { \"y\" : \"Prediction\" , \"x\" : \"Actual\" }) fig . update_layout ( autosize = False , width = 800 , height = 500 , title = 'R2: {:.3f} ' . format ( r2_score ( y_test , y_pred )) ) require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"239a6e0b-8482-49c8-9619-ebde564b91cd\")) { Plotly.newPlot( \"239a6e0b-8482-49c8-9619-ebde564b91cd\", [{\"hovertemplate\":\"Actual=%{x}<br>Prediction=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4528.380503144654,64.01384083044982,15617.489114658929,4528.380503144654,5698.392857142857,124348.89322916669,21939.111111111117,5712.33671988389,64983.42138364781,1483.5,93302.72435897436,1236.25,773.5849056603774,518.4642857142858,3798.928571428572,13671.296296296296,188.7692307692308,4057.832278481013,101149.25,14626.074074074077,12274.009146341465,141456.76315789475,4793.829296424452,74.78571428571429,10861.245674740485,86.24595469255664,19129.33333333333,39307.04113924051,74.78571428571429,46247.0,188.7692307692308,85.32258064516128,90677.41228070176,9306.611111111111,220.55016181229772,466477.57812500006,1158.125,220.55016181229772,1901.1290322580644,19129.33333333333,2637.8993710691825,1868.867924528302,3798.928571428572,2252.261437908497,5222.5,181.5552699228792,5038.387096774193,120.98615916955016,10146.898432174508,85.32258064516128,286.6666666666667,18613.222222222223,818.1891025641025,2499.201741654572,2010.9477124183009,6984.389273356401,42704.25889967639,11715.487421383648,39307.04113924051,440.859375,428.0,96.59546925566345,102.64285714285714,3569.0359477124184,13671.296296296296,773.5849056603774,2010.9477124183009,42704.25889967639,40608.41346153846,83347.75862068967,10796.31640058055,74.78571428571429,80801.72807017544,2540.0,5222.5,131.55526992287918,299024.0885416667,3882.680981595092,895.9082278481012,1180.5379746835442,15617.489114658929,85.32258064516128,6185.59509202454,16749.13157894737,47142.92857142857,3786.966463414634,936.280487804878,15244.632812500002,3272.80163599182,1271.904761904762,47142.92857142857,356.6666666666667,10861.245674740485,108.64285714285714,21438.1875,16918.132716049382,7892.142857142857,1901.1290322580644,2647.5,5038.387096774193,1677.5078125000002,39307.04113924051,51795.97953216374,916.3717948717948,41871.69230769231,12870.766853932584,1271.904761904762,181.5552699228792,211540.7575757576,3798.928571428572,2054.328561690525,3578.592162554427,2854.854368932039,51795.97953216374,11791.8,149.57142857142858,103.3653846153846,119015.43674698794,2499.201741654572,19286.94968553459,2185.3410740203194,19569.69230769231,12317.397660818711,6965.361445783132,86.24595469255664,3071.4691011235955,13867.125,50574.625,14626.074074074077,1005.4545454545454,16918.132716049382,2543.106741573034,10245.87593728698,8608.709677419354,150.3654970760234,3517.9333333333334,14626.074074074077,3177.0,2499.201741654572,345.6428571428572,10736.622807017544,13799.769319492503,12691.209677419354,11.175616835994196,2010.9477124183009,10736.622807017544,71138.51966292135,8104.40251572327,62.168396770472896,211540.7575757576,16569.261006289307,445.54838709677415,16918.132716049382,42.484177215189874,150.3654970760234,11.175616835994196,299024.0885416667,579.0625,10146.898432174508,181.5552699228792,494.57142857142856,411.1303344867359,2355.267295597484,494.57142857142856,79091.23475609756,10146.898432174508,1005.4545454545454,5038.387096774193,191.11111111111111,9304.668674698794,19569.69230769231,8250.491573033707,466477.57812500006,3048.0,1630.1966292134832,64.01384083044982,3569.0359477124184,1901.1290322580644,3272.80163599182,9772.200520833334,2185.3410740203194,28694.0,314.3203883495146,8104.40251572327,18557.572327044025,2231.547169811321,1432.2712418300653,773.5849056603774,40735.10802469136,314.3203883495146,471.91011235955057,42.484177215189874,9194.277108433736,8173.714285714285,837.8787878787879,1604.1437908496732,22.50980392156863,14.663461538461538,663176.1973875181,2854.854368932039,428.0,43.64516129032258,1526.2857142857142,1432.2712418300653,5456.578050443081,22331.935185185182,150.3654970760234,4057.832278481013,1868.867924528302,1630.1966292134832,4723.49129172714,5038.387096774193,19364.70552147239,117.22222222222224,15110.75,4057.832278481013,83347.75862068967,7892.142857142857,19129.33333333333,1968.8904494382025,207.39062500000003,62.168396770472896,3578.592162554427,1677.5078125000002,2499.201741654572,1254.4444444444443,1236.25,3578.592162554427,1992.4528301886792,14626.074074074077,1236.25,6965.361445783132,36.37096774193548,40735.10802469136,20.098039215686278,1432.2712418300653,538.8571428571429,101149.25,93302.72435897436,181.5552699228792,28694.0,2054.328561690525,5872.384615384615,31412.04644412192,5872.384615384615,854.952380952381,31412.04644412192,1253403.0130624091,63142.24137931035,20.098039215686278,3569.0359477124184,16569.261006289307,19286.94968553459,19286.94968553459,837.8787878787879,3665.809768637532,108.64285714285714,56.07911392405064,13671.296296296296,371.2903225806451,4057.832278481013,47142.92857142857,21438.1875,54.833333333333336,26081.56401384083,2540.0,115.76923076923076,16.423076923076923,8608.709677419354,4463.780120481927,8250.491573033707,15110.75,8173.714285714285,157100.37650602407,104499.0512820513,9076.930817610064,2611.25,428.0,14.663461538461538,2694.0,3569.0359477124184,86.24595469255664,678.7183544303797,494.57142857142856,712.4603174603175,663176.1973875181,10245.87593728698,5071.786163522012,1868.867924528302,26081.56401384083,3403.344867358708,4793.829296424452,64983.42138364781,3272.80163599182,5712.33671988389,9194.277108433736,608.4770114942529,42.484177215189874,466477.57812500006,3695.444059976932,3517.9333333333334,345.6428571428572,579.0625,7892.142857142857,5038.387096774193,45601.61516853933,1992.4528301886792,2647.5,5872.384615384615,6965.361445783132,64.01384083044982,45601.61516853933,23123.5,1992.4528301886792,2540.0,9060.337370242214,14.663461538461538,3882.680981595092,36.37096774193548,193984.2734375,2231.547169811321,108.64285714285714,329.7142857142857,117.49826989619376,773.5849056603774,36.37096774193548,8608.709677419354,371.2903225806451,45481.42307692308,10245.87593728698,63142.24137931035,678.7183544303797,11555.9375,4528.380503144654,2499.201741654572,10736.622807017544,5222.5,1901.1290322580644,93302.72435897436,1702.1929824561405,114208.8534107402,343.1394601542417,10245.87593728698,8173.714285714285,140637.14285714287,132.94270833333334,579.0625,663176.1973875181,1180.5379746835442,1236.25,26081.56401384083,854.952380952381,2010.9477124183009,30698.85714285714,11.175616835994196,90677.41228070176,38128.80258899677,663176.1973875181,71138.51966292135,1236.25,31412.04644412192,50574.625,83347.75862068967,12274.009146341465,90677.41228070176,808.2857142857143,11.175616835994196,11555.9375,86.24595469255664,41871.69230769231,19129.33333333333,329.7142857142857,1236.25,2637.8993710691825,579.7777777777778,30698.85714285714,329.7142857142857,2647.5,466477.57812500006,102.64285714285714,12691.209677419354,12317.397660818711,220.55016181229772,19147.454268292684,9304.668674698794,4057.832278481013,47142.92857142857,1630.1966292134832,4463.780120481927,2002.1844660194176,5071.786163522012,7035.866666666667,13504.20634920635,21601.383647798742,10245.87593728698,10861.245674740485,176.36477987421384,12691.209677419354,1432.2712418300653,608.4770114942529,10736.622807017544,3695.444059976932,157100.37650602407,1702.1929824561405,51795.97953216374,2386.449438202247,117.22222222222224,120.98615916955016,777.0363321799308,12274.009146341465,2611.25,2242.446601941748,168764.57142857145,627.2222222222222,40608.41346153846,2002.1844660194176,157.46855345911948,2647.5,119015.43674698794,579.0625,329.7142857142857,13121.345911949686,71138.51966292135,207.39062500000003,30698.85714285714,9060.337370242214,1529.7752808988764,3071.4691011235955,46247.0,538.8571428571429,16347.42857142857,23123.5,132.94270833333334,6984.389273356401,12691.209677419354,2499.201741654572,30221.5,15229.451612903224,191.11111111111111,428.0,3578.592162554427,11715.487421383648,19129.33333333333,30221.5,6432.321799307958,41871.69230769231,42.484177215189874,16918.132716049382,3695.444059976932,13504.20634920635,15617.489114658929,117.49826989619376,16569.261006289307,96.59546925566345,678.7183544303797,10146.898432174508,678.7183544303797,131.55526992287918,47142.92857142857,79091.23475609756,736.1797752808989,1180.5379746835442,29354.53846153846,45481.42307692308,21438.1875,6965.361445783132,31412.04644412192,10146.898432174508,11791.8,15617.489114658929,1677.5078125000002,5456.578050443081,14626.074074074077,94285.85714285714,248.6394601542416,5456.578050443081,26081.56401384083,63142.24137931035,12317.397660818711,12317.397660818711,608.4770114942529,11.175616835994196,10861.245674740485,2566.6935483870966,11555.9375,678.7183544303797,1529.7752808988764,1432.2712418300653,518.4642857142858,8104.40251572327,63142.24137931035,3798.928571428572,4057.832278481013,1526.2857142857142,2854.854368932039,3403.344867358708,9306.611111111111,538.8571428571429,13671.296296296296,117.22222222222224,343.1394601542417],\"xaxis\":\"x\",\"y\":[4785.592155979585,378.9394302913829,15365.600809369838,4785.592155979585,4070.255357333951,134757.3771732384,14409.557558767568,5813.845282617682,61516.49587090381,1510.6755468797405,98738.27282520698,1684.2601507265988,1088.107600326137,647.864369876902,4263.755978756964,14168.403568365671,487.3481032940997,4287.792726026477,51779.415544128475,19934.734830917627,12141.409803201397,87747.52966460225,5818.94510670397,377.43130038813075,12797.394868451114,389.0359797814606,19263.507069512012,40163.79013647013,377.43130038813075,23840.068464074022,487.3481032940997,400.48488833402286,126112.56377290396,11223.080944977459,522.1062342859491,415175.9779430929,894.6750043489748,519.4101103091041,2138.6141864173633,19263.507069512012,2790.085070463646,2196.4290704108694,4263.755978756964,2244.514746383961,2963.0052419712265,602.4723068239653,5217.814575086902,410.08771833436947,16913.133647514336,390.2429375597753,489.6064298303322,16311.253798810561,1109.562627058514,2715.348806915594,2281.9990642021894,6353.789558047462,37073.75373050823,11896.354947646087,38882.42895370776,797.0601491084628,703.1493477716122,388.482661364839,481.8437277075316,3747.013377347842,16794.12337436424,1571.4700357476117,2257.4161561731,37073.75373050823,43146.57407027896,76460.52042984031,5813.845282617682,433.93372568151483,50253.31197444373,2781.8507793892813,2963.0052419712265,450.36262704088375,323624.6828685297,2286.347850033103,959.8167805278175,1463.9293549917084,25867.060321463283,388.50455558540244,3461.34813854915,10658.874464205835,48286.68591679114,4070.54807420052,1208.1895902424544,13863.416557439898,3913.805102706469,1545.439458867743,48286.68591679114,664.1495092797293,12797.394868451114,415.88888264864494,24308.715376885695,17460.789585310762,13878.585909613186,2138.6141864173633,3258.406190332729,5925.265679815453,1797.2371240684145,47713.131047358365,51804.0528093027,1099.5606427530397,40683.104312073505,8508.45844428347,1571.3535403980425,505.4911026847705,213132.54291644722,4263.755978756964,2570.353838719455,3926.506181336219,3317.1418553360736,54758.90192605476,20585.396531692135,377.43130038813075,406.91804655949966,120990.40817472423,4395.853637740449,19387.390319584494,2412.686086825897,19699.926384269966,12552.025861937103,7368.391717428835,390.09029678781303,2262.905415567944,11572.521113313724,56931.5110267007,15545.52362930884,1239.906233381196,17460.789585310762,1926.1552505875645,10673.158532726991,9907.699090223898,454.81639486308137,6355.618165841122,19934.734830917627,2858.35606741023,2834.2664976391197,665.469888483012,11592.823783640883,16177.143075924467,12543.725418313918,317.63569157648635,2257.4161561731,15201.471014315619,45645.23591676211,8323.62833082682,376.8168895165831,213132.54291644722,16698.569941711827,663.3571920970296,20710.099320853376,346.28222029939593,513.9332229141203,323.60536008687086,323624.6828685297,1301.212252741579,16913.133647514336,505.4911026847705,632.0783029798903,778.1760841513764,2635.5625663138067,632.0783029798903,76574.72476531091,11492.974658748008,1239.906233381196,5163.94089137517,494.71513791291954,11527.578799029196,19176.79696136619,8979.131225405608,415175.9779430929,2754.6914615362707,2567.0739167219367,378.9394302913829,3747.013377347842,2138.6141864173633,3913.805102706469,10871.491666793676,2516.669574686992,18752.149151877966,636.9170394143573,8323.62833082682,17785.632844249434,2407.321312452001,1686.4843440391876,1051.2986748894868,49435.50780135217,610.4363857799107,960.0492155948752,346.28222029939593,7141.329503589847,8187.405610699661,1148.288091129662,1686.4843440391876,324.6947768318741,320.7835266802878,639820.6545150073,3317.1418553360736,703.1493477716122,340.3871184326995,1531.8394329987157,1686.4843440391876,9236.305456033022,16619.83993793382,463.3944227377964,4287.792726026477,3364.1619784229933,2019.1544585081797,2715.348806915594,5163.94089137517,10185.63650295469,436.06660273854754,26293.480165383102,4218.370391131691,76460.52042984031,7915.880111460361,25978.487349635714,2375.22637673234,489.7605752448112,376.8168895165831,3756.2286447454576,1797.2371240684145,3060.8574440447824,1384.0395692783663,1535.8631900519042,6162.533552625073,2276.6020512892946,14800.536096877271,1535.8631900519042,7368.391717428835,340.3871184326995,40283.97684677175,325.06940680600604,1695.6732046475156,1028.5053850528614,51779.415544128475,92018.35013115287,505.4911026847705,18752.149151877966,3667.712553203018,5968.182975715722,34939.27854355379,6424.274585977019,992.354894936303,30596.638581578518,639820.6545150073,65413.624508456036,324.82371754886964,3747.013377347842,16698.569941711827,19387.390319584494,19387.390319584494,1148.288091129662,6305.285970941779,492.1628050669814,346.28222029939593,14168.403568365671,663.3571920970296,4218.370391131691,45766.272960899,37175.69967364701,369.63985077669076,22891.92825541111,2754.6914615362707,405.6544524204279,319.54816919005003,8698.931074864986,5689.033000226852,11752.173931839643,26293.480165383102,8187.405610699661,117110.65135820676,90877.76737128492,8855.335527589015,3229.016296504339,703.1493477716122,319.72742324233695,1604.2554168141014,3769.9108375996702,396.30197122916206,959.8167805278175,632.0783029798903,999.9729800621147,731503.1707028296,17075.133945943726,5082.686543693957,2107.5040932279385,22891.92825541111,4219.676962291559,5818.94510670397,61516.49587090381,3617.0754679045876,5813.845282617682,7141.329503589847,932.7370475860574,346.28222029939593,415175.9779430929,4555.635369302835,3885.8140646224183,769.1955699368395,953.6647702716481,8337.819513819635,5163.94089137517,48246.709882894014,2276.6020512892946,2886.664844581823,5968.182975715722,7368.391717428835,378.9394302913829,63573.70285359673,40074.17801737161,2276.6020512892946,3138.4978117622,8151.559019833033,319.72742324233695,2286.347850033103,340.3871184326995,172828.89934304,2407.321312452001,410.08048522816836,747.818087951304,407.0672088952674,1088.107600326137,341.517050774033,8698.931074864986,667.3272676429804,39725.42364104348,10673.158532726991,65413.624508456036,959.8167805278175,13195.11592262944,4785.592155979585,2834.2664976391197,15201.471014315619,2963.0052419712265,2425.884027039221,98738.27282520698,1997.7384398419902,115873.73047883328,489.0303345318461,10673.158532726991,9457.064447629586,137428.5757499219,449.0579108608325,1301.212252741579,1085751.3630379443,1502.4134405828336,1684.2601507265988,22891.92825541111,992.354894936303,2244.514746383961,31550.15438840883,316.0906893952995,90462.3574812228,37318.37237669782,639820.6545150073,45645.23591676211,1684.2601507265988,30596.638581578518,56931.5110267007,76460.52042984031,12141.409803201397,95635.30862544454,839.3500692330289,316.62245060704845,13195.11592262944,389.0359797814606,41802.402147140514,19263.507069512012,747.818087951304,1535.8631900519042,2790.085070463646,803.8781088470796,29908.894508749334,648.872490598366,2858.35606741023,415175.9779430929,409.7821718132848,12543.725418313918,17394.662595168105,522.1062342859491,12141.409803201397,9740.51689224938,5199.4234913728005,45766.272960899,2567.0739167219367,4831.715916465941,2417.5880406486763,5082.686543693957,3697.7340063828487,13472.111607949182,20652.753180263422,11602.10423834572,12797.394868451114,471.44073265789405,12543.725418313918,1695.6732046475156,932.7370475860574,10980.322568203586,4555.635369302835,117110.65135820676,2094.8448847085992,54758.90192605476,1826.31018819446,421.9954832228255,410.08771833436947,978.2266656454766,12141.409803201397,4796.262256309382,2236.064155834039,135924.7927322855,1041.1204286449113,40221.84276082843,2273.3851256392354,461.10966099955675,2858.35606741023,143848.60557702882,953.6647702716481,648.872490598366,12664.975331868296,45645.23591676211,489.7605752448112,29908.894508749334,8151.559019833033,2427.747778574022,2262.905415567944,23840.068464074022,866.7970691016989,8187.405610699661,26195.686125920434,449.0579108608325,6353.789558047462,12679.428015781728,2715.348806915594,15684.810589735318,12543.725418313918,504.4494927620649,703.1493477716122,4250.960414621229,11896.354947646087,25978.487349635714,15684.810589735318,5875.699548389586,41802.402147140514,347.0090494475603,16909.279272683176,4555.635369302835,15368.26716660901,16108.716384819742,407.0672088952674,16698.569941711827,388.482661364839,993.553834752781,10573.002781125038,971.4284514069652,450.36262704088375,45766.272960899,76574.72476531091,774.5157686911542,1443.732437768414,19176.79696136619,39725.42364104348,24308.715376885695,8706.164457317907,32091.295039989156,11492.974658748008,13508.076892121393,16108.716384819742,1797.2371240684145,5826.8479598607,15545.52362930884,45766.272960899,438.4351253031718,5826.8479598607,22891.92825541111,65413.624508456036,12552.025861937103,12552.025861937103,932.7370475860574,316.0906893952995,12797.394868451114,2860.1717851882872,11807.964454280216,1123.9090100996427,1913.5805743985902,1816.3382332975395,647.864369876902,8323.62833082682,65413.624508456036,5403.795319141717,4218.370391131691,1531.8394329987157,3111.527273406017,4219.676962291559,11223.080944977459,866.7970691016989,13722.736128528055,436.06660273854754,489.0303345318461],\"yaxis\":\"y\",\"type\":\"scatter\"}], {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Actual\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prediction\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"autosize\":false,\"width\":800,\"height\":500,\"title\":{\"text\":\"R2: 0.869\"}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('239a6e0b-8482-49c8-9619-ebde564b91cd'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; }); 4.2.3 Image Preprocessing \u00b6 back to top Image preprocessing is beyond the scope of this session. We cover this topic in General Applications of Neural Networks . For now, know that there is a wealth of considerations for how to handle images, and they all fit within the realm of feature engineering. 4.3 Transformed Features \u00b6 back to top Transformed features, are features that we would like to augment based on their relationship within their own distribution or to other (allegedly) independent data within our training set. e.g. we're not deriving new features based on some empirical knowledge of the data, rather we are changing them due to statistical properties that we can assess based on the data itself. 4.3.1 Skewness \u00b6 back to top Skewed data can lead to imbalances in our model prediction. Why? Skewed values in the distribution will bias the mean. When assigning weights to this input feature, therefore, the model will give preferential treatment to these values. To demonstrate, I'm going to use scipy to create some skewed data. from scipy.stats import skewnorm a = 10 x = np . linspace ( skewnorm . ppf ( 0.01 , a ), skewnorm . ppf ( 0.99 , a ), 100 ) plt . plot ( x , skewnorm . pdf ( x , a ), 'r-' , lw = 5 , alpha = 0.6 , label = 'skewnorm pdf' ) [<matplotlib.lines.Line2D at 0x7fa48d14a130>] We can now generate a random population based on this distribution r = skewnorm . rvs ( a , size = 1000 ) plt . hist ( r ) (array([113., 267., 225., 172., 116., 62., 26., 13., 2., 4.]), array([-0.19733964, 0.15303313, 0.50340589, 0.85377866, 1.20415142, 1.55452419, 1.90489696, 2.25526972, 2.60564249, 2.95601526, 3.30638802]), <BarContainer object of 10 artists>) Unskewed data will return something close to 0 from calling df.skew() . When dealing with actual data, we can use df.skew() to determine whether we should transform our data. x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) x [ 'Skewed Data' ] . skew () 0.9141902067398219 There are a handful of ways to deal with skewed data: log transform square root transform Box-Cox transform Let's try the first two print ( 'square root transformed skew: {:.4f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) print ( 'log transformed skew: {:.4f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . legend () square root transformed skew: 0.0561 log transformed skew: -1.6916 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in sqrt /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log <matplotlib.legend.Legend at 0x7fa49157d340> We see we didn't get much traction with the log transform, and the log transform will not be able to handle 0 values, and so we will sometimes have to code exceptions for those. Box-Cox is often a good route to go, but it has the added restriction that the data has to all be above 0. Let's create a new distribution with this added restriction a = 6 r = skewnorm . rvs ( a , size = 1000 ) r = [ i for i in r if i > 0 ] plt . hist ( r ) (array([220., 277., 182., 127., 66., 39., 17., 5., 4., 2.]), array([2.17150536e-03, 3.88613862e-01, 7.75056219e-01, 1.16149858e+00, 1.54794093e+00, 1.93438329e+00, 2.32082565e+00, 2.70726800e+00, 3.09371036e+00, 3.48015272e+00, 3.86659507e+00]), <BarContainer object of 10 artists>) from scipy import stats x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ], alpha = 0.5 , label = 'box-cox: {:.2f} ' . format ( pd . DataFrame ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ])[ 0 ] . skew ())) ax . legend () <matplotlib.legend.Legend at 0x7fa492058f40> \ud83c\udfcb\ufe0f Exercise 3: Transform data from a gamma distribution \u00b6 Repeat section 4.3.1, this time synthesizing a gamma distribution and transforming it. Which transformation best reduces the skew? Do this for a dataset that does not contain values at or below 0. # Cell for exercise 3 from scipy.stats import gamma <matplotlib.legend.Legend at 0x7fa49111dd30> 4.3.2 Colinearity \u00b6 back to top Colinearity can also affect the performance of your machine learning model. In particular, if features are colinear, it can be easy for your model to overfit to your training dataset. This is often mitigated by regularization. If you're curious you can read more about it on this discussion from StackExchange . We will still explore it explicitly here by calculating the Variance Inflation Factor (VIF) on some hypothetical data. VIF = \\frac{1}{1-R^2} Usually we are concerned about data with a VIF above 10 4.3.2.1 Detecting Colinearity \u00b6 back to top from statsmodels.stats.outliers_influence import variance_inflation_factor Step 1: Make some data # we can throttle the error rate random . seed ( 42 ) # x2 will be sqrt of x1 plus some error def func ( x , err ): return x ** .5 + ( err * random . randint ( - 1 , 1 ) * random . random () * x ) x0 = range ( 100 ) x1 = [ func ( i , .05 ) for i in x0 ] # HIGH degree of colinearity with x0 x2 = [ func ( i , 1 ) for i in x0 ] # MED degree of colinearity with x0 x3 = [ random . randint ( 0 , 100 ) for i in x0 ] # NO degree of colinearity with x0 # take a look fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) ax . plot ( x0 , x1 , label = 'x1' ) ax . plot ( x0 , x2 , label = 'x2' ) ax . plot ( x0 , x3 , label = 'x3' ) ax . legend () <matplotlib.legend.Legend at 0x7fa49121ca90> To calculate the colinearities I'm going to aggregate these x's into a dataframe: colin = pd . DataFrame ([ x0 , x1 , x2 , x3 ]) . T colin . columns = [ 'x0' , 'x1' , 'x2' , 'x3' ] colin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 0 0.0 0.000000 0.000000 29.0 1 1.0 1.013751 0.721523 28.0 2 2.0 1.400260 1.414214 3.0 3 3.0 1.630546 -0.438007 84.0 4 4.0 2.017388 4.304847 24.0 Step 2: Calculate VIF factors # calculate VIF factors vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( colin . values , i ) for i in range ( colin . shape [ 1 ])] vif [ \"features\" ] = colin . columns Step 3: Inspect VIF factors # inspect VIF factors display ( vif ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 12.555415 x0 1 15.823872 x1 2 1.030609 x2 3 3.559468 x3 In this case, we may remove either x0 or x1 from the dataset. 4.3.2.2 Fixing Colinearity \u00b6 back to top It is good to aknowledge where colinearity exists as this will influence the interpretability of your model. In most cases, however, it won't have a heavy influence on the performance of your model. A simple method of dealing with colinearity, is to remove the highest VIF features from your model, iteratively, assessing the performance and determining whether to keep the variable or not. Another method is to create some linear combination of the correlated variables. This is encapsulated in the section on dimensionality reduction. 4.3.3 Normalization \u00b6 back to top Many learning algorithms require zero mean and unit variance to behave optimally. Sklearn preprocessing library contains a very usefull class, StandardScaler for handling this automatically for us. from sklearn.preprocessing import StandardScaler scaler = StandardScaler () normed = scaler . fit_transform ( colin ) colin [[ 'x0' , 'x1' , 'x2' , 'x3' ]] . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'> pd . DataFrame ( normed , columns = [[ 'x0' , 'x1' , 'x2' , 'x3' ]]) . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'> \ud83c\udfcb\ufe0f Exercise 4: Normalization affect on VIF \u00b6 In the above, we saw how to scale and center variables. How does this affect VIF? Calculate the VIF for the scaled-centered data # Code Cell for Exercise 4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 3.286048 x0 1 3.296881 x1 2 1.015805 x2 3 1.035537 x3 4.3.4 Dimensionality Reduction \u00b6 back to top Dimensionality reduction is an awesome way to do feature engineering. It is very commonly used. Because it is also an unsupervised machine learning technique, we will visit this topic in that section. 4.4 Missing Data \u00b6 back to top We will often have missing data in our datasets. How do we deal with this? Let's start by making some data with missing data. We'll use a numpy nan datatype to do this from numpy import nan X = np . array ([[ nan , 0 , 3 ], [ 3 , 7 , 9 ], [ 3 , 5 , 2 ], [ 4 , nan , 6 ], [ 8 , 8 , 1 ]]) y = np . array ([ 14 , 16 , - 1 , 8 , - 5 ]) 4.4.1 Imputation \u00b6 back to top A very common strategy is to impute or fill in the missing data, based on basic statistical descriptions of the feature column (mode, mean, and median) from sklearn.impute import SimpleImputer # strategy = 'mean' will replace nan's with mean value # of the column # others are median and most_frequent (mode) imp = SimpleImputer ( strategy = 'mean' ) X2 = imp . fit_transform ( X ) X2 array([[4.5, 0. , 3. ], [3. , 7. , 9. ], [3. , 5. , 2. ], [4. , 5. , 6. ], [8. , 8. , 1. ]]) 4.4.2 Other Strategies \u00b6 back to top Depending on the severity of missing data, you will sometimes opt to remove the whole column, or perhaps apply some simple learning to fill in the missing data. This is a great article on more advanced strategies for handling missing data. References \u00b6 back to top Box Cox Multicolinearity Missing Data","title":"Feature Engineering"},{"location":"S4_Feature_Engineering/#data-science-foundations-session-4-feature-engineering","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we talked about model pipelines and conveniently began with a suitable set of input data. In the real world, this is hardly ever the case! What is constant is this: at the end of the day, our models need numbers. Not only this, but a suitable set of numbers. What does that mean? The answer to that question is the subject of our session today.","title":"Data Science Foundations  Session 4: Feature Engineering"},{"location":"S4_Feature_Engineering/#40-preparing-environment-and-importing-data","text":"back to top","title":"4.0 Preparing Environment and Importing Data"},{"location":"S4_Feature_Engineering/#401-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats as stats from scipy.stats import gamma from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor import seaborn as sns ; sns . set () from sklearn.datasets import load_iris from sklearn.metrics import mean_squared_error , r2_score","title":"4.0.1 Import Packages"},{"location":"S4_Feature_Engineering/#402-load-dataset","text":"back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) orders = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_orders.csv' ) time_cols = [ i for i in orders . columns if '/' in i ] margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 dfcat = margin . columns [: - 2 ] dfcat Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') orders . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer 1/2020 2/2020 3/2020 4/2020 5/2020 6/2020 7/2020 8/2020 9/2020 10/2020 11/2020 12/2020 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146","title":"4.0.2 Load Dataset"},{"location":"S4_Feature_Engineering/#41-categorical-features","text":"back to top At the end of the day, our algorithms operate on numerical values. How do you get from a series of string values to numerical values? margin [ 'Customer' ] . unique () array(['Slugworth', 'Perk-a-Cola', 'Fickelgruber', 'Zebrabar', \"Dandy's Candies\"], dtype=object) A naive way to do it would be to assign a number to every entry 'Slugworth' = 1 'Perk-a-Cola' = 2 'Dandy's Candies' = 3 but we would inadvertently end up with some weird mathematical relationships between these variables, e.g. Dandy's Candies - Perk-a-Cola = Slugworth (3 - 2 = 1). A work around for this is to think multi-dimensionally we express our categorical values as vectors in a hyperspace where they cannot be expressed in terms of one another, i.e. they are orthogonal 'Slugworth' = [1,0,0] 'Perk-a-Cola' = [0,1,0] 'Dandy's Candies' = [0,0,1] such a scheme, in machine learning vernacular, is termed one-hot encoding.","title":"4.1 Categorical Features"},{"location":"S4_Feature_Engineering/#411-one-hot-encoding","text":"back to top sklearn has a couple useful libraries for one-hot encoding. let's start with the OneHotEncoder class in its preprocessing library from sklearn.preprocessing import OneHotEncoder # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ 'Customer' ] . values . reshape ( - 1 , 1 ) # fit our encoder to this data enc . fit ( X_cat ) OneHotEncoder() After fitting our encoder, we can then use this object to create our training array. # as a reference here's our original data display ( X_cat [: 10 ]) print ( X_cat . shape , end = ' \\n\\n ' ) onehotlabels = enc . transform ( X_cat ) . toarray () print ( onehotlabels . shape , end = ' \\n\\n ' ) # And here is our new data onehotlabels [: 10 ] array([['Slugworth'], ['Slugworth'], ['Perk-a-Cola'], ['Fickelgruber'], ['Fickelgruber'], ['Fickelgruber'], ['Slugworth'], ['Zebrabar'], ['Slugworth'], ['Zebrabar']], dtype=object) (1668, 1) (1668, 5) array([[0., 0., 0., 1., 0.], [0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]]) We have our customer information one-hot encoded, we need to do this for all our variables and concatenate them with our regular numerical variables in our original dataframe. # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ dfcat ] . values # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [ \"KG\" ] print ( X_num . shape ) X = np . concatenate (( onehotlabels , X_num . values . reshape ( - 1 , 1 )), axis = 1 ) X . shape (1668,) (1668, 119) And now we grab our EBITDA (margin) data for prediction y = margin [ \"EBITDA/KG\" ]","title":"4.1.1 One-Hot Encoding"},{"location":"S4_Feature_Engineering/#exercise-1-create-a-simple-linear-model","text":"Using the X and y sets, use train_test_split and LinearRegression to make a baseline model based on what we've learned so far. Assess your model performance visually by plottying y_test vs y_test_pred # Cell for Exercise 1 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression [<matplotlib.lines.Line2D at 0x7fa4b01f90d0>]","title":"\ud83c\udfcb\ufe0f Exercise 1: Create a simple linear model"},{"location":"S4_Feature_Engineering/#question-1","text":"How can we assess the relative feature importance of the features in our model? We could be tempted to inspect the coefficients ( linear.coef_ ) of our model to evaluate the relative feature importance, but in order to do this our features need to be scaled (so that the relative coefficient sizes are meaningful). What other issues might there be (think categorical vs continuous variables).","title":"\ud83d\ude4b Question 1:"},{"location":"S4_Feature_Engineering/#42-derived-features","text":"back to top Can we recall an example of where we've seen this previously? That's right earlier on in our session on model selection and validation we derived some polynomial features to create our polynomial model using the linear regression class in sklearn. We actually see this a lot in engineering, where we will describe log relationships or some other transformation of the original variable. Actually let me see if I can find an example in my handy BSL... concentration profiles in continous stirred tank vs plug flow reactors. Notice the y-axis is log scale. Thanks Bird, Stewart, Lightfoot! Can we think of other examples where we would like to derive features from our input data?","title":"4.2 Derived Features"},{"location":"S4_Feature_Engineering/#421-creating-polynomials","text":"back to top Let's revisit our example from the previous session, right before we introduced Grid Search in sklearn # from Model Selection and Validation, 1.2.1 from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) in the above, we use sklearn's convenient tool, make_pipeline to join together the preprocessing tool PolynomialFeatures and the basic model LinearRegression . Let's take a look at what PolynomialFeatures does to some simple data x = np . arange ( 1 , 11 ) y = x ** 3 print ( x ) print ( y ) [ 1 2 3 4 5 6 7 8 9 10] [ 1 8 27 64 125 216 343 512 729 1000] features = PolynomialFeatures ( degree = 3 ) X2 = features . fit_transform ( x . reshape ( - 1 , 1 )) we see our new feature set contains our original features, plus new features up to the nth-degree polynomial we set when creating the features object from PolynomialFeatures print ( X2 ) [[ 1. 1. 1. 1.] [ 1. 2. 4. 8.] [ 1. 3. 9. 27.] [ 1. 4. 16. 64.] [ 1. 5. 25. 125.] [ 1. 6. 36. 216.] [ 1. 7. 49. 343.] [ 1. 8. 64. 512.] [ 1. 9. 81. 729.] [ 1. 10. 100. 1000.]] model = LinearRegression () . fit ( X2 , y ) yhat = model . predict ( X2 ) plt . scatter ( x , y ) plt . plot ( x , yhat );","title":"4.2.1 Creating Polynomials"},{"location":"S4_Feature_Engineering/#422-dealing-with-time-series","text":"back to top Often, we will be dealing with time series data, whether its data generated by machinery, reactors, or sales and customers. In the following we discuss some simple practices for dealing with time series data.","title":"4.2.2 Dealing with Time Series"},{"location":"S4_Feature_Engineering/#4221-enrichment-fast-fourier-transform","text":"Special thanks to Brian Gerwe for his contribution to this section \ud83d\udc68\u200d\ud83c\udf73 back to top Sometimes we'll want to create a more sophisticated transformation of our input data. As engineers, this can often have to do with some empirical knowledge we understand about our process. When working with equipment and machinery, we will often want to convert a signal from the time to frequency domain. Let's cover how we can do that with numpy! img src What I've drawn here in the following is called a square-wave signal t = np . linspace ( 0 , 1 , 501 ) # FFT should be given an integer number of cycles so we leave out last sample t = t [: - 1 ] f = 5 # linear frequency in Hz w = f * 2 * np . pi # radial frequency h = 4 # height of square wave amp = 4 * h / np . pi s = amp * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) # here is the call to numpy FFT F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ], d = t [ 1 ]) # reorder frequency spectrum and frequency bins with 0 Hz at the center F = np . fft . fftshift ( F ) freq = np . fft . fftshift ( freq ) # scale frequency spectrum to correct amplitude F = F / t . size # amplitudes amps = [ max ( np . sin ( w * t )), max ( np . sin ( w * t * 3 ) / 3 ), max ( np . sin ( w * t * 5 ) / 5 )] fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) ax [ 0 ] . set_xlim ( 0 , 1 ) ax [ 0 ] . set_xlabel ( 'Time (s)' ) # tells us about the amplitude of the component at the # corresponding frequency. Multiplied by two because the # signal power is split between (-) and (+) frequency branches # of FFT, but we're only visualizing the (+) branch magnitude = 2 * np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , 30 ) ax [ 1 ] . set_xlabel ( 'Frequency (Hz)' ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain')","title":"\ud83c\udf52 4.2.2.1 Enrichment: Fast Fourier Transform"},{"location":"S4_Feature_Engineering/#4222-rolling-windows","text":"back to top to see an example of this dataset in action visit this link One powerful technique for dealing with time series data, is to create a rolling window of features based on the historical data. The proper window size can usually be determined by trial and error, or constraints around access to the data itself. In the above gif, we have a window size of 7. What that means is for whatever time step units we are in (that could be minutes, days, months, etc.) we will have 7 of them included in a single instance or observation. This instance or observation is then interpreted by our model and used to assess the target value, typically the quantity in the very next time step after the window (the green bar in the gif). Let's take an example with the orders data tidy_orders = orders . melt ( id_vars = orders . columns [: 6 ], var_name = 'Date' , value_name = 'KG' ) display ( tidy_orders . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 In the next exercise, we are going to attempt to predict an order amount, based on the previous order history. We will scrub all categorical labels and only use historical amounts to inform our models. In effect the data that the model will see will look like the following: fig , ax = plt . subplots ( 3 , 2 , figsize = ( 10 , 20 )) indices = np . argwhere ( ax ) color_dict = { 0 : 'tab:blue' , 1 : 'tab:green' , 2 : 'tab:orange' , 3 : 'tab:red' , 4 : 'tab:pink' , 5 : 'tab:brown' } for index , customer in enumerate ( tidy_orders . Customer . unique ()): orders . loc [ orders . Customer == customer ] . iloc [:, 6 :] . reset_index () . T . plot ( c = color_dict [ index ], legend = False , ax = ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]]) ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]] . set_title ( customer ) What we may notice is that there is very little noise or drift in our order history, but there is certainly some periodicity. The question is can we use a linear model to predict the next order amount based on these history orders? def process_data ( Xy , time_cols = 12 , window = 3 , remove_null = False ): \"\"\" This function splits your time series data into the proper windows Parameters ---------- Xy: array The input data. If there are non-time series columns, assumes they are on the left and time columns are on the right. time_cols: int The number of time columns, default 12 window: int The time window size, default 3 Returns ------- X_: array The independent variables, includes time and non-time series columns with the new window y_: array The dependent variable, selected from the time columns at the end of the window labels: The time series labels, can be used in subsequent plot \"\"\" # separate the non-time series columns X_cat = Xy [:,: - time_cols ] # select the columns to apply the sweeping window X = Xy [:, - time_cols :] X_ = [] y = [] for i in range ( X . shape [ 1 ] - window ): # after attaching the current window to the non-time series # columns, add it to a growing list X_ . append ( np . concatenate (( X_cat , X [:, i : i + window ]), axis = 1 )) # add the next time delta after the window to the list of y # values y . append ( X [:, i + window ]) # X_ is 3D: [number of replicates from sweeping window, # length of input data, # size of new feature with categories and time] # we want to reshape X_ so that the replicates due to the sweeping window is # a part of the same dimension as the instances of the input data X_ = np . array ( X_ ) . reshape ( X . shape [ 0 ] * np . array ( X_ ) . shape [ 0 ], window + X_cat . shape [ 1 ]) y = np . array ( y ) . reshape ( X . shape [ 0 ] * np . array ( y ) . shape [ 0 ],) if remove_null : # remove training data where the target is 0 (may be unfair advantage) X_ = X_ [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] y = y [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] # create labels that show the previous month values used to train the model labels = [] for row in X_ : labels . append ( \"X: {} \" . format ( np . array2string ( row [ - window :] . astype ( float ) . round ()))) return X_ , y , labels","title":"4.2.2.2 Rolling Windows"},{"location":"S4_Feature_Engineering/#exercise-2-optimize-rolling-window-size-for-customer-forecasts","text":"For this exercise, you will use the process_data function below to help you optimize the window size for predicting the order quantity in any given month. You will train a LinearRegression model. create a model using a window size of 3 and predict the order quantity for the month immediately following the window create a model for window sizes 1-11 and report the \\(R^2\\) for each model # Cell for Exercise 2 kg_month_data = orders . values [:, 6 :] # use kg_month_data and the function process_data to create your X, y arrays # then use train_test_split to create train and test portions # USE y_test and y_pred for your actual and true test data # change only window parameter in process_data() print ( \"window R2\" ) for window in range ( 1 , 12 ): ###################### ### YOUR CODE HERE ### ###################### pass window R2 1, 0.97 2, 0.88 3, 0.80 4, 0.96 5, 0.90 6, 0.86 7, 0.61 8, 0.89 9, 0.76 10, 0.74 11, 0.87 #### RUN AFTER EXERCISE 2.2.2.2.1 #### fig = px . scatter ( x = y_test , y = y_pred , labels = { \"y\" : \"Prediction\" , \"x\" : \"Actual\" }) fig . update_layout ( autosize = False , width = 800 , height = 500 , title = 'R2: {:.3f} ' . format ( r2_score ( y_test , y_pred )) ) require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"239a6e0b-8482-49c8-9619-ebde564b91cd\")) { Plotly.newPlot( \"239a6e0b-8482-49c8-9619-ebde564b91cd\", [{\"hovertemplate\":\"Actual=%{x}<br>Prediction=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4528.380503144654,64.01384083044982,15617.489114658929,4528.380503144654,5698.392857142857,124348.89322916669,21939.111111111117,5712.33671988389,64983.42138364781,1483.5,93302.72435897436,1236.25,773.5849056603774,518.4642857142858,3798.928571428572,13671.296296296296,188.7692307692308,4057.832278481013,101149.25,14626.074074074077,12274.009146341465,141456.76315789475,4793.829296424452,74.78571428571429,10861.245674740485,86.24595469255664,19129.33333333333,39307.04113924051,74.78571428571429,46247.0,188.7692307692308,85.32258064516128,90677.41228070176,9306.611111111111,220.55016181229772,466477.57812500006,1158.125,220.55016181229772,1901.1290322580644,19129.33333333333,2637.8993710691825,1868.867924528302,3798.928571428572,2252.261437908497,5222.5,181.5552699228792,5038.387096774193,120.98615916955016,10146.898432174508,85.32258064516128,286.6666666666667,18613.222222222223,818.1891025641025,2499.201741654572,2010.9477124183009,6984.389273356401,42704.25889967639,11715.487421383648,39307.04113924051,440.859375,428.0,96.59546925566345,102.64285714285714,3569.0359477124184,13671.296296296296,773.5849056603774,2010.9477124183009,42704.25889967639,40608.41346153846,83347.75862068967,10796.31640058055,74.78571428571429,80801.72807017544,2540.0,5222.5,131.55526992287918,299024.0885416667,3882.680981595092,895.9082278481012,1180.5379746835442,15617.489114658929,85.32258064516128,6185.59509202454,16749.13157894737,47142.92857142857,3786.966463414634,936.280487804878,15244.632812500002,3272.80163599182,1271.904761904762,47142.92857142857,356.6666666666667,10861.245674740485,108.64285714285714,21438.1875,16918.132716049382,7892.142857142857,1901.1290322580644,2647.5,5038.387096774193,1677.5078125000002,39307.04113924051,51795.97953216374,916.3717948717948,41871.69230769231,12870.766853932584,1271.904761904762,181.5552699228792,211540.7575757576,3798.928571428572,2054.328561690525,3578.592162554427,2854.854368932039,51795.97953216374,11791.8,149.57142857142858,103.3653846153846,119015.43674698794,2499.201741654572,19286.94968553459,2185.3410740203194,19569.69230769231,12317.397660818711,6965.361445783132,86.24595469255664,3071.4691011235955,13867.125,50574.625,14626.074074074077,1005.4545454545454,16918.132716049382,2543.106741573034,10245.87593728698,8608.709677419354,150.3654970760234,3517.9333333333334,14626.074074074077,3177.0,2499.201741654572,345.6428571428572,10736.622807017544,13799.769319492503,12691.209677419354,11.175616835994196,2010.9477124183009,10736.622807017544,71138.51966292135,8104.40251572327,62.168396770472896,211540.7575757576,16569.261006289307,445.54838709677415,16918.132716049382,42.484177215189874,150.3654970760234,11.175616835994196,299024.0885416667,579.0625,10146.898432174508,181.5552699228792,494.57142857142856,411.1303344867359,2355.267295597484,494.57142857142856,79091.23475609756,10146.898432174508,1005.4545454545454,5038.387096774193,191.11111111111111,9304.668674698794,19569.69230769231,8250.491573033707,466477.57812500006,3048.0,1630.1966292134832,64.01384083044982,3569.0359477124184,1901.1290322580644,3272.80163599182,9772.200520833334,2185.3410740203194,28694.0,314.3203883495146,8104.40251572327,18557.572327044025,2231.547169811321,1432.2712418300653,773.5849056603774,40735.10802469136,314.3203883495146,471.91011235955057,42.484177215189874,9194.277108433736,8173.714285714285,837.8787878787879,1604.1437908496732,22.50980392156863,14.663461538461538,663176.1973875181,2854.854368932039,428.0,43.64516129032258,1526.2857142857142,1432.2712418300653,5456.578050443081,22331.935185185182,150.3654970760234,4057.832278481013,1868.867924528302,1630.1966292134832,4723.49129172714,5038.387096774193,19364.70552147239,117.22222222222224,15110.75,4057.832278481013,83347.75862068967,7892.142857142857,19129.33333333333,1968.8904494382025,207.39062500000003,62.168396770472896,3578.592162554427,1677.5078125000002,2499.201741654572,1254.4444444444443,1236.25,3578.592162554427,1992.4528301886792,14626.074074074077,1236.25,6965.361445783132,36.37096774193548,40735.10802469136,20.098039215686278,1432.2712418300653,538.8571428571429,101149.25,93302.72435897436,181.5552699228792,28694.0,2054.328561690525,5872.384615384615,31412.04644412192,5872.384615384615,854.952380952381,31412.04644412192,1253403.0130624091,63142.24137931035,20.098039215686278,3569.0359477124184,16569.261006289307,19286.94968553459,19286.94968553459,837.8787878787879,3665.809768637532,108.64285714285714,56.07911392405064,13671.296296296296,371.2903225806451,4057.832278481013,47142.92857142857,21438.1875,54.833333333333336,26081.56401384083,2540.0,115.76923076923076,16.423076923076923,8608.709677419354,4463.780120481927,8250.491573033707,15110.75,8173.714285714285,157100.37650602407,104499.0512820513,9076.930817610064,2611.25,428.0,14.663461538461538,2694.0,3569.0359477124184,86.24595469255664,678.7183544303797,494.57142857142856,712.4603174603175,663176.1973875181,10245.87593728698,5071.786163522012,1868.867924528302,26081.56401384083,3403.344867358708,4793.829296424452,64983.42138364781,3272.80163599182,5712.33671988389,9194.277108433736,608.4770114942529,42.484177215189874,466477.57812500006,3695.444059976932,3517.9333333333334,345.6428571428572,579.0625,7892.142857142857,5038.387096774193,45601.61516853933,1992.4528301886792,2647.5,5872.384615384615,6965.361445783132,64.01384083044982,45601.61516853933,23123.5,1992.4528301886792,2540.0,9060.337370242214,14.663461538461538,3882.680981595092,36.37096774193548,193984.2734375,2231.547169811321,108.64285714285714,329.7142857142857,117.49826989619376,773.5849056603774,36.37096774193548,8608.709677419354,371.2903225806451,45481.42307692308,10245.87593728698,63142.24137931035,678.7183544303797,11555.9375,4528.380503144654,2499.201741654572,10736.622807017544,5222.5,1901.1290322580644,93302.72435897436,1702.1929824561405,114208.8534107402,343.1394601542417,10245.87593728698,8173.714285714285,140637.14285714287,132.94270833333334,579.0625,663176.1973875181,1180.5379746835442,1236.25,26081.56401384083,854.952380952381,2010.9477124183009,30698.85714285714,11.175616835994196,90677.41228070176,38128.80258899677,663176.1973875181,71138.51966292135,1236.25,31412.04644412192,50574.625,83347.75862068967,12274.009146341465,90677.41228070176,808.2857142857143,11.175616835994196,11555.9375,86.24595469255664,41871.69230769231,19129.33333333333,329.7142857142857,1236.25,2637.8993710691825,579.7777777777778,30698.85714285714,329.7142857142857,2647.5,466477.57812500006,102.64285714285714,12691.209677419354,12317.397660818711,220.55016181229772,19147.454268292684,9304.668674698794,4057.832278481013,47142.92857142857,1630.1966292134832,4463.780120481927,2002.1844660194176,5071.786163522012,7035.866666666667,13504.20634920635,21601.383647798742,10245.87593728698,10861.245674740485,176.36477987421384,12691.209677419354,1432.2712418300653,608.4770114942529,10736.622807017544,3695.444059976932,157100.37650602407,1702.1929824561405,51795.97953216374,2386.449438202247,117.22222222222224,120.98615916955016,777.0363321799308,12274.009146341465,2611.25,2242.446601941748,168764.57142857145,627.2222222222222,40608.41346153846,2002.1844660194176,157.46855345911948,2647.5,119015.43674698794,579.0625,329.7142857142857,13121.345911949686,71138.51966292135,207.39062500000003,30698.85714285714,9060.337370242214,1529.7752808988764,3071.4691011235955,46247.0,538.8571428571429,16347.42857142857,23123.5,132.94270833333334,6984.389273356401,12691.209677419354,2499.201741654572,30221.5,15229.451612903224,191.11111111111111,428.0,3578.592162554427,11715.487421383648,19129.33333333333,30221.5,6432.321799307958,41871.69230769231,42.484177215189874,16918.132716049382,3695.444059976932,13504.20634920635,15617.489114658929,117.49826989619376,16569.261006289307,96.59546925566345,678.7183544303797,10146.898432174508,678.7183544303797,131.55526992287918,47142.92857142857,79091.23475609756,736.1797752808989,1180.5379746835442,29354.53846153846,45481.42307692308,21438.1875,6965.361445783132,31412.04644412192,10146.898432174508,11791.8,15617.489114658929,1677.5078125000002,5456.578050443081,14626.074074074077,94285.85714285714,248.6394601542416,5456.578050443081,26081.56401384083,63142.24137931035,12317.397660818711,12317.397660818711,608.4770114942529,11.175616835994196,10861.245674740485,2566.6935483870966,11555.9375,678.7183544303797,1529.7752808988764,1432.2712418300653,518.4642857142858,8104.40251572327,63142.24137931035,3798.928571428572,4057.832278481013,1526.2857142857142,2854.854368932039,3403.344867358708,9306.611111111111,538.8571428571429,13671.296296296296,117.22222222222224,343.1394601542417],\"xaxis\":\"x\",\"y\":[4785.592155979585,378.9394302913829,15365.600809369838,4785.592155979585,4070.255357333951,134757.3771732384,14409.557558767568,5813.845282617682,61516.49587090381,1510.6755468797405,98738.27282520698,1684.2601507265988,1088.107600326137,647.864369876902,4263.755978756964,14168.403568365671,487.3481032940997,4287.792726026477,51779.415544128475,19934.734830917627,12141.409803201397,87747.52966460225,5818.94510670397,377.43130038813075,12797.394868451114,389.0359797814606,19263.507069512012,40163.79013647013,377.43130038813075,23840.068464074022,487.3481032940997,400.48488833402286,126112.56377290396,11223.080944977459,522.1062342859491,415175.9779430929,894.6750043489748,519.4101103091041,2138.6141864173633,19263.507069512012,2790.085070463646,2196.4290704108694,4263.755978756964,2244.514746383961,2963.0052419712265,602.4723068239653,5217.814575086902,410.08771833436947,16913.133647514336,390.2429375597753,489.6064298303322,16311.253798810561,1109.562627058514,2715.348806915594,2281.9990642021894,6353.789558047462,37073.75373050823,11896.354947646087,38882.42895370776,797.0601491084628,703.1493477716122,388.482661364839,481.8437277075316,3747.013377347842,16794.12337436424,1571.4700357476117,2257.4161561731,37073.75373050823,43146.57407027896,76460.52042984031,5813.845282617682,433.93372568151483,50253.31197444373,2781.8507793892813,2963.0052419712265,450.36262704088375,323624.6828685297,2286.347850033103,959.8167805278175,1463.9293549917084,25867.060321463283,388.50455558540244,3461.34813854915,10658.874464205835,48286.68591679114,4070.54807420052,1208.1895902424544,13863.416557439898,3913.805102706469,1545.439458867743,48286.68591679114,664.1495092797293,12797.394868451114,415.88888264864494,24308.715376885695,17460.789585310762,13878.585909613186,2138.6141864173633,3258.406190332729,5925.265679815453,1797.2371240684145,47713.131047358365,51804.0528093027,1099.5606427530397,40683.104312073505,8508.45844428347,1571.3535403980425,505.4911026847705,213132.54291644722,4263.755978756964,2570.353838719455,3926.506181336219,3317.1418553360736,54758.90192605476,20585.396531692135,377.43130038813075,406.91804655949966,120990.40817472423,4395.853637740449,19387.390319584494,2412.686086825897,19699.926384269966,12552.025861937103,7368.391717428835,390.09029678781303,2262.905415567944,11572.521113313724,56931.5110267007,15545.52362930884,1239.906233381196,17460.789585310762,1926.1552505875645,10673.158532726991,9907.699090223898,454.81639486308137,6355.618165841122,19934.734830917627,2858.35606741023,2834.2664976391197,665.469888483012,11592.823783640883,16177.143075924467,12543.725418313918,317.63569157648635,2257.4161561731,15201.471014315619,45645.23591676211,8323.62833082682,376.8168895165831,213132.54291644722,16698.569941711827,663.3571920970296,20710.099320853376,346.28222029939593,513.9332229141203,323.60536008687086,323624.6828685297,1301.212252741579,16913.133647514336,505.4911026847705,632.0783029798903,778.1760841513764,2635.5625663138067,632.0783029798903,76574.72476531091,11492.974658748008,1239.906233381196,5163.94089137517,494.71513791291954,11527.578799029196,19176.79696136619,8979.131225405608,415175.9779430929,2754.6914615362707,2567.0739167219367,378.9394302913829,3747.013377347842,2138.6141864173633,3913.805102706469,10871.491666793676,2516.669574686992,18752.149151877966,636.9170394143573,8323.62833082682,17785.632844249434,2407.321312452001,1686.4843440391876,1051.2986748894868,49435.50780135217,610.4363857799107,960.0492155948752,346.28222029939593,7141.329503589847,8187.405610699661,1148.288091129662,1686.4843440391876,324.6947768318741,320.7835266802878,639820.6545150073,3317.1418553360736,703.1493477716122,340.3871184326995,1531.8394329987157,1686.4843440391876,9236.305456033022,16619.83993793382,463.3944227377964,4287.792726026477,3364.1619784229933,2019.1544585081797,2715.348806915594,5163.94089137517,10185.63650295469,436.06660273854754,26293.480165383102,4218.370391131691,76460.52042984031,7915.880111460361,25978.487349635714,2375.22637673234,489.7605752448112,376.8168895165831,3756.2286447454576,1797.2371240684145,3060.8574440447824,1384.0395692783663,1535.8631900519042,6162.533552625073,2276.6020512892946,14800.536096877271,1535.8631900519042,7368.391717428835,340.3871184326995,40283.97684677175,325.06940680600604,1695.6732046475156,1028.5053850528614,51779.415544128475,92018.35013115287,505.4911026847705,18752.149151877966,3667.712553203018,5968.182975715722,34939.27854355379,6424.274585977019,992.354894936303,30596.638581578518,639820.6545150073,65413.624508456036,324.82371754886964,3747.013377347842,16698.569941711827,19387.390319584494,19387.390319584494,1148.288091129662,6305.285970941779,492.1628050669814,346.28222029939593,14168.403568365671,663.3571920970296,4218.370391131691,45766.272960899,37175.69967364701,369.63985077669076,22891.92825541111,2754.6914615362707,405.6544524204279,319.54816919005003,8698.931074864986,5689.033000226852,11752.173931839643,26293.480165383102,8187.405610699661,117110.65135820676,90877.76737128492,8855.335527589015,3229.016296504339,703.1493477716122,319.72742324233695,1604.2554168141014,3769.9108375996702,396.30197122916206,959.8167805278175,632.0783029798903,999.9729800621147,731503.1707028296,17075.133945943726,5082.686543693957,2107.5040932279385,22891.92825541111,4219.676962291559,5818.94510670397,61516.49587090381,3617.0754679045876,5813.845282617682,7141.329503589847,932.7370475860574,346.28222029939593,415175.9779430929,4555.635369302835,3885.8140646224183,769.1955699368395,953.6647702716481,8337.819513819635,5163.94089137517,48246.709882894014,2276.6020512892946,2886.664844581823,5968.182975715722,7368.391717428835,378.9394302913829,63573.70285359673,40074.17801737161,2276.6020512892946,3138.4978117622,8151.559019833033,319.72742324233695,2286.347850033103,340.3871184326995,172828.89934304,2407.321312452001,410.08048522816836,747.818087951304,407.0672088952674,1088.107600326137,341.517050774033,8698.931074864986,667.3272676429804,39725.42364104348,10673.158532726991,65413.624508456036,959.8167805278175,13195.11592262944,4785.592155979585,2834.2664976391197,15201.471014315619,2963.0052419712265,2425.884027039221,98738.27282520698,1997.7384398419902,115873.73047883328,489.0303345318461,10673.158532726991,9457.064447629586,137428.5757499219,449.0579108608325,1301.212252741579,1085751.3630379443,1502.4134405828336,1684.2601507265988,22891.92825541111,992.354894936303,2244.514746383961,31550.15438840883,316.0906893952995,90462.3574812228,37318.37237669782,639820.6545150073,45645.23591676211,1684.2601507265988,30596.638581578518,56931.5110267007,76460.52042984031,12141.409803201397,95635.30862544454,839.3500692330289,316.62245060704845,13195.11592262944,389.0359797814606,41802.402147140514,19263.507069512012,747.818087951304,1535.8631900519042,2790.085070463646,803.8781088470796,29908.894508749334,648.872490598366,2858.35606741023,415175.9779430929,409.7821718132848,12543.725418313918,17394.662595168105,522.1062342859491,12141.409803201397,9740.51689224938,5199.4234913728005,45766.272960899,2567.0739167219367,4831.715916465941,2417.5880406486763,5082.686543693957,3697.7340063828487,13472.111607949182,20652.753180263422,11602.10423834572,12797.394868451114,471.44073265789405,12543.725418313918,1695.6732046475156,932.7370475860574,10980.322568203586,4555.635369302835,117110.65135820676,2094.8448847085992,54758.90192605476,1826.31018819446,421.9954832228255,410.08771833436947,978.2266656454766,12141.409803201397,4796.262256309382,2236.064155834039,135924.7927322855,1041.1204286449113,40221.84276082843,2273.3851256392354,461.10966099955675,2858.35606741023,143848.60557702882,953.6647702716481,648.872490598366,12664.975331868296,45645.23591676211,489.7605752448112,29908.894508749334,8151.559019833033,2427.747778574022,2262.905415567944,23840.068464074022,866.7970691016989,8187.405610699661,26195.686125920434,449.0579108608325,6353.789558047462,12679.428015781728,2715.348806915594,15684.810589735318,12543.725418313918,504.4494927620649,703.1493477716122,4250.960414621229,11896.354947646087,25978.487349635714,15684.810589735318,5875.699548389586,41802.402147140514,347.0090494475603,16909.279272683176,4555.635369302835,15368.26716660901,16108.716384819742,407.0672088952674,16698.569941711827,388.482661364839,993.553834752781,10573.002781125038,971.4284514069652,450.36262704088375,45766.272960899,76574.72476531091,774.5157686911542,1443.732437768414,19176.79696136619,39725.42364104348,24308.715376885695,8706.164457317907,32091.295039989156,11492.974658748008,13508.076892121393,16108.716384819742,1797.2371240684145,5826.8479598607,15545.52362930884,45766.272960899,438.4351253031718,5826.8479598607,22891.92825541111,65413.624508456036,12552.025861937103,12552.025861937103,932.7370475860574,316.0906893952995,12797.394868451114,2860.1717851882872,11807.964454280216,1123.9090100996427,1913.5805743985902,1816.3382332975395,647.864369876902,8323.62833082682,65413.624508456036,5403.795319141717,4218.370391131691,1531.8394329987157,3111.527273406017,4219.676962291559,11223.080944977459,866.7970691016989,13722.736128528055,436.06660273854754,489.0303345318461],\"yaxis\":\"y\",\"type\":\"scatter\"}], {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Actual\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prediction\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"autosize\":false,\"width\":800,\"height\":500,\"title\":{\"text\":\"R2: 0.869\"}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('239a6e0b-8482-49c8-9619-ebde564b91cd'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; });","title":"\ud83c\udfcb\ufe0f Exercise 2: Optimize Rolling Window Size for Customer Forecasts"},{"location":"S4_Feature_Engineering/#423-image-preprocessing","text":"back to top Image preprocessing is beyond the scope of this session. We cover this topic in General Applications of Neural Networks . For now, know that there is a wealth of considerations for how to handle images, and they all fit within the realm of feature engineering.","title":"4.2.3 Image Preprocessing"},{"location":"S4_Feature_Engineering/#43-transformed-features","text":"back to top Transformed features, are features that we would like to augment based on their relationship within their own distribution or to other (allegedly) independent data within our training set. e.g. we're not deriving new features based on some empirical knowledge of the data, rather we are changing them due to statistical properties that we can assess based on the data itself.","title":"4.3 Transformed Features"},{"location":"S4_Feature_Engineering/#431-skewness","text":"back to top Skewed data can lead to imbalances in our model prediction. Why? Skewed values in the distribution will bias the mean. When assigning weights to this input feature, therefore, the model will give preferential treatment to these values. To demonstrate, I'm going to use scipy to create some skewed data. from scipy.stats import skewnorm a = 10 x = np . linspace ( skewnorm . ppf ( 0.01 , a ), skewnorm . ppf ( 0.99 , a ), 100 ) plt . plot ( x , skewnorm . pdf ( x , a ), 'r-' , lw = 5 , alpha = 0.6 , label = 'skewnorm pdf' ) [<matplotlib.lines.Line2D at 0x7fa48d14a130>] We can now generate a random population based on this distribution r = skewnorm . rvs ( a , size = 1000 ) plt . hist ( r ) (array([113., 267., 225., 172., 116., 62., 26., 13., 2., 4.]), array([-0.19733964, 0.15303313, 0.50340589, 0.85377866, 1.20415142, 1.55452419, 1.90489696, 2.25526972, 2.60564249, 2.95601526, 3.30638802]), <BarContainer object of 10 artists>) Unskewed data will return something close to 0 from calling df.skew() . When dealing with actual data, we can use df.skew() to determine whether we should transform our data. x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) x [ 'Skewed Data' ] . skew () 0.9141902067398219 There are a handful of ways to deal with skewed data: log transform square root transform Box-Cox transform Let's try the first two print ( 'square root transformed skew: {:.4f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) print ( 'log transformed skew: {:.4f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . legend () square root transformed skew: 0.0561 log transformed skew: -1.6916 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in sqrt /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log <matplotlib.legend.Legend at 0x7fa49157d340> We see we didn't get much traction with the log transform, and the log transform will not be able to handle 0 values, and so we will sometimes have to code exceptions for those. Box-Cox is often a good route to go, but it has the added restriction that the data has to all be above 0. Let's create a new distribution with this added restriction a = 6 r = skewnorm . rvs ( a , size = 1000 ) r = [ i for i in r if i > 0 ] plt . hist ( r ) (array([220., 277., 182., 127., 66., 39., 17., 5., 4., 2.]), array([2.17150536e-03, 3.88613862e-01, 7.75056219e-01, 1.16149858e+00, 1.54794093e+00, 1.93438329e+00, 2.32082565e+00, 2.70726800e+00, 3.09371036e+00, 3.48015272e+00, 3.86659507e+00]), <BarContainer object of 10 artists>) from scipy import stats x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ], alpha = 0.5 , label = 'box-cox: {:.2f} ' . format ( pd . DataFrame ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ])[ 0 ] . skew ())) ax . legend () <matplotlib.legend.Legend at 0x7fa492058f40>","title":"4.3.1 Skewness"},{"location":"S4_Feature_Engineering/#exercise-3-transform-data-from-a-gamma-distribution","text":"Repeat section 4.3.1, this time synthesizing a gamma distribution and transforming it. Which transformation best reduces the skew? Do this for a dataset that does not contain values at or below 0. # Cell for exercise 3 from scipy.stats import gamma <matplotlib.legend.Legend at 0x7fa49111dd30>","title":"\ud83c\udfcb\ufe0f Exercise 3: Transform data from a gamma distribution"},{"location":"S4_Feature_Engineering/#432-colinearity","text":"back to top Colinearity can also affect the performance of your machine learning model. In particular, if features are colinear, it can be easy for your model to overfit to your training dataset. This is often mitigated by regularization. If you're curious you can read more about it on this discussion from StackExchange . We will still explore it explicitly here by calculating the Variance Inflation Factor (VIF) on some hypothetical data. VIF = \\frac{1}{1-R^2} Usually we are concerned about data with a VIF above 10","title":"4.3.2 Colinearity"},{"location":"S4_Feature_Engineering/#4321-detecting-colinearity","text":"back to top from statsmodels.stats.outliers_influence import variance_inflation_factor Step 1: Make some data # we can throttle the error rate random . seed ( 42 ) # x2 will be sqrt of x1 plus some error def func ( x , err ): return x ** .5 + ( err * random . randint ( - 1 , 1 ) * random . random () * x ) x0 = range ( 100 ) x1 = [ func ( i , .05 ) for i in x0 ] # HIGH degree of colinearity with x0 x2 = [ func ( i , 1 ) for i in x0 ] # MED degree of colinearity with x0 x3 = [ random . randint ( 0 , 100 ) for i in x0 ] # NO degree of colinearity with x0 # take a look fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) ax . plot ( x0 , x1 , label = 'x1' ) ax . plot ( x0 , x2 , label = 'x2' ) ax . plot ( x0 , x3 , label = 'x3' ) ax . legend () <matplotlib.legend.Legend at 0x7fa49121ca90> To calculate the colinearities I'm going to aggregate these x's into a dataframe: colin = pd . DataFrame ([ x0 , x1 , x2 , x3 ]) . T colin . columns = [ 'x0' , 'x1' , 'x2' , 'x3' ] colin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 0 0.0 0.000000 0.000000 29.0 1 1.0 1.013751 0.721523 28.0 2 2.0 1.400260 1.414214 3.0 3 3.0 1.630546 -0.438007 84.0 4 4.0 2.017388 4.304847 24.0 Step 2: Calculate VIF factors # calculate VIF factors vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( colin . values , i ) for i in range ( colin . shape [ 1 ])] vif [ \"features\" ] = colin . columns Step 3: Inspect VIF factors # inspect VIF factors display ( vif ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 12.555415 x0 1 15.823872 x1 2 1.030609 x2 3 3.559468 x3 In this case, we may remove either x0 or x1 from the dataset.","title":"4.3.2.1 Detecting Colinearity"},{"location":"S4_Feature_Engineering/#4322-fixing-colinearity","text":"back to top It is good to aknowledge where colinearity exists as this will influence the interpretability of your model. In most cases, however, it won't have a heavy influence on the performance of your model. A simple method of dealing with colinearity, is to remove the highest VIF features from your model, iteratively, assessing the performance and determining whether to keep the variable or not. Another method is to create some linear combination of the correlated variables. This is encapsulated in the section on dimensionality reduction.","title":"4.3.2.2 Fixing Colinearity"},{"location":"S4_Feature_Engineering/#433-normalization","text":"back to top Many learning algorithms require zero mean and unit variance to behave optimally. Sklearn preprocessing library contains a very usefull class, StandardScaler for handling this automatically for us. from sklearn.preprocessing import StandardScaler scaler = StandardScaler () normed = scaler . fit_transform ( colin ) colin [[ 'x0' , 'x1' , 'x2' , 'x3' ]] . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'> pd . DataFrame ( normed , columns = [[ 'x0' , 'x1' , 'x2' , 'x3' ]]) . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'>","title":"4.3.3 Normalization"},{"location":"S4_Feature_Engineering/#exercise-4-normalization-affect-on-vif","text":"In the above, we saw how to scale and center variables. How does this affect VIF? Calculate the VIF for the scaled-centered data # Code Cell for Exercise 4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 3.286048 x0 1 3.296881 x1 2 1.015805 x2 3 1.035537 x3","title":"\ud83c\udfcb\ufe0f Exercise 4: Normalization affect on VIF"},{"location":"S4_Feature_Engineering/#434-dimensionality-reduction","text":"back to top Dimensionality reduction is an awesome way to do feature engineering. It is very commonly used. Because it is also an unsupervised machine learning technique, we will visit this topic in that section.","title":"4.3.4 Dimensionality Reduction"},{"location":"S4_Feature_Engineering/#44-missing-data","text":"back to top We will often have missing data in our datasets. How do we deal with this? Let's start by making some data with missing data. We'll use a numpy nan datatype to do this from numpy import nan X = np . array ([[ nan , 0 , 3 ], [ 3 , 7 , 9 ], [ 3 , 5 , 2 ], [ 4 , nan , 6 ], [ 8 , 8 , 1 ]]) y = np . array ([ 14 , 16 , - 1 , 8 , - 5 ])","title":"4.4 Missing Data"},{"location":"S4_Feature_Engineering/#441-imputation","text":"back to top A very common strategy is to impute or fill in the missing data, based on basic statistical descriptions of the feature column (mode, mean, and median) from sklearn.impute import SimpleImputer # strategy = 'mean' will replace nan's with mean value # of the column # others are median and most_frequent (mode) imp = SimpleImputer ( strategy = 'mean' ) X2 = imp . fit_transform ( X ) X2 array([[4.5, 0. , 3. ], [3. , 7. , 9. ], [3. , 5. , 2. ], [4. , 5. , 6. ], [8. , 8. , 1. ]])","title":"4.4.1 Imputation"},{"location":"S4_Feature_Engineering/#442-other-strategies","text":"back to top Depending on the severity of missing data, you will sometimes opt to remove the whole column, or perhaps apply some simple learning to fill in the missing data. This is a great article on more advanced strategies for handling missing data.","title":"4.4.2 Other Strategies"},{"location":"S4_Feature_Engineering/#references","text":"back to top Box Cox Multicolinearity Missing Data","title":"References"},{"location":"S5_Unsupervised_Learning/","text":"Data Science Foundations Session 5: Unsupervised Learning: Clustering and Dimensionality Reduction \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we began our discussion on feature engineering and ended with a sneak peak into dimensionality reduction. This last topic deserves a whole session on its own because its use case is not limited to feature engineering! It can also be used as a tool for visualization, for noise filtering, and much more. In this session we discuss dimensionality reduction along with other unsupervised learning methods. Up until now, the only learning estimators we've looked at were supervised ones: estimators that predict labels based on training data. Here, however, we are interested in uncovering aspects of the data without reference to any known labels. The usefulness for these learners will become immediately apparent when we revist our wine quality models! 5.0 Preparing Environment and Importing Data \u00b6 back to top 5.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score , silhouette_score , calinski_harabasz_score from sklearn.mixture import GaussianMixture from sklearn.impute import SimpleImputer from scipy.spatial.distance import cdist import seaborn as sns ; sns . set () import copy 5.0.2 Load and Process Dataset \u00b6 back to top wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X_wine = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X_wine [:, 2 :] = scaler . fit_transform ( X_wine [:, 2 :]) wine = pd . DataFrame ( X_wine , columns = cols ) wine [ 'density' ] = y 5.1 Principal Component Analysis \u00b6 back to top Principle Component Analysis or PCA is one of the most wide spread implementations of dimensionality reduction. In PCA, we find the principle components, or linear recombinations of the dimensions of the data, that best explain the variance of the data. 5.1.1 The Covariance Matrix \u00b6 back to top In the cell below, we have plotted acidity and density from our familiar wine dataset. fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) wine . loc [ wine [ 'red' ] == 1 ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> X = wine . loc [ wine [ 'red' ] == 1 ][[ 'fixed acidity' , 'density' ]] . values X [: 5 ] array([[0.14156636, 0.9978 ], [0.45029132, 0.9968 ], [0.45029132, 0.997 ], [3.07445349, 0.998 ], [0.14156636, 0.9978 ]]) The principal components, are actually the eigenvectors of the covariance matrix of the standardized data. We will perform this by hand to get an understanding. First we standardize the data from sklearn.preprocessing import StandardScaler X_std = StandardScaler () . fit_transform ( X ) # note I've already done this in 5.0.2 Then we compute the covariance matrix. There is a nice demonstration of computing covariance on stats quest . The covariance can be expressed as: cov(X,Y) = \\frac{1}{n^2}\\sum\\sum(x_i - x_j)(y_i - y_j) \\;\\;\\;\\;\\;\\sf eq. 1 Every \\((x_i - x_j)(y_i - y_j)\\) is the area described by the rectangle between points \\(i\\) and \\(j\\), and we if we deem to color positive changes as red and negative ones as blue, we get a picture like the following: When we view all the pairwise interactions in aggregate we get a sense of how the areas of rectangles made by each observation influence the covariance: As a side note, the covariance term is the numerator in the pearsons correlation we covered last week: \\rho_{x,y} = \\frac{cov(X,Y)}{\\sigma_x\\sigma_y} \\;\\;\\;\\;\\;\\sf eq. 2 Extrapolating \\(Eq. 1\\) across the entire matrix, \\(X\\) of datapoints: C = \\frac{1}{n-1}(X - \\bar{X})^{T} \\cdot (X - \\bar{X}) \\;\\;\\;\\;\\;\\sf eq. 3 The covariance matrix of our wine data can be obtained from \\(Eq. 3\\): import numpy as np mean_vec = np . mean ( X_std , axis = 0 ) cov_mat = ( X_std - mean_vec ) . T . dot (( X_std - mean_vec )) / ( X_std . shape [ 0 ] - 1 ) print ( 'Covariance matrix \\n %s ' % cov_mat ) Covariance matrix [[1.00062578 0.66847772] [0.66847772 1.00062578]] 5.1.2 How Does the Covariance Matrix Relate to the New Coordinate System \u00b6 We desire a new coordinate system that has no covariance between its dimensions (thus each dimension can be sorted by explained variance to isolate key dimensions (i.e. principal components)) Because the covariance matrix in \\(Eq. 3\\) is a square matrix, we can diagonalize it; the new dimensional space whose covariance matrix is expressed by this diagonolized matrix will have the desired properties explained in point 1 (because everything off the diagonal is zero) The difficult and unintuitive part of PCA is that the vector that produces this transformation to the new coordinate space is given by the eigenvectors of \\(C\\). For those who are interested in investigating further I suggest reading this answer by amoeba and this answer by cardinal . For a more geometric explanation of the principal components checkout the grandparent, spouse, daughter parable These arguments coincide with the Spectral theorem explanation of PCA, and you can read more about it in the links provided above In 5.1.2.1-5.1.2.3 I provide a segue into deriving eigenvectors and eigenvalues, feel free to visit these foundational topics, although they are not necessary to reap the value of PCA. For this particular set of wine data, we will see that the corresponding diagonalized matrix will look like: \\begin{bmatrix} 1.67 & 0 \\\\ 0 & 0.33 \\end{bmatrix} At the end of 5.1.3 we will show that this is also the covariance matrix of our data projected into the new coordinate system! \ud83c\udf2d 5.1.2.1 Enrichment: Deriving the Eigenvectors and Eigenvalues \u00b6 The principal components are found mathematically by determining the eigenvectors of the covariance matrix and sorting them by their egienvalues, i.e. their explained variance. There is a mathematical proof 1 , 2 for why this works, but we will not cover that here. So how do we calculate eigenvalues and their correpsonding eigenvectors? This question is usually posited in the following format: A \\cdot v = \\lambda \\cdot v \\;\\;\\;\\;\\;\\sf eq. 3 In this equation A is an n-by-n matrix (our covariance matrix in this case), v is a non-zero n-by-1 vector and \u03bb is a scalar (which may be either real or complex). Any value of \u03bb for which this equation has a solution is known as an eigenvalue of the matrix A. In other words v, is an eigenvector of A if there exists a scalar value such that \\(A \\cdot v\\) and \\(\\lambda \\cdot v\\) will yield the same result In high school or college, we might've solved for these eigenvalues and eigenvectors by hand using simple, integer-bound matrices of A. Here's an example, for this matrix: \\begin{bmatrix} -6 & 3 \\\\ 4 & 5\\end{bmatrix} an eigenvector is: \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} with a corresponding eigenvalue of 6. Taking the requisite dot products for each side of eq. 3, \\(A v\\) gives us: \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} and \\(\\lambda v\\): 6 \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24\\end{bmatrix} \ud83c\udf2d 5.1.2.2: Find the Eigenvalues \u00b6 The trick that is employed to decompose these equality statements is to multiply the right hand side of eq. 3 by an identity matrix and then subtract this quantity from both sides of the equation. In the case of \\(v\\) being non-zero, this becomes the determinant : | A - \\lambda I | = 0 In the case of our simple example \\begin{vmatrix} \\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\end{vmatrix} = 0 simplifies to \\begin{vmatrix} -6-\\lambda & 3 \\\\ 4 & 5-\\lambda \\end{vmatrix} = 0 writing out the determinant (-6-\\lambda)(5-\\lambda) - 3 x 4 = 0 gives the quadratic equation \\lambda^2 + \\lambda - 42 = 0 and solving for \\(\\lambda\\) \\lambda = -7 \\space or \\space 6 from scipy.optimize import fsolve , leastsq A = np . array ([[ - 6 , 3 ], [ 4 , 5 ]]) I = np . array ([[ 1 , 0 ],[ 0 , 1 ]]) # define the determinant def det ( lamb ): \"\"\" A: the covariance matrix I: the identity matrix \"\"\" return ( A [ 0 , 0 ] - lamb ) * ( A [ 1 , 1 ] - lamb ) - ( A [ 0 , 1 ] * A [ 1 , 0 ]) root = fsolve ( det , [ - 10 , 10 ]) print ( np . isclose ( det ( root ), [ 0.0 , 0.0 ])) print ( root ) [ True True] [-7. 6.] \ud83c\udf2d 5.1.2.3: Find the Eigenvectors \u00b6 We find the eigenvector for each corresponding eigenvalue one at a time \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 6 \\begin{bmatrix} x \\\\ y \\end{bmatrix} multiplying out gives the system of equations -6x + 3y = 6x 4x + 5y = 6y bringing to the left hand side -12x + 3y = 0 4x - 1y = 0 solving for either equation yeilds \\(y = 4x\\) so the eigenvector is \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} def eig ( vec ): \"\"\" A: the covariance matrix lamb: the eigen value \"\"\" return [ A [ 0 , 0 ] * vec [ 0 ] + A [ 0 , 1 ] * vec [ 1 ] - lamb * vec [ 0 ], A [ 1 , 0 ] * vec [ 0 ] + A [ 1 , 1 ] * vec [ 1 ] - lamb * vec [ 1 ]] lamb = round ( root [ 0 ]) vector = fsolve ( eig , [ 1 , - 4 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () lamb = round ( root [ 1 ]) vector = fsolve ( eig , [ 1 , 10 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () -7 [ True True] [-3. 1.] 6 [ True True] [1. 4.] # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( A ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.9486833 -0.24253563] [ 0.31622777 -0.9701425 ]] Eigenvalues [-7. 6.] 5.1.3 Projecting onto the Principal Components \u00b6 To complete our principal component analysis, we need to project our data onto the eigenvectors of the covariance matrix. We can obtain the eigenvectors and corresponding eigenvalues using np or scipy . Here I've completed the task with np : cov_mat = np . cov ( X_std . T ) # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( cov_mat ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.70710678 -0.70710678] [ 0.70710678 -0.70710678]] Eigenvalues [0.33214806 1.6691035 ] And now we are just going to sort our vectors by their eigenvalues to get the proper order of principal components: # Make a list of (eigenvalue, eigenvector) tuples eig_pairs = [( np . abs ( eig_vals [ i ]), eig_vecs [:, i ]) for i in range ( len ( eig_vals ))] # Sort the (eigenvalue, eigenvector) tuples from high to low eig_pairs . sort ( key = lambda x : x [ 0 ], reverse = True ) # Visually confirm that the list is correctly sorted by decreasing eigenvalues print ( 'Eigenvalues in descending order:' ) for i in eig_pairs : print ( i [ 0 ]) Eigenvalues in descending order: 1.669103500110071 0.3321480643454986 eig_pairs [(1.669103500110071, array([-0.70710678, -0.70710678])), (0.3321480643454986, array([-0.70710678, 0.70710678]))] For those of us who are familiar with eigenvectors, we should recognize that they are necessarily orthogonal to one another. This is good to know from the PCA point of view, because we wouldn't want the variance along one vector to be explained by another. Let's plot these vectors along with the standardized data: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 7 )) ax . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . set_aspect ( 'equal' ) ax . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) (-2.138871623907465, 4.356979103463171) We indeed see that these vectors are orthogonal. Continuing on with our task of projecting the data onto our principal components, in order to project our data onto the PCs I'll need to reshape eig_pairs : matrix_w = np . hstack (( eig_pairs [ 0 ][ 1 ] . reshape ( 2 , 1 ), eig_pairs [ 1 ][ 1 ] . reshape ( 2 , 1 ))) print ( 'Matrix W: \\n ' , matrix_w ) Matrix W: [[-0.70710678 -0.70710678] [-0.70710678 0.70710678]] And now taking the dot product: Y = X_std . dot ( matrix_w ) plt . scatter ( Y [:, 0 ], Y [:, 1 ]) <matplotlib.collections.PathCollection at 0x7f681bdf3100> We see that our data is dispersed nicely along these PCs. Finally to tie this in with the point made at the end of 5.1.2, we see that the covariance matrix for the data in this new space is described by the diagonalized matrix of the former dimensional space: mean_vec = np . mean ( Y , axis = 0 ) cov_mat = ( Y - mean_vec ) . T . dot (( Y - mean_vec )) / ( Y . shape [ 0 ] - 1 ) cov_mat array([[1.66910350e+00, 1.76746394e-16], [1.76746394e-16, 3.32148064e-01]]) 5.1.4 Cumulative Explained Variance \u00b6 Often we will need to decide just how many principal components are enough, especially with high dimensional data containing many colinear variables. To assist with this, data scientists will plot the cumulative explained variance. The explained variance is captured by the eigenvalues (this is why we sort by the eigenvalues in the first place). tot = sum ( eig_vals ) var_exp = [( i / tot ) * 100 for i in sorted ( eig_vals , reverse = True )] cum_var_exp = np . cumsum ( var_exp ) with plt . style . context ( 'seaborn-whitegrid' ): plt . figure ( figsize = ( 7 , 4 )) plt . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'individual explained variance' ) plt . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'cumulative explained variance' ) plt . ylabel ( 'Explained variance ratio' ) plt . xlabel ( 'Principal components' ) plt . legend ( loc = 'center right' ) plt . tight_layout () 5.1.5 PCA with Scikit-Learn \u00b6 But we can avoid the fancy footwork and do all this in sklearn! from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) We see that the values we get are the same as for the hand-calculated eigenvalues and vectors print ( pca . components_ ) [[ 0.70710678 0.70710678] [ 0.70710678 -0.70710678]] And the eigenvalues are under pca.explained_variance_ print ( pca . explained_variance_ ) [1.6691035 0.33214806] 5.1.6 PCA as Dimensionality Reduction \u00b6 back to top One obvious use case for PCA, is to drop the dimensions with lowest explained variance. Continuing with our acidity/density example this will look like the following from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . set_aspect ( 'equal' ) ax [ 0 ] . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax [ 0 ] . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) ax [ 0 ] . set_ylabel ( 'Normalized density' ) ax [ 0 ] . set_xlabel ( 'Normalized acidity' ) ax [ 1 ] . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'Individual' ) ax [ 1 ] . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'Cumulative' ) ax [ 1 ] . set_ylabel ( 'Explained variance ratio' ) ax [ 1 ] . set_xlabel ( 'Principal components' ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f68189c5520> We can capture 80% of the explained variance along just the first principal component. What does this projection look like? # we set our components to 1 pca = PCA ( n_components = 1 ) pca . fit ( X_std ) # we then project the data onto the first PC # and then rebroadcast this transformation # back onto the orginal dimensions to see # what this looks like in terms of acidity/density X_pca = pca . inverse_transform ( pca . transform ( X_std )) # original data plt . scatter ( X_std [:, 0 ], X_std [:, 1 ], alpha = 0.2 ) # projected data plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.8 ) <matplotlib.collections.PathCollection at 0x7f681c292ee0> \ud83d\ude4b\u200d\u2640\ufe0f Question 1: Differentiating PCA from linear regression \u00b6 How is PCA different from linear regression? PCA tries to find a best fit line through the data points but it minimizes the orthogonal distance to the model line where as Linear Regression minimizes distance from the perspective of the axis you're regressing with respect to. Explore more here 5.1.7 PCA for visualization \u00b6 back to top For classification tasks, PCA lends itself as a useful method for seeing how the classes separate on the highest variance dimensions of the data without consideration to the classes themselves, i.e. do our classes seperate out according to the other variables in the dataset. With our wine dataset, we see that the wine types fall out nicely along the first two principal components X_std = StandardScaler () . fit_transform ( wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = wine [ 'white' ] . values , edgecolor = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC') \ud83c\udf2d 5.1.8 Enrichment: PCA as Outlier Removal and Noise Filtering \u00b6 back to top In some cases, it can be advantageous to use PCA as a method for outlier removal. There are many caveats to this that we will not discuss here. But for those who are interested, this is a great overview article. As for noise filteration, Vanderplas' DS handbook has a good section on the topic as does Guido/Muller's Intro to ML with Python by the same pusblisher. 5.1.9 PCA for Feature Engineering \u00b6 back to top Finally, PCA is actually a commonly used preprocessing technique for supervised machine learning models. In the next exercise, our goal will be to use PCA to generate a new set of features, to feed into our linear model. \ud83c\udfcb\ufe0f Exercise 1: PCA as Preprocessing for Models \u00b6 Using the wine data, select any number of the first principal components and attemp to predict density for the red wine data. Consider that the maximum number of principal components are: print ( f \"Max principal components: { X . shape [ 1 ] } \" ) # Code Cell for Exercise 1 ################################################################################ ##### CREATE A SKLEARN-PCA OBJECT, FIT AND TRANSFORM TO THE WINE DATA ########## ################################################################################ # as you do this, be sure to remove 'density' from the input features ################################################################################ ############################## UNCOMMENT THE BELOW ############################# ################################################################################ # plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2, c=wine['white'].values, # edgecolor='grey') # plt.xlabel('First PC') # plt.ylabel('Second PC') # plt.show() # model = LinearRegression() # X_train, X_test, y_train, y_test = train_test_split(X_pca, y_wine, train_size=0.8, random_state=42) # model.fit(X_train, y_train) # y_pred = model.predict(X_test) # print(r2_score(y_test, y_pred)) # print(r2_score(y_train, model.predict(X_train))) 0.9634516142421967 0.953295487875815 5.2 K-Means Clustering \u00b6 back to top We now embark on a second class of unsupervised learning techinques: clustering. The K-means algorithm works under two assumptions: * every cluster can be defined by an arithmetic mean or cluster center * each point is closer to one arithmetic center than the other centers Let's turn back to our wine dataset: X_std = StandardScaler () . fit_transform ( X_wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC') It is fairly obvious under these two dimensions that there are two clusters. The K-means algorithm automatically pics this out from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = 2 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); wa-la! Our simple unsupervised algorithm does a pretty good job of differentiating between the red and white wine clusters. 5.2.1 The Algorithm: Expectation-Maximization \u00b6 back to top We won't go too far into the implementation of the underlying algorithm here. In its basic steps, however, it performs the following: Initialize cluster centers (random guess) Then repeat: E-Step: assign points to the nearest center (arithmetic distance) M-step: set the new center point for each cluster according to the mean of it's datapoint members More information on K-means algorithm can be explored here 5.2.2 Limitations \u00b6 back to top A few brief notes on limitations: the global optimum may not be achieved (no guarantee of finding the overall best solution) the number of clusters must be guessed beforehand cluster boundaries are unavoidably linear and the cluster assignments are unavoidably circular can be slow for large datasets cluster assignments are non probabilistic 3 and 5 motivate our next section, Gaussian Mixture Models 5.2.3 Determining K with the Elbow Method \u00b6 The elbow method is a popular technique for determining the value of k . It involves looping through a range of k 's and assessing some goodness of fit metric. Intuitively, we might presume those metrics involve some measure of the distance of datapoints to their cluster centers. We have options: Distortion: distance ( typically Euclidean ) from the cluster centers averaged across the respective clusters. Inertia: the sum of squared distances of samples to their closest cluster center. Silhouette: calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of The Silhouette Coefficient for a sample is (b-a) / max(b-a) best value is 1 worst value is -1 Calinski Harabasz Score or Variance Ratio: the ratio between within-cluster dispersion and between-cluster dispersion should sound familiar to our ANOVA discussion higher is better And there are many other methods of evaluating cluster assignment performance \ud83d\ude4b\u200d\u2640\ufe0f Question 2: Comparing Metrics \u00b6 What is the primary difference between Distortion, Inertia vs Silhouette, Calinksi? distortions = [] inertias = [] silhouette = [] variance = [] for k in range ( 1 , 10 ): kmeans = KMeans ( n_clusters = k ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) labels = kmeans . labels_ distortions . append ( sum ( np . min ( cdist ( X_pca , kmeans . cluster_centers_ , 'euclidean' ), axis = 1 )) / X . shape [ 0 ]) inertias . append ( kmeans . inertia_ ) if k > 1 : silhouette . append ( silhouette_score ( X_pca , labels , metric = 'euclidean' )) variance . append ( calinski_harabasz_score ( X_pca , labels )) We note in the following plots that inertia and distortion asymptotically improve with higher k (because they are unregularized) whereas silhouette and calinski metrics are penalized for inter-cluster relatability fig , [[ ax1 , ax2 ], [ ax3 , ax4 ]] = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) ax1 . plot ( range ( 1 , 10 ), distortions ) ax2 . plot ( range ( 1 , 10 ), inertias ) ax3 . plot ( range ( 2 , 10 ), silhouette ) ax4 . plot ( range ( 2 , 10 ), variance ) [<matplotlib.lines.Line2D at 0x7f67fc5d8bb0>] kmeans = KMeans ( n_clusters = 3 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); 5.3 Gaussian Mixture Models \u00b6 back to top in the simplest case, GMMs can be used in the same way as K-means from sklearn.mixture import GaussianMixture gmm = GaussianMixture ( n_components = 2 ) . fit ( X_pca ) labels = gmm . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , alpha = 0.2 , edgecolor = 'grey' ); But because GMM is a probablistic model, we can visualize the certainty with which we assign these clusters. probs = gmm . predict_proba ( X_pca ) print ( probs [ 5 : 20 ] . round ( 3 )) [[0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.]] # convert probs to 1 dimension probs . max ( 1 ) array([1. , 0.99999994, 0.99999999, ..., 1. , 1. , 0.99999189]) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = probs . max ( 1 ), s = 40 , cmap = 'Blues' , alpha = 0.5 , edgecolor = 'grey' ); 5.3.1 Generalizing E-M for GMMs \u00b6 back to top The algorithm for GMMs is very similar to K-means, but now the EM steps are probablistic 1 2 Initialize cluster centers (random guess) Then repeat: E-Step: assign points their probability of belonging to every cluster M-step: set the new center point for each cluster according to the probabilities of all datapoints 5.3.2 GMMs as a Data Generator \u00b6 back to top One particularly fascinating application of GMMs is to use them to generate new data that is similar to the data on which they are modeled. Let's take an example. We're going to use sklearn make_circles function to create some arbitrary data that has a complex relationship along two different axes. # some helper functions borrowed from Jake Vanderplas with a few minor tweaks # https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html from matplotlib.patches import Ellipse def draw_ellipse ( position , covariance , ax = None , ** kwargs ): \"\"\"Draw an ellipse with a given position and covariance\"\"\" ax = ax or plt . gca () # Convert covariance to principal axes if covariance . shape == ( 2 , 2 ): U , s , Vt = np . linalg . svd ( covariance ) angle = np . degrees ( np . arctan2 ( U [ 1 , 0 ], U [ 0 , 0 ])) width , height = 2 * np . sqrt ( s ) else : angle = 0 width , height = 2 * np . sqrt ( covariance ) # Draw the Ellipse for nsig in range ( 1 , 4 ): ax . add_patch ( Ellipse ( position , nsig * width , nsig * height , angle , ** kwargs )) def plot_gmm ( gmm , X , label = True , ax = None , data_alpha = 1 ): ax = ax or plt . gca () labels = gmm . fit ( X ) . predict ( X ) if label : ax . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , zorder = 2 , alpha = data_alpha ) else : ax . scatter ( X [:, 0 ], X [:, 1 ], s = 40 , zorder = 2 , alpha = data_alpha ) ax . axis ( 'equal' ) w_factor = 0.2 / gmm . weights_ . max () for pos , covar , w in zip ( gmm . means_ , gmm . covariances_ , gmm . weights_ ): draw_ellipse ( pos , covar , alpha = w * w_factor ) from sklearn.datasets import make_circles as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ]); As a side note, as a clustering model, the GMM is not particularly useful: gmm2 = GaussianMixture ( n_components = 2 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm2 , X ) But if we use the GMM as a density estimator on the underlying data, rather than as a clustering algorithm, we find a completely different use case: gmm16 = GaussianMixture ( n_components = 16 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm16 , X , label = False ) Now, with the distributions drawn, we can assemble entirely new data: Xnew = gmm16 . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); 5.3.2.1 Determining the number of components \u00b6 back to top Let's think back to session 1 on model selection. How might we determine the best number of components? A couple analytic approaches that we have not much yet discussed, are the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The important thing to note from each of these is that they penalize added complexity to the models, and we would like to minimize their estimated prediction error. n_components = np . arange ( 1 , 42 ) models = [ GaussianMixture ( n , covariance_type = 'full' , random_state = 42 ) . fit ( X ) for n in n_components ] plt . plot ( n_components , [ m . bic ( X ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( X ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); plt . ylabel ( 'est. prediction error' ) Text(0, 0.5, 'est. prediction error') Both AIC and BIC capture a local minimum on the order of 6 Gaussians. This is where the model captures the overall circular modality of the data. However it totally misses the circumscribed nature of the two circles. Increasing this number to 30-40 gaussians captures this feature of the data. The AIC score reflects this while the BIC score (although captures a local minima in the area) does not define this as a global optimum. gmmNew = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmmNew , X , label = True , data_alpha = 0 ) Xnew = gmmNew . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); \ud83c\udfcb\ufe0f Exercise 2: Determine Number of Components for Circular Moons \u00b6 Repeat the above, this time using sklearn.datasets.make_moons # Code Cell for Exercise 2 from sklearn.datasets import make_moons as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) ################################################################################ ##### FIT GMM MODEL(S) TO 1-42 CLUSTER CENTERS AND RECORD THE AIC/BIC ########## ################################################################################ # uncomment these lines # plt.plot(n_components, [m.bic(X) for m in models], label='BIC') # plt.plot(n_components, [m.aic(X) for m in models], label='AIC') # plt.legend(loc='best') # plt.xlabel('n_components'); # plt.ylabel('est. prediction error') Text(0, 0.5, 'est. prediction error') gmm_moon = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm_moon , X ) Xnew = gmm_moon . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); References \u00b6 PCA \u00b6 Intuitive PCA PCA and Eigenvectors/values GMM \u00b6 GMMs Explained Derive GMM Exercise","title":"Unsupervised Learning"},{"location":"S5_Unsupervised_Learning/#data-science-foundations-session-5-unsupervised-learning-clustering-and-dimensionality-reduction","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we began our discussion on feature engineering and ended with a sneak peak into dimensionality reduction. This last topic deserves a whole session on its own because its use case is not limited to feature engineering! It can also be used as a tool for visualization, for noise filtering, and much more. In this session we discuss dimensionality reduction along with other unsupervised learning methods. Up until now, the only learning estimators we've looked at were supervised ones: estimators that predict labels based on training data. Here, however, we are interested in uncovering aspects of the data without reference to any known labels. The usefulness for these learners will become immediately apparent when we revist our wine quality models!","title":"Data Science Foundations  Session 5: Unsupervised Learning: Clustering and Dimensionality Reduction"},{"location":"S5_Unsupervised_Learning/#50-preparing-environment-and-importing-data","text":"back to top","title":"5.0 Preparing Environment and Importing Data"},{"location":"S5_Unsupervised_Learning/#501-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score , silhouette_score , calinski_harabasz_score from sklearn.mixture import GaussianMixture from sklearn.impute import SimpleImputer from scipy.spatial.distance import cdist import seaborn as sns ; sns . set () import copy","title":"5.0.1 Import Packages"},{"location":"S5_Unsupervised_Learning/#502-load-and-process-dataset","text":"back to top wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X_wine = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X_wine [:, 2 :] = scaler . fit_transform ( X_wine [:, 2 :]) wine = pd . DataFrame ( X_wine , columns = cols ) wine [ 'density' ] = y","title":"5.0.2 Load and Process Dataset"},{"location":"S5_Unsupervised_Learning/#51-principal-component-analysis","text":"back to top Principle Component Analysis or PCA is one of the most wide spread implementations of dimensionality reduction. In PCA, we find the principle components, or linear recombinations of the dimensions of the data, that best explain the variance of the data.","title":"5.1 Principal Component Analysis"},{"location":"S5_Unsupervised_Learning/#511-the-covariance-matrix","text":"back to top In the cell below, we have plotted acidity and density from our familiar wine dataset. fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) wine . loc [ wine [ 'red' ] == 1 ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> X = wine . loc [ wine [ 'red' ] == 1 ][[ 'fixed acidity' , 'density' ]] . values X [: 5 ] array([[0.14156636, 0.9978 ], [0.45029132, 0.9968 ], [0.45029132, 0.997 ], [3.07445349, 0.998 ], [0.14156636, 0.9978 ]]) The principal components, are actually the eigenvectors of the covariance matrix of the standardized data. We will perform this by hand to get an understanding. First we standardize the data from sklearn.preprocessing import StandardScaler X_std = StandardScaler () . fit_transform ( X ) # note I've already done this in 5.0.2 Then we compute the covariance matrix. There is a nice demonstration of computing covariance on stats quest . The covariance can be expressed as: cov(X,Y) = \\frac{1}{n^2}\\sum\\sum(x_i - x_j)(y_i - y_j) \\;\\;\\;\\;\\;\\sf eq. 1 Every \\((x_i - x_j)(y_i - y_j)\\) is the area described by the rectangle between points \\(i\\) and \\(j\\), and we if we deem to color positive changes as red and negative ones as blue, we get a picture like the following: When we view all the pairwise interactions in aggregate we get a sense of how the areas of rectangles made by each observation influence the covariance: As a side note, the covariance term is the numerator in the pearsons correlation we covered last week: \\rho_{x,y} = \\frac{cov(X,Y)}{\\sigma_x\\sigma_y} \\;\\;\\;\\;\\;\\sf eq. 2 Extrapolating \\(Eq. 1\\) across the entire matrix, \\(X\\) of datapoints: C = \\frac{1}{n-1}(X - \\bar{X})^{T} \\cdot (X - \\bar{X}) \\;\\;\\;\\;\\;\\sf eq. 3 The covariance matrix of our wine data can be obtained from \\(Eq. 3\\): import numpy as np mean_vec = np . mean ( X_std , axis = 0 ) cov_mat = ( X_std - mean_vec ) . T . dot (( X_std - mean_vec )) / ( X_std . shape [ 0 ] - 1 ) print ( 'Covariance matrix \\n %s ' % cov_mat ) Covariance matrix [[1.00062578 0.66847772] [0.66847772 1.00062578]]","title":"5.1.1 The Covariance Matrix"},{"location":"S5_Unsupervised_Learning/#512-how-does-the-covariance-matrix-relate-to-the-new-coordinate-system","text":"We desire a new coordinate system that has no covariance between its dimensions (thus each dimension can be sorted by explained variance to isolate key dimensions (i.e. principal components)) Because the covariance matrix in \\(Eq. 3\\) is a square matrix, we can diagonalize it; the new dimensional space whose covariance matrix is expressed by this diagonolized matrix will have the desired properties explained in point 1 (because everything off the diagonal is zero) The difficult and unintuitive part of PCA is that the vector that produces this transformation to the new coordinate space is given by the eigenvectors of \\(C\\). For those who are interested in investigating further I suggest reading this answer by amoeba and this answer by cardinal . For a more geometric explanation of the principal components checkout the grandparent, spouse, daughter parable These arguments coincide with the Spectral theorem explanation of PCA, and you can read more about it in the links provided above In 5.1.2.1-5.1.2.3 I provide a segue into deriving eigenvectors and eigenvalues, feel free to visit these foundational topics, although they are not necessary to reap the value of PCA. For this particular set of wine data, we will see that the corresponding diagonalized matrix will look like: \\begin{bmatrix} 1.67 & 0 \\\\ 0 & 0.33 \\end{bmatrix} At the end of 5.1.3 we will show that this is also the covariance matrix of our data projected into the new coordinate system!","title":"5.1.2 How Does the Covariance Matrix Relate to the New Coordinate System"},{"location":"S5_Unsupervised_Learning/#5121-enrichment-deriving-the-eigenvectors-and-eigenvalues","text":"The principal components are found mathematically by determining the eigenvectors of the covariance matrix and sorting them by their egienvalues, i.e. their explained variance. There is a mathematical proof 1 , 2 for why this works, but we will not cover that here. So how do we calculate eigenvalues and their correpsonding eigenvectors? This question is usually posited in the following format: A \\cdot v = \\lambda \\cdot v \\;\\;\\;\\;\\;\\sf eq. 3 In this equation A is an n-by-n matrix (our covariance matrix in this case), v is a non-zero n-by-1 vector and \u03bb is a scalar (which may be either real or complex). Any value of \u03bb for which this equation has a solution is known as an eigenvalue of the matrix A. In other words v, is an eigenvector of A if there exists a scalar value such that \\(A \\cdot v\\) and \\(\\lambda \\cdot v\\) will yield the same result In high school or college, we might've solved for these eigenvalues and eigenvectors by hand using simple, integer-bound matrices of A. Here's an example, for this matrix: \\begin{bmatrix} -6 & 3 \\\\ 4 & 5\\end{bmatrix} an eigenvector is: \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} with a corresponding eigenvalue of 6. Taking the requisite dot products for each side of eq. 3, \\(A v\\) gives us: \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} and \\(\\lambda v\\): 6 \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24\\end{bmatrix}","title":"\ud83c\udf2d 5.1.2.1 Enrichment: Deriving the Eigenvectors and Eigenvalues"},{"location":"S5_Unsupervised_Learning/#5122-find-the-eigenvalues","text":"The trick that is employed to decompose these equality statements is to multiply the right hand side of eq. 3 by an identity matrix and then subtract this quantity from both sides of the equation. In the case of \\(v\\) being non-zero, this becomes the determinant : | A - \\lambda I | = 0 In the case of our simple example \\begin{vmatrix} \\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\end{vmatrix} = 0 simplifies to \\begin{vmatrix} -6-\\lambda & 3 \\\\ 4 & 5-\\lambda \\end{vmatrix} = 0 writing out the determinant (-6-\\lambda)(5-\\lambda) - 3 x 4 = 0 gives the quadratic equation \\lambda^2 + \\lambda - 42 = 0 and solving for \\(\\lambda\\) \\lambda = -7 \\space or \\space 6 from scipy.optimize import fsolve , leastsq A = np . array ([[ - 6 , 3 ], [ 4 , 5 ]]) I = np . array ([[ 1 , 0 ],[ 0 , 1 ]]) # define the determinant def det ( lamb ): \"\"\" A: the covariance matrix I: the identity matrix \"\"\" return ( A [ 0 , 0 ] - lamb ) * ( A [ 1 , 1 ] - lamb ) - ( A [ 0 , 1 ] * A [ 1 , 0 ]) root = fsolve ( det , [ - 10 , 10 ]) print ( np . isclose ( det ( root ), [ 0.0 , 0.0 ])) print ( root ) [ True True] [-7. 6.]","title":"\ud83c\udf2d  5.1.2.2: Find the Eigenvalues"},{"location":"S5_Unsupervised_Learning/#5123-find-the-eigenvectors","text":"We find the eigenvector for each corresponding eigenvalue one at a time \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 6 \\begin{bmatrix} x \\\\ y \\end{bmatrix} multiplying out gives the system of equations -6x + 3y = 6x 4x + 5y = 6y bringing to the left hand side -12x + 3y = 0 4x - 1y = 0 solving for either equation yeilds \\(y = 4x\\) so the eigenvector is \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} def eig ( vec ): \"\"\" A: the covariance matrix lamb: the eigen value \"\"\" return [ A [ 0 , 0 ] * vec [ 0 ] + A [ 0 , 1 ] * vec [ 1 ] - lamb * vec [ 0 ], A [ 1 , 0 ] * vec [ 0 ] + A [ 1 , 1 ] * vec [ 1 ] - lamb * vec [ 1 ]] lamb = round ( root [ 0 ]) vector = fsolve ( eig , [ 1 , - 4 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () lamb = round ( root [ 1 ]) vector = fsolve ( eig , [ 1 , 10 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () -7 [ True True] [-3. 1.] 6 [ True True] [1. 4.] # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( A ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.9486833 -0.24253563] [ 0.31622777 -0.9701425 ]] Eigenvalues [-7. 6.]","title":"\ud83c\udf2d  5.1.2.3: Find the Eigenvectors"},{"location":"S5_Unsupervised_Learning/#513-projecting-onto-the-principal-components","text":"To complete our principal component analysis, we need to project our data onto the eigenvectors of the covariance matrix. We can obtain the eigenvectors and corresponding eigenvalues using np or scipy . Here I've completed the task with np : cov_mat = np . cov ( X_std . T ) # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( cov_mat ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.70710678 -0.70710678] [ 0.70710678 -0.70710678]] Eigenvalues [0.33214806 1.6691035 ] And now we are just going to sort our vectors by their eigenvalues to get the proper order of principal components: # Make a list of (eigenvalue, eigenvector) tuples eig_pairs = [( np . abs ( eig_vals [ i ]), eig_vecs [:, i ]) for i in range ( len ( eig_vals ))] # Sort the (eigenvalue, eigenvector) tuples from high to low eig_pairs . sort ( key = lambda x : x [ 0 ], reverse = True ) # Visually confirm that the list is correctly sorted by decreasing eigenvalues print ( 'Eigenvalues in descending order:' ) for i in eig_pairs : print ( i [ 0 ]) Eigenvalues in descending order: 1.669103500110071 0.3321480643454986 eig_pairs [(1.669103500110071, array([-0.70710678, -0.70710678])), (0.3321480643454986, array([-0.70710678, 0.70710678]))] For those of us who are familiar with eigenvectors, we should recognize that they are necessarily orthogonal to one another. This is good to know from the PCA point of view, because we wouldn't want the variance along one vector to be explained by another. Let's plot these vectors along with the standardized data: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 7 )) ax . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . set_aspect ( 'equal' ) ax . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) (-2.138871623907465, 4.356979103463171) We indeed see that these vectors are orthogonal. Continuing on with our task of projecting the data onto our principal components, in order to project our data onto the PCs I'll need to reshape eig_pairs : matrix_w = np . hstack (( eig_pairs [ 0 ][ 1 ] . reshape ( 2 , 1 ), eig_pairs [ 1 ][ 1 ] . reshape ( 2 , 1 ))) print ( 'Matrix W: \\n ' , matrix_w ) Matrix W: [[-0.70710678 -0.70710678] [-0.70710678 0.70710678]] And now taking the dot product: Y = X_std . dot ( matrix_w ) plt . scatter ( Y [:, 0 ], Y [:, 1 ]) <matplotlib.collections.PathCollection at 0x7f681bdf3100> We see that our data is dispersed nicely along these PCs. Finally to tie this in with the point made at the end of 5.1.2, we see that the covariance matrix for the data in this new space is described by the diagonalized matrix of the former dimensional space: mean_vec = np . mean ( Y , axis = 0 ) cov_mat = ( Y - mean_vec ) . T . dot (( Y - mean_vec )) / ( Y . shape [ 0 ] - 1 ) cov_mat array([[1.66910350e+00, 1.76746394e-16], [1.76746394e-16, 3.32148064e-01]])","title":"5.1.3 Projecting onto the Principal Components"},{"location":"S5_Unsupervised_Learning/#514-cumulative-explained-variance","text":"Often we will need to decide just how many principal components are enough, especially with high dimensional data containing many colinear variables. To assist with this, data scientists will plot the cumulative explained variance. The explained variance is captured by the eigenvalues (this is why we sort by the eigenvalues in the first place). tot = sum ( eig_vals ) var_exp = [( i / tot ) * 100 for i in sorted ( eig_vals , reverse = True )] cum_var_exp = np . cumsum ( var_exp ) with plt . style . context ( 'seaborn-whitegrid' ): plt . figure ( figsize = ( 7 , 4 )) plt . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'individual explained variance' ) plt . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'cumulative explained variance' ) plt . ylabel ( 'Explained variance ratio' ) plt . xlabel ( 'Principal components' ) plt . legend ( loc = 'center right' ) plt . tight_layout ()","title":"5.1.4 Cumulative Explained Variance"},{"location":"S5_Unsupervised_Learning/#515-pca-with-scikit-learn","text":"But we can avoid the fancy footwork and do all this in sklearn! from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) We see that the values we get are the same as for the hand-calculated eigenvalues and vectors print ( pca . components_ ) [[ 0.70710678 0.70710678] [ 0.70710678 -0.70710678]] And the eigenvalues are under pca.explained_variance_ print ( pca . explained_variance_ ) [1.6691035 0.33214806]","title":"5.1.5 PCA with Scikit-Learn"},{"location":"S5_Unsupervised_Learning/#516-pca-as-dimensionality-reduction","text":"back to top One obvious use case for PCA, is to drop the dimensions with lowest explained variance. Continuing with our acidity/density example this will look like the following from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . set_aspect ( 'equal' ) ax [ 0 ] . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax [ 0 ] . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) ax [ 0 ] . set_ylabel ( 'Normalized density' ) ax [ 0 ] . set_xlabel ( 'Normalized acidity' ) ax [ 1 ] . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'Individual' ) ax [ 1 ] . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'Cumulative' ) ax [ 1 ] . set_ylabel ( 'Explained variance ratio' ) ax [ 1 ] . set_xlabel ( 'Principal components' ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f68189c5520> We can capture 80% of the explained variance along just the first principal component. What does this projection look like? # we set our components to 1 pca = PCA ( n_components = 1 ) pca . fit ( X_std ) # we then project the data onto the first PC # and then rebroadcast this transformation # back onto the orginal dimensions to see # what this looks like in terms of acidity/density X_pca = pca . inverse_transform ( pca . transform ( X_std )) # original data plt . scatter ( X_std [:, 0 ], X_std [:, 1 ], alpha = 0.2 ) # projected data plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.8 ) <matplotlib.collections.PathCollection at 0x7f681c292ee0>","title":"5.1.6 PCA as Dimensionality Reduction"},{"location":"S5_Unsupervised_Learning/#question-1-differentiating-pca-from-linear-regression","text":"How is PCA different from linear regression? PCA tries to find a best fit line through the data points but it minimizes the orthogonal distance to the model line where as Linear Regression minimizes distance from the perspective of the axis you're regressing with respect to. Explore more here","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: Differentiating PCA from linear regression"},{"location":"S5_Unsupervised_Learning/#517-pca-for-visualization","text":"back to top For classification tasks, PCA lends itself as a useful method for seeing how the classes separate on the highest variance dimensions of the data without consideration to the classes themselves, i.e. do our classes seperate out according to the other variables in the dataset. With our wine dataset, we see that the wine types fall out nicely along the first two principal components X_std = StandardScaler () . fit_transform ( wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = wine [ 'white' ] . values , edgecolor = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC')","title":"5.1.7 PCA for visualization"},{"location":"S5_Unsupervised_Learning/#518-enrichment-pca-as-outlier-removal-and-noise-filtering","text":"back to top In some cases, it can be advantageous to use PCA as a method for outlier removal. There are many caveats to this that we will not discuss here. But for those who are interested, this is a great overview article. As for noise filteration, Vanderplas' DS handbook has a good section on the topic as does Guido/Muller's Intro to ML with Python by the same pusblisher.","title":"\ud83c\udf2d 5.1.8 Enrichment: PCA as Outlier Removal and Noise Filtering"},{"location":"S5_Unsupervised_Learning/#519-pca-for-feature-engineering","text":"back to top Finally, PCA is actually a commonly used preprocessing technique for supervised machine learning models. In the next exercise, our goal will be to use PCA to generate a new set of features, to feed into our linear model.","title":"5.1.9 PCA for Feature Engineering"},{"location":"S5_Unsupervised_Learning/#exercise-1-pca-as-preprocessing-for-models","text":"Using the wine data, select any number of the first principal components and attemp to predict density for the red wine data. Consider that the maximum number of principal components are: print ( f \"Max principal components: { X . shape [ 1 ] } \" ) # Code Cell for Exercise 1 ################################################################################ ##### CREATE A SKLEARN-PCA OBJECT, FIT AND TRANSFORM TO THE WINE DATA ########## ################################################################################ # as you do this, be sure to remove 'density' from the input features ################################################################################ ############################## UNCOMMENT THE BELOW ############################# ################################################################################ # plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2, c=wine['white'].values, # edgecolor='grey') # plt.xlabel('First PC') # plt.ylabel('Second PC') # plt.show() # model = LinearRegression() # X_train, X_test, y_train, y_test = train_test_split(X_pca, y_wine, train_size=0.8, random_state=42) # model.fit(X_train, y_train) # y_pred = model.predict(X_test) # print(r2_score(y_test, y_pred)) # print(r2_score(y_train, model.predict(X_train))) 0.9634516142421967 0.953295487875815","title":"\ud83c\udfcb\ufe0f Exercise 1: PCA as Preprocessing for Models"},{"location":"S5_Unsupervised_Learning/#52-k-means-clustering","text":"back to top We now embark on a second class of unsupervised learning techinques: clustering. The K-means algorithm works under two assumptions: * every cluster can be defined by an arithmetic mean or cluster center * each point is closer to one arithmetic center than the other centers Let's turn back to our wine dataset: X_std = StandardScaler () . fit_transform ( X_wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC') It is fairly obvious under these two dimensions that there are two clusters. The K-means algorithm automatically pics this out from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = 2 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); wa-la! Our simple unsupervised algorithm does a pretty good job of differentiating between the red and white wine clusters.","title":"5.2 K-Means Clustering"},{"location":"S5_Unsupervised_Learning/#521-the-algorithm-expectation-maximization","text":"back to top We won't go too far into the implementation of the underlying algorithm here. In its basic steps, however, it performs the following: Initialize cluster centers (random guess) Then repeat: E-Step: assign points to the nearest center (arithmetic distance) M-step: set the new center point for each cluster according to the mean of it's datapoint members More information on K-means algorithm can be explored here","title":"5.2.1 The Algorithm: Expectation-Maximization"},{"location":"S5_Unsupervised_Learning/#522-limitations","text":"back to top A few brief notes on limitations: the global optimum may not be achieved (no guarantee of finding the overall best solution) the number of clusters must be guessed beforehand cluster boundaries are unavoidably linear and the cluster assignments are unavoidably circular can be slow for large datasets cluster assignments are non probabilistic 3 and 5 motivate our next section, Gaussian Mixture Models","title":"5.2.2 Limitations"},{"location":"S5_Unsupervised_Learning/#523-determining-k-with-the-elbow-method","text":"The elbow method is a popular technique for determining the value of k . It involves looping through a range of k 's and assessing some goodness of fit metric. Intuitively, we might presume those metrics involve some measure of the distance of datapoints to their cluster centers. We have options: Distortion: distance ( typically Euclidean ) from the cluster centers averaged across the respective clusters. Inertia: the sum of squared distances of samples to their closest cluster center. Silhouette: calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of The Silhouette Coefficient for a sample is (b-a) / max(b-a) best value is 1 worst value is -1 Calinski Harabasz Score or Variance Ratio: the ratio between within-cluster dispersion and between-cluster dispersion should sound familiar to our ANOVA discussion higher is better And there are many other methods of evaluating cluster assignment performance","title":"5.2.3 Determining K with the Elbow Method"},{"location":"S5_Unsupervised_Learning/#question-2-comparing-metrics","text":"What is the primary difference between Distortion, Inertia vs Silhouette, Calinksi? distortions = [] inertias = [] silhouette = [] variance = [] for k in range ( 1 , 10 ): kmeans = KMeans ( n_clusters = k ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) labels = kmeans . labels_ distortions . append ( sum ( np . min ( cdist ( X_pca , kmeans . cluster_centers_ , 'euclidean' ), axis = 1 )) / X . shape [ 0 ]) inertias . append ( kmeans . inertia_ ) if k > 1 : silhouette . append ( silhouette_score ( X_pca , labels , metric = 'euclidean' )) variance . append ( calinski_harabasz_score ( X_pca , labels )) We note in the following plots that inertia and distortion asymptotically improve with higher k (because they are unregularized) whereas silhouette and calinski metrics are penalized for inter-cluster relatability fig , [[ ax1 , ax2 ], [ ax3 , ax4 ]] = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) ax1 . plot ( range ( 1 , 10 ), distortions ) ax2 . plot ( range ( 1 , 10 ), inertias ) ax3 . plot ( range ( 2 , 10 ), silhouette ) ax4 . plot ( range ( 2 , 10 ), variance ) [<matplotlib.lines.Line2D at 0x7f67fc5d8bb0>] kmeans = KMeans ( n_clusters = 3 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 );","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 2: Comparing Metrics"},{"location":"S5_Unsupervised_Learning/#53-gaussian-mixture-models","text":"back to top in the simplest case, GMMs can be used in the same way as K-means from sklearn.mixture import GaussianMixture gmm = GaussianMixture ( n_components = 2 ) . fit ( X_pca ) labels = gmm . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , alpha = 0.2 , edgecolor = 'grey' ); But because GMM is a probablistic model, we can visualize the certainty with which we assign these clusters. probs = gmm . predict_proba ( X_pca ) print ( probs [ 5 : 20 ] . round ( 3 )) [[0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.]] # convert probs to 1 dimension probs . max ( 1 ) array([1. , 0.99999994, 0.99999999, ..., 1. , 1. , 0.99999189]) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = probs . max ( 1 ), s = 40 , cmap = 'Blues' , alpha = 0.5 , edgecolor = 'grey' );","title":"5.3 Gaussian Mixture Models"},{"location":"S5_Unsupervised_Learning/#531-generalizing-e-m-for-gmms","text":"back to top The algorithm for GMMs is very similar to K-means, but now the EM steps are probablistic 1 2 Initialize cluster centers (random guess) Then repeat: E-Step: assign points their probability of belonging to every cluster M-step: set the new center point for each cluster according to the probabilities of all datapoints","title":"5.3.1 Generalizing E-M for GMMs"},{"location":"S5_Unsupervised_Learning/#532-gmms-as-a-data-generator","text":"back to top One particularly fascinating application of GMMs is to use them to generate new data that is similar to the data on which they are modeled. Let's take an example. We're going to use sklearn make_circles function to create some arbitrary data that has a complex relationship along two different axes. # some helper functions borrowed from Jake Vanderplas with a few minor tweaks # https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html from matplotlib.patches import Ellipse def draw_ellipse ( position , covariance , ax = None , ** kwargs ): \"\"\"Draw an ellipse with a given position and covariance\"\"\" ax = ax or plt . gca () # Convert covariance to principal axes if covariance . shape == ( 2 , 2 ): U , s , Vt = np . linalg . svd ( covariance ) angle = np . degrees ( np . arctan2 ( U [ 1 , 0 ], U [ 0 , 0 ])) width , height = 2 * np . sqrt ( s ) else : angle = 0 width , height = 2 * np . sqrt ( covariance ) # Draw the Ellipse for nsig in range ( 1 , 4 ): ax . add_patch ( Ellipse ( position , nsig * width , nsig * height , angle , ** kwargs )) def plot_gmm ( gmm , X , label = True , ax = None , data_alpha = 1 ): ax = ax or plt . gca () labels = gmm . fit ( X ) . predict ( X ) if label : ax . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , zorder = 2 , alpha = data_alpha ) else : ax . scatter ( X [:, 0 ], X [:, 1 ], s = 40 , zorder = 2 , alpha = data_alpha ) ax . axis ( 'equal' ) w_factor = 0.2 / gmm . weights_ . max () for pos , covar , w in zip ( gmm . means_ , gmm . covariances_ , gmm . weights_ ): draw_ellipse ( pos , covar , alpha = w * w_factor ) from sklearn.datasets import make_circles as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ]); As a side note, as a clustering model, the GMM is not particularly useful: gmm2 = GaussianMixture ( n_components = 2 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm2 , X ) But if we use the GMM as a density estimator on the underlying data, rather than as a clustering algorithm, we find a completely different use case: gmm16 = GaussianMixture ( n_components = 16 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm16 , X , label = False ) Now, with the distributions drawn, we can assemble entirely new data: Xnew = gmm16 . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]);","title":"5.3.2 GMMs as a Data Generator"},{"location":"S5_Unsupervised_Learning/#5321-determining-the-number-of-components","text":"back to top Let's think back to session 1 on model selection. How might we determine the best number of components? A couple analytic approaches that we have not much yet discussed, are the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The important thing to note from each of these is that they penalize added complexity to the models, and we would like to minimize their estimated prediction error. n_components = np . arange ( 1 , 42 ) models = [ GaussianMixture ( n , covariance_type = 'full' , random_state = 42 ) . fit ( X ) for n in n_components ] plt . plot ( n_components , [ m . bic ( X ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( X ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); plt . ylabel ( 'est. prediction error' ) Text(0, 0.5, 'est. prediction error') Both AIC and BIC capture a local minimum on the order of 6 Gaussians. This is where the model captures the overall circular modality of the data. However it totally misses the circumscribed nature of the two circles. Increasing this number to 30-40 gaussians captures this feature of the data. The AIC score reflects this while the BIC score (although captures a local minima in the area) does not define this as a global optimum. gmmNew = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmmNew , X , label = True , data_alpha = 0 ) Xnew = gmmNew . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]);","title":"5.3.2.1 Determining the number of components"},{"location":"S5_Unsupervised_Learning/#exercise-2-determine-number-of-components-for-circular-moons","text":"Repeat the above, this time using sklearn.datasets.make_moons # Code Cell for Exercise 2 from sklearn.datasets import make_moons as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) ################################################################################ ##### FIT GMM MODEL(S) TO 1-42 CLUSTER CENTERS AND RECORD THE AIC/BIC ########## ################################################################################ # uncomment these lines # plt.plot(n_components, [m.bic(X) for m in models], label='BIC') # plt.plot(n_components, [m.aic(X) for m in models], label='AIC') # plt.legend(loc='best') # plt.xlabel('n_components'); # plt.ylabel('est. prediction error') Text(0, 0.5, 'est. prediction error') gmm_moon = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm_moon , X ) Xnew = gmm_moon . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]);","title":"\ud83c\udfcb\ufe0f Exercise 2: Determine Number of Components for Circular Moons"},{"location":"S5_Unsupervised_Learning/#references","text":"","title":"References"},{"location":"S5_Unsupervised_Learning/#pca","text":"Intuitive PCA PCA and Eigenvectors/values","title":"PCA"},{"location":"S5_Unsupervised_Learning/#gmm","text":"GMMs Explained Derive GMM Exercise","title":"GMM"},{"location":"S6_Bagging/","text":"Data Science Foundations Session 6: Bagging Decision Trees and Random Forests \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're going back to the topic of supervised learning models. These models however, belong to a special class of methods called bagging, or bootstrap aggregation. Bagging is an ensemble learning method. In this method, many weak classifiers cast their votes in a general election for the final prediction. The weak learners that random forests are made of, are called decision trees. 6.0 Preparing Environment and Importing Data \u00b6 back to top 6.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score from sklearn.model_selection import train_test_split from sklearn import metrics 6.0.2 Load Dataset \u00b6 back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) print ( margin . shape , end = ' \\n\\n ' ) display ( margin . head ()) (1668, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 We're going to recreate the same operations we employed in Session 4, Feature Engineering: # identify categorical columns cat_cols = margin . columns [: 7 ] # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ cat_cols ] # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [[ 'KG' ]] X_truf = np . concatenate (( onehotlabels , X_num . values ), axis = 1 ) # grab our y data y_truf = margin [ 'EBITDA/KG' ] . values Lastly, to create a classification task, we're going to identify high, med, and low value products: print ( 'bad less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .25 )), end = ' \\n\\n ' ) print ( 'low less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .5 )), end = ' \\n\\n ' ) print ( 'med less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .75 )), end = ' \\n\\n ' ) pd . DataFrame ( margin [ margin . columns [ - 2 ]]) . boxplot ( showfliers = False ) bad less than: 0.12 low less than: 0.22 med less than: 0.35 <AxesSubplot:> margin [ 'profitability' ] = margin [ margin . columns [ - 1 ]] . apply ( lambda x : 'bad' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .25 ) else 'low' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .50 ) else 'med' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .75 ) else 'high' ) margin [ 'profitability' ] . hist () <AxesSubplot:> class_profit = { 'bad' : 0 , 'low' : 1 , 'med' : 2 , 'high' : 3 } y_truf_class = margin [ 'profitability' ] . map ( class_profit ) . values margin [ 'profitability_encoding' ] = y_truf_class margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG profitability profitability_encoding 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 high 3 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 med 2 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 low 1 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 med 2 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 high 3 6.1 Decision Trees \u00b6 back to top In essence, a decision tree is a series of binary questions. Let's begin this discussion by talking about how we make decision trees in sklearn. 6.1.1 Creating a Decision Tree \u00b6 back to top from sklearn import tree X = [[ 0 , 0 ], [ 1 , 1 ]] y = [ 0 , 1 ] clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) After fitting the model we can use the predict method to show the output for a sample clf . predict ([[ 2. , 2. ]]) array([1]) Similar to what we saw with GMMs, we also have access to the probabilities of the outcomes: clf . predict_proba ([[ 2. , 2. ]]) array([[0., 1.]]) Let's now go on to using visual strategies to interpreting trees. 6.1.2 Interpreting a Decision Tree \u00b6 back to top Throughout today, we will discuss many ways to view both a single tree and a random forest of trees. 6.1.2.1 Node & Branch Diagram \u00b6 back to top We can visualize the decision tree: tree . plot_tree ( clf ) [Text(0.5, 0.75, 'X[1] <= 0.5\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'), Text(0.25, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'), Text(0.75, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]')] or, more prettily: import graphviz dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph The gini label, also known as Gini impurity , is a measure of how often a sample passing through the node would be incorrectly labeled if it was randomly assigned a label based on the proportion of all labels passing through the node. So it is a measure of the progress of our tree. Let's take a more complex example from sklearn.datasets import make_classification as gen X , y = gen ( random_state = 42 ) Let's inspect our generated data: print ( X . shape ) print ( y . shape ) y [: 5 ] # a binary classification (100, 20) (100,) array([0, 0, 1, 1, 0]) And now let's train our tree: clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) How do we interpret this graph? dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph Can we confirm the observations in the tree by manually inspecting X and y? y [ X [:, 10 ] < .203 ] array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) We can confirm the gini score of the top left node by hand... scr = [] for j in range ( 1000 ): y_pred = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] y_true = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] scr . append ( mean_squared_error ( y_pred , y_true )) np . mean ( scr ) 0.1091346153846154 Let's take a look at this with our truffle dataset Vary the parameter max_depth what do you notice? Does the term greedy mean anything to you? Do nodes higher in the tree change based on decisions lower in the tree? clf = tree . DecisionTreeClassifier ( max_depth = 1 ) clf . fit ( X_truf , y_truf_class ) DecisionTreeClassifier(max_depth=1) And now lets look at the graph: dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph What is X[4] ??? # It's those tasty sponge cake truffles! enc . get_feature_names_out ()[ 4 ] 'Base Cake_Sponge' This is one great aspect of decision trees, their interpretability . We will perform this analysis again, for now, let's proceed with simpler datasets while exploring the features of decision trees. 6.1.2.1 Decision Boundaries \u00b6 back to top Let's make some random blobs from sklearn.datasets import make_blobs as gen X , y = gen ( random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f850da667c0> Let's call up our Classifier again, this time setting the max_depth to two clf = tree . DecisionTreeClassifier ( max_depth = 2 , random_state = 42 ) clf = clf . fit ( X , y ) # Parameters plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) <matplotlib.collections.PathCollection at 0x7f850cc0f9d0> dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph We can see from the output of this graph, that the tree attempts to create the class boundaries as far from the cluster centers as possible. What happens when these clusters overlap? X , y = gen ( random_state = 42 , cluster_std = 3 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f850cb877f0> Let's go ahead and write our plot into a function def plot_tree ( X , clf ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return plt We see that the boundaries mislabel some points fig = plot_tree ( X , clf ) 6.1.3 Overfitting a Decision Tree \u00b6 back to top Let's increase the max_depth clf = tree . DecisionTreeClassifier ( max_depth = 5 , random_state = 42 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> What we notice is that while the model accurately predicts the training data, we see some spurious labels, noteably the trailing purple bar that extends into the otherwise green region of the data. This is a well known fact about decision trees, that they tend to overfit their training data. In fact, this is a major motivation for why decision trees, a weak classifier, are conveniently packaged into ensembles. We combine the idea of bootstrapping, with decision trees, to come up with an overall better classifier. \ud83c\udfcb\ufe0f Exercise 1: Minimize Overfitting \u00b6 Repeat 6.1.3 with different max_depth settings, also read the docstring and play with any other hyperparameters available to you. What settings do you feel minimize overfitting? The documentation for DecisionTreeClassifier may be helpful # Code Cell for 1 ################################################################################ ##### CHANGE THE HYPERPARAMETERS IN THE CALL TO DECISIONTREECLASSIFIER ######### ################################################################################ clf = tree . DecisionTreeClassifier ( random_state = 42 , max_depth = None , min_samples_split = 3 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = None , max_leaf_nodes = None , min_impurity_decrease = 0.1 , class_weight = None , ccp_alpha = 0.0 ,) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> 6.2 Random Forests and Bagging \u00b6 back to top 6.2.1 What is Bagging? \u00b6 back to top Bagging , or Bootstrap AGGregation is the process of creating subsets of your data and training separate models on them, and using the aggregate votes of the models to make a final prediction. Bootstrapping is a topic in and of itself that we will just touch on here. Without going through the statistical rigor of proof, bootstrapping, or sampling from your observations with replacement, simulates having drawn additional data from the true population. We use this method to create many new datasets that are then used to train separate learners in parallel. This overall approach is called Bagging . A Random Forest is an instance of bagging where the separate learners are decision trees. 6.2.2 Random Forests for Classification \u00b6 back to top from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier tree = DecisionTreeClassifier () bag = BaggingClassifier ( tree , n_estimators = 10 , max_samples = 0.8 , random_state = 1 ) bag . fit ( X , y ) plot_tree ( X , bag ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> In the above, we have bootstrapped by providing each individual tree with 80% of the population data. In practice, Random Forests can achieve even better results by randomizing how the individual classifiers are constructed. In fact there are many unique methods of training individual trees and you can learn more about them here . This randomness is done automatically in sklearn's RandomForestClassifier from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier ( n_estimators = 10 , random_state = 2 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> 6.2.2.1 Interpreting a Random Forest \u00b6 back to top Let's revisit our truffle dataset again, this time with random forests # fit the model clf = RandomForestClassifier ( n_estimators = 100 , min_samples_leaf = 6 ) clf = clf . fit ( X_truf , y_truf_class ) We get a fairly high accuracy when our min_samples_leaf is low and an accuracy that leaves room for improvement when min_samples_leaf is high. This indicates to us the model may be prown to overfitting if we are not careful: accuracy_score ( clf . predict ( X_truf ), y_truf_class ) 0.6127098321342925 We can grab the original feature names with get_feature_names_out() : feats = enc . get_feature_names_out () The feature importances are stored in clf.feature_importances_ . These are calculated from the Mean Decrease in Impurity or MDI also called the Gini Importance . It is the sum of the number of nodes across all trees that include the feature, weighted by the number of samples passing through the node. One downside of estimating feature importance in this way is that it doesn't play well with highly cardinal features (features with many unique values such as mailing addresses, are highly cardinal features) len ( feats ) 118 # grab feature importances imp = clf . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in clf . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ feats , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.109284 0.095848 2 Base Cake_Chiffon 0.049163 0.049299 3 Base Cake_Pound 0.041666 0.03948 4 Base Cake_Butter 0.038501 0.038294 5 Base Cake_Cheese 0.033326 0.037235 I'm going to use plotly to create this chart: px . bar ( feat [: 20 ], x = 'feature' , y = 'importance' , error_y = 'std' , title = 'Feature Importance' ) \ud83d\ude4b\u200d\u2640\ufe0f Question 1: Feature Importance and Cardinality \u00b6 How does feature importance change in the above plot when we change the minimum leaf size from 6 to 1? \ud83d\ude4b\u200d Question 2: Compare to Moods Median \u00b6 We can then go and look at the different EBITDAs when selecting for each of these features. What do you notice as the primary difference between these results and those from Session 2: Inferential Statistics Exercise 1, Part C when we ran Mood's Median test on this same data? feat . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.109284 0.095848 2 Base Cake_Chiffon 0.049163 0.049299 3 Base Cake_Pound 0.041666 0.03948 4 Base Cake_Butter 0.038501 0.038294 5 Base Cake_Cheese 0.033326 0.037235 for feature in feat . iloc [: 10 , 0 ]: group = feature . split ( '_' )[ 0 ] sel = \" \" . join ( feature . split ( '_' )[ 1 :]) pos = margin . loc [( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () neg = margin . loc [ ~ ( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () print ( group + \": \" + sel ) print ( \" \\t with: {:.2f} \" . format ( pos )) print ( \" \\t without: {:.2f} \" . format ( neg )) Base Cake: Sponge with: 0.70 without: 0.20 Base Cake: Chiffon with: 0.13 without: 0.24 Base Cake: Pound with: 0.24 without: 0.20 Base Cake: Butter with: 0.14 without: 0.26 Base Cake: Cheese with: 0.44 without: 0.21 Primary Flavor: Doughnut with: 0.38 without: 0.20 Primary Flavor: Butter Toffee with: 0.46 without: 0.21 Color Group: Olive with: 0.67 without: 0.21 Secondary Flavor: Egg Nog with: 0.23 without: 0.21 Truffle Type: Candy Outer with: 0.20 without: 0.22 6.2.3 Random Forests for Regression \u00b6 back to top from sklearn.ensemble import RandomForestRegressor clf = RandomForestRegressor ( n_estimators = 10 ) Because our labels on our blob data were numerical, we can apply and view the estimator in the same way: clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> I want to revisit a dataset we brought up in Session 2 on feature engineering: t = np . linspace ( 0 , 5 , 200 ) w = 5 h = 4 s = 4 * h / np . pi * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ]) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) # tells us about the amplitude of the component at the # corresponding frequency magnitude = np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , .15 ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain') Let's see if a random forest regression model can capture the wave behavior of the time-series data clf = RandomForestRegressor ( n_estimators = 10 ) clf . fit ( t . reshape ( - 1 , 1 ), s ) RandomForestRegressor(n_estimators=10) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . plot ( t , s ) ax . plot ( t , clf . predict ( t . reshape ( - 1 , 1 ))) [<matplotlib.lines.Line2D at 0x7f850c12adc0>] Nice! without specifying any perdiodicity, the random forest does a good job of embedding this periodicity in the final output. \ud83c\udfcb\ufe0f Exercise 2: Practice with Random Forests \u00b6 With the wine dataset: predict: density create a learning curve of train/test score vs model complexity for your random forest model(s) I have provided the cleaned dataset as well as starter code for training the model and making parity plots Do not change the following 3 cells: wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ 'density' ] = y model = RandomForestRegressor ( n_estimators = 100 , criterion = 'squared_error' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , bootstrap = True , oob_score = False , n_jobs = None , random_state = None , verbose = 0 , warm_start = False , ccp_alpha = 0.0 , max_samples = None ,) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax . set_ylabel ( 'Predicted' ) ax . set_xlabel ( 'Actual' ) ax_ . set_xlabel ( 'Actual' ) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.973') Compare these results with our linear model from Lab 3. Recall that we can quickly grab the names of the paramters in our sklearn model: RandomForestRegressor () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Cell for Exercise 2 <matplotlib.legend.Legend at 0x7f1fe5063f40>","title":"Bagging"},{"location":"S6_Bagging/#data-science-foundations-session-6-bagging-decision-trees-and-random-forests","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're going back to the topic of supervised learning models. These models however, belong to a special class of methods called bagging, or bootstrap aggregation. Bagging is an ensemble learning method. In this method, many weak classifiers cast their votes in a general election for the final prediction. The weak learners that random forests are made of, are called decision trees.","title":"Data Science Foundations  Session 6: Bagging  Decision Trees and Random Forests"},{"location":"S6_Bagging/#60-preparing-environment-and-importing-data","text":"back to top","title":"6.0 Preparing Environment and Importing Data"},{"location":"S6_Bagging/#601-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score from sklearn.model_selection import train_test_split from sklearn import metrics","title":"6.0.1 Import Packages"},{"location":"S6_Bagging/#602-load-dataset","text":"back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) print ( margin . shape , end = ' \\n\\n ' ) display ( margin . head ()) (1668, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 We're going to recreate the same operations we employed in Session 4, Feature Engineering: # identify categorical columns cat_cols = margin . columns [: 7 ] # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ cat_cols ] # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [[ 'KG' ]] X_truf = np . concatenate (( onehotlabels , X_num . values ), axis = 1 ) # grab our y data y_truf = margin [ 'EBITDA/KG' ] . values Lastly, to create a classification task, we're going to identify high, med, and low value products: print ( 'bad less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .25 )), end = ' \\n\\n ' ) print ( 'low less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .5 )), end = ' \\n\\n ' ) print ( 'med less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .75 )), end = ' \\n\\n ' ) pd . DataFrame ( margin [ margin . columns [ - 2 ]]) . boxplot ( showfliers = False ) bad less than: 0.12 low less than: 0.22 med less than: 0.35 <AxesSubplot:> margin [ 'profitability' ] = margin [ margin . columns [ - 1 ]] . apply ( lambda x : 'bad' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .25 ) else 'low' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .50 ) else 'med' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .75 ) else 'high' ) margin [ 'profitability' ] . hist () <AxesSubplot:> class_profit = { 'bad' : 0 , 'low' : 1 , 'med' : 2 , 'high' : 3 } y_truf_class = margin [ 'profitability' ] . map ( class_profit ) . values margin [ 'profitability_encoding' ] = y_truf_class margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG profitability profitability_encoding 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 high 3 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 med 2 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 low 1 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 med 2 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 high 3","title":"6.0.2 Load Dataset"},{"location":"S6_Bagging/#61-decision-trees","text":"back to top In essence, a decision tree is a series of binary questions. Let's begin this discussion by talking about how we make decision trees in sklearn.","title":"6.1 Decision Trees"},{"location":"S6_Bagging/#611-creating-a-decision-tree","text":"back to top from sklearn import tree X = [[ 0 , 0 ], [ 1 , 1 ]] y = [ 0 , 1 ] clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) After fitting the model we can use the predict method to show the output for a sample clf . predict ([[ 2. , 2. ]]) array([1]) Similar to what we saw with GMMs, we also have access to the probabilities of the outcomes: clf . predict_proba ([[ 2. , 2. ]]) array([[0., 1.]]) Let's now go on to using visual strategies to interpreting trees.","title":"6.1.1 Creating a Decision Tree"},{"location":"S6_Bagging/#612-interpreting-a-decision-tree","text":"back to top Throughout today, we will discuss many ways to view both a single tree and a random forest of trees.","title":"6.1.2 Interpreting a Decision Tree"},{"location":"S6_Bagging/#6121-node-branch-diagram","text":"back to top We can visualize the decision tree: tree . plot_tree ( clf ) [Text(0.5, 0.75, 'X[1] <= 0.5\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'), Text(0.25, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'), Text(0.75, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]')] or, more prettily: import graphviz dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph The gini label, also known as Gini impurity , is a measure of how often a sample passing through the node would be incorrectly labeled if it was randomly assigned a label based on the proportion of all labels passing through the node. So it is a measure of the progress of our tree. Let's take a more complex example from sklearn.datasets import make_classification as gen X , y = gen ( random_state = 42 ) Let's inspect our generated data: print ( X . shape ) print ( y . shape ) y [: 5 ] # a binary classification (100, 20) (100,) array([0, 0, 1, 1, 0]) And now let's train our tree: clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) How do we interpret this graph? dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph Can we confirm the observations in the tree by manually inspecting X and y? y [ X [:, 10 ] < .203 ] array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) We can confirm the gini score of the top left node by hand... scr = [] for j in range ( 1000 ): y_pred = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] y_true = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] scr . append ( mean_squared_error ( y_pred , y_true )) np . mean ( scr ) 0.1091346153846154 Let's take a look at this with our truffle dataset Vary the parameter max_depth what do you notice? Does the term greedy mean anything to you? Do nodes higher in the tree change based on decisions lower in the tree? clf = tree . DecisionTreeClassifier ( max_depth = 1 ) clf . fit ( X_truf , y_truf_class ) DecisionTreeClassifier(max_depth=1) And now lets look at the graph: dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph What is X[4] ??? # It's those tasty sponge cake truffles! enc . get_feature_names_out ()[ 4 ] 'Base Cake_Sponge' This is one great aspect of decision trees, their interpretability . We will perform this analysis again, for now, let's proceed with simpler datasets while exploring the features of decision trees.","title":"6.1.2.1 Node &amp; Branch Diagram"},{"location":"S6_Bagging/#6121-decision-boundaries","text":"back to top Let's make some random blobs from sklearn.datasets import make_blobs as gen X , y = gen ( random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f850da667c0> Let's call up our Classifier again, this time setting the max_depth to two clf = tree . DecisionTreeClassifier ( max_depth = 2 , random_state = 42 ) clf = clf . fit ( X , y ) # Parameters plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) <matplotlib.collections.PathCollection at 0x7f850cc0f9d0> dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph We can see from the output of this graph, that the tree attempts to create the class boundaries as far from the cluster centers as possible. What happens when these clusters overlap? X , y = gen ( random_state = 42 , cluster_std = 3 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f850cb877f0> Let's go ahead and write our plot into a function def plot_tree ( X , clf ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return plt We see that the boundaries mislabel some points fig = plot_tree ( X , clf )","title":"6.1.2.1 Decision Boundaries"},{"location":"S6_Bagging/#613-overfitting-a-decision-tree","text":"back to top Let's increase the max_depth clf = tree . DecisionTreeClassifier ( max_depth = 5 , random_state = 42 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> What we notice is that while the model accurately predicts the training data, we see some spurious labels, noteably the trailing purple bar that extends into the otherwise green region of the data. This is a well known fact about decision trees, that they tend to overfit their training data. In fact, this is a major motivation for why decision trees, a weak classifier, are conveniently packaged into ensembles. We combine the idea of bootstrapping, with decision trees, to come up with an overall better classifier.","title":"6.1.3 Overfitting a Decision Tree"},{"location":"S6_Bagging/#exercise-1-minimize-overfitting","text":"Repeat 6.1.3 with different max_depth settings, also read the docstring and play with any other hyperparameters available to you. What settings do you feel minimize overfitting? The documentation for DecisionTreeClassifier may be helpful # Code Cell for 1 ################################################################################ ##### CHANGE THE HYPERPARAMETERS IN THE CALL TO DECISIONTREECLASSIFIER ######### ################################################################################ clf = tree . DecisionTreeClassifier ( random_state = 42 , max_depth = None , min_samples_split = 3 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = None , max_leaf_nodes = None , min_impurity_decrease = 0.1 , class_weight = None , ccp_alpha = 0.0 ,) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'>","title":"\ud83c\udfcb\ufe0f Exercise 1: Minimize Overfitting"},{"location":"S6_Bagging/#62-random-forests-and-bagging","text":"back to top","title":"6.2 Random Forests and Bagging"},{"location":"S6_Bagging/#621-what-is-bagging","text":"back to top Bagging , or Bootstrap AGGregation is the process of creating subsets of your data and training separate models on them, and using the aggregate votes of the models to make a final prediction. Bootstrapping is a topic in and of itself that we will just touch on here. Without going through the statistical rigor of proof, bootstrapping, or sampling from your observations with replacement, simulates having drawn additional data from the true population. We use this method to create many new datasets that are then used to train separate learners in parallel. This overall approach is called Bagging . A Random Forest is an instance of bagging where the separate learners are decision trees.","title":"6.2.1 What is Bagging?"},{"location":"S6_Bagging/#622-random-forests-for-classification","text":"back to top from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier tree = DecisionTreeClassifier () bag = BaggingClassifier ( tree , n_estimators = 10 , max_samples = 0.8 , random_state = 1 ) bag . fit ( X , y ) plot_tree ( X , bag ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> In the above, we have bootstrapped by providing each individual tree with 80% of the population data. In practice, Random Forests can achieve even better results by randomizing how the individual classifiers are constructed. In fact there are many unique methods of training individual trees and you can learn more about them here . This randomness is done automatically in sklearn's RandomForestClassifier from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier ( n_estimators = 10 , random_state = 2 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'>","title":"6.2.2 Random Forests for Classification"},{"location":"S6_Bagging/#6221-interpreting-a-random-forest","text":"back to top Let's revisit our truffle dataset again, this time with random forests # fit the model clf = RandomForestClassifier ( n_estimators = 100 , min_samples_leaf = 6 ) clf = clf . fit ( X_truf , y_truf_class ) We get a fairly high accuracy when our min_samples_leaf is low and an accuracy that leaves room for improvement when min_samples_leaf is high. This indicates to us the model may be prown to overfitting if we are not careful: accuracy_score ( clf . predict ( X_truf ), y_truf_class ) 0.6127098321342925 We can grab the original feature names with get_feature_names_out() : feats = enc . get_feature_names_out () The feature importances are stored in clf.feature_importances_ . These are calculated from the Mean Decrease in Impurity or MDI also called the Gini Importance . It is the sum of the number of nodes across all trees that include the feature, weighted by the number of samples passing through the node. One downside of estimating feature importance in this way is that it doesn't play well with highly cardinal features (features with many unique values such as mailing addresses, are highly cardinal features) len ( feats ) 118 # grab feature importances imp = clf . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in clf . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ feats , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.109284 0.095848 2 Base Cake_Chiffon 0.049163 0.049299 3 Base Cake_Pound 0.041666 0.03948 4 Base Cake_Butter 0.038501 0.038294 5 Base Cake_Cheese 0.033326 0.037235 I'm going to use plotly to create this chart: px . bar ( feat [: 20 ], x = 'feature' , y = 'importance' , error_y = 'std' , title = 'Feature Importance' )","title":"6.2.2.1 Interpreting a Random Forest"},{"location":"S6_Bagging/#question-1-feature-importance-and-cardinality","text":"How does feature importance change in the above plot when we change the minimum leaf size from 6 to 1?","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: Feature Importance and Cardinality"},{"location":"S6_Bagging/#question-2-compare-to-moods-median","text":"We can then go and look at the different EBITDAs when selecting for each of these features. What do you notice as the primary difference between these results and those from Session 2: Inferential Statistics Exercise 1, Part C when we ran Mood's Median test on this same data? feat . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.109284 0.095848 2 Base Cake_Chiffon 0.049163 0.049299 3 Base Cake_Pound 0.041666 0.03948 4 Base Cake_Butter 0.038501 0.038294 5 Base Cake_Cheese 0.033326 0.037235 for feature in feat . iloc [: 10 , 0 ]: group = feature . split ( '_' )[ 0 ] sel = \" \" . join ( feature . split ( '_' )[ 1 :]) pos = margin . loc [( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () neg = margin . loc [ ~ ( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () print ( group + \": \" + sel ) print ( \" \\t with: {:.2f} \" . format ( pos )) print ( \" \\t without: {:.2f} \" . format ( neg )) Base Cake: Sponge with: 0.70 without: 0.20 Base Cake: Chiffon with: 0.13 without: 0.24 Base Cake: Pound with: 0.24 without: 0.20 Base Cake: Butter with: 0.14 without: 0.26 Base Cake: Cheese with: 0.44 without: 0.21 Primary Flavor: Doughnut with: 0.38 without: 0.20 Primary Flavor: Butter Toffee with: 0.46 without: 0.21 Color Group: Olive with: 0.67 without: 0.21 Secondary Flavor: Egg Nog with: 0.23 without: 0.21 Truffle Type: Candy Outer with: 0.20 without: 0.22","title":"\ud83d\ude4b\u200d Question 2: Compare to Moods Median"},{"location":"S6_Bagging/#623-random-forests-for-regression","text":"back to top from sklearn.ensemble import RandomForestRegressor clf = RandomForestRegressor ( n_estimators = 10 ) Because our labels on our blob data were numerical, we can apply and view the estimator in the same way: clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> I want to revisit a dataset we brought up in Session 2 on feature engineering: t = np . linspace ( 0 , 5 , 200 ) w = 5 h = 4 s = 4 * h / np . pi * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ]) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) # tells us about the amplitude of the component at the # corresponding frequency magnitude = np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , .15 ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain') Let's see if a random forest regression model can capture the wave behavior of the time-series data clf = RandomForestRegressor ( n_estimators = 10 ) clf . fit ( t . reshape ( - 1 , 1 ), s ) RandomForestRegressor(n_estimators=10) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . plot ( t , s ) ax . plot ( t , clf . predict ( t . reshape ( - 1 , 1 ))) [<matplotlib.lines.Line2D at 0x7f850c12adc0>] Nice! without specifying any perdiodicity, the random forest does a good job of embedding this periodicity in the final output.","title":"6.2.3 Random Forests for Regression"},{"location":"S6_Bagging/#exercise-2-practice-with-random-forests","text":"With the wine dataset: predict: density create a learning curve of train/test score vs model complexity for your random forest model(s) I have provided the cleaned dataset as well as starter code for training the model and making parity plots Do not change the following 3 cells: wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ 'density' ] = y model = RandomForestRegressor ( n_estimators = 100 , criterion = 'squared_error' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , bootstrap = True , oob_score = False , n_jobs = None , random_state = None , verbose = 0 , warm_start = False , ccp_alpha = 0.0 , max_samples = None ,) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax . set_ylabel ( 'Predicted' ) ax . set_xlabel ( 'Actual' ) ax_ . set_xlabel ( 'Actual' ) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.973') Compare these results with our linear model from Lab 3. Recall that we can quickly grab the names of the paramters in our sklearn model: RandomForestRegressor () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Cell for Exercise 2 <matplotlib.legend.Legend at 0x7f1fe5063f40>","title":"\ud83c\udfcb\ufe0f Exercise 2: Practice with Random Forests"},{"location":"S7_Boosting/","text":"Data Science Foundations Session 7: Boosting \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're continuing on the topic of supervised learning an ensemble learning method called boosting. 7.1 Preparing Environment and Importing Data \u00b6 back to top 7.1.1 Import Packages \u00b6 back to top from sklearn import svm from sklearn.datasets import make_blobs , make_circles from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier , GradientBoostingClassifier import matplotlib.pyplot as plt import numpy as np import plotly.express as px from ipywidgets import interact , FloatSlider , interactive def plot_boundaries ( X , clf , ax = False ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) if ax : cs = ax . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) ax . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return ax else : cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) 7.1.2 Load Dataset \u00b6 back to top For this session, we will use dummy datasets from sklearn. 7.2 Boosting \u00b6 back to top The last supervised learning algorithms we will cover, are the boosting learners. Similar to Bagging, Boosting algorithms leverage the idea of training on variations of the available data, only this time they do so in serial rather than parallel. What do I mean by this? It's a little nuanced, but the idea is straight forward. The first model trains on the dataset, it generates some error. The datapoints creating the greatest amount of error are emphasized in the second round of training, and so on, as the sequence of models proceeds, ever troublesome datapoints receive ever increasing influence. 7.2.1 AdaBoost \u00b6 Back to Top AdaBoost was the first boosting learner of its kind. It's weak learners (the things that are stitched together in serial) are typically stumps or really shallow decision trees. Lets create some data and fit an AdaBoostClassifier to it: X , y = make_circles ( random_state = 42 , noise = .01 ) clf = AdaBoostClassifier ( DecisionTreeClassifier ( max_depth = 3 )) clf . fit ( X , y ) plot_boundaries ( X , clf ) Trying with a different distribution of data: X , y = make_blobs ( random_state = 42 , centers = 2 , cluster_std = 2.5 ) clf = AdaBoostClassifier ( DecisionTreeClassifier ( max_depth = 5 )) clf . fit ( X , y ) plot_boundaries ( X , clf ) And now with make_moons : from sklearn.datasets import make_moons X , y = make_moons ( random_state = 42 , noise = .05 ) clf = AdaBoostClassifier () clf . fit ( X , y ) plot_boundaries ( X , clf ) 7.2.1 Gradient Boosting \u00b6 Back to Top Gradient Boosting builds on the idea of AdaBoost . The term gradient implies that 2 or more derivatives are being taken somewhere. What this is referring to, is while AdaBoost is subject to a predefined loss function, Gradient Boosting can take in any arbitrary (as long as it is differentiable) loss function, to coordinate the training of its weak learners. X , y = make_circles ( random_state = 42 , noise = .01 ) clf = GradientBoostingClassifier ( loss = 'deviance' ) clf . fit ( X , y ) plot_boundaries ( X , clf ) X , y = make_blobs ( random_state = 42 , centers = 2 , cluster_std = 2.5 ) clf = GradientBoostingClassifier () clf . fit ( X , y ) plot_boundaries ( X , clf ) from sklearn.datasets import make_moons X , y = make_moons ( random_state = 42 , noise = .05 ) clf . fit ( X , y ) plot_boundaries ( X , clf ) References \u00b6 back to top Generative vs Discriminative Models","title":"Boosting"},{"location":"S7_Boosting/#data-science-foundations-session-7-boosting","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're continuing on the topic of supervised learning an ensemble learning method called boosting.","title":"Data Science Foundations  Session 7: Boosting"},{"location":"S7_Boosting/#71-preparing-environment-and-importing-data","text":"back to top","title":"7.1 Preparing Environment and Importing Data"},{"location":"S7_Boosting/#711-import-packages","text":"back to top from sklearn import svm from sklearn.datasets import make_blobs , make_circles from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier , GradientBoostingClassifier import matplotlib.pyplot as plt import numpy as np import plotly.express as px from ipywidgets import interact , FloatSlider , interactive def plot_boundaries ( X , clf , ax = False ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) if ax : cs = ax . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) ax . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return ax else : cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 )","title":"7.1.1 Import Packages"},{"location":"S7_Boosting/#712-load-dataset","text":"back to top For this session, we will use dummy datasets from sklearn.","title":"7.1.2 Load Dataset"},{"location":"S7_Boosting/#72-boosting","text":"back to top The last supervised learning algorithms we will cover, are the boosting learners. Similar to Bagging, Boosting algorithms leverage the idea of training on variations of the available data, only this time they do so in serial rather than parallel. What do I mean by this? It's a little nuanced, but the idea is straight forward. The first model trains on the dataset, it generates some error. The datapoints creating the greatest amount of error are emphasized in the second round of training, and so on, as the sequence of models proceeds, ever troublesome datapoints receive ever increasing influence.","title":"7.2 Boosting"},{"location":"S7_Boosting/#721-adaboost","text":"Back to Top AdaBoost was the first boosting learner of its kind. It's weak learners (the things that are stitched together in serial) are typically stumps or really shallow decision trees. Lets create some data and fit an AdaBoostClassifier to it: X , y = make_circles ( random_state = 42 , noise = .01 ) clf = AdaBoostClassifier ( DecisionTreeClassifier ( max_depth = 3 )) clf . fit ( X , y ) plot_boundaries ( X , clf ) Trying with a different distribution of data: X , y = make_blobs ( random_state = 42 , centers = 2 , cluster_std = 2.5 ) clf = AdaBoostClassifier ( DecisionTreeClassifier ( max_depth = 5 )) clf . fit ( X , y ) plot_boundaries ( X , clf ) And now with make_moons : from sklearn.datasets import make_moons X , y = make_moons ( random_state = 42 , noise = .05 ) clf = AdaBoostClassifier () clf . fit ( X , y ) plot_boundaries ( X , clf )","title":"7.2.1 AdaBoost"},{"location":"S7_Boosting/#721-gradient-boosting","text":"Back to Top Gradient Boosting builds on the idea of AdaBoost . The term gradient implies that 2 or more derivatives are being taken somewhere. What this is referring to, is while AdaBoost is subject to a predefined loss function, Gradient Boosting can take in any arbitrary (as long as it is differentiable) loss function, to coordinate the training of its weak learners. X , y = make_circles ( random_state = 42 , noise = .01 ) clf = GradientBoostingClassifier ( loss = 'deviance' ) clf . fit ( X , y ) plot_boundaries ( X , clf ) X , y = make_blobs ( random_state = 42 , centers = 2 , cluster_std = 2.5 ) clf = GradientBoostingClassifier () clf . fit ( X , y ) plot_boundaries ( X , clf ) from sklearn.datasets import make_moons X , y = make_moons ( random_state = 42 , noise = .05 ) clf . fit ( X , y ) plot_boundaries ( X , clf )","title":"7.2.1 Gradient Boosting"},{"location":"S7_Boosting/#references","text":"back to top Generative vs Discriminative Models","title":"References"},{"location":"introduction/","text":"Data Science Foundations \u00b6 Cut through the deluge of data science material by focusing on the essentials. This course uses illustrations, code-based examples and case studies to demonstrate essential data science topics and the practical application of existing machine learning frameworks. By the end of the course, you will have trained and validated machine learning algorithms to make continuous-value as well as discrete-value predictions from data sources relevant in business and engineering. You will also be able to make statistically sound, data-driven decisions in business from sales and production data. The breakdown for this course is as follows: Data Topics Bias-variance tradeoff; regression: linear, logistic, and multivariate; regularization: L1 and L2; inferential statistics: moods median, t-tests, f-tests, ANOVA; descriptive statistics: mean, median, mode, kurtosis, skew; beyond regression coefficients: tree-based and resampling methods; unsupervised learning: clustering and dimensionality reduction Software Topics Unit Tests Sessions S1: Regression and Analysis S2: Inferential Statistics S3: Model Selection and Validation S4: Feature Engineering S5: Unsupervised Learning: Clustering and Dimensionality Reduction S6: Bagging: Decision Trees and Random Forests S7: Boosting: AdaBoost and XGBoost Labs L1: Descriptive Statistics Data Hunt L2: Inferential Statistics Data Hunt L3: Feature Engineering L4: Supervised Learners L5: Writing Unit Tests Project P1: Statistical Analysis of Tic-Tac-Toe Games P2: Heuristical Tic-Tac-Toe Agents P3: 1-Step Look Ahead Agents P4: N-Step Look Ahead Agents Extras X1: Thinking Data Reading JVDP chapter 5","title":"Introduction"},{"location":"introduction/#data-science-foundations","text":"Cut through the deluge of data science material by focusing on the essentials. This course uses illustrations, code-based examples and case studies to demonstrate essential data science topics and the practical application of existing machine learning frameworks. By the end of the course, you will have trained and validated machine learning algorithms to make continuous-value as well as discrete-value predictions from data sources relevant in business and engineering. You will also be able to make statistically sound, data-driven decisions in business from sales and production data. The breakdown for this course is as follows: Data Topics Bias-variance tradeoff; regression: linear, logistic, and multivariate; regularization: L1 and L2; inferential statistics: moods median, t-tests, f-tests, ANOVA; descriptive statistics: mean, median, mode, kurtosis, skew; beyond regression coefficients: tree-based and resampling methods; unsupervised learning: clustering and dimensionality reduction Software Topics Unit Tests Sessions S1: Regression and Analysis S2: Inferential Statistics S3: Model Selection and Validation S4: Feature Engineering S5: Unsupervised Learning: Clustering and Dimensionality Reduction S6: Bagging: Decision Trees and Random Forests S7: Boosting: AdaBoost and XGBoost Labs L1: Descriptive Statistics Data Hunt L2: Inferential Statistics Data Hunt L3: Feature Engineering L4: Supervised Learners L5: Writing Unit Tests Project P1: Statistical Analysis of Tic-Tac-Toe Games P2: Heuristical Tic-Tac-Toe Agents P3: 1-Step Look Ahead Agents P4: N-Step Look Ahead Agents Extras X1: Thinking Data Reading JVDP chapter 5","title":"Data Science Foundations"},{"location":"extras/Probability/","text":"Measuring Uncertainty \u00b6 One strategy is to just count occurance of outcomes The probability of either of two mutually exclusive events occuring is the sum of their probabilities P(A \\space or \\space B) = P(A \\cup B) = P(A) + P(B) The probability of two mutually exclusive events occuring together is the product of their probabilities P(A \\space and \\space B) = P(A \\cap B) = P(A) * P(B) For non-mutually exclusive events: P (A \\cup B) = P(A) + P(B) - P(A \\cap B) Q1 \u00b6 In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that their sum will be at most 9 tot_outcomes = 6 ** 2 sum_less_than_9 = 6 + 6 + 6 + 5 + 4 + 3 sum_less_than_9 / tot_outcomes 0.8333333333333334 Q2 \u00b6 In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that the values rolled by each die will be different and the two dice have a sum of 6. # only 5 outcomes will sum to 6 # one of those has equal numbers # so there are 4/36 chances or 1/9 probability Q3 \u00b6 There are 3 urns labeled X, Y, and Z. Urn X contains 4 red balls and 3 black balls. Urn Y contains 5 red balls and 4 black balls. Urn Z contains 4 red balls and 4 black balls. One ball is drawn from each of the 3 urns. What is the probability that, of the 3 balls drawn, 2 are red and 1 is black? # multiply and sum probabilities # RRB 4/7 * 5/9 * 4/8 # RBR 4/7 * 4/9 * 4/8 # BRR 3/7 * 5/9 * 4/8 ( 4 / 7 * 5 / 9 * 1 / 2 ) + \\ ( 4 / 7 * 4 / 9 * 1 / 2 ) + \\ ( 3 / 7 * 5 / 9 * 1 / 2 ) 0.40476190476190477 Conditional Probability \u00b6 The flagship expression here is Bayes Rule or Bayesian Inference: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\). Q1 \u00b6 Suppose a family has 2 children, one of which is a boy. What is the probability that both children are boys? # child1 child2 # a boy; a girl # a girl; a boy # a boy; a boy Q2 \u00b6 You draw 2 cards from a standard 52-card deck without replacing them. What is the probability that both cards are of the same suit? # suites # 13 13 13 13 # hearts ( 13 - 1 ) / 51 #12/51 # spades 4/17 # clubs 4/17 # diamonds 4/17 # (4*4)/(4*17) # multiplying out still yields # 12/51 0.9411764705882353 Q3 \u00b6 If the probability of student A passing an exam is 2/7 and the probability of student B failing the exam is 3/7, then find the probability that at least 1 of the 2 students will pass the exam # P(A) = 2/7 # P(B) = 4/7 # All outcomes - sum to 1 # A pass B pass 2/7 * 4/7 # A fail B fail 5/7 * 3/7 # A pass B fail 2/7 * 3/7 # A fail B pass 5/7 * 4/7 # all outcomes ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 5 / 7 ) * ( 3 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) # outcomes we care about ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) 34 / 49 0.6938775510204082 Q4 \u00b6 Historical data shows that it has only rained 5 days per year in some desert region (assuming a 365 day year). A meteorologist predicts that it will rain today. When it actually rains, the meteorologist correctly predicts rain 90% of the time. When it doesn't rain, the meteorologist incorrectly predicts rain 10% of the time. Find the probability that it will rain today. # P(A|B) = probability that it will rain today given that the meteorologist has predicted it will rain # P(B|A) = probability that the meteoroligist will say it will rain when it rains; 90% # P(A) = probability that it will rain; 5/365 # P(B) = probability that meteoroligist will say it will rain # what is P(B) then? # P(B) = (5/365*.90) + ((365-5)/365*.1) P_B = ( 5 / 365 * .90 ) + (( 365 - 5 ) / 365 * .1 ) P_A = 5 / 365 P_BA = 0.9 P_AB = P_BA * P_A / P_B print ( f \"P(B|A): { P_BA } \" ) print ( f \"P(B): { P_B } \" ) print ( f \"P(A): { P_A } \" ) print ( f \"P(A|B): { P_AB } \" ) P(B|A): 0.9 P(B): 0.11095890410958904 P(A): 0.0136986301369863 P(A|B): 0.1111111111111111 Binomial Probabilities \u00b6 Operates on PMF (Probability Mass Functions) for discrete values answer key B(K;n,p) = \\binom{n}{k} \\times p^k \\times (1 - p)^{n-k} We can calculate the total number of outcomes we care about from a total number of trials using the binomial coefficient (this field of study is called combinatorics): \\binom{n}{k} = \\frac{n!}{k! \\times (n - k)!} This allows us to calculate the probability of an event: B(K;n,p) = \\binom{n}{k} \\times P(desired \\space outcome) def fact ( x ): \"\"\" return the factorial of a number using recursion \"\"\" if x == 1 or x == 0 : return 1 else : return fact ( x - 1 ) * x def n_choose_k ( n , k ): \"\"\" Returns the number of outcomes we care about of all possible outcomes \"\"\" return fact ( n ) / ( fact ( k ) * fact ( n - k )) def binom ( n , k , p ): \"\"\" Returns the probability of an event occuring K times in a total number of n trials having a probability of p \"\"\" return n_choose_k ( n , k ) * p ** k * ( 1 - p ) ** ( n - k ) def k_or_more ( n , k , p ): \"\"\" we can solve the K or more problem recursively \"\"\" if k == n : return binom ( n , k , p ) else : return k_or_more ( n , k + 1 , p ) + binom ( n , k , p ) Q1 \u00b6 When you're searching for a new job, it's always helpful to have more than one offer on the table so you can use it in negotiations. If you have \u2155 probability of receiving a job offer when you interview, and you interview iwth seven companies in a month, what is the probability you'll have at least two competing offers by the end of that month? p = 1 / 5 n = 7 k = 2 offers1 = k_or_more ( n , k , p ) print ( offers1 ) 0.4232832000000002 Q2 \u00b6 You get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you're tired. You really don't want to go on this many interviews unless you are at least twice as likely to get a least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7? p = 1 / 10 n = 25 k = 2 offers2 = k_or_more ( n , k , p ) print ( offers2 ) print ( offers2 / offers1 ) 0.7287940935386341 1.7217647512082543 The ratio of boys to girls for babies born in Russia is 1.09:1. If there is 1 child born per birth, what proportion of Russian families with exactly 6 children will have at least 3 boys? br , gr = 1.09 , 1 p = br / ( br + gr ) n = 6 k = 3 k_or_more ( n , k , p ) 0.6957033161509107 The Beta Distribution \u00b6 Operates on PDF (Probability Density Function) for continuous values Think: Probability of probabilities Beta(\\rho; \\alpha, \\beta) = \\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)} where \\(\\rho\\) is the probability of an event. This corresponds to the different hypotheses for the possible probabilities that could be generating our observed data; \\(\\alpha\\) represents how many times we observe an event we care about such as winning a coin toss; \\(\\beta\\) represents how many times the event we care about didn't happen, such as losing a coin toss. The total number of trials is \\(\\alpha + \\beta\\) (contrast this with \\(n\\) and \\(k\\) in the binomial distribution). The beta (lowercase) distribution: \\int_0^1{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}} Putting this all together. The probability that an event occurs in a specific range: Beta(\\rho; \\alpha, \\beta) = \\int_{lower \\space bound}^{upper \\space bound}{\\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)}} Q1 \u00b6 You want to use the beta distribution to determine whether or not a coin you have is a fair coin - meaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time? from scipy.stats import beta _alpha = 4 _beta = 6 model = beta ( _alpha , _beta ) model . pdf ( 0.6 ) 1.1147673600000005","title":"Probability"},{"location":"extras/Probability/#measuring-uncertainty","text":"One strategy is to just count occurance of outcomes The probability of either of two mutually exclusive events occuring is the sum of their probabilities P(A \\space or \\space B) = P(A \\cup B) = P(A) + P(B) The probability of two mutually exclusive events occuring together is the product of their probabilities P(A \\space and \\space B) = P(A \\cap B) = P(A) * P(B) For non-mutually exclusive events: P (A \\cup B) = P(A) + P(B) - P(A \\cap B)","title":"Measuring Uncertainty"},{"location":"extras/Probability/#q1","text":"In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that their sum will be at most 9 tot_outcomes = 6 ** 2 sum_less_than_9 = 6 + 6 + 6 + 5 + 4 + 3 sum_less_than_9 / tot_outcomes 0.8333333333333334","title":"Q1"},{"location":"extras/Probability/#q2","text":"In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that the values rolled by each die will be different and the two dice have a sum of 6. # only 5 outcomes will sum to 6 # one of those has equal numbers # so there are 4/36 chances or 1/9 probability","title":"Q2"},{"location":"extras/Probability/#q3","text":"There are 3 urns labeled X, Y, and Z. Urn X contains 4 red balls and 3 black balls. Urn Y contains 5 red balls and 4 black balls. Urn Z contains 4 red balls and 4 black balls. One ball is drawn from each of the 3 urns. What is the probability that, of the 3 balls drawn, 2 are red and 1 is black? # multiply and sum probabilities # RRB 4/7 * 5/9 * 4/8 # RBR 4/7 * 4/9 * 4/8 # BRR 3/7 * 5/9 * 4/8 ( 4 / 7 * 5 / 9 * 1 / 2 ) + \\ ( 4 / 7 * 4 / 9 * 1 / 2 ) + \\ ( 3 / 7 * 5 / 9 * 1 / 2 ) 0.40476190476190477","title":"Q3"},{"location":"extras/Probability/#conditional-probability","text":"The flagship expression here is Bayes Rule or Bayesian Inference: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\).","title":"Conditional Probability"},{"location":"extras/Probability/#q1_1","text":"Suppose a family has 2 children, one of which is a boy. What is the probability that both children are boys? # child1 child2 # a boy; a girl # a girl; a boy # a boy; a boy","title":"Q1"},{"location":"extras/Probability/#q2_1","text":"You draw 2 cards from a standard 52-card deck without replacing them. What is the probability that both cards are of the same suit? # suites # 13 13 13 13 # hearts ( 13 - 1 ) / 51 #12/51 # spades 4/17 # clubs 4/17 # diamonds 4/17 # (4*4)/(4*17) # multiplying out still yields # 12/51 0.9411764705882353","title":"Q2"},{"location":"extras/Probability/#q3_1","text":"If the probability of student A passing an exam is 2/7 and the probability of student B failing the exam is 3/7, then find the probability that at least 1 of the 2 students will pass the exam # P(A) = 2/7 # P(B) = 4/7 # All outcomes - sum to 1 # A pass B pass 2/7 * 4/7 # A fail B fail 5/7 * 3/7 # A pass B fail 2/7 * 3/7 # A fail B pass 5/7 * 4/7 # all outcomes ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 5 / 7 ) * ( 3 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) # outcomes we care about ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) 34 / 49 0.6938775510204082","title":"Q3"},{"location":"extras/Probability/#q4","text":"Historical data shows that it has only rained 5 days per year in some desert region (assuming a 365 day year). A meteorologist predicts that it will rain today. When it actually rains, the meteorologist correctly predicts rain 90% of the time. When it doesn't rain, the meteorologist incorrectly predicts rain 10% of the time. Find the probability that it will rain today. # P(A|B) = probability that it will rain today given that the meteorologist has predicted it will rain # P(B|A) = probability that the meteoroligist will say it will rain when it rains; 90% # P(A) = probability that it will rain; 5/365 # P(B) = probability that meteoroligist will say it will rain # what is P(B) then? # P(B) = (5/365*.90) + ((365-5)/365*.1) P_B = ( 5 / 365 * .90 ) + (( 365 - 5 ) / 365 * .1 ) P_A = 5 / 365 P_BA = 0.9 P_AB = P_BA * P_A / P_B print ( f \"P(B|A): { P_BA } \" ) print ( f \"P(B): { P_B } \" ) print ( f \"P(A): { P_A } \" ) print ( f \"P(A|B): { P_AB } \" ) P(B|A): 0.9 P(B): 0.11095890410958904 P(A): 0.0136986301369863 P(A|B): 0.1111111111111111","title":"Q4"},{"location":"extras/Probability/#binomial-probabilities","text":"Operates on PMF (Probability Mass Functions) for discrete values answer key B(K;n,p) = \\binom{n}{k} \\times p^k \\times (1 - p)^{n-k} We can calculate the total number of outcomes we care about from a total number of trials using the binomial coefficient (this field of study is called combinatorics): \\binom{n}{k} = \\frac{n!}{k! \\times (n - k)!} This allows us to calculate the probability of an event: B(K;n,p) = \\binom{n}{k} \\times P(desired \\space outcome) def fact ( x ): \"\"\" return the factorial of a number using recursion \"\"\" if x == 1 or x == 0 : return 1 else : return fact ( x - 1 ) * x def n_choose_k ( n , k ): \"\"\" Returns the number of outcomes we care about of all possible outcomes \"\"\" return fact ( n ) / ( fact ( k ) * fact ( n - k )) def binom ( n , k , p ): \"\"\" Returns the probability of an event occuring K times in a total number of n trials having a probability of p \"\"\" return n_choose_k ( n , k ) * p ** k * ( 1 - p ) ** ( n - k ) def k_or_more ( n , k , p ): \"\"\" we can solve the K or more problem recursively \"\"\" if k == n : return binom ( n , k , p ) else : return k_or_more ( n , k + 1 , p ) + binom ( n , k , p )","title":"Binomial Probabilities"},{"location":"extras/Probability/#q1_2","text":"When you're searching for a new job, it's always helpful to have more than one offer on the table so you can use it in negotiations. If you have \u2155 probability of receiving a job offer when you interview, and you interview iwth seven companies in a month, what is the probability you'll have at least two competing offers by the end of that month? p = 1 / 5 n = 7 k = 2 offers1 = k_or_more ( n , k , p ) print ( offers1 ) 0.4232832000000002","title":"Q1"},{"location":"extras/Probability/#q2_2","text":"You get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you're tired. You really don't want to go on this many interviews unless you are at least twice as likely to get a least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7? p = 1 / 10 n = 25 k = 2 offers2 = k_or_more ( n , k , p ) print ( offers2 ) print ( offers2 / offers1 ) 0.7287940935386341 1.7217647512082543 The ratio of boys to girls for babies born in Russia is 1.09:1. If there is 1 child born per birth, what proportion of Russian families with exactly 6 children will have at least 3 boys? br , gr = 1.09 , 1 p = br / ( br + gr ) n = 6 k = 3 k_or_more ( n , k , p ) 0.6957033161509107","title":"Q2"},{"location":"extras/Probability/#the-beta-distribution","text":"Operates on PDF (Probability Density Function) for continuous values Think: Probability of probabilities Beta(\\rho; \\alpha, \\beta) = \\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)} where \\(\\rho\\) is the probability of an event. This corresponds to the different hypotheses for the possible probabilities that could be generating our observed data; \\(\\alpha\\) represents how many times we observe an event we care about such as winning a coin toss; \\(\\beta\\) represents how many times the event we care about didn't happen, such as losing a coin toss. The total number of trials is \\(\\alpha + \\beta\\) (contrast this with \\(n\\) and \\(k\\) in the binomial distribution). The beta (lowercase) distribution: \\int_0^1{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}} Putting this all together. The probability that an event occurs in a specific range: Beta(\\rho; \\alpha, \\beta) = \\int_{lower \\space bound}^{upper \\space bound}{\\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)}}","title":"The Beta Distribution"},{"location":"extras/Probability/#q1_3","text":"You want to use the beta distribution to determine whether or not a coin you have is a fair coin - meaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time? from scipy.stats import beta _alpha = 4 _beta = 6 model = beta ( _alpha , _beta ) model . pdf ( 0.6 ) 1.1147673600000005","title":"Q1"},{"location":"extras/X1_Bayesian_Probability/","text":"Data Science Foundations X1: Common Probability Topics in Data Science \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com This notebook is meant to summarize and provide some basic examples and problems of probability Measuring Uncertainty \u00b6 One strategy is to just count occurance of outcomes The probability of either of two mutually exclusive events occuring is the sum of their probabilities P(A \\space or \\space B) = P(A \\cup B) = P(A) + P(B) The probability of two mutually exclusive events occuring together is the product of their probabilities P(A \\space and \\space B) = P(A \\cap B) = P(A) * P(B) For non-mutually exclusive events: P (A \\cup B) = P(A) + P(B) - P(A \\cap B) Q1 \u00b6 In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that their sum will be at most 9 tot_outcomes = 6 ** 2 sum_less_than_9 = 6 + 6 + 6 + 5 + 4 + 3 sum_less_than_9 / tot_outcomes 0.8333333333333334 Q2 \u00b6 In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that the values rolled by each die will be different and the two dice have a sum of 6. # only 5 outcomes will sum to 6 # one of those has equal numbers # so there are 4/36 chances or 1/9 probability Q3 \u00b6 There are 3 urns labeled X, Y, and Z. Urn X contains 4 red balls and 3 black balls. Urn Y contains 5 red balls and 4 black balls. Urn Z contains 4 red balls and 4 black balls. One ball is drawn from each of the 3 urns. What is the probability that, of the 3 balls drawn, 2 are red and 1 is black? # multiply and sum probabilities # RRB 4/7 * 5/9 * 4/8 # RBR 4/7 * 4/9 * 4/8 # BRR 3/7 * 5/9 * 4/8 ( 4 / 7 * 5 / 9 * 1 / 2 ) + \\ ( 4 / 7 * 4 / 9 * 1 / 2 ) + \\ ( 3 / 7 * 5 / 9 * 1 / 2 ) 0.40476190476190477 Conditional Probability \u00b6 The flagship expression here is Bayes Rule or Bayesian Inference: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Also formulated this way P(belief|data) = \\frac{P(data|belief) * P(belief)}{P(data)} The posterior, the likelihood, and the prior The \\space posterior \\space probability = \\frac{Likelihood * prior \\space probability}{observed \\space data} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\). Q1 \u00b6 Suppose a family has 2 children, one of which is a boy. What is the probability that both children are boys? # child1 child2 # a boy; a girl # a girl; a boy # a boy; a boy # a girl; a girl - not included # answer - 1/3 Q2 \u00b6 You draw 2 cards from a standard 52-card deck without replacing them. What is the probability that both cards are of the same suit? # suites # 13 13 13 13 # hearts ( 13 - 1 ) / 51 #12/51 # spades 4/17 # clubs 4/17 # diamonds 4/17 # (4*4)/(4*17) # multiplying out still yields # 12/51 # 1/4 * 12/51 + 1/4 * 12/51 + 1/4 * 12/51 + 1/4 * 12/51 = 12/51 0.23529411764705882 # prob(a|b) = (prob(b|a)*prob(a)) / prob(b) = intersect(a|b) / prob(b) # (1/4) * (12/51) / Q3 \u00b6 If the probability of student A passing an exam is 2/7 and the probability of student B failing the exam is 3/7, then find the probability that at least 1 of the 2 students will pass the exam # P(A) = 2/7 # P(B) = 4/7 # All outcomes - sum to 1 # A pass B pass 2/7 * 4/7 # A fail B fail 5/7 * 3/7 # A pass B fail 2/7 * 3/7 # A fail B pass 5/7 * 4/7 # all outcomes ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 5 / 7 ) * ( 3 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) # outcomes we care about ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) 34 / 49 0.6938775510204082 Q4 \u00b6 Historical data shows that it has only rained 5 days per year in some desert region (assuming a 365 day year). A meteorologist predicts that it will rain today. When it actually rains, the meteorologist correctly predicts rain 90% of the time. When it doesn't rain, the meteorologist incorrectly predicts rain 10% of the time. Find the probability that it will rain today. # P(A|B) = probability that it will rain today given that the meteorologist has predicted it will rain # P(B|A) = probability that the meteoroligist will say it will rain when it rains; 90% # P(A) = probability that it will rain; 5/365 # P(B) = probability that meteoroligist will say it will rain # what is P(B) then? # P(B) = (5/365*.90) + ((365-5)/365*.1) P_B = ( 5 / 365 * .90 ) + (( 365 - 5 ) / 365 * .1 ) P_A = 5 / 365 P_BA = 0.9 P_AB = P_BA * P_A / P_B print ( f \"P(B|A): { P_BA } \" ) print ( f \"P(B): { P_B } \" ) print ( f \"P(A): { P_A } \" ) print ( f \"P(A|B): { P_AB } \" ) P(B|A): 0.9 P(B): 0.11095890410958904 P(A): 0.0136986301369863 P(A|B): 0.1111111111111111 Binomial Probabilities \u00b6 Operates on PMF (Probability Mass Functions) for discrete values answer key B(K;n,p) = \\binom{n}{k} \\times p^k \\times (1 - p)^{n-k} We can calculate the total number of outcomes we care about from a total number of trials using the binomial coefficient (this field of study is called combinatorics): \\binom{n}{k} = \\frac{n!}{k! \\times (n - k)!} This allows us to calculate the probability of an event: B(K;n,p) = \\binom{n}{k} \\times P(desired \\space outcome) def fact ( x ): \"\"\" return the factorial of a number using recursion \"\"\" if x == 1 or x == 0 : return 1 else : return fact ( x - 1 ) * x def n_choose_k ( n , k ): \"\"\" Returns the number of outcomes we care about of all possible outcomes \"\"\" return fact ( n ) / ( fact ( k ) * fact ( n - k )) def binom ( n , k , p ): \"\"\" Returns the probability of an event occuring K times in a total number of n trials having a probability of p \"\"\" return n_choose_k ( n , k ) * p ** k * ( 1 - p ) ** ( n - k ) def k_or_more ( n , k , p ): \"\"\" we can solve the K or more problem recursively \"\"\" if k == n : return binom ( n , k , p ) else : return k_or_more ( n , k + 1 , p ) + binom ( n , k , p ) Q1 \u00b6 When you're searching for a new job, it's always helpful to have more than one offer on the table so you can use it in negotiations. If you have \u2155 probability of receiving a job offer when you interview, and you interview iwth seven companies in a month, what is the probability you'll have at least two competing offers by the end of that month? p = 1 / 5 n = 7 k = 2 offers1 = k_or_more ( n , k , p ) print ( offers1 ) 0.4232832000000002 Q2 \u00b6 You get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you're tired. You really don't want to go on this many interviews unless you are at least twice as likely to get a least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7? p = 1 / 10 n = 25 k = 2 offers2 = k_or_more ( n , k , p ) print ( offers2 ) print ( offers2 / offers1 ) 0.7287940935386341 1.7217647512082543 The ratio of boys to girls for babies born in Russia is 1.09:1. If there is 1 child born per birth, what proportion of Russian families with exactly 6 children will have at least 3 boys? br , gr = 1.09 , 1 p = br / ( br + gr ) n = 6 k = 3 k_or_more ( n , k , p ) 0.6957033161509107 The Beta Distribution \u00b6 Operates on PDF (Probability Density Function) for continuous values Think: Probability of probabilities Beta(\\rho; \\alpha, \\beta) = \\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)} where \\(\\rho\\) is the probability of an event. This corresponds to the different hypotheses for the possible probabilities that could be generating our observed data; \\(\\alpha\\) represents how many times we observe an event we care about such as winning a coin toss; \\(\\beta\\) represents how many times the event we care about didn't happen, such as losing a coin toss. The total number of trials is \\(\\alpha + \\beta\\) (contrast this with \\(n\\) and \\(k\\) in the binomial distribution). The beta (lowercase) distribution: \\int_0^1{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}} Putting this all together. The probability that an event occurs in a specific range: Beta(\\rho; \\alpha, \\beta) = \\int_{lower \\space bound}^{upper \\space bound}{\\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)}} Q1 \u00b6 You want to use the beta distribution to determine whether or not a coin you have is a fair coin - meaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time? from scipy.stats import beta _alpha = 4 _beta = 6 model = beta ( _alpha , _beta ) model . pdf ( 0.6 ) 1.1147673600000005 Poisson Random Variables \u00b6 What happens when we can't estimate \\(p\\) for a binomial probability? Enter the poisson experiemnt A Poisson experiment is a statistical experiment that has the following properties: The outcome of each trial is either success or failure. The average number of successes that occurs in a specified region is known. The probability that a success will occur is proportional to the size of the region. The probability that a success will occur in an extremely small region is virtually zero. The Poisson distribution is: P(k,\\lambda) = \\frac{\\lambda^k \\exp^{-\\lambda}}{k!} where \\(\\lambda\\) is the average number of successes that occur in a specified region, \\(k\\) is the actual number of successes that occur in a specified region, \\(P(K,\\lambda)\\) is the Poisson probability, which is the probability of getting exactly successes when the average number of successes is \\(\\lambda\\). from math import exp def poisson ( k , lamb ): return ( lamb ** k * exp ( - lamb )) / ( fact ( k )) Q1 \u00b6 A random variable, \\(X\\), follows Poisson distribution with mean of 2.5. Find the probability with which the random variable \\(X\\) is equal to 5.","title":"Probability"},{"location":"extras/X1_Bayesian_Probability/#data-science-foundations-x1-common-probability-topics-in-data-science","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com This notebook is meant to summarize and provide some basic examples and problems of probability","title":"Data Science Foundations  X1: Common Probability Topics in Data Science"},{"location":"extras/X1_Bayesian_Probability/#measuring-uncertainty","text":"One strategy is to just count occurance of outcomes The probability of either of two mutually exclusive events occuring is the sum of their probabilities P(A \\space or \\space B) = P(A \\cup B) = P(A) + P(B) The probability of two mutually exclusive events occuring together is the product of their probabilities P(A \\space and \\space B) = P(A \\cap B) = P(A) * P(B) For non-mutually exclusive events: P (A \\cup B) = P(A) + P(B) - P(A \\cap B)","title":"Measuring Uncertainty"},{"location":"extras/X1_Bayesian_Probability/#q1","text":"In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that their sum will be at most 9 tot_outcomes = 6 ** 2 sum_less_than_9 = 6 + 6 + 6 + 5 + 4 + 3 sum_less_than_9 / tot_outcomes 0.8333333333333334","title":"Q1"},{"location":"extras/X1_Bayesian_Probability/#q2","text":"In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that the values rolled by each die will be different and the two dice have a sum of 6. # only 5 outcomes will sum to 6 # one of those has equal numbers # so there are 4/36 chances or 1/9 probability","title":"Q2"},{"location":"extras/X1_Bayesian_Probability/#q3","text":"There are 3 urns labeled X, Y, and Z. Urn X contains 4 red balls and 3 black balls. Urn Y contains 5 red balls and 4 black balls. Urn Z contains 4 red balls and 4 black balls. One ball is drawn from each of the 3 urns. What is the probability that, of the 3 balls drawn, 2 are red and 1 is black? # multiply and sum probabilities # RRB 4/7 * 5/9 * 4/8 # RBR 4/7 * 4/9 * 4/8 # BRR 3/7 * 5/9 * 4/8 ( 4 / 7 * 5 / 9 * 1 / 2 ) + \\ ( 4 / 7 * 4 / 9 * 1 / 2 ) + \\ ( 3 / 7 * 5 / 9 * 1 / 2 ) 0.40476190476190477","title":"Q3"},{"location":"extras/X1_Bayesian_Probability/#conditional-probability","text":"The flagship expression here is Bayes Rule or Bayesian Inference: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Also formulated this way P(belief|data) = \\frac{P(data|belief) * P(belief)}{P(data)} The posterior, the likelihood, and the prior The \\space posterior \\space probability = \\frac{Likelihood * prior \\space probability}{observed \\space data} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\).","title":"Conditional Probability"},{"location":"extras/X1_Bayesian_Probability/#q1_1","text":"Suppose a family has 2 children, one of which is a boy. What is the probability that both children are boys? # child1 child2 # a boy; a girl # a girl; a boy # a boy; a boy # a girl; a girl - not included # answer - 1/3","title":"Q1"},{"location":"extras/X1_Bayesian_Probability/#q2_1","text":"You draw 2 cards from a standard 52-card deck without replacing them. What is the probability that both cards are of the same suit? # suites # 13 13 13 13 # hearts ( 13 - 1 ) / 51 #12/51 # spades 4/17 # clubs 4/17 # diamonds 4/17 # (4*4)/(4*17) # multiplying out still yields # 12/51 # 1/4 * 12/51 + 1/4 * 12/51 + 1/4 * 12/51 + 1/4 * 12/51 = 12/51 0.23529411764705882 # prob(a|b) = (prob(b|a)*prob(a)) / prob(b) = intersect(a|b) / prob(b) # (1/4) * (12/51) /","title":"Q2"},{"location":"extras/X1_Bayesian_Probability/#q3_1","text":"If the probability of student A passing an exam is 2/7 and the probability of student B failing the exam is 3/7, then find the probability that at least 1 of the 2 students will pass the exam # P(A) = 2/7 # P(B) = 4/7 # All outcomes - sum to 1 # A pass B pass 2/7 * 4/7 # A fail B fail 5/7 * 3/7 # A pass B fail 2/7 * 3/7 # A fail B pass 5/7 * 4/7 # all outcomes ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 5 / 7 ) * ( 3 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) # outcomes we care about ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) 34 / 49 0.6938775510204082","title":"Q3"},{"location":"extras/X1_Bayesian_Probability/#q4","text":"Historical data shows that it has only rained 5 days per year in some desert region (assuming a 365 day year). A meteorologist predicts that it will rain today. When it actually rains, the meteorologist correctly predicts rain 90% of the time. When it doesn't rain, the meteorologist incorrectly predicts rain 10% of the time. Find the probability that it will rain today. # P(A|B) = probability that it will rain today given that the meteorologist has predicted it will rain # P(B|A) = probability that the meteoroligist will say it will rain when it rains; 90% # P(A) = probability that it will rain; 5/365 # P(B) = probability that meteoroligist will say it will rain # what is P(B) then? # P(B) = (5/365*.90) + ((365-5)/365*.1) P_B = ( 5 / 365 * .90 ) + (( 365 - 5 ) / 365 * .1 ) P_A = 5 / 365 P_BA = 0.9 P_AB = P_BA * P_A / P_B print ( f \"P(B|A): { P_BA } \" ) print ( f \"P(B): { P_B } \" ) print ( f \"P(A): { P_A } \" ) print ( f \"P(A|B): { P_AB } \" ) P(B|A): 0.9 P(B): 0.11095890410958904 P(A): 0.0136986301369863 P(A|B): 0.1111111111111111","title":"Q4"},{"location":"extras/X1_Bayesian_Probability/#binomial-probabilities","text":"Operates on PMF (Probability Mass Functions) for discrete values answer key B(K;n,p) = \\binom{n}{k} \\times p^k \\times (1 - p)^{n-k} We can calculate the total number of outcomes we care about from a total number of trials using the binomial coefficient (this field of study is called combinatorics): \\binom{n}{k} = \\frac{n!}{k! \\times (n - k)!} This allows us to calculate the probability of an event: B(K;n,p) = \\binom{n}{k} \\times P(desired \\space outcome) def fact ( x ): \"\"\" return the factorial of a number using recursion \"\"\" if x == 1 or x == 0 : return 1 else : return fact ( x - 1 ) * x def n_choose_k ( n , k ): \"\"\" Returns the number of outcomes we care about of all possible outcomes \"\"\" return fact ( n ) / ( fact ( k ) * fact ( n - k )) def binom ( n , k , p ): \"\"\" Returns the probability of an event occuring K times in a total number of n trials having a probability of p \"\"\" return n_choose_k ( n , k ) * p ** k * ( 1 - p ) ** ( n - k ) def k_or_more ( n , k , p ): \"\"\" we can solve the K or more problem recursively \"\"\" if k == n : return binom ( n , k , p ) else : return k_or_more ( n , k + 1 , p ) + binom ( n , k , p )","title":"Binomial Probabilities"},{"location":"extras/X1_Bayesian_Probability/#q1_2","text":"When you're searching for a new job, it's always helpful to have more than one offer on the table so you can use it in negotiations. If you have \u2155 probability of receiving a job offer when you interview, and you interview iwth seven companies in a month, what is the probability you'll have at least two competing offers by the end of that month? p = 1 / 5 n = 7 k = 2 offers1 = k_or_more ( n , k , p ) print ( offers1 ) 0.4232832000000002","title":"Q1"},{"location":"extras/X1_Bayesian_Probability/#q2_2","text":"You get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you're tired. You really don't want to go on this many interviews unless you are at least twice as likely to get a least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7? p = 1 / 10 n = 25 k = 2 offers2 = k_or_more ( n , k , p ) print ( offers2 ) print ( offers2 / offers1 ) 0.7287940935386341 1.7217647512082543 The ratio of boys to girls for babies born in Russia is 1.09:1. If there is 1 child born per birth, what proportion of Russian families with exactly 6 children will have at least 3 boys? br , gr = 1.09 , 1 p = br / ( br + gr ) n = 6 k = 3 k_or_more ( n , k , p ) 0.6957033161509107","title":"Q2"},{"location":"extras/X1_Bayesian_Probability/#the-beta-distribution","text":"Operates on PDF (Probability Density Function) for continuous values Think: Probability of probabilities Beta(\\rho; \\alpha, \\beta) = \\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)} where \\(\\rho\\) is the probability of an event. This corresponds to the different hypotheses for the possible probabilities that could be generating our observed data; \\(\\alpha\\) represents how many times we observe an event we care about such as winning a coin toss; \\(\\beta\\) represents how many times the event we care about didn't happen, such as losing a coin toss. The total number of trials is \\(\\alpha + \\beta\\) (contrast this with \\(n\\) and \\(k\\) in the binomial distribution). The beta (lowercase) distribution: \\int_0^1{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}} Putting this all together. The probability that an event occurs in a specific range: Beta(\\rho; \\alpha, \\beta) = \\int_{lower \\space bound}^{upper \\space bound}{\\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)}}","title":"The Beta Distribution"},{"location":"extras/X1_Bayesian_Probability/#q1_3","text":"You want to use the beta distribution to determine whether or not a coin you have is a fair coin - meaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time? from scipy.stats import beta _alpha = 4 _beta = 6 model = beta ( _alpha , _beta ) model . pdf ( 0.6 ) 1.1147673600000005","title":"Q1"},{"location":"extras/X1_Bayesian_Probability/#poisson-random-variables","text":"What happens when we can't estimate \\(p\\) for a binomial probability? Enter the poisson experiemnt A Poisson experiment is a statistical experiment that has the following properties: The outcome of each trial is either success or failure. The average number of successes that occurs in a specified region is known. The probability that a success will occur is proportional to the size of the region. The probability that a success will occur in an extremely small region is virtually zero. The Poisson distribution is: P(k,\\lambda) = \\frac{\\lambda^k \\exp^{-\\lambda}}{k!} where \\(\\lambda\\) is the average number of successes that occur in a specified region, \\(k\\) is the actual number of successes that occur in a specified region, \\(P(K,\\lambda)\\) is the Poisson probability, which is the probability of getting exactly successes when the average number of successes is \\(\\lambda\\). from math import exp def poisson ( k , lamb ): return ( lamb ** k * exp ( - lamb )) / ( fact ( k ))","title":"Poisson Random Variables"},{"location":"extras/X1_Bayesian_Probability/#q1_4","text":"A random variable, \\(X\\), follows Poisson distribution with mean of 2.5. Find the probability with which the random variable \\(X\\) is equal to 5.","title":"Q1"},{"location":"extras/X1_Thinking_Data/","text":"Data Science Foundations X1: Thinking Data \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to take our newfound knowledge from the course, and practice how we can leverage data to build predictive models. We'll start with a feature engineering problem on some dummy data. This will get us thinking creatively about problem solving. We will then pivot over to an Airbnb dataset . After performing some general, exploratory data analysis, we will solve the following business case: Airbnb is interested in using historical list prices from their airbnb hosts, to make pricing suggestions to new hosts. How can we use this existing datset to assist with this price listing suggestion? Prepare Environment and Import Data \u00b6 # basic packages import pandas as pd import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score from sklearn.model_selection import GridSearchCV , cross_val_score Warm Up \u00b6 Add aditional feature(s) to X to predict y with a linear classifier (e.g. logistic regression) from sklearn.datasets import make_circles X , y = make_circles ( random_state = 42 , noise = .01 ) relabel = dict ( zip ([ 0 , 1 , 2 , 3 ],[ 0 , 1 , 0 , 1 ])) y = np . vectorize ( relabel . get )( y ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f20ec66d280> and now predict # consider using # LogisticRegression() # r2_score Exploratory Data Analysis \u00b6 which columns are numerical, string; which contain nans/nulls; what is the correlation between features airbnb = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/datasets/main/datasets/airbnb/AB_NYC_2019.csv\" ) # recall these attributes # shape dtypes # and these methods # head() tail() isnull() sum() nunique() copy() select_dtypes() describe() drop() what visualizations would be useful to make? plt . figure ( figsize = ( 10 , 6 )) sns . scatterplot ( x = airbnb . longitude , y = airbnb . latitude , hue = airbnb . neighbourhood_group ) <AxesSubplot:xlabel='longitude', ylabel='latitude'> Feature Engineering \u00b6 Say we want to predict price , using an ML model. How would you build your features? Based on the number of null values, what would you do with the last_review and reviews_per_month column? X = airbnb . copy () How will we deal with the categorical features? # Recall # OneHotEncoder() X_cat = X . select_dtypes ( include = 'object' ) display ( X_cat . head ()) print ( X_cat . nunique ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name host_name neighbourhood_group neighbourhood room_type last_review 0 Clean & quiet apt home by the park John Brooklyn Kensington Private room 2018-10-19 1 Skylit Midtown Castle Jennifer Manhattan Midtown Entire home/apt 2019-05-21 2 THE VILLAGE OF HARLEM....NEW YORK ! Elisabeth Manhattan Harlem Private room NaN 3 Cozy Entire Floor of Brownstone LisaRoxanne Brooklyn Clinton Hill Entire home/apt 2019-07-05 4 Entire Apt: Spacious Studio/Loft by central park Laura Manhattan East Harlem Entire home/apt 2018-11-19 name 47905 host_name 11452 neighbourhood_group 5 neighbourhood 221 room_type 3 last_review 1764 dtype: int64 X_num = X . select_dtypes ( exclude = 'object' ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 149 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 225 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 150 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 89 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 80 10 9 0.10 1 0 Feature Transformation \u00b6 What features do you think will cause the most problems if untransformed? Scale and Center all but the target variable, price # Recall # StandardScaler() df.pop() Model Baseline \u00b6 # Recall # X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42) # LinearRegression() # r2_score() try a model that captures non-linear relationships # Recall # RandomForestRegressor() both of these results from the LinearRegression and RandomForest models should indicate something to you (think back to Model Selection and Validation ) Additional Strategies \u00b6 After this first pass, what are some additional strategies to consider for improving the model?","title":"X1 Thinking Data"},{"location":"extras/X1_Thinking_Data/#data-science-foundations-x1-thinking-data","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to take our newfound knowledge from the course, and practice how we can leverage data to build predictive models. We'll start with a feature engineering problem on some dummy data. This will get us thinking creatively about problem solving. We will then pivot over to an Airbnb dataset . After performing some general, exploratory data analysis, we will solve the following business case: Airbnb is interested in using historical list prices from their airbnb hosts, to make pricing suggestions to new hosts. How can we use this existing datset to assist with this price listing suggestion?","title":"Data Science Foundations  X1: Thinking Data"},{"location":"extras/X1_Thinking_Data/#prepare-environment-and-import-data","text":"# basic packages import pandas as pd import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score from sklearn.model_selection import GridSearchCV , cross_val_score","title":"Prepare Environment and Import Data"},{"location":"extras/X1_Thinking_Data/#warm-up","text":"Add aditional feature(s) to X to predict y with a linear classifier (e.g. logistic regression) from sklearn.datasets import make_circles X , y = make_circles ( random_state = 42 , noise = .01 ) relabel = dict ( zip ([ 0 , 1 , 2 , 3 ],[ 0 , 1 , 0 , 1 ])) y = np . vectorize ( relabel . get )( y ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f20ec66d280> and now predict # consider using # LogisticRegression() # r2_score","title":"Warm Up"},{"location":"extras/X1_Thinking_Data/#exploratory-data-analysis","text":"which columns are numerical, string; which contain nans/nulls; what is the correlation between features airbnb = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/datasets/main/datasets/airbnb/AB_NYC_2019.csv\" ) # recall these attributes # shape dtypes # and these methods # head() tail() isnull() sum() nunique() copy() select_dtypes() describe() drop() what visualizations would be useful to make? plt . figure ( figsize = ( 10 , 6 )) sns . scatterplot ( x = airbnb . longitude , y = airbnb . latitude , hue = airbnb . neighbourhood_group ) <AxesSubplot:xlabel='longitude', ylabel='latitude'>","title":"Exploratory Data Analysis"},{"location":"extras/X1_Thinking_Data/#feature-engineering","text":"Say we want to predict price , using an ML model. How would you build your features? Based on the number of null values, what would you do with the last_review and reviews_per_month column? X = airbnb . copy () How will we deal with the categorical features? # Recall # OneHotEncoder() X_cat = X . select_dtypes ( include = 'object' ) display ( X_cat . head ()) print ( X_cat . nunique ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name host_name neighbourhood_group neighbourhood room_type last_review 0 Clean & quiet apt home by the park John Brooklyn Kensington Private room 2018-10-19 1 Skylit Midtown Castle Jennifer Manhattan Midtown Entire home/apt 2019-05-21 2 THE VILLAGE OF HARLEM....NEW YORK ! Elisabeth Manhattan Harlem Private room NaN 3 Cozy Entire Floor of Brownstone LisaRoxanne Brooklyn Clinton Hill Entire home/apt 2019-07-05 4 Entire Apt: Spacious Studio/Loft by central park Laura Manhattan East Harlem Entire home/apt 2018-11-19 name 47905 host_name 11452 neighbourhood_group 5 neighbourhood 221 room_type 3 last_review 1764 dtype: int64 X_num = X . select_dtypes ( exclude = 'object' ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 149 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 225 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 150 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 89 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 80 10 9 0.10 1 0","title":"Feature Engineering"},{"location":"extras/X1_Thinking_Data/#feature-transformation","text":"What features do you think will cause the most problems if untransformed? Scale and Center all but the target variable, price # Recall # StandardScaler() df.pop()","title":"Feature Transformation"},{"location":"extras/X1_Thinking_Data/#model-baseline","text":"# Recall # X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42) # LinearRegression() # r2_score() try a model that captures non-linear relationships # Recall # RandomForestRegressor() both of these results from the LinearRegression and RandomForest models should indicate something to you (think back to Model Selection and Validation )","title":"Model Baseline"},{"location":"extras/X1_Thinking_Data/#additional-strategies","text":"After this first pass, what are some additional strategies to consider for improving the model?","title":"Additional Strategies"},{"location":"extras/X2_Airbnb/","text":"Data Science Foundations X2: Airbnb \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to take our newfound knowledge from the course, and practice how we can leverage data to build predictive models. We'll start with a feature engineering problem on some dummy data. This will get us thinking creatively about problem solving. We will then pivot over to an Airbnb dataset . After performing some general, exploratory data analysis, we will solve the following business case: Airbnb is interested in using historical list prices from their airbnb hosts, to make pricing suggestions to new hosts. How can we use this existing datset to assist with this price listing suggestion? Prepare Environment and Import Data \u00b6 # basic packages import pandas as pd import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score from sklearn.model_selection import GridSearchCV , cross_val_score Warm Up \u00b6 Add aditional feature(s) to X to predict y with a linear classifier (e.g. logistic regression) from sklearn.datasets import make_circles X , y = make_circles ( random_state = 42 , noise = .01 ) relabel = dict ( zip ([ 0 , 1 , 2 , 3 ],[ 0 , 1 , 0 , 1 ])) y = np . vectorize ( relabel . get )( y ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f20ec66d280> and now predict # consider using # LogisticRegression() # r2_score Exploratory Data Analysis \u00b6 which columns are numerical, string; which contain nans/nulls; what is the correlation between features airbnb = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/datasets/main/datasets/airbnb/AB_NYC_2019.csv\" ) # recall these attributes # shape dtypes # and these methods # head() tail() isnull() sum() nunique() copy() select_dtypes() describe() drop() what visualizations would be useful to make? plt . figure ( figsize = ( 10 , 6 )) sns . scatterplot ( x = airbnb . longitude , y = airbnb . latitude , hue = airbnb . neighbourhood_group ) <AxesSubplot:xlabel='longitude', ylabel='latitude'> Feature Engineering \u00b6 Say we want to predict price , using an ML model. How would you build your features? Based on the number of null values, what would you do with the last_review and reviews_per_month column? X = airbnb . copy () How will we deal with the categorical features? # Recall # OneHotEncoder() X_cat = X . select_dtypes ( include = 'object' ) display ( X_cat . head ()) print ( X_cat . nunique ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name host_name neighbourhood_group neighbourhood room_type last_review 0 Clean & quiet apt home by the park John Brooklyn Kensington Private room 2018-10-19 1 Skylit Midtown Castle Jennifer Manhattan Midtown Entire home/apt 2019-05-21 2 THE VILLAGE OF HARLEM....NEW YORK ! Elisabeth Manhattan Harlem Private room NaN 3 Cozy Entire Floor of Brownstone LisaRoxanne Brooklyn Clinton Hill Entire home/apt 2019-07-05 4 Entire Apt: Spacious Studio/Loft by central park Laura Manhattan East Harlem Entire home/apt 2018-11-19 name 47905 host_name 11452 neighbourhood_group 5 neighbourhood 221 room_type 3 last_review 1764 dtype: int64 X_num = X . select_dtypes ( exclude = 'object' ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 149 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 225 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 150 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 89 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 80 10 9 0.10 1 0 Feature Transformation \u00b6 What features do you think will cause the most problems if untransformed? Scale and Center all but the target variable, price # Recall # StandardScaler() df.pop() Model Baseline \u00b6 # Recall # X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42) # LinearRegression() # r2_score() try a model that captures non-linear relationships # Recall # RandomForestRegressor() both of these results from the LinearRegression and RandomForest models should indicate something to you (think back to Model Selection and Validation ) Additional Strategies \u00b6 After this first pass, what are some additional strategies to consider for improving the model?","title":"Airbnb"},{"location":"extras/X2_Airbnb/#data-science-foundations-x2-airbnb","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to take our newfound knowledge from the course, and practice how we can leverage data to build predictive models. We'll start with a feature engineering problem on some dummy data. This will get us thinking creatively about problem solving. We will then pivot over to an Airbnb dataset . After performing some general, exploratory data analysis, we will solve the following business case: Airbnb is interested in using historical list prices from their airbnb hosts, to make pricing suggestions to new hosts. How can we use this existing datset to assist with this price listing suggestion?","title":"Data Science Foundations  X2: Airbnb"},{"location":"extras/X2_Airbnb/#prepare-environment-and-import-data","text":"# basic packages import pandas as pd import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score from sklearn.model_selection import GridSearchCV , cross_val_score","title":"Prepare Environment and Import Data"},{"location":"extras/X2_Airbnb/#warm-up","text":"Add aditional feature(s) to X to predict y with a linear classifier (e.g. logistic regression) from sklearn.datasets import make_circles X , y = make_circles ( random_state = 42 , noise = .01 ) relabel = dict ( zip ([ 0 , 1 , 2 , 3 ],[ 0 , 1 , 0 , 1 ])) y = np . vectorize ( relabel . get )( y ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f20ec66d280> and now predict # consider using # LogisticRegression() # r2_score","title":"Warm Up"},{"location":"extras/X2_Airbnb/#exploratory-data-analysis","text":"which columns are numerical, string; which contain nans/nulls; what is the correlation between features airbnb = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/datasets/main/datasets/airbnb/AB_NYC_2019.csv\" ) # recall these attributes # shape dtypes # and these methods # head() tail() isnull() sum() nunique() copy() select_dtypes() describe() drop() what visualizations would be useful to make? plt . figure ( figsize = ( 10 , 6 )) sns . scatterplot ( x = airbnb . longitude , y = airbnb . latitude , hue = airbnb . neighbourhood_group ) <AxesSubplot:xlabel='longitude', ylabel='latitude'>","title":"Exploratory Data Analysis"},{"location":"extras/X2_Airbnb/#feature-engineering","text":"Say we want to predict price , using an ML model. How would you build your features? Based on the number of null values, what would you do with the last_review and reviews_per_month column? X = airbnb . copy () How will we deal with the categorical features? # Recall # OneHotEncoder() X_cat = X . select_dtypes ( include = 'object' ) display ( X_cat . head ()) print ( X_cat . nunique ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name host_name neighbourhood_group neighbourhood room_type last_review 0 Clean & quiet apt home by the park John Brooklyn Kensington Private room 2018-10-19 1 Skylit Midtown Castle Jennifer Manhattan Midtown Entire home/apt 2019-05-21 2 THE VILLAGE OF HARLEM....NEW YORK ! Elisabeth Manhattan Harlem Private room NaN 3 Cozy Entire Floor of Brownstone LisaRoxanne Brooklyn Clinton Hill Entire home/apt 2019-07-05 4 Entire Apt: Spacious Studio/Loft by central park Laura Manhattan East Harlem Entire home/apt 2018-11-19 name 47905 host_name 11452 neighbourhood_group 5 neighbourhood 221 room_type 3 last_review 1764 dtype: int64 X_num = X . select_dtypes ( exclude = 'object' ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 149 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 225 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 150 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 89 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 80 10 9 0.10 1 0","title":"Feature Engineering"},{"location":"extras/X2_Airbnb/#feature-transformation","text":"What features do you think will cause the most problems if untransformed? Scale and Center all but the target variable, price # Recall # StandardScaler() df.pop()","title":"Feature Transformation"},{"location":"extras/X2_Airbnb/#model-baseline","text":"# Recall # X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42) # LinearRegression() # r2_score() try a model that captures non-linear relationships # Recall # RandomForestRegressor() both of these results from the LinearRegression and RandomForest models should indicate something to you (think back to Model Selection and Validation )","title":"Model Baseline"},{"location":"extras/X2_Airbnb/#additional-strategies","text":"After this first pass, what are some additional strategies to consider for improving the model?","title":"Additional Strategies"},{"location":"extras/X2_Probability/","text":"Data Science Foundations X2: Common Probability Topics in Data Science \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com This notebook is meant to summarize and provide some basic examples and problems of probability Measuring Uncertainty \u00b6 One strategy is to just count occurance of outcomes The probability of either of two mutually exclusive events occuring is the sum of their probabilities P(A \\space or \\space B) = P(A \\cup B) = P(A) + P(B) The probability of two mutually exclusive events occuring together is the product of their probabilities P(A \\space and \\space B) = P(A \\cap B) = P(A) * P(B) For non-mutually exclusive events: P (A \\cup B) = P(A) + P(B) - P(A \\cap B) Q1 \u00b6 In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that their sum will be at most 9 tot_outcomes = 6 ** 2 sum_less_than_9 = 6 + 6 + 6 + 5 + 4 + 3 sum_less_than_9 / tot_outcomes 0.8333333333333334 Q2 \u00b6 In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that the values rolled by each die will be different and the two dice have a sum of 6. # only 5 outcomes will sum to 6 # one of those has equal numbers # so there are 4/36 chances or 1/9 probability Q3 \u00b6 There are 3 urns labeled X, Y, and Z. Urn X contains 4 red balls and 3 black balls. Urn Y contains 5 red balls and 4 black balls. Urn Z contains 4 red balls and 4 black balls. One ball is drawn from each of the 3 urns. What is the probability that, of the 3 balls drawn, 2 are red and 1 is black? # multiply and sum probabilities # RRB 4/7 * 5/9 * 4/8 # RBR 4/7 * 4/9 * 4/8 # BRR 3/7 * 5/9 * 4/8 ( 4 / 7 * 5 / 9 * 1 / 2 ) + \\ ( 4 / 7 * 4 / 9 * 1 / 2 ) + \\ ( 3 / 7 * 5 / 9 * 1 / 2 ) 0.40476190476190477 Conditional Probability \u00b6 The flagship expression here is Bayes Rule or Bayesian Inference: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Also formulated this way P(belief|data) = \\frac{P(data|belief) * P(belief)}{P(data)} The posterior, the likelihood, and the prior The \\space posterior \\space probability = \\frac{Likelihood * prior \\space probability}{observed \\space data} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\). Q1 \u00b6 Suppose a family has 2 children, one of which is a boy. What is the probability that both children are boys? # child1 child2 # a boy; a girl # a girl; a boy # a boy; a boy # a girl; a girl - not included # answer - 1/3 Q2 \u00b6 You draw 2 cards from a standard 52-card deck without replacing them. What is the probability that both cards are of the same suit? # suites # 13 13 13 13 # hearts ( 13 - 1 ) / 51 #12/51 # spades 4/17 # clubs 4/17 # diamonds 4/17 # (4*4)/(4*17) # multiplying out still yields # 12/51 # 1/4 * 12/51 + 1/4 * 12/51 + 1/4 * 12/51 + 1/4 * 12/51 = 12/51 0.23529411764705882 # prob(a|b) = (prob(b|a)*prob(a)) / prob(b) = intersect(a|b) / prob(b) # (1/4) * (12/51) / Q3 \u00b6 If the probability of student A passing an exam is 2/7 and the probability of student B failing the exam is 3/7, then find the probability that at least 1 of the 2 students will pass the exam # P(A) = 2/7 # P(B) = 4/7 # All outcomes - sum to 1 # A pass B pass 2/7 * 4/7 # A fail B fail 5/7 * 3/7 # A pass B fail 2/7 * 3/7 # A fail B pass 5/7 * 4/7 # all outcomes ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 5 / 7 ) * ( 3 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) # outcomes we care about ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) 34 / 49 0.6938775510204082 Q4 \u00b6 Historical data shows that it has only rained 5 days per year in some desert region (assuming a 365 day year). A meteorologist predicts that it will rain today. When it actually rains, the meteorologist correctly predicts rain 90% of the time. When it doesn't rain, the meteorologist incorrectly predicts rain 10% of the time. Find the probability that it will rain today. # P(A|B) = probability that it will rain today given that the meteorologist has predicted it will rain # P(B|A) = probability that the meteoroligist will say it will rain when it rains; 90% # P(A) = probability that it will rain; 5/365 # P(B) = probability that meteoroligist will say it will rain # what is P(B) then? # P(B) = (5/365*.90) + ((365-5)/365*.1) P_B = ( 5 / 365 * .90 ) + (( 365 - 5 ) / 365 * .1 ) P_A = 5 / 365 P_BA = 0.9 P_AB = P_BA * P_A / P_B print ( f \"P(B|A): { P_BA } \" ) print ( f \"P(B): { P_B } \" ) print ( f \"P(A): { P_A } \" ) print ( f \"P(A|B): { P_AB } \" ) P(B|A): 0.9 P(B): 0.11095890410958904 P(A): 0.0136986301369863 P(A|B): 0.1111111111111111 Binomial Probabilities \u00b6 Operates on PMF (Probability Mass Functions) for discrete values answer key B(K;n,p) = \\binom{n}{k} \\times p^k \\times (1 - p)^{n-k} We can calculate the total number of outcomes we care about from a total number of trials using the binomial coefficient (this field of study is called combinatorics): \\binom{n}{k} = \\frac{n!}{k! \\times (n - k)!} This allows us to calculate the probability of an event: B(K;n,p) = \\binom{n}{k} \\times P(desired \\space outcome) def fact ( x ): \"\"\" return the factorial of a number using recursion \"\"\" if x == 1 or x == 0 : return 1 else : return fact ( x - 1 ) * x def n_choose_k ( n , k ): \"\"\" Returns the number of outcomes we care about of all possible outcomes \"\"\" return fact ( n ) / ( fact ( k ) * fact ( n - k )) def binom ( n , k , p ): \"\"\" Returns the probability of an event occuring K times in a total number of n trials having a probability of p \"\"\" return n_choose_k ( n , k ) * p ** k * ( 1 - p ) ** ( n - k ) def k_or_more ( n , k , p ): \"\"\" we can solve the K or more problem recursively \"\"\" if k == n : return binom ( n , k , p ) else : return k_or_more ( n , k + 1 , p ) + binom ( n , k , p ) Q1 \u00b6 When you're searching for a new job, it's always helpful to have more than one offer on the table so you can use it in negotiations. If you have \u2155 probability of receiving a job offer when you interview, and you interview iwth seven companies in a month, what is the probability you'll have at least two competing offers by the end of that month? p = 1 / 5 n = 7 k = 2 offers1 = k_or_more ( n , k , p ) print ( offers1 ) 0.4232832000000002 Q2 \u00b6 You get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you're tired. You really don't want to go on this many interviews unless you are at least twice as likely to get a least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7? p = 1 / 10 n = 25 k = 2 offers2 = k_or_more ( n , k , p ) print ( offers2 ) print ( offers2 / offers1 ) 0.7287940935386341 1.7217647512082543 The ratio of boys to girls for babies born in Russia is 1.09:1. If there is 1 child born per birth, what proportion of Russian families with exactly 6 children will have at least 3 boys? br , gr = 1.09 , 1 p = br / ( br + gr ) n = 6 k = 3 k_or_more ( n , k , p ) 0.6957033161509107 The Beta Distribution \u00b6 Operates on PDF (Probability Density Function) for continuous values Think: Probability of probabilities Beta(\\rho; \\alpha, \\beta) = \\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)} where \\(\\rho\\) is the probability of an event. This corresponds to the different hypotheses for the possible probabilities that could be generating our observed data; \\(\\alpha\\) represents how many times we observe an event we care about such as winning a coin toss; \\(\\beta\\) represents how many times the event we care about didn't happen, such as losing a coin toss. The total number of trials is \\(\\alpha + \\beta\\) (contrast this with \\(n\\) and \\(k\\) in the binomial distribution). The beta (lowercase) distribution: \\int_0^1{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}} Putting this all together. The probability that an event occurs in a specific range: Beta(\\rho; \\alpha, \\beta) = \\int_{lower \\space bound}^{upper \\space bound}{\\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)}} Q1 \u00b6 You want to use the beta distribution to determine whether or not a coin you have is a fair coin - meaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time? from scipy.stats import beta _alpha = 4 _beta = 6 model = beta ( _alpha , _beta ) model . pdf ( 0.6 ) 1.1147673600000005 Poisson Random Variables \u00b6 What happens when we can't estimate \\(p\\) for a binomial probability? Enter the poisson experiemnt A Poisson experiment is a statistical experiment that has the following properties: The outcome of each trial is either success or failure. The average number of successes that occurs in a specified region is known. The probability that a success will occur is proportional to the size of the region. The probability that a success will occur in an extremely small region is virtually zero. The Poisson distribution is: P(k,\\lambda) = \\frac{\\lambda^k \\exp^{-\\lambda}}{k!} where \\(\\lambda\\) is the average number of successes that occur in a specified region, \\(k\\) is the actual number of successes that occur in a specified region, \\(P(K,\\lambda)\\) is the Poisson probability, which is the probability of getting exactly successes when the average number of successes is \\(\\lambda\\). from math import exp def poisson ( k , lamb ): return ( lamb ** k * exp ( - lamb )) / ( fact ( k )) Q1 \u00b6 A random variable, \\(X\\), follows Poisson distribution with mean of 2.5. Find the probability with which the random variable \\(X\\) is equal to 5.","title":"X2 Probability"},{"location":"extras/X2_Probability/#data-science-foundations-x2-common-probability-topics-in-data-science","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com This notebook is meant to summarize and provide some basic examples and problems of probability","title":"Data Science Foundations  X2: Common Probability Topics in Data Science"},{"location":"extras/X2_Probability/#measuring-uncertainty","text":"One strategy is to just count occurance of outcomes The probability of either of two mutually exclusive events occuring is the sum of their probabilities P(A \\space or \\space B) = P(A \\cup B) = P(A) + P(B) The probability of two mutually exclusive events occuring together is the product of their probabilities P(A \\space and \\space B) = P(A \\cap B) = P(A) * P(B) For non-mutually exclusive events: P (A \\cup B) = P(A) + P(B) - P(A \\cap B)","title":"Measuring Uncertainty"},{"location":"extras/X2_Probability/#q1","text":"In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that their sum will be at most 9 tot_outcomes = 6 ** 2 sum_less_than_9 = 6 + 6 + 6 + 5 + 4 + 3 sum_less_than_9 / tot_outcomes 0.8333333333333334","title":"Q1"},{"location":"extras/X2_Probability/#q2","text":"In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that the values rolled by each die will be different and the two dice have a sum of 6. # only 5 outcomes will sum to 6 # one of those has equal numbers # so there are 4/36 chances or 1/9 probability","title":"Q2"},{"location":"extras/X2_Probability/#q3","text":"There are 3 urns labeled X, Y, and Z. Urn X contains 4 red balls and 3 black balls. Urn Y contains 5 red balls and 4 black balls. Urn Z contains 4 red balls and 4 black balls. One ball is drawn from each of the 3 urns. What is the probability that, of the 3 balls drawn, 2 are red and 1 is black? # multiply and sum probabilities # RRB 4/7 * 5/9 * 4/8 # RBR 4/7 * 4/9 * 4/8 # BRR 3/7 * 5/9 * 4/8 ( 4 / 7 * 5 / 9 * 1 / 2 ) + \\ ( 4 / 7 * 4 / 9 * 1 / 2 ) + \\ ( 3 / 7 * 5 / 9 * 1 / 2 ) 0.40476190476190477","title":"Q3"},{"location":"extras/X2_Probability/#conditional-probability","text":"The flagship expression here is Bayes Rule or Bayesian Inference: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Also formulated this way P(belief|data) = \\frac{P(data|belief) * P(belief)}{P(data)} The posterior, the likelihood, and the prior The \\space posterior \\space probability = \\frac{Likelihood * prior \\space probability}{observed \\space data} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\).","title":"Conditional Probability"},{"location":"extras/X2_Probability/#q1_1","text":"Suppose a family has 2 children, one of which is a boy. What is the probability that both children are boys? # child1 child2 # a boy; a girl # a girl; a boy # a boy; a boy # a girl; a girl - not included # answer - 1/3","title":"Q1"},{"location":"extras/X2_Probability/#q2_1","text":"You draw 2 cards from a standard 52-card deck without replacing them. What is the probability that both cards are of the same suit? # suites # 13 13 13 13 # hearts ( 13 - 1 ) / 51 #12/51 # spades 4/17 # clubs 4/17 # diamonds 4/17 # (4*4)/(4*17) # multiplying out still yields # 12/51 # 1/4 * 12/51 + 1/4 * 12/51 + 1/4 * 12/51 + 1/4 * 12/51 = 12/51 0.23529411764705882 # prob(a|b) = (prob(b|a)*prob(a)) / prob(b) = intersect(a|b) / prob(b) # (1/4) * (12/51) /","title":"Q2"},{"location":"extras/X2_Probability/#q3_1","text":"If the probability of student A passing an exam is 2/7 and the probability of student B failing the exam is 3/7, then find the probability that at least 1 of the 2 students will pass the exam # P(A) = 2/7 # P(B) = 4/7 # All outcomes - sum to 1 # A pass B pass 2/7 * 4/7 # A fail B fail 5/7 * 3/7 # A pass B fail 2/7 * 3/7 # A fail B pass 5/7 * 4/7 # all outcomes ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 5 / 7 ) * ( 3 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) # outcomes we care about ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) 34 / 49 0.6938775510204082","title":"Q3"},{"location":"extras/X2_Probability/#q4","text":"Historical data shows that it has only rained 5 days per year in some desert region (assuming a 365 day year). A meteorologist predicts that it will rain today. When it actually rains, the meteorologist correctly predicts rain 90% of the time. When it doesn't rain, the meteorologist incorrectly predicts rain 10% of the time. Find the probability that it will rain today. # P(A|B) = probability that it will rain today given that the meteorologist has predicted it will rain # P(B|A) = probability that the meteoroligist will say it will rain when it rains; 90% # P(A) = probability that it will rain; 5/365 # P(B) = probability that meteoroligist will say it will rain # what is P(B) then? # P(B) = (5/365*.90) + ((365-5)/365*.1) P_B = ( 5 / 365 * .90 ) + (( 365 - 5 ) / 365 * .1 ) P_A = 5 / 365 P_BA = 0.9 P_AB = P_BA * P_A / P_B print ( f \"P(B|A): { P_BA } \" ) print ( f \"P(B): { P_B } \" ) print ( f \"P(A): { P_A } \" ) print ( f \"P(A|B): { P_AB } \" ) P(B|A): 0.9 P(B): 0.11095890410958904 P(A): 0.0136986301369863 P(A|B): 0.1111111111111111","title":"Q4"},{"location":"extras/X2_Probability/#binomial-probabilities","text":"Operates on PMF (Probability Mass Functions) for discrete values answer key B(K;n,p) = \\binom{n}{k} \\times p^k \\times (1 - p)^{n-k} We can calculate the total number of outcomes we care about from a total number of trials using the binomial coefficient (this field of study is called combinatorics): \\binom{n}{k} = \\frac{n!}{k! \\times (n - k)!} This allows us to calculate the probability of an event: B(K;n,p) = \\binom{n}{k} \\times P(desired \\space outcome) def fact ( x ): \"\"\" return the factorial of a number using recursion \"\"\" if x == 1 or x == 0 : return 1 else : return fact ( x - 1 ) * x def n_choose_k ( n , k ): \"\"\" Returns the number of outcomes we care about of all possible outcomes \"\"\" return fact ( n ) / ( fact ( k ) * fact ( n - k )) def binom ( n , k , p ): \"\"\" Returns the probability of an event occuring K times in a total number of n trials having a probability of p \"\"\" return n_choose_k ( n , k ) * p ** k * ( 1 - p ) ** ( n - k ) def k_or_more ( n , k , p ): \"\"\" we can solve the K or more problem recursively \"\"\" if k == n : return binom ( n , k , p ) else : return k_or_more ( n , k + 1 , p ) + binom ( n , k , p )","title":"Binomial Probabilities"},{"location":"extras/X2_Probability/#q1_2","text":"When you're searching for a new job, it's always helpful to have more than one offer on the table so you can use it in negotiations. If you have \u2155 probability of receiving a job offer when you interview, and you interview iwth seven companies in a month, what is the probability you'll have at least two competing offers by the end of that month? p = 1 / 5 n = 7 k = 2 offers1 = k_or_more ( n , k , p ) print ( offers1 ) 0.4232832000000002","title":"Q1"},{"location":"extras/X2_Probability/#q2_2","text":"You get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you're tired. You really don't want to go on this many interviews unless you are at least twice as likely to get a least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7? p = 1 / 10 n = 25 k = 2 offers2 = k_or_more ( n , k , p ) print ( offers2 ) print ( offers2 / offers1 ) 0.7287940935386341 1.7217647512082543 The ratio of boys to girls for babies born in Russia is 1.09:1. If there is 1 child born per birth, what proportion of Russian families with exactly 6 children will have at least 3 boys? br , gr = 1.09 , 1 p = br / ( br + gr ) n = 6 k = 3 k_or_more ( n , k , p ) 0.6957033161509107","title":"Q2"},{"location":"extras/X2_Probability/#the-beta-distribution","text":"Operates on PDF (Probability Density Function) for continuous values Think: Probability of probabilities Beta(\\rho; \\alpha, \\beta) = \\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)} where \\(\\rho\\) is the probability of an event. This corresponds to the different hypotheses for the possible probabilities that could be generating our observed data; \\(\\alpha\\) represents how many times we observe an event we care about such as winning a coin toss; \\(\\beta\\) represents how many times the event we care about didn't happen, such as losing a coin toss. The total number of trials is \\(\\alpha + \\beta\\) (contrast this with \\(n\\) and \\(k\\) in the binomial distribution). The beta (lowercase) distribution: \\int_0^1{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}} Putting this all together. The probability that an event occurs in a specific range: Beta(\\rho; \\alpha, \\beta) = \\int_{lower \\space bound}^{upper \\space bound}{\\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)}}","title":"The Beta Distribution"},{"location":"extras/X2_Probability/#q1_3","text":"You want to use the beta distribution to determine whether or not a coin you have is a fair coin - meaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time? from scipy.stats import beta _alpha = 4 _beta = 6 model = beta ( _alpha , _beta ) model . pdf ( 0.6 ) 1.1147673600000005","title":"Q1"},{"location":"extras/X2_Probability/#poisson-random-variables","text":"What happens when we can't estimate \\(p\\) for a binomial probability? Enter the poisson experiemnt A Poisson experiment is a statistical experiment that has the following properties: The outcome of each trial is either success or failure. The average number of successes that occurs in a specified region is known. The probability that a success will occur is proportional to the size of the region. The probability that a success will occur in an extremely small region is virtually zero. The Poisson distribution is: P(k,\\lambda) = \\frac{\\lambda^k \\exp^{-\\lambda}}{k!} where \\(\\lambda\\) is the average number of successes that occur in a specified region, \\(k\\) is the actual number of successes that occur in a specified region, \\(P(K,\\lambda)\\) is the Poisson probability, which is the probability of getting exactly successes when the average number of successes is \\(\\lambda\\). from math import exp def poisson ( k , lamb ): return ( lamb ** k * exp ( - lamb )) / ( fact ( k ))","title":"Poisson Random Variables"},{"location":"extras/X2_Probability/#q1_4","text":"A random variable, \\(X\\), follows Poisson distribution with mean of 2.5. Find the probability with which the random variable \\(X\\) is equal to 5.","title":"Q1"},{"location":"extras/X3_AB_Testing/","text":"Data Science Foundations X3: The Nitty Gritty of AB Tests \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com This notebook is meant to summarize and provide some basic examples of AB testing import numpy as np from scipy.stats import norm import matplotlib.pyplot as plt def compute_z ( alpha = 0.05 , tail = \"two\" ): if tail == \"two\" : p = 1 - ( alpha / 2 ) else : p = 1 - alpha z = norm . ppf ( p ) # the critical value return z def compute_power ( h_0 , h_1 , se , alpha = 0.05 , tail = \"two\" , verbiose = 1 ): z = compute_z ( alpha , tail ) # upper and lower limits of the region of acceptance # M (parameter value in the null hypothesis) + SE (standard error) * CV (critical value) lower = h_0 - z * se upper = h_0 + z * se # the areas show the probability associated with these regions lower_a = norm . cdf ( lower , h_1 , se ) upper_a = 1 - norm . cdf ( upper , h_1 , se ) if tail == \"two\" : acceptance_region = [ lower , upper ] power = lower_a + upper_a if verbiose == 1 : print ( f \"acceptance region { lower : .3f } , { upper : .2f } \" ) print ( power ) if tail == \"left\" : acceptance_region = lower power = lower_a if verbiose == 1 : print ( f \"acceptance region > { lower : .3f } \" ) elif tail == \"right\" : acceptance_region = upper power = upper_a if verbiose == 1 : print ( f \"acceptance region < { upper : .3f } \" ) beta = 1 - power return power , beta , acceptance_region BEAN (No Ender's Game) \u00b6 BEAN is a useful acronym to remember the four variables that affect the statistical significance of an AB test. B - beta E - effect size A - alpha N - sample size Beta, or 1- power, indicates the false negative rate, i.e. the probability that the null hypothesis is accepted when the alternative hypothesis is true. The effect size is the difference between the two hypotheses, i.e. if the control conversion rate is 50% and the expected, new, conversion rate is 52%, then the effect size is 2%. Alpha indicates the false positive rate, i.e. the probability that the null hypothesis is rejected when in fact it is true. Finally, N, is the total number of samples used to conduct the test. We can look at how BEA effect N: Variable relationship with N E inverse - as the spread between the hypotheses increases, we will require less data to capture the same significance A inverse - as the tollerance for false positives decreases, we will require more data to reject the null hypothesis B inverse - as the tollerance for false negatives decreases, we will require more data to accept the null hypothesis Note: although \"effect size\" is used in the BEAN acronym, in actuality the localization of the effect size is important as well. For example it is easier to detect a lift from 2% to 4% than it is to detect a lift from 50% to 52%. N vs A and E \u00b6 we can explore the relationship between alpha, N, and effect size using the formula for N, below For example, say we are checking whether a variant of a newsletter email lifts our click through rate from 50% to 52%. We set our alpha (significance level) to 0.05 and our tail set to \"right\" because we are sure that the the variant will create a positive lift (for reasons). # https://medium.com/swlh/the-ultimate-guide-to-a-b-testing-part-1-experiment-design-8315a2470c63 tail = \"right\" alpha = 0.05 # significance level h_0 = 0.5 h_1 = 0.52 def compute_n ( h_0 , h_1 , alpha , tail ): z = compute_z ( alpha , tail ) if tail == \"two\" : w = ( ( h_1 - h_0 ) * 2 ) ** 2 else : w = ( h_1 - h_0 ) ** 2 # squared effect size return ( z ** 2 * h_0 * ( 1 - h_0 )) / w n = compute_n ( h_0 , h_1 , alpha , tail ) print ( f \"required samples: { n : .0f } \" ) required samples: 1691 holding our null hypothesis at 50%, our alternative hypothesis at 52%, and ignoring our beta or our power level for now, we can track the relationship between alpha and n. alphas = np . linspace ( 0.01 , 0.05 , 100 ) ns = [ compute_n ( h_0 , h_1 , alpha , tail ) for alpha in alphas ] fig , ax = plt . subplots () ax . plot ( alphas , ns ) ax . set_xlabel ( \"alpha\" ) ax . set_ylabel ( \"sample size\" ) ax . set_title ( \"required sample size for AB test\" ) Text(0.5, 1.0, 'required sample size for AB test') holding alpha and our null hypothesis constant, we can track the required sample size with respect to the desired conversion rate we'd like to detect. The larger effect we wish to see, the fewer samples we require. At first this may seem counterintuitive. Remember, however, that holding standard deviation constant, it is easier to discern two piles the further apart they are. alpha = 0.05 h_0 = 0.5 h_1s = np . linspace ( 0.51 , 0.6 , 100 ) ns = [ compute_n ( h_0 , h_1 , alpha , tail ) for h_1 in h_1s ] fig , ax = plt . subplots () ax . plot ( h_1s , ns ) ax . set_xlabel ( \"h_1\" ) ax . set_ylabel ( \"sample size\" ) ax . set_title ( \"required sample size for AB test\" ) Text(0.5, 1.0, 'required sample size for AB test') N vs B \u00b6 in AB testing we often would like to consider the power level of our test. That is, the ability with which we can reduce the likelihood of accepting the null hypothesis when it is false. A common power level is 80%. This means that 80% of the time the null hypothesis is false we will safely reject it. tail = \"right\" alpha = 0.05 # significance level h_0 = 0.5 h_1 = 0.52 n = 3200 ns = np . arange ( 1000 , 10000 ) powers = [] for n in ns : # note, this se taken from https://towardsdatascience.com/the-power-of-a-b-testing-3387c04a14e3 se = np . sqrt ( h_0 * ( 1 - h_0 ) / n ) # standard error power , beta , ar = compute_power ( h_0 , h_1 , se , alpha , tail , verbiose = 0 ) powers . append ( power ) fig , ax = plt . subplots () ax . plot ( powers , ns ) ax . set_xlabel ( \"power level\" ) ax . set_ylabel ( \"sample size\" ) ax . set_title ( \"required sample size for AB test\" ) Text(0.5, 1.0, 'required sample size for AB test') Visualizing the Hypotheses \u00b6 It can be helpful to visualize the distributions of the two hypotheses we are testing. The Tradeoff between alpha and power level \u00b6 Type I error: False positive, is the area under the null hypothesis to the right of the acceptance boundary (for right tailed tests) Type II error: False negative, is the area under the alternative hypothesis to the left of the acceptance boundary (for right tailed tests) As we shall see, there is a tradeoff between alpha and power level. tail = \"right\" alpha = 0.05 # significance level h_0 = 0.5 h_1 = 0.52 n = 3200 se = np . sqrt ( h_0 * ( 1 - h_0 ) / n ) # standard error print ( f \"est. std error: { se : .3f } \" ) power , beta , acceptance_region = compute_power ( h_0 , h_1 , se , alpha , tail ) print ( f \"power: { power : .2f } , beta: { beta : .2f } \" ) fig , ( ax , ax_ ) = plt . subplots ( 2 , 1 , figsize = ( 10 , 7.5 )) x = np . linspace ( norm . ppf ( 0.01 ), norm . ppf ( 0.99 ), 5000 ) y1 = norm . pdf ( x , loc = h_0 , scale = se ) y2 = norm . pdf ( x , loc = h_1 , scale = se ) y3 = norm . cdf ( x , loc = h_0 , scale = se ) ax . plot ( x , y1 , c = 'tab:blue' , lw = 2 , alpha = 0.6 , label = '$H_0$' ) ax . plot ( x , y2 , c = 'tab:orange' , lw = 2 , alpha = 0.6 , label = '$H_1$' ) ax . axvline ( acceptance_region , ls = '--' , label = 'Acceptance boundary' ) ax . fill_between ( x , y2 , where = x < acceptance_region , facecolor = 'tab:orange' , alpha = 0.3 , label = \"Type II error\" ) ax . fill_between ( x , y1 , where = x > acceptance_region , facecolor = 'tab:blue' , alpha = 0.3 , label = \"Type I error\" ) ax . set_xlim ( x [ min ( np . argwhere ( y1 > 0.001 ))[ 0 ]], x [ max ( np . argwhere ( y2 > 0.001 ))[ 0 ]]) alpha = 0.01 # significance level h_0 = 0.5 h_1 = 0.52 n = 3200 se = np . sqrt ( h_0 * ( 1 - h_0 ) / n ) # standard error print ( f \"est. std error: { se : .3f } \" ) power , beta , acceptance_region = compute_power ( h_0 , h_1 , se , alpha , tail ) print ( f \"power: { power : .2f } , beta: { beta : .2f } \" ) x = np . linspace ( norm . ppf ( 0.01 ), norm . ppf ( 0.99 ), 5000 ) y1 = norm . pdf ( x , loc = h_0 , scale = se ) y2 = norm . pdf ( x , loc = h_1 , scale = se ) y3 = norm . cdf ( x , loc = h_0 , scale = se ) ax_ . plot ( x , y1 , c = 'tab:blue' , lw = 2 , alpha = 0.6 , label = '$H_0$' ) ax_ . plot ( x , y2 , c = 'tab:orange' , lw = 2 , alpha = 0.6 , label = '$H_1$' ) ax_ . axvline ( acceptance_region , ls = '--' , label = 'Acceptance boundary' ) ax_ . fill_between ( x , y2 , where = x < acceptance_region , facecolor = 'tab:orange' , alpha = 0.3 , label = \"Type II error\" ) ax_ . fill_between ( x , y1 , where = x > acceptance_region , facecolor = 'tab:blue' , alpha = 0.3 , label = \"Type I error\" ) ax_ . set_xlim ( x [ min ( np . argwhere ( y1 > 0.001 ))[ 0 ]], x [ max ( np . argwhere ( y2 > 0.001 ))[ 0 ]]) ax_ . legend () est. std error: 0.009 acceptance region < 0.515 power: 0.73, beta: 0.27 est. std error: 0.009 acceptance region < 0.521 power: 0.47, beta: 0.53 <matplotlib.legend.Legend at 0x7fcc782e5f40> Greater lift, fewer tests, for the same power \u00b6 The greater the amount of lift we are trying to detect, the fewer sample sizes we will need tail = \"right\" alpha = 0.05 # significance level h_0 = 0.5 h_1 = 0.52 n = 3200 se = np . sqrt ( h_0 * ( 1 - h_0 ) / n ) # standard error print ( f \"est. std error: { se : .3f } \" ) power , beta , acceptance_region = compute_power ( h_0 , h_1 , se , alpha , tail ) print ( f \"power: { power : .2f } , beta: { beta : .2f } \" ) fig , ( ax , ax_ ) = plt . subplots ( 2 , 1 , figsize = ( 10 , 7.5 )) x = np . linspace ( norm . ppf ( 0.01 ), norm . ppf ( 0.99 ), 5000 ) y1 = norm . pdf ( x , loc = h_0 , scale = se ) y2 = norm . pdf ( x , loc = h_1 , scale = se ) y3 = norm . cdf ( x , loc = h_0 , scale = se ) ax . plot ( x , y1 , c = 'tab:blue' , lw = 2 , alpha = 0.6 , label = '$H_0$' ) ax . plot ( x , y2 , c = 'tab:orange' , lw = 2 , alpha = 0.6 , label = '$H_1$' ) ax . axvline ( acceptance_region , ls = '--' , label = 'Acceptance boundary' ) ax . fill_between ( x , y2 , where = x < acceptance_region , facecolor = 'tab:orange' , alpha = 0.3 , label = \"Type II error\" ) ax . fill_between ( x , y1 , where = x > acceptance_region , facecolor = 'tab:blue' , alpha = 0.3 , label = \"Type I error\" ) ax . set_xlim ( x [ min ( np . argwhere ( y1 > 0.001 ))[ 0 ]], x [ max ( np . argwhere ( y2 > 0.001 ))[ 0 ]]) alpha = 0.05 # significance level h_0 = 0.5 h_1 = 0.54 n = 900 se = np . sqrt ( h_0 * ( 1 - h_0 ) / n ) # standard error print ( f \"est. std error: { se : .3f } \" ) power , beta , acceptance_region = compute_power ( h_0 , h_1 , se , alpha , tail ) print ( f \"power: { power : .2f } , beta: { beta : .2f } \" ) x = np . linspace ( norm . ppf ( 0.01 ), norm . ppf ( 0.99 ), 5000 ) y1 = norm . pdf ( x , loc = h_0 , scale = se ) y2 = norm . pdf ( x , loc = h_1 , scale = se ) y3 = norm . cdf ( x , loc = h_0 , scale = se ) ax_ . plot ( x , y1 , c = 'tab:blue' , lw = 2 , alpha = 0.6 , label = '$H_0$' ) ax_ . plot ( x , y2 , c = 'tab:orange' , lw = 2 , alpha = 0.6 , label = '$H_1$' ) ax_ . axvline ( acceptance_region , ls = '--' , label = 'Acceptance boundary' ) ax_ . fill_between ( x , y2 , where = x < acceptance_region , facecolor = 'tab:orange' , alpha = 0.3 , label = \"Type II error\" ) ax_ . fill_between ( x , y1 , where = x > acceptance_region , facecolor = 'tab:blue' , alpha = 0.3 , label = \"Type I error\" ) ax_ . set_xlim ( x [ min ( np . argwhere ( y1 > 0.001 ))[ 0 ]], x [ max ( np . argwhere ( y2 > 0.001 ))[ 0 ]]) ax_ . legend () est. std error: 0.009 acceptance region < 0.515 power: 0.73, beta: 0.27 est. std error: 0.017 acceptance region < 0.527 power: 0.77, beta: 0.23 <matplotlib.legend.Legend at 0x7fcc77f1d730> Closing Notes \u00b6 In the power law examples, we were estimating the standard error of the tests. In real AB test cases, we can use historical data to more accurately estiamte the standard error around the performance metric. In addition, we can update the estimations of N as the test is running, to more accurately determine when our test should conclude!","title":"AB Tests"},{"location":"extras/X3_AB_Testing/#data-science-foundations-x3-the-nitty-gritty-of-ab-tests","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com This notebook is meant to summarize and provide some basic examples of AB testing import numpy as np from scipy.stats import norm import matplotlib.pyplot as plt def compute_z ( alpha = 0.05 , tail = \"two\" ): if tail == \"two\" : p = 1 - ( alpha / 2 ) else : p = 1 - alpha z = norm . ppf ( p ) # the critical value return z def compute_power ( h_0 , h_1 , se , alpha = 0.05 , tail = \"two\" , verbiose = 1 ): z = compute_z ( alpha , tail ) # upper and lower limits of the region of acceptance # M (parameter value in the null hypothesis) + SE (standard error) * CV (critical value) lower = h_0 - z * se upper = h_0 + z * se # the areas show the probability associated with these regions lower_a = norm . cdf ( lower , h_1 , se ) upper_a = 1 - norm . cdf ( upper , h_1 , se ) if tail == \"two\" : acceptance_region = [ lower , upper ] power = lower_a + upper_a if verbiose == 1 : print ( f \"acceptance region { lower : .3f } , { upper : .2f } \" ) print ( power ) if tail == \"left\" : acceptance_region = lower power = lower_a if verbiose == 1 : print ( f \"acceptance region > { lower : .3f } \" ) elif tail == \"right\" : acceptance_region = upper power = upper_a if verbiose == 1 : print ( f \"acceptance region < { upper : .3f } \" ) beta = 1 - power return power , beta , acceptance_region","title":"Data Science Foundations  X3: The Nitty Gritty of AB Tests"},{"location":"extras/X3_AB_Testing/#bean-no-enders-game","text":"BEAN is a useful acronym to remember the four variables that affect the statistical significance of an AB test. B - beta E - effect size A - alpha N - sample size Beta, or 1- power, indicates the false negative rate, i.e. the probability that the null hypothesis is accepted when the alternative hypothesis is true. The effect size is the difference between the two hypotheses, i.e. if the control conversion rate is 50% and the expected, new, conversion rate is 52%, then the effect size is 2%. Alpha indicates the false positive rate, i.e. the probability that the null hypothesis is rejected when in fact it is true. Finally, N, is the total number of samples used to conduct the test. We can look at how BEA effect N: Variable relationship with N E inverse - as the spread between the hypotheses increases, we will require less data to capture the same significance A inverse - as the tollerance for false positives decreases, we will require more data to reject the null hypothesis B inverse - as the tollerance for false negatives decreases, we will require more data to accept the null hypothesis Note: although \"effect size\" is used in the BEAN acronym, in actuality the localization of the effect size is important as well. For example it is easier to detect a lift from 2% to 4% than it is to detect a lift from 50% to 52%.","title":"BEAN (No Ender's Game)"},{"location":"extras/X3_AB_Testing/#n-vs-a-and-e","text":"we can explore the relationship between alpha, N, and effect size using the formula for N, below For example, say we are checking whether a variant of a newsletter email lifts our click through rate from 50% to 52%. We set our alpha (significance level) to 0.05 and our tail set to \"right\" because we are sure that the the variant will create a positive lift (for reasons). # https://medium.com/swlh/the-ultimate-guide-to-a-b-testing-part-1-experiment-design-8315a2470c63 tail = \"right\" alpha = 0.05 # significance level h_0 = 0.5 h_1 = 0.52 def compute_n ( h_0 , h_1 , alpha , tail ): z = compute_z ( alpha , tail ) if tail == \"two\" : w = ( ( h_1 - h_0 ) * 2 ) ** 2 else : w = ( h_1 - h_0 ) ** 2 # squared effect size return ( z ** 2 * h_0 * ( 1 - h_0 )) / w n = compute_n ( h_0 , h_1 , alpha , tail ) print ( f \"required samples: { n : .0f } \" ) required samples: 1691 holding our null hypothesis at 50%, our alternative hypothesis at 52%, and ignoring our beta or our power level for now, we can track the relationship between alpha and n. alphas = np . linspace ( 0.01 , 0.05 , 100 ) ns = [ compute_n ( h_0 , h_1 , alpha , tail ) for alpha in alphas ] fig , ax = plt . subplots () ax . plot ( alphas , ns ) ax . set_xlabel ( \"alpha\" ) ax . set_ylabel ( \"sample size\" ) ax . set_title ( \"required sample size for AB test\" ) Text(0.5, 1.0, 'required sample size for AB test') holding alpha and our null hypothesis constant, we can track the required sample size with respect to the desired conversion rate we'd like to detect. The larger effect we wish to see, the fewer samples we require. At first this may seem counterintuitive. Remember, however, that holding standard deviation constant, it is easier to discern two piles the further apart they are. alpha = 0.05 h_0 = 0.5 h_1s = np . linspace ( 0.51 , 0.6 , 100 ) ns = [ compute_n ( h_0 , h_1 , alpha , tail ) for h_1 in h_1s ] fig , ax = plt . subplots () ax . plot ( h_1s , ns ) ax . set_xlabel ( \"h_1\" ) ax . set_ylabel ( \"sample size\" ) ax . set_title ( \"required sample size for AB test\" ) Text(0.5, 1.0, 'required sample size for AB test')","title":"N vs A and E"},{"location":"extras/X3_AB_Testing/#n-vs-b","text":"in AB testing we often would like to consider the power level of our test. That is, the ability with which we can reduce the likelihood of accepting the null hypothesis when it is false. A common power level is 80%. This means that 80% of the time the null hypothesis is false we will safely reject it. tail = \"right\" alpha = 0.05 # significance level h_0 = 0.5 h_1 = 0.52 n = 3200 ns = np . arange ( 1000 , 10000 ) powers = [] for n in ns : # note, this se taken from https://towardsdatascience.com/the-power-of-a-b-testing-3387c04a14e3 se = np . sqrt ( h_0 * ( 1 - h_0 ) / n ) # standard error power , beta , ar = compute_power ( h_0 , h_1 , se , alpha , tail , verbiose = 0 ) powers . append ( power ) fig , ax = plt . subplots () ax . plot ( powers , ns ) ax . set_xlabel ( \"power level\" ) ax . set_ylabel ( \"sample size\" ) ax . set_title ( \"required sample size for AB test\" ) Text(0.5, 1.0, 'required sample size for AB test')","title":"N vs B"},{"location":"extras/X3_AB_Testing/#visualizing-the-hypotheses","text":"It can be helpful to visualize the distributions of the two hypotheses we are testing.","title":"Visualizing the Hypotheses"},{"location":"extras/X3_AB_Testing/#the-tradeoff-between-alpha-and-power-level","text":"Type I error: False positive, is the area under the null hypothesis to the right of the acceptance boundary (for right tailed tests) Type II error: False negative, is the area under the alternative hypothesis to the left of the acceptance boundary (for right tailed tests) As we shall see, there is a tradeoff between alpha and power level. tail = \"right\" alpha = 0.05 # significance level h_0 = 0.5 h_1 = 0.52 n = 3200 se = np . sqrt ( h_0 * ( 1 - h_0 ) / n ) # standard error print ( f \"est. std error: { se : .3f } \" ) power , beta , acceptance_region = compute_power ( h_0 , h_1 , se , alpha , tail ) print ( f \"power: { power : .2f } , beta: { beta : .2f } \" ) fig , ( ax , ax_ ) = plt . subplots ( 2 , 1 , figsize = ( 10 , 7.5 )) x = np . linspace ( norm . ppf ( 0.01 ), norm . ppf ( 0.99 ), 5000 ) y1 = norm . pdf ( x , loc = h_0 , scale = se ) y2 = norm . pdf ( x , loc = h_1 , scale = se ) y3 = norm . cdf ( x , loc = h_0 , scale = se ) ax . plot ( x , y1 , c = 'tab:blue' , lw = 2 , alpha = 0.6 , label = '$H_0$' ) ax . plot ( x , y2 , c = 'tab:orange' , lw = 2 , alpha = 0.6 , label = '$H_1$' ) ax . axvline ( acceptance_region , ls = '--' , label = 'Acceptance boundary' ) ax . fill_between ( x , y2 , where = x < acceptance_region , facecolor = 'tab:orange' , alpha = 0.3 , label = \"Type II error\" ) ax . fill_between ( x , y1 , where = x > acceptance_region , facecolor = 'tab:blue' , alpha = 0.3 , label = \"Type I error\" ) ax . set_xlim ( x [ min ( np . argwhere ( y1 > 0.001 ))[ 0 ]], x [ max ( np . argwhere ( y2 > 0.001 ))[ 0 ]]) alpha = 0.01 # significance level h_0 = 0.5 h_1 = 0.52 n = 3200 se = np . sqrt ( h_0 * ( 1 - h_0 ) / n ) # standard error print ( f \"est. std error: { se : .3f } \" ) power , beta , acceptance_region = compute_power ( h_0 , h_1 , se , alpha , tail ) print ( f \"power: { power : .2f } , beta: { beta : .2f } \" ) x = np . linspace ( norm . ppf ( 0.01 ), norm . ppf ( 0.99 ), 5000 ) y1 = norm . pdf ( x , loc = h_0 , scale = se ) y2 = norm . pdf ( x , loc = h_1 , scale = se ) y3 = norm . cdf ( x , loc = h_0 , scale = se ) ax_ . plot ( x , y1 , c = 'tab:blue' , lw = 2 , alpha = 0.6 , label = '$H_0$' ) ax_ . plot ( x , y2 , c = 'tab:orange' , lw = 2 , alpha = 0.6 , label = '$H_1$' ) ax_ . axvline ( acceptance_region , ls = '--' , label = 'Acceptance boundary' ) ax_ . fill_between ( x , y2 , where = x < acceptance_region , facecolor = 'tab:orange' , alpha = 0.3 , label = \"Type II error\" ) ax_ . fill_between ( x , y1 , where = x > acceptance_region , facecolor = 'tab:blue' , alpha = 0.3 , label = \"Type I error\" ) ax_ . set_xlim ( x [ min ( np . argwhere ( y1 > 0.001 ))[ 0 ]], x [ max ( np . argwhere ( y2 > 0.001 ))[ 0 ]]) ax_ . legend () est. std error: 0.009 acceptance region < 0.515 power: 0.73, beta: 0.27 est. std error: 0.009 acceptance region < 0.521 power: 0.47, beta: 0.53 <matplotlib.legend.Legend at 0x7fcc782e5f40>","title":"The Tradeoff between alpha and power level"},{"location":"extras/X3_AB_Testing/#greater-lift-fewer-tests-for-the-same-power","text":"The greater the amount of lift we are trying to detect, the fewer sample sizes we will need tail = \"right\" alpha = 0.05 # significance level h_0 = 0.5 h_1 = 0.52 n = 3200 se = np . sqrt ( h_0 * ( 1 - h_0 ) / n ) # standard error print ( f \"est. std error: { se : .3f } \" ) power , beta , acceptance_region = compute_power ( h_0 , h_1 , se , alpha , tail ) print ( f \"power: { power : .2f } , beta: { beta : .2f } \" ) fig , ( ax , ax_ ) = plt . subplots ( 2 , 1 , figsize = ( 10 , 7.5 )) x = np . linspace ( norm . ppf ( 0.01 ), norm . ppf ( 0.99 ), 5000 ) y1 = norm . pdf ( x , loc = h_0 , scale = se ) y2 = norm . pdf ( x , loc = h_1 , scale = se ) y3 = norm . cdf ( x , loc = h_0 , scale = se ) ax . plot ( x , y1 , c = 'tab:blue' , lw = 2 , alpha = 0.6 , label = '$H_0$' ) ax . plot ( x , y2 , c = 'tab:orange' , lw = 2 , alpha = 0.6 , label = '$H_1$' ) ax . axvline ( acceptance_region , ls = '--' , label = 'Acceptance boundary' ) ax . fill_between ( x , y2 , where = x < acceptance_region , facecolor = 'tab:orange' , alpha = 0.3 , label = \"Type II error\" ) ax . fill_between ( x , y1 , where = x > acceptance_region , facecolor = 'tab:blue' , alpha = 0.3 , label = \"Type I error\" ) ax . set_xlim ( x [ min ( np . argwhere ( y1 > 0.001 ))[ 0 ]], x [ max ( np . argwhere ( y2 > 0.001 ))[ 0 ]]) alpha = 0.05 # significance level h_0 = 0.5 h_1 = 0.54 n = 900 se = np . sqrt ( h_0 * ( 1 - h_0 ) / n ) # standard error print ( f \"est. std error: { se : .3f } \" ) power , beta , acceptance_region = compute_power ( h_0 , h_1 , se , alpha , tail ) print ( f \"power: { power : .2f } , beta: { beta : .2f } \" ) x = np . linspace ( norm . ppf ( 0.01 ), norm . ppf ( 0.99 ), 5000 ) y1 = norm . pdf ( x , loc = h_0 , scale = se ) y2 = norm . pdf ( x , loc = h_1 , scale = se ) y3 = norm . cdf ( x , loc = h_0 , scale = se ) ax_ . plot ( x , y1 , c = 'tab:blue' , lw = 2 , alpha = 0.6 , label = '$H_0$' ) ax_ . plot ( x , y2 , c = 'tab:orange' , lw = 2 , alpha = 0.6 , label = '$H_1$' ) ax_ . axvline ( acceptance_region , ls = '--' , label = 'Acceptance boundary' ) ax_ . fill_between ( x , y2 , where = x < acceptance_region , facecolor = 'tab:orange' , alpha = 0.3 , label = \"Type II error\" ) ax_ . fill_between ( x , y1 , where = x > acceptance_region , facecolor = 'tab:blue' , alpha = 0.3 , label = \"Type I error\" ) ax_ . set_xlim ( x [ min ( np . argwhere ( y1 > 0.001 ))[ 0 ]], x [ max ( np . argwhere ( y2 > 0.001 ))[ 0 ]]) ax_ . legend () est. std error: 0.009 acceptance region < 0.515 power: 0.73, beta: 0.27 est. std error: 0.017 acceptance region < 0.527 power: 0.77, beta: 0.23 <matplotlib.legend.Legend at 0x7fcc77f1d730>","title":"Greater lift, fewer tests, for the same power"},{"location":"extras/X3_AB_Testing/#closing-notes","text":"In the power law examples, we were estimating the standard error of the tests. In real AB test cases, we can use historical data to more accurately estiamte the standard error around the performance metric. In addition, we can update the estimations of N as the test is running, to more accurately determine when our test should conclude!","title":"Closing Notes"},{"location":"extras/X3_Spotify-Copy1/","text":"What makes a playlist successful? \u00b6 Analysis Simple metric (dependent variable) mau mau_previous_month mau_both_months monthly_stream30s stream30s Design metric (dependent variable) 30s listens/tot listens (listen conversions) Users both months/users prev month (user conversions) Best small time performers (less than X total monthly listens + high conversion) Best new user playlist (owner has only 1 popular playlist) Define \"top\" Top 10% mau_previous_month: 9.0 mau_both_months: 2.0 mau: 9.0 monthly_stream30s: 432.0 stream30s: 17.0 Top 1% mau_previous_month: 130.0 mau_both_months: 19.0 mau: 143.0 monthly_stream30s: 2843.0 stream30s: 113.0 Independent variables moods and genres (categorical) number of tracks, albums, artists, and local tracks (continuous) The analysis will consist of: understand the distribution characteristics of the dependent and independent variables quantify the dependency of the dependent/independent variables for each of the simple and design metrics chi-square test bootstrap/t-test Key Conclusions for the simple metrics, key genres and moods were Romantic, Latin, Children's, Lively, Traditional, and Jazz . Playlists that included these genres/moods had a positive multiplier effect (usually in the vicinicty of 2x more likely) on the key simple metric (i.e. playlists with latin as a primary genre were 2.5x more likely to be in the top 10% of streams longer than 30 seconds) skippers - is this associated with the current playlist Column Name Description playlist_uri The key, Spotify uri of the playlist owner Playlist owner, Spotify username streams Number of streams from the playlist today stream30s Number of streams over 30 seconds from playlist today dau Number of Daily Active Users, i.e. users with a stream over 30 seconds from playlist today wau Number of Weekly Active Users, i.e. users with a stream over 30 seconds from playlist in past week mau Number of Monthly Active Users, i.e. users with a stream over 30 seconds from playlist in the past month mau_previous_months Number of Monthly Active users in the month prior to this one mau_both_months Number of users that were active on the playlist both this and the previous month users Number of users streaming (all streams) from this playlist this month skippers Number of users who skipped more than 90 percent of their streams today owner_country Country of the playlist owner n_tracks Number of tracks in playlist n_local_tracks Change in number of tracks on playlist since yesterday n_artists Number of unique artists in playlist n_albums Number of unique albums in playlist monthly_stream30s Number of streams over 30 seconds this month monthly_owner_stream30s Number of streams over 30 seconds by playlist owner this month tokens List of playlist title tokens, stopwords and punctuation removed genre_1 No. 1 Genre by weight of playlist tracks, from Gracenote metadata genre_2 No. 2 Genre by weight of playlist tracks, from Gracenote metadata genre_3 No. 3 Genre by weight of playlist tracks, from Gracenote metadata mood_1 No. 1 Mood by weight of playlist tracks, from Gracenote metadata mood_2 No. 2 Mood by weight of playlist tracks, from Gracenote metadata mood_3 No. 3 Mood by weight of playlist tracks, from Gracenote metadata Imports \u00b6 # basic packages import pandas as pd pd . set_option ( 'display.max_columns' , 500 ) import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.discrete.discrete_model import Logit from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.utils.class_weight import compute_class_weight # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score df = pd . read_csv ( \"../../data/playlist_summary_external-4.txt\" , delimiter = ' \\t ' ) Putting it All Together \u00b6 sub_targets = [ 'mau_previous_month' , 'mau_both_months' , 'monthly_stream30s' , 'stream30s' ] des_features = [ 'mood_1' , 'mood_2' , 'mood_3' , 'genre_1' , 'genre_2' , 'genre_3' ] Models (Multi-Feature Analysis) \u00b6 Deciles - Random Forest \u00b6 sub_targets target = sub_targets [ - 2 ] y = df [ target ] . values labels = y . copy () names = [] for idx , quant in zip ( range ( 11 ), np . linspace ( 0 , 1 , num = 11 )): if idx == 0 : prev = quant continue if idx == 1 : labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels names X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 42 , train_size = 0.8 ) model = RandomForestClassifier () model . fit ( X_train , y_train ) y_hat_test = model . predict ( X_test ) print ( f \"Train Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) # grab feature importances imp = model . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in model . estimators_ ], axis = 0 ) # build feature names feature_names = con_features + list ( enc . get_feature_names_out ()) # create new dataframe feat = pd . DataFrame ([ feature_names , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head ( 20 ) Quartiles - Random Forest \u00b6 ### Create Categories y = df [ target ] . values labels = y . copy () names = [] lim = 5 for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx == 0 : prev = quant continue if idx == 1 : labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels ### Create Training Data X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 42 , train_size = 0.8 ) ### Train Model model = RandomForestClassifier () model . fit ( X_train , y_train ) ### Asses Performance y_hat_test = model . predict ( X_test ) y_hat_train = model . predict ( X_train ) print ( f \"Train Acc: { accuracy_score ( y_train , y_hat_train ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax ) Binary, 90 th Percentile, Random Forest \u00b6 ### Create Categories y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 5 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels ### Create Training Data X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test , weight_train , weight_test = train_test_split ( X , y , weights , random_state = 42 , train_size = 0.8 ) ### Strateification Code # strat_y0_idx = np.array(random.sample(list(np.argwhere(y_train==3).reshape(-1)), np.unique(y_train, return_counts=True)[1][1])) # strat_y1_idx = np.argwhere(y_train==4).reshape(-1) # strat_idx = np.hstack((strat_y0_idx, strat_y1_idx)) # X_train = X_train[strat_idx] # y_train = y_train[strat_idx] ### Train Model model = RandomForestClassifier () model . fit ( X_train , y_train ) ### Assess Performance y_hat_test = model . predict ( X_test ) y_hat_train = model . predict ( X_train ) print ( f \"Train Acc: { accuracy_score ( y_train , y_hat_train ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax ) Forward Selection Model \u00b6 ### y print ( target ) y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = 0 names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = 1 weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels #### X X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( np . ones (( X_con . shape [ 0 ], 1 )), X_con , X_cat )) feature_names = [ 'intercept' ] + con_features + list ( enc . get_feature_names_out ()) data = pd . DataFrame ( X , columns = feature_names ) print ( names ) def add_feature ( feature_names , basemodel , data , y , r2max = 0 , model = 'linear' , disp = 0 ): feature_max = None bestsum = None newmodel = None for feature in feature_names : basemodel [ feature ] = data [ feature ] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () score = float ( str ( pd . DataFrame ( summ . tables [ 0 ]) . loc [ 3 , 3 ])) if ( score > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = score feature_max = feature bestsum = est2 . summary () newmodel = basemodel . copy () if disp == 1 : print ( f \"new r2max, { feature_max } , { r2max } \" ) basemodel . drop ( labels = feature , axis = 1 , inplace = True ) return r2max , feature_max , bestsum , newmodel candidates = feature_names . copy () basemodel = pd . DataFrame () r2max = 0 with open ( \"canidates.txt\" , \"w+\" ) as f : file_data = f . read () for i in candidates : f . write ( f \" { i } \\n \" ) basemodel . to_csv ( \"basemodel.csv\" ) with open ( \"canidates.txt\" , \"r\" ) as f : # file_data = f.read() new = [] for line in f : current_place = line [: - 1 ] new . append ( current_place ) new = pd . read_csv ( \"basemodel.csv\" , index_col = 0 ) with open ( \"fwd_selection_results.txt\" , \"r+\" ) as f : for line in f : pass lastline = line [: - 1 ] stuff = lastline . split ( \", \" ) new = float ( stuff [ - 1 ]) new while True : newr2max , feature_max , bestsum , newmodel = add_feature ( feature_names = candidates , basemodel = basemodel , data = data , y = y , r2max = r2max ) if newr2max > r2max : r2max = newr2max print ( f \"new r2max, { feature_max } , { r2max } \" ) with open ( \"fwd_selection_results.txt\" , \"a+\" ) as f : file_data = f . read () f . write ( f \"new r2max, { feature_max } , { r2max } \\n \" ) candidates . remove ( feature_max ) with open ( \"canidates.txt\" , \"w+\" ) as f : file_data = f . read () for i in candidates : f . write ( f \" { i } \\n \" ) basemodel = newmodel basemodel . to_csv ( \"basemodel.csv\" ) continue else : break X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns res_df Binary, 99 th Percentile \u00b6 ### Create Categories y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels ### Create Training Data X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test , weight_train , weight_test = train_test_split ( X , y , weights , random_state = 42 , train_size = 0.8 ) ### Train Model model = RandomForestClassifier () model . fit ( X_train , y_train , weight_train ) ### Asses Performance y_hat_test = model . predict ( X_test ) y_hat_train = model . predict ( X_train ) print ( f \"Train Acc: { accuracy_score ( y_train , y_hat_train ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax ) Other Metrics \u00b6 30s listens/tot listens (listen conversions) also like a bounce rate Users both months/users prev month (user conversions) combine with mau > mau_previous_month Best small time performers (less than X total monthly listens + high conversion) Best new user playlist (owner has only 1 popular playlist) Listen and User Conversions, MAU Growing \u00b6 df [ 'listen_conversions' ] = df [ 'stream30s' ] / df [ 'streams' ] df [ 'listen_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'user_retention' ] = df [ 'mau_both_months' ] / df [ 'mau_previous_month' ] df [ 'user_retention' ] . fillna ( value = 0 , inplace = True ) df [ 'user_conversions' ] = df [ 'mau' ] / df [ 'users' ] df [ 'user_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growing' ] = df [ 'mau' ] > df [ 'mau_previous_month' ] df [ 'mau_growth' ] = df [ 'mau' ] / df [ 'mau_previous_month' ] df [ 'mau_growth' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growth' ] . replace ([ np . inf , - np . inf ], 1 , inplace = True ) new_metrics = [ 'listen_conversions' , 'user_conversions' , 'user_retention' , 'mau_growth' ] df [ new_metrics ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } listen_conversions user_conversions user_retention mau_growth count 403366.000000 403366.000000 403366.000000 403366.000000 mean 0.334701 0.724072 0.571070 1.513218 std 0.399968 0.261708 0.392073 17.459669 min 0.000000 0.020348 0.000000 0.031250 25% 0.000000 0.500000 0.200000 1.000000 50% 0.000000 0.666667 0.500000 1.066667 75% 0.730769 1.000000 1.000000 2.000000 max 1.000000 1.000000 1.000000 7859.000000 df [ 'listen_conversions' ] . plot ( kind = 'hist' , bins = 10 ) <AxesSubplot:ylabel='Frequency'> df [ 'user_conversions' ] . plot ( kind = 'hist' , bins = 10 ) <AxesSubplot:ylabel='Frequency'> df [ 'user_retention' ] . plot ( kind = 'hist' , bins = 10 ) <AxesSubplot:ylabel='Frequency'> df . loc [ df [ 'mau_growth' ] < 10 ][ 'mau_growth' ] . plot ( kind = 'hist' , bins = 20 ) <AxesSubplot:ylabel='Frequency'> df [ 'mau_growing' ] . value_counts () . plot ( kind = 'bar' ) <AxesSubplot:> df [ 'new_success' ] = df [ new_metrics ] . apply ( lambda x : ( x > 0.5 ) if ( max ( x ) == 1 ) else ( x > 1 )) . all ( axis = 1 ) df [ 'new_success' ] . value_counts () False 362869 True 40497 Name: new_success, dtype: int64 df . loc [ df [ 'new_success' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playlist_uri owner streams stream30s dau wau mau mau_previous_month mau_both_months users skippers owner_country n_tracks n_local_tracks n_artists n_albums monthly_stream30s monthly_owner_stream30s tokens genre_1 genre_2 genre_3 mood_1 mood_2 mood_3 listen_conversions user_retention user_conversions mau_growing mau_growth new_success 14 spotify:user:9a3580868994077be27d244788d494cd:... 9a3580868994077be27d244788d494cd 28 15 1 1 2 1 1 2 0 US 321 0 170 205 83 77 [\"sunny\", \"daze\"] Alternative Indie Rock Electronica Brooding Excited Sensual 0.535714 1.0 1.000000 True 2.000000 True 18 spotify:user:7abbdbd3119687473b8f2986e73e2ad6:... 7abbdbd3119687473b8f2986e73e2ad6 9 5 1 2 2 1 1 2 0 US 373 8 1 1 18 11 [] Pop Alternative Indie Rock Empowering Excited Urgent 0.555556 1.0 1.000000 True 2.000000 True 20 spotify:user:838141e861005b6a955cb389c19671a5:... 838141e861005b6a955cb389c19671a5 32 25 2 3 4 3 3 5 1 US 904 0 81 125 327 253 [\"metalcore\", \"forever\"] Punk Metal Rock Defiant Urgent Aggressive 0.781250 1.0 0.800000 True 1.333333 True 36 spotify:user:2217942070bcaa5f1e651e27744b4402:... 2217942070bcaa5f1e651e27744b4402 18 17 1 2 4 3 3 5 1 US 141 1 122 131 567 0 [\"chill\"] Rap Dance & House Alternative Excited Defiant Energizing 0.944444 1.0 0.800000 True 1.333333 True 59 spotify:user:dfde15dd16b4ad87a75036276b4c9f66:... dfde15dd16b4ad87a75036276b4c9f66 5 5 1 1 2 1 1 3 0 US 84 0 73 78 254 239 [\"vegas\"] Rock Pop R&B Upbeat Excited Empowering 1.000000 1.0 0.666667 True 2.000000 True ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 403329 spotify:user:358b83239c6a2557fbfb053330d49a41:... 358b83239c6a2557fbfb053330d49a41 4 4 1 1 3 1 1 3 0 US 33 0 28 31 271 32 [\"one\", \"dirt\", \"road\"] Country & Folk Rock - Yearning Empowering Gritty 1.000000 1.0 1.000000 True 3.000000 True 403336 spotify:user:a0781a2de47beb8bd693f3022f316327:... a0781a2de47beb8bd693f3022f316327 856 855 3 10 10 5 5 10 0 US 168 0 6 9 33747 1391 [\"evning\", \"song\"] - - - - - - 0.998832 1.0 1.000000 True 2.000000 True 403338 spotify:user:06f6dd666f1bbf9148c792b87ed4d22f:... 06f6dd666f1bbf9148c792b87ed4d22f 5 4 1 1 2 1 1 2 0 US 59 0 34 46 21 9 [\"rhc\"] Religious Pop Alternative Empowering Upbeat Brooding 0.800000 1.0 1.000000 True 2.000000 True 403348 spotify:user:c6af258245d55221cebedb1175f08d83:... c6af258245d55221cebedb1175f08d83 13 11 1 1 2 1 1 2 0 US 31 0 30 29 208 206 [\"zumba\", \"val\", \"silva\", \"playlist\"] Latin Pop Dance & House Aggressive Excited Defiant 0.846154 1.0 1.000000 True 2.000000 True 403353 spotify:user:5461b6b460dd512d7b4fd4fb488f3520:... 5461b6b460dd512d7b4fd4fb488f3520 2 2 1 1 2 1 1 2 0 US 146 0 115 123 405 321 [\"myfavorites\"] Indie Rock Electronica Alternative Yearning Energizing Brooding 1.000000 1.0 1.000000 True 2.000000 True 40497 rows \u00d7 31 columns chidf = pd . DataFrame () target = 'new_success' chidf [ target ] = df [ target ] # quant_value = 0.90 # tar_value = np.quantile(chidf[target], quant_value) # chidf[target] = chidf[target] > tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum . loc [ chisum [ 'reject null' ] == True ] . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 6 genre_1 Dance & House 231.225731 3.221322e-52 [[334768, 28101], [36487, 4010]] 1.309267 True 2 genre_1 Indie Rock 386.328998 5.212769e-86 [[300809, 62060], [31986, 8511]] 1.289733 True 3 mood_1 Excited 289.821405 5.438394e-65 [[306376, 56493], [32871, 7626]] 1.258184 True 4 mood_1 Defiant 285.014998 6.064223e-64 [[291222, 71647], [31065, 9432]] 1.234123 True 16 genre_2 Electronica 124.733558 5.820843e-29 [[335186, 27683], [36772, 3725]] 1.226540 True ... ... ... ... ... ... ... ... 70 mood_1 Somber 30.852148 2.784538e-08 [[361994, 875], [40456, 41]] 0.419270 True 0 genre_3 - 1404.327669 2.410008e-307 [[324633, 38236], [38610, 1887]] 0.414947 True 1 genre_2 - 861.809401 1.968786e-189 [[342541, 20328], [39619, 878]] 0.373430 True 24 mood_1 Other 81.806778 1.500630e-19 [[361232, 1637], [40439, 58]] 0.316494 True 42 genre_1 Spoken & Audio 58.779116 1.764037e-14 [[361755, 1114], [40460, 37]] 0.296965 True 101 rows \u00d7 7 columns chisum . loc [ chisum [ 'reject null' ] == True ] . sort_values ( 'multiplier' , ascending = True )[: 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 42 genre_1 Spoken & Audio 58.779116 1.764037e-14 [[361755, 1114], [40460, 37]] 0.296965 True 24 mood_1 Other 81.806778 1.500630e-19 [[361232, 1637], [40439, 58]] 0.316494 True 1 genre_2 - 861.809401 1.968786e-189 [[342541, 20328], [39619, 878]] 0.373430 True 0 genre_3 - 1404.327669 2.410008e-307 [[324633, 38236], [38610, 1887]] 0.414947 True 70 mood_1 Somber 30.852148 2.784538e-08 [[361994, 875], [40456, 41]] 0.419270 True 73 genre_1 Easy Listening 30.613123 3.149562e-08 [[361984, 885], [40455, 42]] 0.424642 True 40 mood_2 - 60.796108 6.330294e-15 [[361087, 1782], [40411, 86]] 0.431224 True 43 mood_1 - 57.600397 3.211607e-14 [[361161, 1708], [40414, 83]] 0.434269 True 37 mood_3 - 64.489845 9.703118e-16 [[360957, 1912], [40404, 93]] 0.434536 True 48 genre_1 Children's 52.188042 5.043231e-13 [[361298, 1571], [40420, 77]] 0.438111 True 32 mood_1 Easygoing 72.784800 1.445861e-17 [[360451, 2418], [40371, 126]] 0.465255 True 56 mood_3 Serious 43.083601 5.245004e-11 [[361404, 1465], [40420, 77]] 0.469948 True 59 genre_2 Other 41.614387 1.111721e-10 [[361446, 1423], [40422, 75]] 0.471283 True 82 mood_2 Other 25.423296 4.603257e-07 [[361970, 899], [40449, 48]] 0.477800 True 60 genre_1 Traditional 39.228043 3.770852e-10 [[361402, 1467], [40416, 81]] 0.493733 True 39 genre_3 Easy Listening 61.357952 4.758655e-15 [[360552, 2317], [40368, 129]] 0.497272 True 47 genre_2 Easy Listening 53.106215 3.159911e-13 [[360858, 2011], [40385, 112]] 0.497648 True 65 mood_2 Stirring 34.226638 4.905289e-09 [[361548, 1321], [40423, 74]] 0.501033 True 57 mood_1 Serious 42.044137 8.923632e-11 [[361247, 1622], [40406, 91]] 0.501590 True 10 genre_1 Soundtrack 169.038371 1.200050e-38 [[356345, 6524], [40127, 370]] 0.503642 True fig , ax = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 ), sharex = 'col' , sharey = 'row' ) ind_feature = 'genre_1' target = 'new_success' genre_list = chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False )[ 'group' ] . values chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False ) . to_excel ( 'table2.xlsx' ) dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'percent' ] dff = dff . reset_index () dff . loc [ dff [ target ] == True , 'percent' ] = dff . loc [ dff [ target ] == True , 'percent' ] / dff . loc [ dff [ target ] == True , 'percent' ] . sum () dff . loc [ dff [ target ] == False , 'percent' ] = dff . loc [ dff [ target ] == False , 'percent' ] / dff . loc [ dff [ target ] == False , 'percent' ] . sum () dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 0 , 0 ]) ax [ 0 , 0 ] . set_title ( 'Best and Worst Genres, Percent' ) ax [ 0 , 0 ] . set_ylabel ( '' ) ax [ 0 , 0 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 1 , 0 ]) # ax[1,0].set_title('Worst Primary Genres') ax [ 1 , 0 ] . set_ylabel ( '' ) dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'count' ] dff = dff . reset_index () # dff.loc[dff[target] == True, 'percent'] = dff.loc[dff[target] == True, 'percent'] / dff.loc[dff[target] == True, 'percent'].sum() # dff.loc[dff[target] == False, 'percent'] = dff.loc[dff[target] == False, 'percent'] / dff.loc[dff[target] == False, 'percent'].sum() dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () # fix, ax = plt.subplots(2, 2, figsize=(10,10)) sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 0 , 1 ]) ax [ 0 , 1 ] . set_title ( 'Best and Worst Genres, Count' ) ax [ 0 , 1 ] . set_ylabel ( '' ) ax [ 0 , 1 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 1 , 1 ]) # ax[1,1].set_title('Worst Primary Genres') ax [ 1 , 1 ] . set_ylabel ( '' ) plt . tight_layout () ax [ 0 , 0 ] . get_legend () . remove () ax [ 1 , 1 ] . get_legend () . remove () ax [ 1 , 0 ] . get_legend () . remove () ax [ 0 , 1 ] . legend ( framealpha = 1 , facecolor = 'white' , title = \"Success\" ) fig . savefig ( \"new_categorical_dependency.svg\" ) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 ), sharex = 'col' , sharey = 'row' ) ind_feature = 'mood_1' target = 'new_success' genre_list = chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False )[ 'group' ] . values chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False ) . to_excel ( 'table.xlsx' ) dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'percent' ] dff = dff . reset_index () dff . loc [ dff [ target ] == True , 'percent' ] = dff . loc [ dff [ target ] == True , 'percent' ] / dff . loc [ dff [ target ] == True , 'percent' ] . sum () dff . loc [ dff [ target ] == False , 'percent' ] = dff . loc [ dff [ target ] == False , 'percent' ] / dff . loc [ dff [ target ] == False , 'percent' ] . sum () dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 0 , 0 ]) ax [ 0 , 0 ] . set_title ( 'Best and Worst Genres, Percent' ) ax [ 0 , 0 ] . set_ylabel ( '' ) ax [ 0 , 0 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 1 , 0 ]) # ax[1,0].set_title('Worst Primary Genres') ax [ 1 , 0 ] . set_ylabel ( '' ) dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'count' ] dff = dff . reset_index () # dff.loc[dff[target] == True, 'percent'] = dff.loc[dff[target] == True, 'percent'] / dff.loc[dff[target] == True, 'percent'].sum() # dff.loc[dff[target] == False, 'percent'] = dff.loc[dff[target] == False, 'percent'] / dff.loc[dff[target] == False, 'percent'].sum() dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () # fix, ax = plt.subplots(2, 2, figsize=(10,10)) sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 0 , 1 ]) ax [ 0 , 1 ] . set_title ( 'Best and Worst Genres, Count' ) ax [ 0 , 1 ] . set_ylabel ( '' ) ax [ 0 , 1 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 1 , 1 ]) # ax[1,1].set_title('Worst Primary Genres') ax [ 1 , 1 ] . set_ylabel ( '' ) plt . tight_layout () ax [ 0 , 0 ] . get_legend () . remove () ax [ 1 , 1 ] . get_legend () . remove () ax [ 1 , 0 ] . get_legend () . remove () ax [ 0 , 1 ] . legend ( framealpha = 1 , facecolor = 'white' , title = \"Success\" ) fig . savefig ( \"new_categorical_dependency_mood.svg\" ) fig , (( ax1 , ax2 ),( ax3 , ax4 )) = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) con_features = [ 'n_albums' , 'n_artists' , 'n_tracks' , 'n_local_tracks' ] chidf = pd . DataFrame () target = \"new_success\" chidf [ target ] = df [ target ] welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind , ax in zip ( con_features , [ ax1 , ax2 , ax3 , ax4 ]): chidf [ ind ] = df [ ind ] a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } == True\" , ax = ax ) sns . histplot ( b , label = f \" { target } == False\" , ax = ax ) ax . set_title ( ind ) welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) welchsum . to_excel ( \"new_ttest.xlsx\" ) ax . legend () fig . savefig ( \"new_ttest.svg\" ) chidf = pd . DataFrame () target = \"success\" chidf [ target ] = df [ target ] # chidf.iloc[:int(chidf.shape[0]/2),:] = True # chidf.iloc[int(chidf.shape[0]/2):,:] = False # quant_value = 0.99 # tar_value = np.quantile(chidf[target], quant_value) # chidf[target] = chidf[target] > tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } == True\" ) sns . histplot ( b , label = f \" { target } == False\" ) plt . title ( ind ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) Considering outliers \u00b6 df = df . loc [ df [ targets ] . apply ( lambda x : ( x < 3 * x . std ()) if ( x . dtype == int or x . dtype == float ) else x ) . all ( axis = 1 )] df = df . loc [ df [ 'owner' ] != 'spotify' ] Multiple Criteria for Success \u00b6 df [ 'success' ] = df [ sub_targets ] . apply ( lambda x : x > np . quantile ( x , 0.75 )) . all ( axis = 1 ) chidf = pd . DataFrame () target = 'success' chidf [ target ] = df [ target ] # quant_value = 0.90 # tar_value = np.quantile(chidf[target], quant_value) # chidf[target] = chidf[target] > tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum . loc [ chisum [ 'reject null' ] == True ] . sort_values ( 'multiplier' , ascending = False ) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 ), sharex = 'col' , sharey = 'row' ) genre_list = chisum . loc [ chisum [ 'feature' ] == 'genre_1' ] . sort_values ( 'multiplier' , ascending = False )[ 'group' ] . values dff = pd . DataFrame ( df . groupby ([ 'genre_1' ])[ 'success' ] . value_counts ( sort = False )) dff . columns = [ 'percent' ] dff = dff . reset_index () dff . loc [ dff [ 'success' ] == True , 'percent' ] = dff . loc [ dff [ 'success' ] == True , 'percent' ] / dff . loc [ dff [ 'success' ] == True , 'percent' ] . sum () dff . loc [ dff [ 'success' ] == False , 'percent' ] = dff . loc [ dff [ 'success' ] == False , 'percent' ] / dff . loc [ dff [ 'success' ] == False , 'percent' ] . sum () dff = dff . set_index ( 'genre_1' ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = 'success' , y = 'genre_1' , x = 'percent' , ax = ax [ 0 , 0 ]) ax [ 0 , 0 ] . set_title ( 'Best and Worst Genres, Percent' ) ax [ 0 , 0 ] . set_ylabel ( '' ) ax [ 0 , 0 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = 'success' , y = 'genre_1' , x = 'percent' , ax = ax [ 1 , 0 ]) # ax[1,0].set_title('Worst Primary Genres') ax [ 1 , 0 ] . set_ylabel ( '' ) dff = pd . DataFrame ( df . groupby ([ 'genre_1' ])[ 'success' ] . value_counts ( sort = False )) dff . columns = [ 'count' ] dff = dff . reset_index () # dff.loc[dff['success'] == True, 'percent'] = dff.loc[dff['success'] == True, 'percent'] / dff.loc[dff['success'] == True, 'percent'].sum() # dff.loc[dff['success'] == False, 'percent'] = dff.loc[dff['success'] == False, 'percent'] / dff.loc[dff['success'] == False, 'percent'].sum() dff = dff . set_index ( 'genre_1' ) . loc [ genre_list ,:] dff = dff . reset_index () # fix, ax = plt.subplots(2, 2, figsize=(10,10)) sns . barplot ( data = dff . iloc [: 10 ,:], hue = 'success' , y = 'genre_1' , x = 'count' , ax = ax [ 0 , 1 ]) ax [ 0 , 1 ] . set_title ( 'Best and Worst Genres, Count' ) ax [ 0 , 1 ] . set_ylabel ( '' ) ax [ 0 , 1 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = 'success' , y = 'genre_1' , x = 'count' , ax = ax [ 1 , 1 ]) # ax[1,1].set_title('Worst Primary Genres') ax [ 1 , 1 ] . set_ylabel ( '' ) plt . tight_layout () ax [ 0 , 0 ] . get_legend () . remove () ax [ 1 , 1 ] . get_legend () . remove () ax [ 1 , 0 ] . get_legend () . remove () ax [ 0 , 1 ] . legend ( framealpha = 1 , facecolor = 'white' , title = \"Success\" ) fig . savefig ( \"categorical_dependency.svg\" ) ind = 'n_tracks' target = 'wau' mean_wau_vs_track = [] for track in range ( 1 , 201 ): means = [] for i in range ( 10 ): boot = random . sample ( list ( df . loc [ ( df [ 'success' ] == True ) & ( df [ ind ] == track ) ][ target ] . values ), k = min ( len ( list ( df . loc [ ( df [ 'success' ] == True ) & ( df [ ind ] == track ) ][ target ] . values )), 1000 )) means . append ( np . mean ( boot )) mean_wau_vs_track . append ( np . mean ( means )) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) plt . plot ( range ( len ( mean_wau_vs_track )), mean_wau_vs_track , ls = '' , marker = '.' ) # ax.set_ylim(0,5) len ( df . loc [ ( df [ 'success' ] == True ) & ( df [ ind ] == track ) ][ target ] . values ) Dependency \u00b6 master = pd . DataFrame () for target in new_metrics : # target = sub_targets[0] chidf = pd . DataFrame () chidf [ target ] = df [ target ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) tar_value = 0.8 chidf [ target ] = chidf [ target ] >= tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ target , ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'target' , 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) # chisum = chisum.loc[(chisum['reject null'] == True) & (chisum['multiplier'] > 2)].sort_values('multiplier', ascending=False) master = pd . concat (( master , chisum )) master master . loc [( master [ 'reject null' ] == True ) & ( master [ 'multiplier' ] > 1.5 )] master . loc [( master [ 'reject null' ] == True ) & ( master [ 'multiplier' ] < .5 )] new_master = pd . DataFrame () for target in new_metrics : # target = sub_targets[2] chidf = pd . DataFrame () chidf [ target ] = df [ target ] chidf [ 'n_tracks' ] = df [ 'n_tracks' ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) tar_value = 0.8 chidf [ target ] = chidf [ target ] >= tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ target , ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } >= { tar_value : .0f } \" ) sns . histplot ( b , label = f \" { target } < { tar_value : .0f } \" ) plt . title ( f \" { target } , { ind } \" ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'target' , 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) new_master = pd . concat (( new_master , welchsum )) new_master Conclusions \u00b6 Discrete, Independent Variables \u00b6 We note that there is class imbalance in the discrete independent variables: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 10 )) dff = pd . DataFrame ( df [ des_features [ 0 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 1 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 2 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'mood' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'mood' , x = 'count' , orient = 'h' , ax = ax [ 0 ]) dff = pd . DataFrame ( df [ des_features [ 3 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 4 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 5 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'genre' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'genre' , x = 'count' , orient = 'h' , ax = ax [ 1 ]) plt . tight_layout () This class imbalance can have a variety of effects (and might be derived from a variety of sources). For example, users will have more choice when listening to popular genres likeIndie Rock and Rap, and less choice with genres like Blues and Easy listening. As it so happens, when we look to the relationship between genre/mood and the dependent variables, many of the genre/moods with smaller class sizes will have a positive multiplier effect on the dependent variable Continuous, Independent Variables \u00b6 The four continuous variables of focus in this dataset are highly tailed. Due to this, our statistical tests will require bootstrapping. quant = 0.999 con_features = [ 'n_albums' , 'n_artists' , 'n_tracks' , 'n_local_tracks' ] for target in con_features : cutoff = np . quantile ( df [ target ], quant ) y = df . loc [ df [ target ] < cutoff ] removed = df . loc [ ~ ( df [ target ] < cutoff )] print ( f \"removed items: { removed . shape [ 0 ] } \" ) y . plot ( kind = 'hist' , y = target , bins = 100 , density = True ) plt . show () removed items: 404 removed items: 405 removed items: 404 removed items: 406 an example of bootstrapping n_albums means = [] ind = con_features [ 0 ] for i in range ( 100 ): boot = random . sample ( list ( df . loc [ ( df [ ind ] > 9 ) & ( df [ ind ] < 999 ) ][ ind ] . values ), k = 1000 ) means . append ( np . mean ( boot )) stuff = plt . hist ( means , bins = 100 , density = True ) Discrete, Dependent Variables \u00b6 For the purposes of investigating a \"successful\" playlist, there are 5 primary metrics: targets df [ sub_targets ] . describe () . round ( 1 ) . to_excel ( \"file.xlsx\" ) and \"top\" performers in each of these metrics were based on top 10% and top 1% quantiles: print ( 'p99 targets' ) for target in sub_targets : space = ' ' * ( 20 - len ( str ( target ))) print ( f \" { target } : { space } { np . quantile ( df [ target ], 0.99 ) } \" ) print () print ( 'p90 targets' ) for target in sub_targets : space = ' ' * ( 20 - len ( str ( target ))) print ( f \" { target } : { space } { np . quantile ( df [ target ], 0.90 ) } \" ) You can imagine with these metrics, some concerns are: what if a playlist was made in the current month, or even current day? playlist is not properly represented by the data how do we normalize by playlists that already have a high visibility? i.e. what if a playlist is \"good\" but just isn't getting noticed? can compute conversion metrics: 30 second listens / total listens mau both months / mau previous month While noting these shortcomings, to keep the analysis focused I singled out the previously mentioned targets, with a focus on monthly_stream30s as the north star metric. monthly_stream30s is advantageous as a nort star metric since it contains data from the entire month (reducing variance) only contains relevant listens (greater than 30 seconds long). Some disadvantages of this metric are that it doesn't account for just a few listeners who may be providing the majority of listens, and playlists that were made in the current month will be undervalued. Dependency \u00b6 Chi Square \u00b6 In the chi-square test, the contigency table was used to calculate a multiplier effect. This is a ratio of ratios: the count of upper quantile over bottom quantile for the given group over the count of upper quantile over bottom quantile for non-group. In other words, it articulates how much more likely a sample in the given group is likely to be in the upper quantile vs a sample not in the given group chisq_results = pd . read_csv ( \"chi_square_results.csv\" , index_col = 0 ) chisq_results . head () chisq_results [ 'target' ] . unique () chisq_results [ 'upper q' ] . unique () Taking together the five targets, the two upper quantiles, and the six categorical independent variables, we can identify which group occured the most frequently as a variable of influence: chisq_results . loc [( chisq_results [ 'feature' ] . str . contains ( 'genre' )) & ( chisq_results [ 'group' ] != '-' )][ 'group' ] . value_counts () Using these value counts as a \"rank\" we can then groupby this rank and see how each group is influencing the propensity to be in the upper quadrant Taking \"Romantic\" as an example, we see that it's multiplier effect is relatively consistent across the five targets and two quantiles: sort_key = { i : j for i , j in zip ( chisq_results [ 'group' ] . value_counts () . index . values , range ( chisq_results [ 'group' ] . nunique ()))} chisq_results [ 'rank' ] = chisq_results [ 'group' ] . apply ( lambda x : sort_key [ x ]) chisq_results . sort_values ( 'rank' , inplace = True ) # chisq_results.drop('rank', axis=1, inplace=True) chisq_results . loc [ chisq_results [ 'group' ] != '-' ][: 20 ] chisq_results . loc [( chisq_results [ 'group' ] == 'Traditional' ) & ( chisq_results [ 'target' ] == 'monthly_stream30s' )] Let's use this idea of average multiplier effect, and average chi-square statistic to summarize by group. Sorting by the test statistic, we see the top 5 most influential groups: chisq_results . groupby ( 'group' )[[ 'chi' , 'multiplier' , 'rank' ]] . mean () . sort_values ( 'chi' , ascending = False )[: 10 ] Sorting instead by the multiplier, we can see which group has the heaviest influence chisq_results . groupby ( 'group' )[[ 'chi' , 'multiplier' , 'rank' ]] . mean () . sort_values ( 'multiplier' , ascending = False )[: 10 ] Sorting instead by rank, we see which groups show up most frequently chisq_results . groupby ( 'group' )[[ 'chi' , 'multiplier' , 'rank' ]] . mean () . sort_values ( 'rank' , ascending = True )[: 10 ] chisq_results . loc [ chisq_results [ 'target' ] == 'monthly_stream30s' ] It creates some fog to jumble together mood/genres this way. We can instead separate them and ask questions like: What is the most influential primary genre on monthly streams over 30 seconds? \u00b6 Answer: Children's followed by Latin Reason: both genre's appear as influential in other guardrail metrics (high rank), have high test statistics, and are influential in both p99 and p90 with multiplier effects of [4.8, 2.6] and [3.1, 2.1], respectively. chisq_results . loc [( chisq_results [ 'feature' ] == 'genre_1' ) & ( chisq_results [ 'target' ] == 'monthly_stream30s' )] What is the most influential primary mood on monthly streams over 30 seconds? \u00b6 Answer: Romantic and Lively Reason: Romantic and Lively moods appear multiple times as highly influential (high rank) they have high multipliers. A contendent may be Tender, as it has a high multiplier effect as well at 3.75 chisq_results . loc [( chisq_results [ 'feature' ] == 'mood_1' ) & ( chisq_results [ 'target' ] == 'monthly_stream30s' )] Which Categorical Feature is most influential overall? \u00b6 Answer: genre_1, followed by genre_2 and mood_1 Reason: we see that these features appear multiple times across the 5 different targets and 2 different quantiles chisq_results [ 'feature' ] . value_counts () What are the shortcomings of this analysis? \u00b6 We haven't taken into account confounding variables. For example, perhaps Latin genre is typically associated with Lively mood. Then which variable is it that actually contributes to a highly performing playlist? We have strategies for dealing with this. We can stratify the confounding variables by over or under sampling. We can also consider them together in a forward selection logistic model. We will take the latter approach later on in the analysis. We haven't considered the categorical variables alongside the continuous variables, so we don't know how they fit overall in terms of relative improtance. We will approach this the same way as the confounding variables issue, and incorporate all variables in a logistic regression. t-Test \u00b6 ttest_results = pd . read_csv ( \"t_test_results.csv\" , index_col = 0 ) ttest_results . head () Models \u00b6 log_results = pd . read_csv ( \"../../scripts/fwd_selection_results.txt\" , header = None , index_col = 0 ) log_results . columns = [ 'feature' , 'pseudo r2' ] log_results . reset_index ( inplace = True , drop = True ) log_results . drop ( 0 , axis = 0 , inplace = True ) log_results target = \"monthly_stream30s\" y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = 0 names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = 1 weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels basemodel = pd . read_csv ( \"../../scripts/basemodel.csv\" , index_col = 0 ) X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns res_df","title":"X3 Spotify Copy1"},{"location":"extras/X3_Spotify-Copy1/#what-makes-a-playlist-successful","text":"Analysis Simple metric (dependent variable) mau mau_previous_month mau_both_months monthly_stream30s stream30s Design metric (dependent variable) 30s listens/tot listens (listen conversions) Users both months/users prev month (user conversions) Best small time performers (less than X total monthly listens + high conversion) Best new user playlist (owner has only 1 popular playlist) Define \"top\" Top 10% mau_previous_month: 9.0 mau_both_months: 2.0 mau: 9.0 monthly_stream30s: 432.0 stream30s: 17.0 Top 1% mau_previous_month: 130.0 mau_both_months: 19.0 mau: 143.0 monthly_stream30s: 2843.0 stream30s: 113.0 Independent variables moods and genres (categorical) number of tracks, albums, artists, and local tracks (continuous) The analysis will consist of: understand the distribution characteristics of the dependent and independent variables quantify the dependency of the dependent/independent variables for each of the simple and design metrics chi-square test bootstrap/t-test Key Conclusions for the simple metrics, key genres and moods were Romantic, Latin, Children's, Lively, Traditional, and Jazz . Playlists that included these genres/moods had a positive multiplier effect (usually in the vicinicty of 2x more likely) on the key simple metric (i.e. playlists with latin as a primary genre were 2.5x more likely to be in the top 10% of streams longer than 30 seconds) skippers - is this associated with the current playlist Column Name Description playlist_uri The key, Spotify uri of the playlist owner Playlist owner, Spotify username streams Number of streams from the playlist today stream30s Number of streams over 30 seconds from playlist today dau Number of Daily Active Users, i.e. users with a stream over 30 seconds from playlist today wau Number of Weekly Active Users, i.e. users with a stream over 30 seconds from playlist in past week mau Number of Monthly Active Users, i.e. users with a stream over 30 seconds from playlist in the past month mau_previous_months Number of Monthly Active users in the month prior to this one mau_both_months Number of users that were active on the playlist both this and the previous month users Number of users streaming (all streams) from this playlist this month skippers Number of users who skipped more than 90 percent of their streams today owner_country Country of the playlist owner n_tracks Number of tracks in playlist n_local_tracks Change in number of tracks on playlist since yesterday n_artists Number of unique artists in playlist n_albums Number of unique albums in playlist monthly_stream30s Number of streams over 30 seconds this month monthly_owner_stream30s Number of streams over 30 seconds by playlist owner this month tokens List of playlist title tokens, stopwords and punctuation removed genre_1 No. 1 Genre by weight of playlist tracks, from Gracenote metadata genre_2 No. 2 Genre by weight of playlist tracks, from Gracenote metadata genre_3 No. 3 Genre by weight of playlist tracks, from Gracenote metadata mood_1 No. 1 Mood by weight of playlist tracks, from Gracenote metadata mood_2 No. 2 Mood by weight of playlist tracks, from Gracenote metadata mood_3 No. 3 Mood by weight of playlist tracks, from Gracenote metadata","title":"What makes a playlist successful?"},{"location":"extras/X3_Spotify-Copy1/#imports","text":"# basic packages import pandas as pd pd . set_option ( 'display.max_columns' , 500 ) import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.discrete.discrete_model import Logit from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.utils.class_weight import compute_class_weight # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score df = pd . read_csv ( \"../../data/playlist_summary_external-4.txt\" , delimiter = ' \\t ' )","title":"Imports"},{"location":"extras/X3_Spotify-Copy1/#putting-it-all-together","text":"sub_targets = [ 'mau_previous_month' , 'mau_both_months' , 'monthly_stream30s' , 'stream30s' ] des_features = [ 'mood_1' , 'mood_2' , 'mood_3' , 'genre_1' , 'genre_2' , 'genre_3' ]","title":"Putting it All Together"},{"location":"extras/X3_Spotify-Copy1/#models-multi-feature-analysis","text":"","title":"Models (Multi-Feature Analysis)"},{"location":"extras/X3_Spotify-Copy1/#deciles-random-forest","text":"sub_targets target = sub_targets [ - 2 ] y = df [ target ] . values labels = y . copy () names = [] for idx , quant in zip ( range ( 11 ), np . linspace ( 0 , 1 , num = 11 )): if idx == 0 : prev = quant continue if idx == 1 : labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels names X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 42 , train_size = 0.8 ) model = RandomForestClassifier () model . fit ( X_train , y_train ) y_hat_test = model . predict ( X_test ) print ( f \"Train Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) # grab feature importances imp = model . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in model . estimators_ ], axis = 0 ) # build feature names feature_names = con_features + list ( enc . get_feature_names_out ()) # create new dataframe feat = pd . DataFrame ([ feature_names , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head ( 20 )","title":"Deciles - Random Forest"},{"location":"extras/X3_Spotify-Copy1/#quartiles-random-forest","text":"### Create Categories y = df [ target ] . values labels = y . copy () names = [] lim = 5 for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx == 0 : prev = quant continue if idx == 1 : labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels ### Create Training Data X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 42 , train_size = 0.8 ) ### Train Model model = RandomForestClassifier () model . fit ( X_train , y_train ) ### Asses Performance y_hat_test = model . predict ( X_test ) y_hat_train = model . predict ( X_train ) print ( f \"Train Acc: { accuracy_score ( y_train , y_hat_train ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax )","title":"Quartiles - Random Forest"},{"location":"extras/X3_Spotify-Copy1/#binary-90th-percentile-random-forest","text":"### Create Categories y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 5 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels ### Create Training Data X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test , weight_train , weight_test = train_test_split ( X , y , weights , random_state = 42 , train_size = 0.8 ) ### Strateification Code # strat_y0_idx = np.array(random.sample(list(np.argwhere(y_train==3).reshape(-1)), np.unique(y_train, return_counts=True)[1][1])) # strat_y1_idx = np.argwhere(y_train==4).reshape(-1) # strat_idx = np.hstack((strat_y0_idx, strat_y1_idx)) # X_train = X_train[strat_idx] # y_train = y_train[strat_idx] ### Train Model model = RandomForestClassifier () model . fit ( X_train , y_train ) ### Assess Performance y_hat_test = model . predict ( X_test ) y_hat_train = model . predict ( X_train ) print ( f \"Train Acc: { accuracy_score ( y_train , y_hat_train ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax )","title":"Binary, 90th Percentile, Random Forest"},{"location":"extras/X3_Spotify-Copy1/#forward-selection-model","text":"### y print ( target ) y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = 0 names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = 1 weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels #### X X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( np . ones (( X_con . shape [ 0 ], 1 )), X_con , X_cat )) feature_names = [ 'intercept' ] + con_features + list ( enc . get_feature_names_out ()) data = pd . DataFrame ( X , columns = feature_names ) print ( names ) def add_feature ( feature_names , basemodel , data , y , r2max = 0 , model = 'linear' , disp = 0 ): feature_max = None bestsum = None newmodel = None for feature in feature_names : basemodel [ feature ] = data [ feature ] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () score = float ( str ( pd . DataFrame ( summ . tables [ 0 ]) . loc [ 3 , 3 ])) if ( score > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = score feature_max = feature bestsum = est2 . summary () newmodel = basemodel . copy () if disp == 1 : print ( f \"new r2max, { feature_max } , { r2max } \" ) basemodel . drop ( labels = feature , axis = 1 , inplace = True ) return r2max , feature_max , bestsum , newmodel candidates = feature_names . copy () basemodel = pd . DataFrame () r2max = 0 with open ( \"canidates.txt\" , \"w+\" ) as f : file_data = f . read () for i in candidates : f . write ( f \" { i } \\n \" ) basemodel . to_csv ( \"basemodel.csv\" ) with open ( \"canidates.txt\" , \"r\" ) as f : # file_data = f.read() new = [] for line in f : current_place = line [: - 1 ] new . append ( current_place ) new = pd . read_csv ( \"basemodel.csv\" , index_col = 0 ) with open ( \"fwd_selection_results.txt\" , \"r+\" ) as f : for line in f : pass lastline = line [: - 1 ] stuff = lastline . split ( \", \" ) new = float ( stuff [ - 1 ]) new while True : newr2max , feature_max , bestsum , newmodel = add_feature ( feature_names = candidates , basemodel = basemodel , data = data , y = y , r2max = r2max ) if newr2max > r2max : r2max = newr2max print ( f \"new r2max, { feature_max } , { r2max } \" ) with open ( \"fwd_selection_results.txt\" , \"a+\" ) as f : file_data = f . read () f . write ( f \"new r2max, { feature_max } , { r2max } \\n \" ) candidates . remove ( feature_max ) with open ( \"canidates.txt\" , \"w+\" ) as f : file_data = f . read () for i in candidates : f . write ( f \" { i } \\n \" ) basemodel = newmodel basemodel . to_csv ( \"basemodel.csv\" ) continue else : break X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns res_df","title":"Forward Selection Model"},{"location":"extras/X3_Spotify-Copy1/#binary-99th-percentile","text":"### Create Categories y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels ### Create Training Data X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test , weight_train , weight_test = train_test_split ( X , y , weights , random_state = 42 , train_size = 0.8 ) ### Train Model model = RandomForestClassifier () model . fit ( X_train , y_train , weight_train ) ### Asses Performance y_hat_test = model . predict ( X_test ) y_hat_train = model . predict ( X_train ) print ( f \"Train Acc: { accuracy_score ( y_train , y_hat_train ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax )","title":"Binary, 99th Percentile"},{"location":"extras/X3_Spotify-Copy1/#other-metrics","text":"30s listens/tot listens (listen conversions) also like a bounce rate Users both months/users prev month (user conversions) combine with mau > mau_previous_month Best small time performers (less than X total monthly listens + high conversion) Best new user playlist (owner has only 1 popular playlist)","title":"Other Metrics"},{"location":"extras/X3_Spotify-Copy1/#listen-and-user-conversions-mau-growing","text":"df [ 'listen_conversions' ] = df [ 'stream30s' ] / df [ 'streams' ] df [ 'listen_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'user_retention' ] = df [ 'mau_both_months' ] / df [ 'mau_previous_month' ] df [ 'user_retention' ] . fillna ( value = 0 , inplace = True ) df [ 'user_conversions' ] = df [ 'mau' ] / df [ 'users' ] df [ 'user_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growing' ] = df [ 'mau' ] > df [ 'mau_previous_month' ] df [ 'mau_growth' ] = df [ 'mau' ] / df [ 'mau_previous_month' ] df [ 'mau_growth' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growth' ] . replace ([ np . inf , - np . inf ], 1 , inplace = True ) new_metrics = [ 'listen_conversions' , 'user_conversions' , 'user_retention' , 'mau_growth' ] df [ new_metrics ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } listen_conversions user_conversions user_retention mau_growth count 403366.000000 403366.000000 403366.000000 403366.000000 mean 0.334701 0.724072 0.571070 1.513218 std 0.399968 0.261708 0.392073 17.459669 min 0.000000 0.020348 0.000000 0.031250 25% 0.000000 0.500000 0.200000 1.000000 50% 0.000000 0.666667 0.500000 1.066667 75% 0.730769 1.000000 1.000000 2.000000 max 1.000000 1.000000 1.000000 7859.000000 df [ 'listen_conversions' ] . plot ( kind = 'hist' , bins = 10 ) <AxesSubplot:ylabel='Frequency'> df [ 'user_conversions' ] . plot ( kind = 'hist' , bins = 10 ) <AxesSubplot:ylabel='Frequency'> df [ 'user_retention' ] . plot ( kind = 'hist' , bins = 10 ) <AxesSubplot:ylabel='Frequency'> df . loc [ df [ 'mau_growth' ] < 10 ][ 'mau_growth' ] . plot ( kind = 'hist' , bins = 20 ) <AxesSubplot:ylabel='Frequency'> df [ 'mau_growing' ] . value_counts () . plot ( kind = 'bar' ) <AxesSubplot:> df [ 'new_success' ] = df [ new_metrics ] . apply ( lambda x : ( x > 0.5 ) if ( max ( x ) == 1 ) else ( x > 1 )) . all ( axis = 1 ) df [ 'new_success' ] . value_counts () False 362869 True 40497 Name: new_success, dtype: int64 df . loc [ df [ 'new_success' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playlist_uri owner streams stream30s dau wau mau mau_previous_month mau_both_months users skippers owner_country n_tracks n_local_tracks n_artists n_albums monthly_stream30s monthly_owner_stream30s tokens genre_1 genre_2 genre_3 mood_1 mood_2 mood_3 listen_conversions user_retention user_conversions mau_growing mau_growth new_success 14 spotify:user:9a3580868994077be27d244788d494cd:... 9a3580868994077be27d244788d494cd 28 15 1 1 2 1 1 2 0 US 321 0 170 205 83 77 [\"sunny\", \"daze\"] Alternative Indie Rock Electronica Brooding Excited Sensual 0.535714 1.0 1.000000 True 2.000000 True 18 spotify:user:7abbdbd3119687473b8f2986e73e2ad6:... 7abbdbd3119687473b8f2986e73e2ad6 9 5 1 2 2 1 1 2 0 US 373 8 1 1 18 11 [] Pop Alternative Indie Rock Empowering Excited Urgent 0.555556 1.0 1.000000 True 2.000000 True 20 spotify:user:838141e861005b6a955cb389c19671a5:... 838141e861005b6a955cb389c19671a5 32 25 2 3 4 3 3 5 1 US 904 0 81 125 327 253 [\"metalcore\", \"forever\"] Punk Metal Rock Defiant Urgent Aggressive 0.781250 1.0 0.800000 True 1.333333 True 36 spotify:user:2217942070bcaa5f1e651e27744b4402:... 2217942070bcaa5f1e651e27744b4402 18 17 1 2 4 3 3 5 1 US 141 1 122 131 567 0 [\"chill\"] Rap Dance & House Alternative Excited Defiant Energizing 0.944444 1.0 0.800000 True 1.333333 True 59 spotify:user:dfde15dd16b4ad87a75036276b4c9f66:... dfde15dd16b4ad87a75036276b4c9f66 5 5 1 1 2 1 1 3 0 US 84 0 73 78 254 239 [\"vegas\"] Rock Pop R&B Upbeat Excited Empowering 1.000000 1.0 0.666667 True 2.000000 True ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 403329 spotify:user:358b83239c6a2557fbfb053330d49a41:... 358b83239c6a2557fbfb053330d49a41 4 4 1 1 3 1 1 3 0 US 33 0 28 31 271 32 [\"one\", \"dirt\", \"road\"] Country & Folk Rock - Yearning Empowering Gritty 1.000000 1.0 1.000000 True 3.000000 True 403336 spotify:user:a0781a2de47beb8bd693f3022f316327:... a0781a2de47beb8bd693f3022f316327 856 855 3 10 10 5 5 10 0 US 168 0 6 9 33747 1391 [\"evning\", \"song\"] - - - - - - 0.998832 1.0 1.000000 True 2.000000 True 403338 spotify:user:06f6dd666f1bbf9148c792b87ed4d22f:... 06f6dd666f1bbf9148c792b87ed4d22f 5 4 1 1 2 1 1 2 0 US 59 0 34 46 21 9 [\"rhc\"] Religious Pop Alternative Empowering Upbeat Brooding 0.800000 1.0 1.000000 True 2.000000 True 403348 spotify:user:c6af258245d55221cebedb1175f08d83:... c6af258245d55221cebedb1175f08d83 13 11 1 1 2 1 1 2 0 US 31 0 30 29 208 206 [\"zumba\", \"val\", \"silva\", \"playlist\"] Latin Pop Dance & House Aggressive Excited Defiant 0.846154 1.0 1.000000 True 2.000000 True 403353 spotify:user:5461b6b460dd512d7b4fd4fb488f3520:... 5461b6b460dd512d7b4fd4fb488f3520 2 2 1 1 2 1 1 2 0 US 146 0 115 123 405 321 [\"myfavorites\"] Indie Rock Electronica Alternative Yearning Energizing Brooding 1.000000 1.0 1.000000 True 2.000000 True 40497 rows \u00d7 31 columns chidf = pd . DataFrame () target = 'new_success' chidf [ target ] = df [ target ] # quant_value = 0.90 # tar_value = np.quantile(chidf[target], quant_value) # chidf[target] = chidf[target] > tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum . loc [ chisum [ 'reject null' ] == True ] . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 6 genre_1 Dance & House 231.225731 3.221322e-52 [[334768, 28101], [36487, 4010]] 1.309267 True 2 genre_1 Indie Rock 386.328998 5.212769e-86 [[300809, 62060], [31986, 8511]] 1.289733 True 3 mood_1 Excited 289.821405 5.438394e-65 [[306376, 56493], [32871, 7626]] 1.258184 True 4 mood_1 Defiant 285.014998 6.064223e-64 [[291222, 71647], [31065, 9432]] 1.234123 True 16 genre_2 Electronica 124.733558 5.820843e-29 [[335186, 27683], [36772, 3725]] 1.226540 True ... ... ... ... ... ... ... ... 70 mood_1 Somber 30.852148 2.784538e-08 [[361994, 875], [40456, 41]] 0.419270 True 0 genre_3 - 1404.327669 2.410008e-307 [[324633, 38236], [38610, 1887]] 0.414947 True 1 genre_2 - 861.809401 1.968786e-189 [[342541, 20328], [39619, 878]] 0.373430 True 24 mood_1 Other 81.806778 1.500630e-19 [[361232, 1637], [40439, 58]] 0.316494 True 42 genre_1 Spoken & Audio 58.779116 1.764037e-14 [[361755, 1114], [40460, 37]] 0.296965 True 101 rows \u00d7 7 columns chisum . loc [ chisum [ 'reject null' ] == True ] . sort_values ( 'multiplier' , ascending = True )[: 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 42 genre_1 Spoken & Audio 58.779116 1.764037e-14 [[361755, 1114], [40460, 37]] 0.296965 True 24 mood_1 Other 81.806778 1.500630e-19 [[361232, 1637], [40439, 58]] 0.316494 True 1 genre_2 - 861.809401 1.968786e-189 [[342541, 20328], [39619, 878]] 0.373430 True 0 genre_3 - 1404.327669 2.410008e-307 [[324633, 38236], [38610, 1887]] 0.414947 True 70 mood_1 Somber 30.852148 2.784538e-08 [[361994, 875], [40456, 41]] 0.419270 True 73 genre_1 Easy Listening 30.613123 3.149562e-08 [[361984, 885], [40455, 42]] 0.424642 True 40 mood_2 - 60.796108 6.330294e-15 [[361087, 1782], [40411, 86]] 0.431224 True 43 mood_1 - 57.600397 3.211607e-14 [[361161, 1708], [40414, 83]] 0.434269 True 37 mood_3 - 64.489845 9.703118e-16 [[360957, 1912], [40404, 93]] 0.434536 True 48 genre_1 Children's 52.188042 5.043231e-13 [[361298, 1571], [40420, 77]] 0.438111 True 32 mood_1 Easygoing 72.784800 1.445861e-17 [[360451, 2418], [40371, 126]] 0.465255 True 56 mood_3 Serious 43.083601 5.245004e-11 [[361404, 1465], [40420, 77]] 0.469948 True 59 genre_2 Other 41.614387 1.111721e-10 [[361446, 1423], [40422, 75]] 0.471283 True 82 mood_2 Other 25.423296 4.603257e-07 [[361970, 899], [40449, 48]] 0.477800 True 60 genre_1 Traditional 39.228043 3.770852e-10 [[361402, 1467], [40416, 81]] 0.493733 True 39 genre_3 Easy Listening 61.357952 4.758655e-15 [[360552, 2317], [40368, 129]] 0.497272 True 47 genre_2 Easy Listening 53.106215 3.159911e-13 [[360858, 2011], [40385, 112]] 0.497648 True 65 mood_2 Stirring 34.226638 4.905289e-09 [[361548, 1321], [40423, 74]] 0.501033 True 57 mood_1 Serious 42.044137 8.923632e-11 [[361247, 1622], [40406, 91]] 0.501590 True 10 genre_1 Soundtrack 169.038371 1.200050e-38 [[356345, 6524], [40127, 370]] 0.503642 True fig , ax = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 ), sharex = 'col' , sharey = 'row' ) ind_feature = 'genre_1' target = 'new_success' genre_list = chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False )[ 'group' ] . values chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False ) . to_excel ( 'table2.xlsx' ) dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'percent' ] dff = dff . reset_index () dff . loc [ dff [ target ] == True , 'percent' ] = dff . loc [ dff [ target ] == True , 'percent' ] / dff . loc [ dff [ target ] == True , 'percent' ] . sum () dff . loc [ dff [ target ] == False , 'percent' ] = dff . loc [ dff [ target ] == False , 'percent' ] / dff . loc [ dff [ target ] == False , 'percent' ] . sum () dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 0 , 0 ]) ax [ 0 , 0 ] . set_title ( 'Best and Worst Genres, Percent' ) ax [ 0 , 0 ] . set_ylabel ( '' ) ax [ 0 , 0 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 1 , 0 ]) # ax[1,0].set_title('Worst Primary Genres') ax [ 1 , 0 ] . set_ylabel ( '' ) dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'count' ] dff = dff . reset_index () # dff.loc[dff[target] == True, 'percent'] = dff.loc[dff[target] == True, 'percent'] / dff.loc[dff[target] == True, 'percent'].sum() # dff.loc[dff[target] == False, 'percent'] = dff.loc[dff[target] == False, 'percent'] / dff.loc[dff[target] == False, 'percent'].sum() dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () # fix, ax = plt.subplots(2, 2, figsize=(10,10)) sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 0 , 1 ]) ax [ 0 , 1 ] . set_title ( 'Best and Worst Genres, Count' ) ax [ 0 , 1 ] . set_ylabel ( '' ) ax [ 0 , 1 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 1 , 1 ]) # ax[1,1].set_title('Worst Primary Genres') ax [ 1 , 1 ] . set_ylabel ( '' ) plt . tight_layout () ax [ 0 , 0 ] . get_legend () . remove () ax [ 1 , 1 ] . get_legend () . remove () ax [ 1 , 0 ] . get_legend () . remove () ax [ 0 , 1 ] . legend ( framealpha = 1 , facecolor = 'white' , title = \"Success\" ) fig . savefig ( \"new_categorical_dependency.svg\" ) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 ), sharex = 'col' , sharey = 'row' ) ind_feature = 'mood_1' target = 'new_success' genre_list = chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False )[ 'group' ] . values chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False ) . to_excel ( 'table.xlsx' ) dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'percent' ] dff = dff . reset_index () dff . loc [ dff [ target ] == True , 'percent' ] = dff . loc [ dff [ target ] == True , 'percent' ] / dff . loc [ dff [ target ] == True , 'percent' ] . sum () dff . loc [ dff [ target ] == False , 'percent' ] = dff . loc [ dff [ target ] == False , 'percent' ] / dff . loc [ dff [ target ] == False , 'percent' ] . sum () dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 0 , 0 ]) ax [ 0 , 0 ] . set_title ( 'Best and Worst Genres, Percent' ) ax [ 0 , 0 ] . set_ylabel ( '' ) ax [ 0 , 0 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 1 , 0 ]) # ax[1,0].set_title('Worst Primary Genres') ax [ 1 , 0 ] . set_ylabel ( '' ) dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'count' ] dff = dff . reset_index () # dff.loc[dff[target] == True, 'percent'] = dff.loc[dff[target] == True, 'percent'] / dff.loc[dff[target] == True, 'percent'].sum() # dff.loc[dff[target] == False, 'percent'] = dff.loc[dff[target] == False, 'percent'] / dff.loc[dff[target] == False, 'percent'].sum() dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () # fix, ax = plt.subplots(2, 2, figsize=(10,10)) sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 0 , 1 ]) ax [ 0 , 1 ] . set_title ( 'Best and Worst Genres, Count' ) ax [ 0 , 1 ] . set_ylabel ( '' ) ax [ 0 , 1 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 1 , 1 ]) # ax[1,1].set_title('Worst Primary Genres') ax [ 1 , 1 ] . set_ylabel ( '' ) plt . tight_layout () ax [ 0 , 0 ] . get_legend () . remove () ax [ 1 , 1 ] . get_legend () . remove () ax [ 1 , 0 ] . get_legend () . remove () ax [ 0 , 1 ] . legend ( framealpha = 1 , facecolor = 'white' , title = \"Success\" ) fig . savefig ( \"new_categorical_dependency_mood.svg\" ) fig , (( ax1 , ax2 ),( ax3 , ax4 )) = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) con_features = [ 'n_albums' , 'n_artists' , 'n_tracks' , 'n_local_tracks' ] chidf = pd . DataFrame () target = \"new_success\" chidf [ target ] = df [ target ] welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind , ax in zip ( con_features , [ ax1 , ax2 , ax3 , ax4 ]): chidf [ ind ] = df [ ind ] a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } == True\" , ax = ax ) sns . histplot ( b , label = f \" { target } == False\" , ax = ax ) ax . set_title ( ind ) welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) welchsum . to_excel ( \"new_ttest.xlsx\" ) ax . legend () fig . savefig ( \"new_ttest.svg\" ) chidf = pd . DataFrame () target = \"success\" chidf [ target ] = df [ target ] # chidf.iloc[:int(chidf.shape[0]/2),:] = True # chidf.iloc[int(chidf.shape[0]/2):,:] = False # quant_value = 0.99 # tar_value = np.quantile(chidf[target], quant_value) # chidf[target] = chidf[target] > tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } == True\" ) sns . histplot ( b , label = f \" { target } == False\" ) plt . title ( ind ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True )","title":"Listen and User Conversions, MAU Growing"},{"location":"extras/X3_Spotify-Copy1/#considering-outliers","text":"df = df . loc [ df [ targets ] . apply ( lambda x : ( x < 3 * x . std ()) if ( x . dtype == int or x . dtype == float ) else x ) . all ( axis = 1 )] df = df . loc [ df [ 'owner' ] != 'spotify' ]","title":"Considering outliers"},{"location":"extras/X3_Spotify-Copy1/#multiple-criteria-for-success","text":"df [ 'success' ] = df [ sub_targets ] . apply ( lambda x : x > np . quantile ( x , 0.75 )) . all ( axis = 1 ) chidf = pd . DataFrame () target = 'success' chidf [ target ] = df [ target ] # quant_value = 0.90 # tar_value = np.quantile(chidf[target], quant_value) # chidf[target] = chidf[target] > tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum . loc [ chisum [ 'reject null' ] == True ] . sort_values ( 'multiplier' , ascending = False ) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 ), sharex = 'col' , sharey = 'row' ) genre_list = chisum . loc [ chisum [ 'feature' ] == 'genre_1' ] . sort_values ( 'multiplier' , ascending = False )[ 'group' ] . values dff = pd . DataFrame ( df . groupby ([ 'genre_1' ])[ 'success' ] . value_counts ( sort = False )) dff . columns = [ 'percent' ] dff = dff . reset_index () dff . loc [ dff [ 'success' ] == True , 'percent' ] = dff . loc [ dff [ 'success' ] == True , 'percent' ] / dff . loc [ dff [ 'success' ] == True , 'percent' ] . sum () dff . loc [ dff [ 'success' ] == False , 'percent' ] = dff . loc [ dff [ 'success' ] == False , 'percent' ] / dff . loc [ dff [ 'success' ] == False , 'percent' ] . sum () dff = dff . set_index ( 'genre_1' ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = 'success' , y = 'genre_1' , x = 'percent' , ax = ax [ 0 , 0 ]) ax [ 0 , 0 ] . set_title ( 'Best and Worst Genres, Percent' ) ax [ 0 , 0 ] . set_ylabel ( '' ) ax [ 0 , 0 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = 'success' , y = 'genre_1' , x = 'percent' , ax = ax [ 1 , 0 ]) # ax[1,0].set_title('Worst Primary Genres') ax [ 1 , 0 ] . set_ylabel ( '' ) dff = pd . DataFrame ( df . groupby ([ 'genre_1' ])[ 'success' ] . value_counts ( sort = False )) dff . columns = [ 'count' ] dff = dff . reset_index () # dff.loc[dff['success'] == True, 'percent'] = dff.loc[dff['success'] == True, 'percent'] / dff.loc[dff['success'] == True, 'percent'].sum() # dff.loc[dff['success'] == False, 'percent'] = dff.loc[dff['success'] == False, 'percent'] / dff.loc[dff['success'] == False, 'percent'].sum() dff = dff . set_index ( 'genre_1' ) . loc [ genre_list ,:] dff = dff . reset_index () # fix, ax = plt.subplots(2, 2, figsize=(10,10)) sns . barplot ( data = dff . iloc [: 10 ,:], hue = 'success' , y = 'genre_1' , x = 'count' , ax = ax [ 0 , 1 ]) ax [ 0 , 1 ] . set_title ( 'Best and Worst Genres, Count' ) ax [ 0 , 1 ] . set_ylabel ( '' ) ax [ 0 , 1 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = 'success' , y = 'genre_1' , x = 'count' , ax = ax [ 1 , 1 ]) # ax[1,1].set_title('Worst Primary Genres') ax [ 1 , 1 ] . set_ylabel ( '' ) plt . tight_layout () ax [ 0 , 0 ] . get_legend () . remove () ax [ 1 , 1 ] . get_legend () . remove () ax [ 1 , 0 ] . get_legend () . remove () ax [ 0 , 1 ] . legend ( framealpha = 1 , facecolor = 'white' , title = \"Success\" ) fig . savefig ( \"categorical_dependency.svg\" ) ind = 'n_tracks' target = 'wau' mean_wau_vs_track = [] for track in range ( 1 , 201 ): means = [] for i in range ( 10 ): boot = random . sample ( list ( df . loc [ ( df [ 'success' ] == True ) & ( df [ ind ] == track ) ][ target ] . values ), k = min ( len ( list ( df . loc [ ( df [ 'success' ] == True ) & ( df [ ind ] == track ) ][ target ] . values )), 1000 )) means . append ( np . mean ( boot )) mean_wau_vs_track . append ( np . mean ( means )) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) plt . plot ( range ( len ( mean_wau_vs_track )), mean_wau_vs_track , ls = '' , marker = '.' ) # ax.set_ylim(0,5) len ( df . loc [ ( df [ 'success' ] == True ) & ( df [ ind ] == track ) ][ target ] . values )","title":"Multiple Criteria for Success"},{"location":"extras/X3_Spotify-Copy1/#dependency","text":"master = pd . DataFrame () for target in new_metrics : # target = sub_targets[0] chidf = pd . DataFrame () chidf [ target ] = df [ target ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) tar_value = 0.8 chidf [ target ] = chidf [ target ] >= tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ target , ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'target' , 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) # chisum = chisum.loc[(chisum['reject null'] == True) & (chisum['multiplier'] > 2)].sort_values('multiplier', ascending=False) master = pd . concat (( master , chisum )) master master . loc [( master [ 'reject null' ] == True ) & ( master [ 'multiplier' ] > 1.5 )] master . loc [( master [ 'reject null' ] == True ) & ( master [ 'multiplier' ] < .5 )] new_master = pd . DataFrame () for target in new_metrics : # target = sub_targets[2] chidf = pd . DataFrame () chidf [ target ] = df [ target ] chidf [ 'n_tracks' ] = df [ 'n_tracks' ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) tar_value = 0.8 chidf [ target ] = chidf [ target ] >= tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ target , ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } >= { tar_value : .0f } \" ) sns . histplot ( b , label = f \" { target } < { tar_value : .0f } \" ) plt . title ( f \" { target } , { ind } \" ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'target' , 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) new_master = pd . concat (( new_master , welchsum )) new_master","title":"Dependency"},{"location":"extras/X3_Spotify-Copy1/#conclusions","text":"","title":"Conclusions"},{"location":"extras/X3_Spotify-Copy1/#discrete-independent-variables","text":"We note that there is class imbalance in the discrete independent variables: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 10 )) dff = pd . DataFrame ( df [ des_features [ 0 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 1 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 2 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'mood' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'mood' , x = 'count' , orient = 'h' , ax = ax [ 0 ]) dff = pd . DataFrame ( df [ des_features [ 3 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 4 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 5 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'genre' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'genre' , x = 'count' , orient = 'h' , ax = ax [ 1 ]) plt . tight_layout () This class imbalance can have a variety of effects (and might be derived from a variety of sources). For example, users will have more choice when listening to popular genres likeIndie Rock and Rap, and less choice with genres like Blues and Easy listening. As it so happens, when we look to the relationship between genre/mood and the dependent variables, many of the genre/moods with smaller class sizes will have a positive multiplier effect on the dependent variable","title":"Discrete, Independent Variables"},{"location":"extras/X3_Spotify-Copy1/#continuous-independent-variables","text":"The four continuous variables of focus in this dataset are highly tailed. Due to this, our statistical tests will require bootstrapping. quant = 0.999 con_features = [ 'n_albums' , 'n_artists' , 'n_tracks' , 'n_local_tracks' ] for target in con_features : cutoff = np . quantile ( df [ target ], quant ) y = df . loc [ df [ target ] < cutoff ] removed = df . loc [ ~ ( df [ target ] < cutoff )] print ( f \"removed items: { removed . shape [ 0 ] } \" ) y . plot ( kind = 'hist' , y = target , bins = 100 , density = True ) plt . show () removed items: 404 removed items: 405 removed items: 404 removed items: 406 an example of bootstrapping n_albums means = [] ind = con_features [ 0 ] for i in range ( 100 ): boot = random . sample ( list ( df . loc [ ( df [ ind ] > 9 ) & ( df [ ind ] < 999 ) ][ ind ] . values ), k = 1000 ) means . append ( np . mean ( boot )) stuff = plt . hist ( means , bins = 100 , density = True )","title":"Continuous, Independent Variables"},{"location":"extras/X3_Spotify-Copy1/#discrete-dependent-variables","text":"For the purposes of investigating a \"successful\" playlist, there are 5 primary metrics: targets df [ sub_targets ] . describe () . round ( 1 ) . to_excel ( \"file.xlsx\" ) and \"top\" performers in each of these metrics were based on top 10% and top 1% quantiles: print ( 'p99 targets' ) for target in sub_targets : space = ' ' * ( 20 - len ( str ( target ))) print ( f \" { target } : { space } { np . quantile ( df [ target ], 0.99 ) } \" ) print () print ( 'p90 targets' ) for target in sub_targets : space = ' ' * ( 20 - len ( str ( target ))) print ( f \" { target } : { space } { np . quantile ( df [ target ], 0.90 ) } \" ) You can imagine with these metrics, some concerns are: what if a playlist was made in the current month, or even current day? playlist is not properly represented by the data how do we normalize by playlists that already have a high visibility? i.e. what if a playlist is \"good\" but just isn't getting noticed? can compute conversion metrics: 30 second listens / total listens mau both months / mau previous month While noting these shortcomings, to keep the analysis focused I singled out the previously mentioned targets, with a focus on monthly_stream30s as the north star metric. monthly_stream30s is advantageous as a nort star metric since it contains data from the entire month (reducing variance) only contains relevant listens (greater than 30 seconds long). Some disadvantages of this metric are that it doesn't account for just a few listeners who may be providing the majority of listens, and playlists that were made in the current month will be undervalued.","title":"Discrete, Dependent Variables"},{"location":"extras/X3_Spotify-Copy1/#dependency_1","text":"","title":"Dependency"},{"location":"extras/X3_Spotify-Copy1/#chi-square","text":"In the chi-square test, the contigency table was used to calculate a multiplier effect. This is a ratio of ratios: the count of upper quantile over bottom quantile for the given group over the count of upper quantile over bottom quantile for non-group. In other words, it articulates how much more likely a sample in the given group is likely to be in the upper quantile vs a sample not in the given group chisq_results = pd . read_csv ( \"chi_square_results.csv\" , index_col = 0 ) chisq_results . head () chisq_results [ 'target' ] . unique () chisq_results [ 'upper q' ] . unique () Taking together the five targets, the two upper quantiles, and the six categorical independent variables, we can identify which group occured the most frequently as a variable of influence: chisq_results . loc [( chisq_results [ 'feature' ] . str . contains ( 'genre' )) & ( chisq_results [ 'group' ] != '-' )][ 'group' ] . value_counts () Using these value counts as a \"rank\" we can then groupby this rank and see how each group is influencing the propensity to be in the upper quadrant Taking \"Romantic\" as an example, we see that it's multiplier effect is relatively consistent across the five targets and two quantiles: sort_key = { i : j for i , j in zip ( chisq_results [ 'group' ] . value_counts () . index . values , range ( chisq_results [ 'group' ] . nunique ()))} chisq_results [ 'rank' ] = chisq_results [ 'group' ] . apply ( lambda x : sort_key [ x ]) chisq_results . sort_values ( 'rank' , inplace = True ) # chisq_results.drop('rank', axis=1, inplace=True) chisq_results . loc [ chisq_results [ 'group' ] != '-' ][: 20 ] chisq_results . loc [( chisq_results [ 'group' ] == 'Traditional' ) & ( chisq_results [ 'target' ] == 'monthly_stream30s' )] Let's use this idea of average multiplier effect, and average chi-square statistic to summarize by group. Sorting by the test statistic, we see the top 5 most influential groups: chisq_results . groupby ( 'group' )[[ 'chi' , 'multiplier' , 'rank' ]] . mean () . sort_values ( 'chi' , ascending = False )[: 10 ] Sorting instead by the multiplier, we can see which group has the heaviest influence chisq_results . groupby ( 'group' )[[ 'chi' , 'multiplier' , 'rank' ]] . mean () . sort_values ( 'multiplier' , ascending = False )[: 10 ] Sorting instead by rank, we see which groups show up most frequently chisq_results . groupby ( 'group' )[[ 'chi' , 'multiplier' , 'rank' ]] . mean () . sort_values ( 'rank' , ascending = True )[: 10 ] chisq_results . loc [ chisq_results [ 'target' ] == 'monthly_stream30s' ] It creates some fog to jumble together mood/genres this way. We can instead separate them and ask questions like:","title":"Chi Square"},{"location":"extras/X3_Spotify-Copy1/#what-is-the-most-influential-primary-genre-on-monthly-streams-over-30-seconds","text":"Answer: Children's followed by Latin Reason: both genre's appear as influential in other guardrail metrics (high rank), have high test statistics, and are influential in both p99 and p90 with multiplier effects of [4.8, 2.6] and [3.1, 2.1], respectively. chisq_results . loc [( chisq_results [ 'feature' ] == 'genre_1' ) & ( chisq_results [ 'target' ] == 'monthly_stream30s' )]","title":"What is the most influential primary genre on monthly streams over 30 seconds?"},{"location":"extras/X3_Spotify-Copy1/#what-is-the-most-influential-primary-mood-on-monthly-streams-over-30-seconds","text":"Answer: Romantic and Lively Reason: Romantic and Lively moods appear multiple times as highly influential (high rank) they have high multipliers. A contendent may be Tender, as it has a high multiplier effect as well at 3.75 chisq_results . loc [( chisq_results [ 'feature' ] == 'mood_1' ) & ( chisq_results [ 'target' ] == 'monthly_stream30s' )]","title":"What is the most influential primary mood on monthly streams over 30 seconds?"},{"location":"extras/X3_Spotify-Copy1/#which-categorical-feature-is-most-influential-overall","text":"Answer: genre_1, followed by genre_2 and mood_1 Reason: we see that these features appear multiple times across the 5 different targets and 2 different quantiles chisq_results [ 'feature' ] . value_counts ()","title":"Which Categorical Feature is most influential overall?"},{"location":"extras/X3_Spotify-Copy1/#what-are-the-shortcomings-of-this-analysis","text":"We haven't taken into account confounding variables. For example, perhaps Latin genre is typically associated with Lively mood. Then which variable is it that actually contributes to a highly performing playlist? We have strategies for dealing with this. We can stratify the confounding variables by over or under sampling. We can also consider them together in a forward selection logistic model. We will take the latter approach later on in the analysis. We haven't considered the categorical variables alongside the continuous variables, so we don't know how they fit overall in terms of relative improtance. We will approach this the same way as the confounding variables issue, and incorporate all variables in a logistic regression.","title":"What are the shortcomings of this analysis?"},{"location":"extras/X3_Spotify-Copy1/#t-test","text":"ttest_results = pd . read_csv ( \"t_test_results.csv\" , index_col = 0 ) ttest_results . head ()","title":"t-Test"},{"location":"extras/X3_Spotify-Copy1/#models","text":"log_results = pd . read_csv ( \"../../scripts/fwd_selection_results.txt\" , header = None , index_col = 0 ) log_results . columns = [ 'feature' , 'pseudo r2' ] log_results . reset_index ( inplace = True , drop = True ) log_results . drop ( 0 , axis = 0 , inplace = True ) log_results target = \"monthly_stream30s\" y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = 0 names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = 1 weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels basemodel = pd . read_csv ( \"../../scripts/basemodel.csv\" , index_col = 0 ) X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns res_df","title":"Models"},{"location":"extras/X3_Truffletopia/","text":"Causality Analysis \u00b6 TODO \u00b6 ols assumptions for univariate analyses residual plots forward selection lines up more or less with chi square tests include some colinearity measure? logistic regression (binned seconds, binned total outs) random forest does little better than random guessing business impact reorganize hypothesis tests section univariate vs multivariate cat v vat, cat v quant, quant v quant, quant v cat 1.0 Imports \u00b6 Import Libraries \u00b6 # basic packages import pandas as pd import numpy as np import random import copy # visualization packages from ipywidgets import interact import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures , MinMaxScaler from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , silhouette_score , calinski_harabasz_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score def make_patch_spines_invisible ( ax ): ax . set_frame_on ( True ) ax . patch . set_visible ( False ) for sp in ax . spines . values (): sp . set_visible ( False ) from matplotlib import colors from itertools import cycle def my_colors (): tab = cycle ( colors . TABLEAU_COLORS ) return tab Load Data \u00b6 consider only feeds consider feeds and operating conditions # df = pd.read_csv('../../data/Dupont - start dataset for hypothesis - 20190524.csv', header=0) df = pd . read_csv ( '../../../data/dupont/TyparMasterReportWithSampleDataImprovement2017-today.csv' , header = 1 ) df = df . loc [ df [ 'Product' ] . notnull ()] df [ feeds ] = df [ feeds ] . div ( df [ feeds ] . sum ( axis = 1 ), axis = 0 ) #normalize feed rates df [ 'Time Delta' ] = pd . to_datetime ( df [ 'Doff Time' ]) . diff () df [ 'Total Seconds Out' ] = df [[ i for i in df . columns if 'Sec' in i ]] . sum ( 1 ) # df.dropna(inplace=True) hangs = [ df . columns [ i ] for i , j in enumerate ( 'Hang' in col for col in df . columns ) if j ] speeds = [ df . columns [ i ] for i , j in enumerate ( 'speed' in col for col in df . columns ) if j ] outs = [ df . columns [ i ] for i , j in enumerate ( 'Out' in col for col in df . columns ) if j ] prods = df [ 'Product' ] . unique () counts = [ i for i in df . columns if 'Count' in i ] #from LOEWEN virgin_polymer = [ 'PP INEOS 100 GA04' , 'PP POLYCHIM HB12XF' , 'PP TOTAL 7089' , #INEOS 100 GA09 until 2018 (2k rolls) 'PP TOTAL PPH 4065' , 'PP UNIPETROL GB005' , 'PP POLYCHIM HL10XF' ] #HL10XF not used after 2016, #BOREALIS virgin_polymer = [] for col in df . columns : if ( 'PP INEOS' in col ) or ( 'PP TOTAL' in col ) or ( 'PP UNI' in col ) or ( 'PP BORE' in col ) or ( 'PP POLY' in col ): virgin_polymer . append ( col ) pressures = [] for col in df . columns : if 'Press' in col or 'PRESS' in col : pressures . append ( col ) silicates = [] for col in df . columns : if 'SIL' in col : silicates . append ( col ) uvs = [] for col in df . columns : if 'UV ' in col : uvs . append ( col ) recycle = [] for col in df . columns : if 'MFR' in col : recycle . append ( col ) seconds = [] for col in df . columns : if 'Sec' in col : seconds . append ( col ) feeds = [ 'ADD ARGUS ARGUSTAT AT 31 HK' , 'ADD ARGUS ARGUTHERM AO 77 PP' , 'ADD POLYPLAST PPM AO 01 UV PP' , 'ADD SCHULMAN DTY 20 AOUV' , 'BEIGE SCHULMAN PZ 91738 1 5 ZA' , 'BLACK SCHULMAN P7381' , 'DEVAN 2SPIN PP 106' , 'GREEN POLYPLAST COLOUR MB PP 6821' , 'GREEN SCHULMAN PZ 302446 1 5 T' , 'High MFR Black' , 'GREEN SCHULMAN PZ 34198 1 5 T' , 'MFR 4 Black' , 'MFR 4 Black Brown' , 'High MFR Grey' , 'MFR 4 Green' , 'PP BOREALIS HE465FB' , 'MFR 4 Black Colour Blending' , 'PP INEOS 100 GA02' , 'PP INEOS 100 GA04' , 'PP INEOS 100 GA09' , 'MFR 4 Grey Colour Blending' , 'PP INEOS GA012' , 'PP POLYCHIM HB12XF' , 'MFR 4 White' , 'PP TOTAL 7089' , 'PP TOTAL PPH 4065' , 'MFR 4 White Colour Blending' , 'PP BOREALIS HE370FB' , 'PP UNIPETROL GB005' , 'SILIKAT PBH ABPP 05' , 'SILIKAT POLYPLUS AB 1001 PP' , 'UV ARGUS ARGUVIN UV 361 PP' , 'UV ARGUS ARX 904 11 PP' , 'UV ARGUS ARX 904 11 PP.1' , 'UV ARGUS ARX 904 11 PP.2' , 'UV POLYPLAST 6005 PP' , 'UV SCHULMAN FPPUV 38' , 'MFR 4 Grey' , 'PP POLYCHIM HL10XF' , 'MFR 4 Grey Grey' , 'PP POLYMER TEST MFI4' , 'PP TOTAL 4069' , 'RR MASTERBATCH TEST' , 'UV ARGUS ARX V 16 706 UV PP' ] master_batch = list ( set ( feeds ) - set ( recycle ) - set ( virgin_polymer )) operation = list ( set ( df . columns ) - set ( master_batch ) - set ( virgin_polymer ) - set ( recycle ) - set ( hangs ) - set ( outs )) operation . sort () A summary of the columns print ( f 'pressures: { len ( pressures ) } ' ) print ( f 'silicates: { len ( silicates ) } ' ) print ( f 'uvs: { len ( uvs ) } ' ) print ( f 'feeds: { len ( feeds ) } ' ) print ( f 'master_batch: { len ( master_batch ) } ' ) print ( f 'virgin_polymer: { len ( virgin_polymer ) } ' ) print ( f 'recycle: { len ( recycle ) } ' ) print ( f 'operation: { len ( operation ) } ' ) print () print ( f 'hangs: { len ( hangs ) } ' ) print ( f 'speeds: { len ( speeds ) } ' ) print ( f 'outs: { len ( outs ) } ' ) print ( f 'seconds: { len ( seconds ) } ' ) print () print ( 'summary target columns: TotalHangs, Total Position Out' ) print () print ( df . shape ) pressures: 8 silicates: 2 uvs: 8 feeds: 44 master_batch: 20 virgin_polymer: 13 recycle: 11 operation: 180 hangs: 9 speeds: 9 outs: 10 seconds: 9 summary target columns: TotalHangs, Total Position Out (17897, 243) all_feats = df . select_dtypes ( float ) . columns [ df . select_dtypes ( float ) . columns . isin ( feeds + operation )] . values 2.0 Understand The Dependent Variable \u00b6 What is the hit rate in these upset columns? \u00b6 Around 7500 cases with no problem; 10000 cases with a position out target = \"Total Position Out\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 10355 False 7542 Name: Total Position Out, dtype: int64 There are 30% higher incidence of hangs target = \"TotalHangs\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 13954 False 3943 Name: TotalHangs, dtype: int64 target1 = \"TotalHangs\" target2 = \"Total Position Out\" print ( f \"both upsets: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"only hangs: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) print ( f \"only outs: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"neither upset: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) both upsets: 10161 only hangs: 3793 only outs: 194 neither upset: 3749 What is the magnitude of the outages? \u00b6 target = \"Total Position Out\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f163a9867c0>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total Position Out Unnamed 0 Total Position Out 1.00000 0.62591 Unnamed 0 0.62591 1.00000 target = \"TotalHangs\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f163caf6190>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TotalHangs Unnamed 0 TotalHangs 1.000000 0.390949 Unnamed 0 0.390949 1.000000 Distribution around the target variable, total seconds \u00b6 We will certainly encounter trouble in an OLS estimate with this shape of the target variable; extreme skewness, upper tail, perhaps outliers tot_seconds = df [ seconds ] . sum ( 1 ) tot_seconds . kurt () 923.6728632470827 tot_seconds . skew () 25.09323820232841 tot_seconds . plot . kde () <AxesSubplot:ylabel='Density'> tot_seconds . describe () count 17897.000000 mean 2093.512879 std 8060.466090 min 0.000000 25% 20.000000 50% 666.000000 75% 1970.000000 max 396570.000000 dtype: float64 Do block positions have the same behavior? \u00b6 target = \"TotalHangs\" df . groupby ( target )[ hangs ] . count () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs TotalHangs 0.0 0 0 0 0 0 0 0 0 3943 1.0 510 682 745 977 510 682 745 977 2914 2.0 709 761 901 1109 709 761 901 1109 2424 3.0 721 776 930 1103 721 776 930 1103 1929 4.0 700 773 875 993 700 773 875 993 1577 5.0 609 668 744 828 609 668 745 828 1189 6.0 523 498 599 631 523 498 599 631 883 7.0 387 414 472 505 387 414 472 505 640 8.0 321 338 376 409 321 338 376 409 503 9.0 286 275 305 339 286 275 305 339 405 10.0 219 206 244 248 219 206 244 248 296 11.0 144 154 168 171 144 154 168 171 203 12.0 148 139 159 171 148 139 159 171 191 13.0 105 100 117 106 105 100 117 106 131 14.0 93 88 95 96 93 88 95 96 109 15.0 97 89 94 97 97 89 94 97 108 16.0 70 64 72 76 70 64 72 76 81 17.0 51 50 51 47 51 50 51 47 55 18.0 29 30 32 33 29 30 32 33 35 19.0 32 32 33 32 32 32 33 32 35 20.0 32 32 31 34 32 32 31 34 35 21.0 26 27 25 24 26 27 25 24 28 22.0 21 20 24 24 21 20 24 24 24 23.0 23 23 24 23 23 23 24 23 25 24.0 16 17 18 19 16 17 18 19 20 25.0 7 8 8 7 7 8 8 7 8 26.0 11 10 10 11 11 10 10 11 11 27.0 8 9 9 8 8 9 9 8 9 28.0 10 10 10 10 10 10 10 10 10 29.0 6 5 6 6 6 5 6 6 6 30.0 4 3 4 4 4 3 4 4 4 31.0 8 8 8 8 8 8 8 8 8 32.0 4 3 4 4 4 3 4 4 4 33.0 7 5 7 6 7 5 7 6 7 34.0 9 8 9 8 9 8 9 8 9 35.0 6 6 6 6 6 6 6 6 6 36.0 6 5 6 6 6 5 6 6 6 37.0 3 3 3 3 3 3 3 3 3 38.0 1 1 1 0 1 1 1 0 1 39.0 1 1 1 1 1 1 1 1 1 40.0 2 2 2 2 2 2 2 2 2 42.0 1 1 1 1 1 1 1 1 1 43.0 3 3 3 3 3 3 3 3 3 45.0 3 3 3 3 3 3 3 3 3 46.0 2 2 2 2 2 2 2 2 2 48.0 1 1 1 1 1 1 1 1 1 52.0 1 1 1 1 1 1 1 1 1 53.0 1 1 1 1 1 1 1 1 1 54.0 1 1 1 1 1 1 1 1 1 58.0 1 1 1 1 1 1 1 1 1 62.0 1 1 1 1 1 1 1 1 1 63.0 1 1 1 1 1 1 1 1 1 70.0 1 1 1 1 1 1 1 1 1 72.0 1 1 1 1 1 1 1 1 1 140.0 1 1 1 1 1 1 1 1 1 df [[ i for i in outs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:> df [[ i for i in hangs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:> 3.0 Look for dependent-independent signal \u00b6 Are there linear relationships between the dependent and independent variables? \u00b6 corr = df . corr () ( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 ) Product False Roll Width False Block1Pos Out Seconds True Block2Pos Out Seconds True Block3Pos Out Seconds True ... UPEX - UWU PLOT EXTREM (UPEX) False UWG - UWU GRAVIMETRIC WITH SCALE (UWG) False UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP) False WATP - WATER PERMEABILITY (WATP) False Total Seconds Out True Length: 237, dtype: bool cor_filt = corr . loc [( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 )] . drop ( labels = outs + hangs , axis = 0 )[ outs + hangs ] cor_filt .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Pos Out Seconds Block2Pos Out Seconds Block3Pos Out Seconds Block4Pos Out Seconds Block1Pos Out Count Block2Pos Out Count Block3Pos Out Count Block4Pos Out Count Total Position Out Total Seconds Out Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs ADD ARGUS ARGUSTAT AT 31 HK -0.103999 -0.417688 -0.383222 0.202131 -0.056194 -0.412274 0.155126 0.145535 0.014216 0.029563 0.121682 -0.256723 -0.012676 0.099690 -0.069354 -0.041492 0.094150 0.041900 0.344896 PP INEOS 100 GA02 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 -1.000000 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 MFR 4 White 0.289648 -0.016268 0.451446 -0.155932 0.297256 -0.000098 0.376795 -0.003129 0.225425 0.182687 0.231764 -0.045076 0.283286 -0.119481 0.108396 -0.057188 0.317089 0.008018 0.189109 PP TOTAL 7089 0.160268 0.171023 0.210796 0.093720 0.192159 0.210942 0.206859 0.076058 0.260505 0.207522 0.087954 0.050670 0.052622 -0.050506 0.218148 0.251676 0.134881 0.121080 0.331897 SILIKAT POLYPLUS AB 1001 PP 0.015132 0.125801 0.379559 0.255662 -0.018715 0.012019 0.161698 0.067661 0.127830 0.137376 -0.016317 -0.018187 -0.018956 0.086781 0.011207 -0.020307 0.092291 0.109259 0.069217 RJET15219WG1 - Corona Charge I AVG BLK#2 -0.120204 -0.120509 -0.056377 -0.002054 -0.198138 -0.221189 -0.193393 -0.024541 -0.232863 -0.149862 -0.090896 -0.060404 -0.054169 -0.052683 -0.215325 -0.216545 -0.182990 -0.120387 -0.308217 BLFL - COLOUR BLACK FILAMENT L value (BLFL) -0.360086 -0.157845 -0.245565 -0.268837 -0.153321 -0.162697 -0.212481 -0.247721 -0.067632 -0.121690 0.097783 0.146515 -0.346923 -0.110987 0.402310 -0.274153 -0.378670 -0.116264 -0.117501 BRFA - COLOUR BROWN FILAMENT a value (BRFa) 0.159056 0.250229 -0.384880 0.109024 0.134248 0.112836 -0.380715 -0.054074 0.099040 0.166926 0.170184 -0.065344 -0.195062 0.094596 0.195404 -0.333632 -0.026209 0.177576 0.173995 BRFB - COLOUR BROWN FILAMENT b value (BRFb) 0.413876 0.047748 0.332752 0.342115 0.145794 -0.028170 0.246787 0.234624 0.216156 0.311363 0.148441 -0.210193 0.118275 0.285286 -0.027055 -0.275750 0.100830 0.328240 0.164345 BRFL - COLOUR BROWN FILAMENT L value (BRFL) 0.198864 -0.178050 0.483242 0.205796 0.017456 -0.220432 0.411110 0.090885 0.039031 0.128326 0.337130 -0.326368 0.075738 0.145878 0.195055 -0.467536 -0.075130 0.188013 -0.000575 E20M AVG - SST ELONGATION 20X20 MD (E20M) 0.106870 -0.096775 -0.249986 -0.039633 0.009518 -0.049131 -0.367065 -0.074018 -0.070058 -0.071425 -0.124456 -0.016993 -0.070312 -0.036314 -0.145788 -0.075968 -0.197153 -0.069477 -0.072872 EF1M AVG - SST ELONGATION FOLDED 10 CM MD (EF1M) 1.000000 NaN 1.000000 -0.081738 1.000000 NaN NaN NaN -0.101606 -0.178993 0.313364 -0.936781 -1.000000 -0.517401 0.306235 -0.950303 -1.000000 -0.324932 -0.112334 EF1X AVG - SST ELONGATION FOLDED 10 CM XD (EF1X) 1.000000 NaN 1.000000 0.126777 1.000000 NaN NaN NaN 0.010039 -0.063399 0.896084 -0.961383 -1.000000 -0.609904 0.892730 -0.971830 -1.000000 -0.340772 0.050483 NATM AVG - Nail Tear MD EN 13859-1 (NATM) -0.517421 -0.835147 0.511409 -0.258360 -0.296500 -0.906038 NaN -0.333754 -0.100244 0.013436 -0.338129 0.580493 0.826191 -0.322585 -0.348782 0.063309 0.929207 -0.430201 0.086484 NATX AVG - Nail Tear XD EN 13859-1 (NATX) 0.269389 -0.864128 0.542651 0.071499 -0.053990 -0.927938 NaN -0.056465 -0.084131 0.046366 -0.190830 0.541297 0.777131 -0.114506 -0.096640 0.051081 0.902927 -0.295087 0.093804 O90 - O90 EN ISO 12956 (O90E) NaN NaN NaN 0.807962 NaN NaN NaN NaN -0.265093 -0.268107 -0.280791 -0.378899 NaN -0.451610 -0.307148 -0.267261 NaN -0.269680 -0.193539 PU AVG - PUNCTURE \"US Rod\" (PU) 0.173625 -0.790779 -0.165265 0.451194 -0.127912 -0.952390 -0.184515 0.734933 -0.040323 -0.021124 0.096383 -0.998695 -0.288786 0.156274 0.066142 0.363427 -0.297118 0.479459 -0.239496 TKO4 AVG - Tack-Tear Ost 4Meter -0.170480 -0.383667 -0.326335 0.006590 -0.104058 -0.308260 -0.329544 0.047734 -0.171814 -0.195662 -0.214633 -0.440748 -0.182875 -0.027353 -0.077638 -0.447423 -0.187893 0.114764 -0.160340 TKO5 AVG - Tack - Tear Ost 5 Meter 1.000000 0.134107 0.560717 -0.070116 NaN 0.106485 0.152594 -0.525375 -0.373817 -0.306252 0.338272 0.155577 0.034557 -0.453643 0.286066 0.056270 -0.066270 -0.400941 -0.387297 TKW4 AVG - Tack-Tear West 4Meter -0.334291 -0.282220 -0.265607 -0.134467 -0.320379 -0.231417 -0.300679 -0.265049 -0.274024 -0.250617 -0.277325 -0.375559 -0.168965 -0.154119 -0.236424 -0.282687 -0.168865 -0.162801 -0.316193 TKW5 AVG - Tack - Tear West 5 Meter 1.000000 0.051583 0.100609 0.018368 NaN 0.014194 -0.167032 -0.366762 -0.279911 -0.190366 0.069184 0.108657 0.224892 -0.326866 -0.003422 0.043730 0.039670 -0.362928 -0.270898 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt , cmap = cmap ) <AxesSubplot:> Filter N > 100 \u00b6 cor_filt = cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]] fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]], cmap = cmap ) <AxesSubplot:> Operating Conditions \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Operations Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Operations Correlations to Hangs and Outs') Feeds \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ ~ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Feeds Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Feeds Correlations to Hangs and Outs') ind_vars = list ( cor_filt . index [ ~ cor_filt . index . isin ( operation )]) ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] fig , ax = plt . subplots ( 1 , 3 , figsize = ( 30 , 10 )) indices = np . argwhere ( ax ) for index , ind_var in enumerate ( ind_vars ): df [[ ind_var ] + seconds ] . melt ( id_vars = ind_var , value_vars = None , var_name = None ,) . plot ( x = ind_var , y = 'value' , ax = ax [ index ], ls = '' , marker = '.' ) 4.0 Understand the Independent Variable \u00b6 Descriptive Stats on Ind Var \u00b6 all the feeds sum to 1 (feeds are mass or volume fractions), so we probably won't need to worry about scaling df [ feeds ] . sum ( 1 ) 0 1.0 1 1.0 2 1.0 3 1.0 4 1.0 ... 17892 1.0 17893 1.0 17894 1.0 17895 1.0 17896 1.0 Length: 17897, dtype: float64 many of the feeds are not used at all skew = df [ feeds ] . skew () kurt = df [ feeds ] . kurtosis () null = df [ feeds ] . isnull () . sum () n = df [ feeds ] . notnull () . sum () med = df [ feeds ] . median () men = df [ feeds ] . mean () dff = pd . DataFrame ([ skew , kurt , null , n , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'null cnt' , 'n' , 'median' , 'mean' ] dff . loc [ dff [ 'n' ] > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis null cnt n median mean ADD ARGUS ARGUSTAT AT 31 HK -1.122018 -0.275303 17798.0 99.0 0.006560 0.005080 ADD ARGUS ARGUTHERM AO 77 PP 14.927951 248.300227 15627.0 2270.0 0.005249 0.010705 ADD POLYPLAST PPM AO 01 UV PP 5.788514 87.051660 16769.0 1128.0 0.006397 0.006218 ADD SCHULMAN DTY 20 AOUV 6.573319 99.052816 13484.0 4413.0 0.004997 0.005411 BEIGE SCHULMAN PZ 91738 1 5 ZA -0.455858 10.108094 16975.0 922.0 0.008922 0.008708 BLACK SCHULMAN P7381 2.301415 16.313957 4471.0 13426.0 0.000317 0.002040 DEVAN 2SPIN PP 106 0.770601 11.066370 13279.0 4618.0 0.006658 0.005554 GREEN POLYPLAST COLOUR MB PP 6821 -1.856631 3.540741 16894.0 1003.0 0.007519 0.006822 GREEN SCHULMAN PZ 302446 1 5 T 0.363239 8.864015 17150.0 747.0 0.018764 0.018006 High MFR Black 0.473484 -1.518082 14862.0 3035.0 0.047883 0.146677 MFR 4 Black 1.612264 4.261601 12595.0 5302.0 0.030099 0.098792 MFR 4 Black Brown -0.086521 -0.342944 16360.0 1537.0 0.189244 0.150273 High MFR Grey -0.364616 -1.483643 15900.0 1997.0 0.226358 0.199138 MFR 4 Green 0.149918 8.244109 15985.0 1912.0 0.279622 0.262574 PP INEOS 100 GA02 NaN NaN 17895.0 2.0 0.107927 0.107927 PP INEOS 100 GA04 1.294449 5.764334 6684.0 11213.0 0.208757 0.216941 PP INEOS 100 GA09 -0.449540 -0.968103 16802.0 1095.0 0.224353 0.219208 PP POLYCHIM HB12XF -0.222658 -0.684893 16525.0 1372.0 0.275755 0.291827 MFR 4 White 1.136619 2.053071 17559.0 338.0 0.098961 0.111129 PP TOTAL 7089 -0.216726 -1.333059 16346.0 1551.0 0.276430 0.288643 PP TOTAL PPH 4065 0.353911 0.740658 611.0 17286.0 0.441009 0.424684 PP UNIPETROL GB005 0.117291 1.045121 11444.0 6453.0 0.324773 0.295906 SILIKAT PBH ABPP 05 2.690146 26.471577 15967.0 1930.0 0.005001 0.005229 SILIKAT POLYPLUS AB 1001 PP 6.874010 77.288671 17360.0 537.0 0.002506 0.002642 UV ARGUS ARX 904 11 PP 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.1 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.2 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV POLYPLAST 6005 PP 1.426790 1.634549 17506.0 391.0 0.020009 0.026245 UV SCHULMAN FPPUV 38 2.114261 14.372541 17141.0 756.0 0.029986 0.032261 MFR 4 Grey 1.718802 10.970192 8567.0 9330.0 0.269445 0.263401 MFR 4 Grey Grey 3.607370 20.342313 15865.0 2032.0 0.040035 0.068547 UV ARGUS ARX V 16 706 UV PP 1.889895 6.474782 7972.0 9925.0 0.002110 0.007994 our ind_vars feeds have some strong correlations with other feeds In particular, PP TOTAL 7089 and SILIKAT POLYPLUS AB 1001 PP have a correlation w/ eachother of .923 ind_corr = df [ feeds ] . corr ()[ ind_vars ] . dropna ( how = 'all' ) ind_corr . loc [( abs ( ind_corr ) > 0.5 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 ind_vars feeds exhibit bimodal distributions (2+ recipes) for ind in ind_vars : df [ ind ] . plot . kde () plt . title ( ind ) plt . xlim ( 0 ) plt . show () 5.0 Hypothesis Tests \u00b6 Non-Parametric \u00b6 Univariate Categorical to Categorical (Chi-Square) \u00b6 ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 for ind in ind_vars : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 10182 False 7377 True True 173 False 165 Name: Total Position Out, dtype: int64 [[10182 173] [ 7377 165]] chi2 stat: 6.02 p-value: 1.41e-02 PP TOTAL 7089 Total Position Out False True 9171 False 7175 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[9171 1184] [7175 367]] chi2 stat: 237.00 p-value: 1.78e-53 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 9996 False 7364 True True 359 False 178 Name: Total Position Out, dtype: int64 [[9996 359] [7364 178]] chi2 stat: 17.99 p-value: 2.22e-05 chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 chisum = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum [ 'type' ] = None chisum . loc [ chisum [ 'feed' ] . isin ( master_batch ), 'type' ] = 'master_batch' chisum . loc [ chisum [ 'feed' ] . isin ( recycle ), 'type' ] = 'recyle' chisum . loc [ chisum [ 'feed' ] . isin ( virgin_polymer ), 'type' ] = 'virgin_polymer' chisum . loc [ chisum [ 'feed' ] . isin ( silicates ), 'type' ] = 'silicates' chisum . loc [ chisum [ 'feed' ] . isin ( uvs ), 'type' ] = 'uvs' chisum . loc [ chisum [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null type 0 PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1 PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2 High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3 SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 4 BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 5 PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 6 ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 7 DEVAN 2SPIN PP 106 35.237690 2.918209e-09 [[7511, 2844], [5768, 1774]] False True master_batch 8 High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 9 GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 10 MFR 4 Black Brown 20.494540 5.980160e-06 [[9550, 805], [6810, 732]] True True recyle 11 UV ARGUS ARX V 16 706 UV PP 20.048858 7.548859e-06 [[4465, 5890], [3507, 4035]] False True uvs 12 ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 13 SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 14 GREEN SCHULMAN PZ 302446 1 5 T 14.155700 1.682865e-04 [[9973, 382], [7177, 365]] True True master_batch 15 MFR 4 Grey Grey 9.004184 2.693622e-03 [[9464, 1141], [6401, 891]] True True recyle Check confounding variables \u00b6 Isolate Products (product conflation with feed) Let's isolate the comparison by only products made with the feed ( ind_var ) in question We see that MFR 4 White and SILIKAT POLYPLUS AB 1001 PP are very imbalanced compared to PP TOTAL 7089 target = 'Total Position Out' for ind in ind_vars : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 3978 False 2933 True True 173 False 165 Name: Total Position Out, dtype: int64 [[3978 173] [2933 165]] chi2 stat: 5.10 p-value: 2.40e-02 PP TOTAL 7089 Total Position Out False True 2301 False 1393 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[2301 1184] [1393 367]] chi2 stat: 96.05 p-value: 1.12e-22 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 2418 False 1602 True True 359 False 178 Name: Total Position Out, dtype: int64 [[2418 359] [1602 178]] chi2 stat: 8.66 p-value: 3.25e-03 target = 'Total Position Out' chisum2 = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum2 = pd . concat ([ chisum2 , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum2 . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum2 = chisum2 . sort_values ( 'p-value' ) . reset_index ( drop = True ) tests = chisum . set_index ( 'feed' ) . join ( chisum2 . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], rsuffix = ', confound' ) tests . loc [( tests [ 'reject null' ] == True ) & ( tests [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value cTable + reject null type p-value, confound reject null, confound feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True we can compare this with our feed correlation table - there may be other explanatory variables 'hiding' ind_corr . loc [( abs ( ind_corr ) > 0.3 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 BLACK SCHULMAN P7381 0.196431 -0.470838 0.082699 DEVAN 2SPIN PP 106 NaN -0.415896 0.097590 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA04 0.342077 -0.248904 0.402788 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP TOTAL PPH 4065 -0.419569 -0.160492 0.089303 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 UV ARGUS ARX 904 11 PP NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.1 NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.2 NaN -0.461986 NaN MFR 4 Grey -0.303323 -0.381548 0.038125 MFR 4 Grey Grey 0.395658 -0.102141 -0.086338 UV ARGUS ARX V 16 706 UV PP 0.246254 -0.439913 -0.084811 Univariate Categorical to Quantitative (Moods Median) \u00b6 A limitation of the previous analysis is that our threshold for counting an upset in the line was pretty low - any delay (position out > 0 seconds). A way we can naturally increase this threshold is to compare medians (of position out (secs)) across our groups. chidf = pd . DataFrame () target = 'Seconds' chidf [ target ] = df [ seconds ] . sum ( 1 ) moods = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind in feeds : try : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) grp = chidf . loc [ chidf [ ind ] == True ][ target ] . values chi2 , p , m , cTable = stats . median_test ( grp , pop , correction = False ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] < 1 moods = pd . concat ([ moods , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass moods . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] moods = moods . sort_values ( 'p-value' ) . reset_index ( drop = True ) moods . loc [ moods [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null 0 PP TOTAL 7089 284.071422 9.736103e-64 [[1093, 8620], [458, 9277]] False True 1 MFR 4 Grey Grey 161.505327 5.305934e-37 [[744, 9215], [1288, 8682]] True True 2 PP POLYCHIM HB12XF 140.079653 2.557382e-32 [[897, 8734], [475, 9163]] False True 3 High MFR Grey 134.851343 3.558380e-31 [[1244, 8697], [753, 9200]] False True 4 SILIKAT PBH ABPP 05 132.735135 1.033157e-30 [[1205, 8704], [725, 9193]] False True 5 SILIKAT POLYPLUS AB 1001 PP 40.522176 1.943972e-10 [[341, 8870], [196, 9027]] False True 6 DEVAN 2SPIN PP 106 39.379816 3.488847e-10 [[2499, 8758], [2119, 9139]] False True 7 BLACK SCHULMAN P7381 33.697655 6.437826e-09 [[6961, 8686], [6465, 9211]] False True 8 GREEN POLYPLAST COLOUR MB PP 6821 32.518968 1.180348e-08 [[589, 8854], [414, 9043]] False True 9 PP INEOS 100 GA09 26.828649 2.223170e-07 [[630, 8854], [465, 9043]] False True 10 ADD ARGUS ARGUTHERM AO 77 PP 26.802325 2.253660e-07 [[1250, 8823], [1020, 9074]] False True 11 UV ARGUS ARX V 16 706 UV PP 23.787198 1.075957e-06 [[5152, 8744], [4773, 9153]] False True 12 High MFR Black 18.389746 1.800242e-05 [[1626, 8835], [1409, 9062]] False True 13 ADD POLYPLAST PPM AO 01 UV PP 15.536764 8.091620e-05 [[499, 9000], [629, 8897]] True True 14 UV SCHULMAN FPPUV 38 12.417442 4.253424e-04 [[330, 8983], [426, 8914]] True True 15 MFR 4 White 11.470090 7.072526e-04 [[138, 8971], [200, 8926]] True True 16 GREEN SCHULMAN PZ 302446 1 5 T 9.924320 1.631091e-03 [[331, 8983], [416, 8914]] True True we see that overall the moods test is slightly less pessimistic chisum [ 'reject null' ] . value_counts () True 16 False 16 Name: reject null, dtype: int64 moods [ 'reject null' ] . value_counts () True 17 False 15 Name: reject null, dtype: int64 testsf = tests . join ( moods . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], lsuffix = ', chi' , rsuffix = ', moods' ) testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True 2.223170e-07 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True 2.253660e-07 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True 1.800242e-05 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True 1.180348e-08 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True 8.091620e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True 1.943972e-10 True Non-Parametric Conclusions \u00b6 problem_vars = testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] . index print ( f \"Compared with correlation \\n the dependency tests show that the top 5 problem variables are: \\n { [ f ' { i } ' for i in problem_vars ] } \" ) Compared with correlation the dependency tests show that the top 5 problem variables are: ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True Parametric \u00b6 Univariate Quantitative to Quantitative (Linear Regression) \u00b6 for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () plt . plot ( x , ( y - y_pred ), ls = '' , marker = '.' ) plt . ylabel ( 'Residual' ) plt . xlabel ( 'x' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( f \"test statistic: { t } \" ) print ( f \"p-value: { stats . t . sf ( x = t , df = n - 2 ) } \" ) R2: 3.34e-02 RSE: 9.97e-03 b: -2.51e-03 m: 3.61e-01 n: 338 x_bar: 1.11e-01 SE_b: 1.51e-03 SE_m: 1.26e-02 test statistic: 28.56535956265302 p-value: 3.2661498371310494e-92 R2: 4.31e-02 RSE: 5.27e-03 b: -3.19e-03 m: 9.08e-02 n: 1551 x_bar: 2.89e-01 SE_b: 3.09e-04 SE_m: 9.66e-04 test statistic: 94.02166426512674 p-value: 0.0 R2: 1.89e-02 RSE: 5.94e-03 b: -1.36e-02 m: 1.57e+01 n: 537 x_bar: 2.64e-03 SE_b: 1.04e-03 SE_m: 3.81e-01 test statistic: 41.11483903422569 p-value: 4.981863349397906e-168 for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.033 Model: OLS Adj. R-squared: 0.030 Method: Least Squares F-statistic: 11.60 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.000739 Time: 10:02:50 Log-Likelihood: 360.28 No. Observations: 338 AIC: -716.6 Df Residuals: 336 BIC: -708.9 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0025 0.013 -0.199 0.843 -0.027 0.022 x1 0.3612 0.106 3.406 0.001 0.153 0.570 ============================================================================== Omnibus: 457.321 Durbin-Watson: 1.128 Prob(Omnibus): 0.000 Jarque-Bera (JB): 55378.392 Skew: 6.475 Prob(JB): 0.00 Kurtosis: 64.356 Cond. No. 23.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.043 Model: OLS Adj. R-squared: 0.042 Method: Least Squares F-statistic: 69.71 Date: Sat, 02 Apr 2022 Prob (F-statistic): 1.50e-16 Time: 10:02:50 Log-Likelihood: 2180.0 No. Observations: 1551 AIC: -4356. Df Residuals: 1549 BIC: -4345. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0032 0.003 -0.917 0.359 -0.010 0.004 x1 0.0908 0.011 8.349 0.000 0.069 0.112 ============================================================================== Omnibus: 2262.527 Durbin-Watson: 1.176 Prob(Omnibus): 0.000 Jarque-Bera (JB): 660230.502 Skew: 8.562 Prob(JB): 0.00 Kurtosis: 102.615 Cond. No. 7.82 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.019 Model: OLS Adj. R-squared: 0.017 Method: Least Squares F-statistic: 10.29 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.00142 Time: 10:02:50 Log-Likelihood: 622.03 No. Observations: 537 AIC: -1240. Df Residuals: 535 BIC: -1231. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0136 0.013 -1.020 0.308 -0.040 0.013 x1 15.6795 4.888 3.208 0.001 6.078 25.281 ============================================================================== Omnibus: 739.911 Durbin-Watson: 1.268 Prob(Omnibus): 0.000 Jarque-Bera (JB): 108127.158 Skew: 7.213 Prob(JB): 0.00 Kurtosis: 71.003 Cond. No. 1.49e+03 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.49e+03. This might indicate that there are strong multicollinearity or other numerical problems. Feature Engineering \u00b6 Introducing polynomial features results in poor coefficient estimates ind = ind_vars [ 1 ] lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) features = PolynomialFeatures ( degree = 3 ) x = features . fit_transform ( x . reshape ( - 1 , 1 )) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.056 Model: OLS Adj. R-squared: 0.055 Method: Least Squares F-statistic: 30.83 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.35e-19 Time: 10:03:01 Log-Likelihood: 2190.9 No. Observations: 1551 AIC: -4374. Df Residuals: 1547 BIC: -4352. Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0030 0.009 0.334 0.739 -0.014 0.020 x1 0.1612 0.124 1.301 0.193 -0.082 0.404 x2 -0.9203 0.523 -1.761 0.078 -1.945 0.105 x3 1.6922 0.664 2.550 0.011 0.390 2.994 ============================================================================== Omnibus: 2252.329 Durbin-Watson: 1.188 Prob(Omnibus): 0.000 Jarque-Bera (JB): 651735.784 Skew: 8.487 Prob(JB): 0.00 Kurtosis: 101.979 Cond. No. 594. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. R2: 5.64e-02 RSE: 6.03e-03 b: 2.95e-03 m: 0.00e+00 n: 1551 x_bar: 3.58e-01 SE_b: 1.69e-04 SE_m: 1.96e-04 0.0 0.5 Multivariate Quantitative to Quantitative (Multivariate Linear Regression) \u00b6 lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) lindf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ADD ARGUS ARGUSTAT AT 31 HK ADD ARGUS ARGUTHERM AO 77 PP ADD POLYPLAST PPM AO 01 UV PP ADD SCHULMAN DTY 20 AOUV BEIGE SCHULMAN PZ 91738 1 5 ZA BLACK SCHULMAN P7381 DEVAN 2SPIN PP 106 GREEN POLYPLAST COLOUR MB PP 6821 GREEN SCHULMAN PZ 302446 1 5 T High MFR Black ... UV POLYPLAST 6005 PP UV SCHULMAN FPPUV 38 MFR 4 Grey PP POLYCHIM HL10XF MFR 4 Grey Grey PP POLYMER TEST MFI4 PP TOTAL 4069 RR MASTERBATCH TEST UV ARGUS ARX V 16 706 UV PP seconds 0 0.0 0.0 0.0 0.005337 0.0 0.000253 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.409353 0.0 0.0 0.0 0.0 0.0 0.000000 9.0 1 0.0 0.0 0.0 0.005867 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.452007 0.0 0.0 0.0 0.0 0.0 0.000000 536.0 2 0.0 0.0 0.0 0.005895 0.0 0.000270 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.449608 0.0 0.0 0.0 0.0 0.0 0.000000 20.0 3 0.0 0.0 0.0 0.005743 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.440883 0.0 0.0 0.0 0.0 0.0 0.000000 1001.0 4 0.0 0.0 0.0 0.004553 0.0 0.000211 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.349459 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.339726 0.0 0.0 0.0 0.0 0.0 0.002062 0.0 17893 0.0 0.0 0.0 0.000000 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.342659 0.0 0.0 0.0 0.0 0.0 0.002071 0.0 17894 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345886 0.0 0.0 0.0 0.0 0.0 0.002107 0.0 17895 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.344945 0.0 0.0 0.0 0.0 0.0 0.002095 799.0 17896 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345288 0.0 0.0 0.0 0.0 0.0 0.002105 429.0 17897 rows \u00d7 45 columns lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) x = lindf [ feeds ] . values # features = PolynomialFeatures(degree=3) # x = features.fit_transform(x.reshape(-1,1)) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.050 Model: OLS Adj. R-squared: 0.049 Method: Least Squares F-statistic: 31.47 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.76e-174 Time: 10:03:19 Log-Likelihood: 44791. No. Observations: 17897 AIC: -8.952e+04 Df Residuals: 17866 BIC: -8.928e+04 Df Model: 30 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0123 0.002 5.316 0.000 0.008 0.017 x1 -1.0553 0.382 -2.760 0.006 -1.805 -0.306 x2 -0.0119 0.010 -1.241 0.215 -0.031 0.007 x3 0.0351 0.100 0.352 0.725 -0.161 0.231 x4 -0.2377 0.069 -3.438 0.001 -0.373 -0.102 x5 0.4001 0.145 2.750 0.006 0.115 0.685 x6 -0.0224 0.128 -0.175 0.861 -0.272 0.228 x7 -0.3174 0.096 -3.305 0.001 -0.506 -0.129 x8 -0.5909 0.209 -2.825 0.005 -1.001 -0.181 x9 -0.4059 0.087 -4.655 0.000 -0.577 -0.235 x10 -0.0241 0.005 -5.206 0.000 -0.033 -0.015 x11 -2.473e-14 1.17e-14 -2.107 0.035 -4.77e-14 -1.73e-15 x12 -0.0050 0.004 -1.371 0.171 -0.012 0.002 x13 -0.0288 0.007 -4.319 0.000 -0.042 -0.016 x14 -0.0268 0.005 -5.644 0.000 -0.036 -0.018 x15 -0.0200 0.006 -3.620 0.000 -0.031 -0.009 x16 -1.023e-14 3.5e-15 -2.923 0.003 -1.71e-14 -3.37e-15 x17 -9.109e-16 2.5e-16 -3.649 0.000 -1.4e-15 -4.22e-16 x18 -0.0994 0.093 -1.073 0.283 -0.281 0.082 x19 -0.0055 0.003 -2.044 0.041 -0.011 -0.000 x20 0.0003 0.005 0.058 0.954 -0.009 0.010 x21 1.325e-15 1.2e-16 11.078 0.000 1.09e-15 1.56e-15 x22 -2.494e-16 8.68e-17 -2.873 0.004 -4.2e-16 -7.93e-17 x23 0.0145 0.004 3.827 0.000 0.007 0.022 x24 -0.0100 0.010 -1.051 0.293 -0.029 0.009 x25 0.0216 0.004 5.724 0.000 0.014 0.029 x26 -0.0101 0.002 -4.198 0.000 -0.015 -0.005 x27 -1.03e-16 8.73e-17 -1.180 0.238 -2.74e-16 6.8e-17 x28 9.329e-16 7.53e-17 12.396 0.000 7.85e-16 1.08e-15 x29 -0.0074 0.003 -2.881 0.004 -0.012 -0.002 x30 1.7091 0.132 12.994 0.000 1.451 1.967 x31 4.1538 0.376 11.053 0.000 3.417 4.890 x32 0 0 nan nan 0 0 x33 0.0031 0.012 0.263 0.792 -0.020 0.026 x34 0.0031 0.012 0.263 0.792 -0.020 0.026 x35 0.0031 0.012 0.263 0.792 -0.020 0.026 x36 0.0790 0.042 1.868 0.062 -0.004 0.162 x37 0.0525 0.030 1.774 0.076 -0.006 0.111 x38 -0.0064 0.003 -2.521 0.012 -0.011 -0.001 x39 0 0 nan nan 0 0 x40 -0.0136 0.005 -2.762 0.006 -0.023 -0.004 x41 0 0 nan nan 0 0 x42 0 0 nan nan 0 0 x43 0 0 nan nan 0 0 x44 0.1990 0.048 4.119 0.000 0.104 0.294 ============================================================================== Omnibus: 43479.020 Durbin-Watson: 1.074 Prob(Omnibus): 0.000 Jarque-Bera (JB): 693938974.691 Skew: 25.434 Prob(JB): 0.00 Kurtosis: 966.322 Cond. No. 1.11e+16 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 1.79e-28. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. R2: 5.02e-02 RSE: 1.67e-03 b: 1.23e-02 m: -1.06e+00 n: 17897 x_bar: 2.26e-02 SE_b: 1.25e-05 SE_m: 2.14e-05 -49284.75476441197 1.0 Forward Selection \u00b6 def add_feature ( features , basemodel , data , y , r2max ): for feed in features : basemodel [ feed ] = data [ feed ] basemodel . fillna ( 0 , inplace = True ) x = basemodel [[ col for col in basemodel . columns if col != 'seconds' ]] . values ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () if ( est2 . rsquared > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = est2 . rsquared feedmax = feed bestsum = est2 . summary () newmodel = basemodel . copy () else : pass basemodel . drop ( labels = feed , axis = 1 , inplace = True ) return r2max , feedmax , bestsum , newmodel lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) r2max = 0 candidates = feeds . copy () basemodel = lindf while True : newr2max , feedmax , bestsum , newmodel = add_feature ( features = candidates , basemodel = basemodel , data = df , y = y , r2max = 0 ) if newr2max > r2max : # print(newr2max, feedmax) candidates . remove ( feedmax ) r2max = newr2max basemodel = newmodel continue else : break /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) we see some of the usual suspects from the chi-square tests. Notably some are missing: High MFR Grey , BLACK SCHULMAN P7381 , PP INEOS 100 GA09 and others. basemodel . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } seconds PP TOTAL 7089 PP POLYCHIM HB12XF SILIKAT PBH ABPP 05 SILIKAT POLYPLUS AB 1001 PP MFR 4 Green ADD SCHULMAN DTY 20 AOUV GREEN SCHULMAN PZ 302446 1 5 T MFR 4 Grey UV ARGUS ARX V 16 706 UV PP GREEN SCHULMAN PZ 34198 1 5 T 0 9.0 0.0 0.0 0.0 0.0 0.0 0.005337 0.0 0.409353 0.0 0.0 1 536.0 0.0 0.0 0.0 0.0 0.0 0.005867 0.0 0.452007 0.0 0.0 2 20.0 0.0 0.0 0.0 0.0 0.0 0.005895 0.0 0.449608 0.0 0.0 3 1001.0 0.0 0.0 0.0 0.0 0.0 0.005743 0.0 0.440883 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.004553 0.0 0.349459 0.0 0.0 forward_selection_feeds = basemodel . columns [ 1 :] bestsum OLS Regression Results Dep. Variable: y R-squared: 0.046 Model: OLS Adj. R-squared: 0.045 Method: Least Squares F-statistic: 94.99 Date: Sat, 02 Apr 2022 Prob (F-statistic): 5.51e-174 Time: 10:04:41 Log-Likelihood: 44748. No. Observations: 17897 AIC: -8.948e+04 Df Residuals: 17887 BIC: -8.940e+04 Df Model: 9 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 0.0034 0.000 11.288 0.000 0.003 0.004 x1 0.0214 0.002 11.535 0.000 0.018 0.025 x2 0.0146 0.002 7.837 0.000 0.011 0.018 x3 1.5742 0.110 14.375 0.000 1.360 1.789 x4 4.1700 0.363 11.497 0.000 3.459 4.881 x5 -0.0181 0.003 -6.276 0.000 -0.024 -0.012 x6 -0.2879 0.065 -4.450 0.000 -0.415 -0.161 x7 -0.1932 0.052 -3.698 0.000 -0.296 -0.091 x8 0.0037 0.001 3.322 0.001 0.002 0.006 x9 0.0736 0.028 2.641 0.008 0.019 0.128 x10 0 0 nan nan 0 0 x11 0 0 nan nan 0 0 Omnibus: 43415.763 Durbin-Watson: 1.068 Prob(Omnibus): 0.000 Jarque-Bera (JB): 683838684.139 Skew: 25.346 Prob(JB): 0.00 Kurtosis: 959.275 Cond. No. inf Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 0. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular. Multivariate Conclusions \u00b6 y poses a problem with out of the box regression. Extreme tails will upset the residual plots. We some overlap of the parametric and non-parametric approaches: [ i for i in basemodel . columns if i in ( problem_vars )] ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05'] Multivariate Quantitative to Categorical (Binned Output Variable) \u00b6 Logistic regression does little better than random guessing lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) x = lindf [ ind ] . values #.reshape(-1,1) model = LogisticRegression () model . fit ( x , y ) y_pred = model . predict ( x ) print ( f \"Train Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) Train Acc: 0.37 Test Acc: 0.37 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( print ( classification_report ( y , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.35 0.80 0.49 4527 2.0 0.36 0.32 0.34 4434 3.0 0.30 0.03 0.06 4463 4.0 0.43 0.32 0.37 4473 accuracy 0.37 17897 macro avg 0.36 0.37 0.31 17897 weighted avg 0.36 0.37 0.31 17897 <AxesSubplot:> If we are careful about regularizing the random forest model, it does a little bit better lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) X = lindf [ ind ] . values #.reshape(-1,1) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 0.99 Test Acc: 0.39 y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.41 0.48 0.44 898 2.0 0.33 0.30 0.31 881 3.0 0.29 0.27 0.28 893 4.0 0.50 0.51 0.51 908 accuracy 0.39 3580 macro avg 0.38 0.39 0.38 3580 weighted avg 0.38 0.39 0.38 3580 <AxesSubplot:> # grab feature importances imp = model . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in model . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ ind , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 PP TOTAL PPH 4065 0.150387 0.008053 1 BLACK SCHULMAN P7381 0.118158 0.008725 2 PP INEOS 100 GA04 0.100635 0.005217 3 UV ARGUS ARX V 16 706 UV PP 0.090776 0.006273 4 MFR 4 Grey 0.08828 0.005656 5 PP UNIPETROL GB005 0.060558 0.003569 6 MFR 4 Black 0.052589 0.003695 7 ADD SCHULMAN DTY 20 AOUV 0.041478 0.003554 8 DEVAN 2SPIN PP 106 0.038438 0.004061 9 High MFR Black 0.028838 0.002515 10 MFR 4 Grey Grey 0.025758 0.002319 11 ADD ARGUS ARGUTHERM AO 77 PP 0.024526 0.002253 12 High MFR Grey 0.020652 0.004188 13 MFR 4 Green 0.016508 0.002725 14 SILIKAT PBH ABPP 05 0.016283 0.002291 15 PP TOTAL 7089 0.01624 0.004897 16 ADD POLYPLAST PPM AO 01 UV PP 0.013499 0.001685 17 MFR 4 Black Brown 0.013401 0.002131 18 PP POLYCHIM HB12XF 0.01245 0.002185 19 PP INEOS 100 GA09 0.010354 0.00156 6.0 Business Impact \u00b6 What is the so what? From Hyp tests - target is either 'Total Position Out' or 'Total Seconds Out' We had culprits from pearsons: ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] culprits from non parametric tests: list ( problem_vars ) ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] and culprits from linear regression forward_selection_feeds Index(['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'MFR 4 Green', 'ADD SCHULMAN DTY 20 AOUV', 'GREEN SCHULMAN PZ 302446 1 5 T', 'MFR 4 Grey', 'UV ARGUS ARX V 16 706 UV PP', 'GREEN SCHULMAN PZ 34198 1 5 T'], dtype='object') and we can check the overlap between non-parametric and linear regression (parametric): predict_vars = [ i for i in basemodel . columns if i in ( problem_vars )] predict_vars ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05'] all_vars = set ( list ( problem_vars ) + list ( forward_selection_feeds )) all_vars {'ADD SCHULMAN DTY 20 AOUV', 'BLACK SCHULMAN P7381', 'GREEN SCHULMAN PZ 302446 1 5 T', 'GREEN SCHULMAN PZ 34198 1 5 T', 'High MFR Grey', 'MFR 4 Green', 'MFR 4 Grey', 'PP POLYCHIM HB12XF', 'PP TOTAL 7089', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'UV ARGUS ARX V 16 706 UV PP'} # add to DF business = pd . DataFrame () value_line_by_day = 99000 for ind in problem_vars : try : # products, prods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique () nprods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . nunique () dff = df . loc [ df [ 'Product' ] . isin ( prods )] # total runtime, tot_runtime = dff [ 'Time Delta' ] . sum () # total runtime with feed, tot_runtime_wfeed = dff [ dff [ ind ] > 0 ][ 'Time Delta' ] . sum () #downtime w/o feed, downtime w/ feed, avg_downtime_min = dff . groupby ( dff [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 avg_downtime_min_wfeed = avg_downtime_min [ True ] avg_downtime_min_w_ofeed = avg_downtime_min [ False ] ratio = avg_downtime_min_wfeed / avg_downtime_min_w_ofeed # average runtime, avg_runtime_min = dff [ 'Time Delta' ] . mean () . total_seconds () / 60 # days saved, dollars saved days_saved = ( tot_runtime_wfeed . total_seconds () / 60 / avg_runtime_min * avg_downtime_min . diff () . values [ - 1 ]) / 60 / 24 dollars_saved_per_batch = value_line_by_day * avg_downtime_min . diff () . values [ - 1 ] / ( 60 * 24 ) business = pd . concat (( business , pd . DataFrame ([[ ind , nprods , tot_runtime , tot_runtime_wfeed , avg_downtime_min_wfeed , avg_downtime_min_w_ofeed , ratio , avg_runtime_min , days_saved , days_saved * value_line_by_day , dollars_saved_per_batch ]], columns = [ 'Feed' , 'Products' , 'Total Runtime, Products' , 'Total Runtime With Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Average Runtime' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]))) except : print ( ind ) print ( dff . groupby ( df [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 ) business = business . sort_values ( 'Dollars Saved (Per Batch)' , ascending = False ) . reset_index ( drop = True ) business # workshop downtime ratio # feed products, feed products # Dollars saved per batch .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Products Total Runtime, Products Total Runtime With Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Average Runtime Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26 409 days 20:43:21 94 days 04:36:05 26.250639 13.671063 1.920161 70.835736 16.727319 1.656005e+06 864.845823 1 High MFR Grey 36 564 days 04:13:41 94 days 12:26:59 23.623539 12.809036 1.844287 65.459164 15.615433 1.545928e+06 743.497080 2 PP POLYCHIM HB12XF 17 383 days 00:03:51 67 days 09:13:53 21.043525 15.600015 1.348943 70.908183 5.173013 5.121283e+05 374.241303 3 BLACK SCHULMAN P7381 64 751 days 20:01:57 590 days 12:21:50 14.902871 12.363943 1.205349 63.216276 23.716605 2.347944e+06 174.551301 4 PP TOTAL 7089 13 291 days 22:06:22 79 days 15:56:47 20.651050 18.269305 1.130369 80.146114 2.367430 2.343756e+05 163.744933 pd . set_option ( 'display.precision' , 2 ) display ( business [[ 'Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]], ) pd . set_option ( 'display.precision' , 6 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26.25 13.67 1.92 16.73 1.66e+06 864.85 1 High MFR Grey 23.62 12.81 1.84 15.62 1.55e+06 743.50 2 PP POLYCHIM HB12XF 21.04 15.60 1.35 5.17 5.12e+05 374.24 3 BLACK SCHULMAN P7381 14.90 12.36 1.21 23.72 2.35e+06 174.55 4 PP TOTAL 7089 20.65 18.27 1.13 2.37 2.34e+05 163.74 We see that 'SILKAT PBH ABPP 05' as well as 'High MFR Grey' have a large amount of days saved over the two years, a strong downtime multiplier effect and the largest dollars saved per batch. Since High MFR Grey is a recycle feed, we may not have the ability to remove it from the feed list. Leaving the SILKAT (Silicate) feed the prime feedstock to remove 7.0 Visualizations \u00b6 PP TOTAL 7089 \u00b6 predict_var = predict_vars [ 1 ] outcome_var = [ i for i in df . columns if 'Seconds' in i ] def myplot ( col = list ( df . loc [ df [ predict_var ] > 0 ][ 'Product' ] . unique ())): fig , ax = plt . subplots ( figsize = ( 20 , 10 )) axt = ax . twinx () # other feeds df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True ) . dropna ( axis = 1 , how = 'all' ) . plot ( ls = '' , marker = '*' , ax = ax , alpha = 0.2 ) # predict_var feed df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True )[ predict_var ] . plot ( ls = '--' , marker = '+' , ax = ax , lw = 2 , c = 'red' ) # outcome_var df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ) . plot ( ls = '-' , marker = '+' , lw = 1 , ax = axt , alpha = 1 , c = 'blue' , label = 'Total Seconds Out' ) # outliers ax . set_ylim ( 0 ) axt . set_ylim ( 0 , np . quantile ( df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ), 0.997 )) h1 , l1 = ax . get_legend_handles_labels () h2 , l2 = axt . get_legend_handles_labels () ax . legend ( h1 + h2 , l1 + l2 , loc = 0 ) interact ( myplot ) interactive(children=(Dropdown(description='col', options=(0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.\u2026 <function __main__.myplot(col=[0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.0, 136215.0, 125215.0, 110215.0, 100215.0, 80215.0, 100617.0, 90617.0, 90215.0, 90216.0, 110520.0, 80617.0])>","title":"X3 Truffletopia"},{"location":"extras/X3_Truffletopia/#causality-analysis","text":"","title":"Causality Analysis"},{"location":"extras/X3_Truffletopia/#todo","text":"ols assumptions for univariate analyses residual plots forward selection lines up more or less with chi square tests include some colinearity measure? logistic regression (binned seconds, binned total outs) random forest does little better than random guessing business impact reorganize hypothesis tests section univariate vs multivariate cat v vat, cat v quant, quant v quant, quant v cat","title":"TODO"},{"location":"extras/X3_Truffletopia/#10-imports","text":"","title":"1.0 Imports"},{"location":"extras/X3_Truffletopia/#import-libraries","text":"# basic packages import pandas as pd import numpy as np import random import copy # visualization packages from ipywidgets import interact import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures , MinMaxScaler from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , silhouette_score , calinski_harabasz_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score def make_patch_spines_invisible ( ax ): ax . set_frame_on ( True ) ax . patch . set_visible ( False ) for sp in ax . spines . values (): sp . set_visible ( False ) from matplotlib import colors from itertools import cycle def my_colors (): tab = cycle ( colors . TABLEAU_COLORS ) return tab","title":"Import Libraries"},{"location":"extras/X3_Truffletopia/#load-data","text":"consider only feeds consider feeds and operating conditions # df = pd.read_csv('../../data/Dupont - start dataset for hypothesis - 20190524.csv', header=0) df = pd . read_csv ( '../../../data/dupont/TyparMasterReportWithSampleDataImprovement2017-today.csv' , header = 1 ) df = df . loc [ df [ 'Product' ] . notnull ()] df [ feeds ] = df [ feeds ] . div ( df [ feeds ] . sum ( axis = 1 ), axis = 0 ) #normalize feed rates df [ 'Time Delta' ] = pd . to_datetime ( df [ 'Doff Time' ]) . diff () df [ 'Total Seconds Out' ] = df [[ i for i in df . columns if 'Sec' in i ]] . sum ( 1 ) # df.dropna(inplace=True) hangs = [ df . columns [ i ] for i , j in enumerate ( 'Hang' in col for col in df . columns ) if j ] speeds = [ df . columns [ i ] for i , j in enumerate ( 'speed' in col for col in df . columns ) if j ] outs = [ df . columns [ i ] for i , j in enumerate ( 'Out' in col for col in df . columns ) if j ] prods = df [ 'Product' ] . unique () counts = [ i for i in df . columns if 'Count' in i ] #from LOEWEN virgin_polymer = [ 'PP INEOS 100 GA04' , 'PP POLYCHIM HB12XF' , 'PP TOTAL 7089' , #INEOS 100 GA09 until 2018 (2k rolls) 'PP TOTAL PPH 4065' , 'PP UNIPETROL GB005' , 'PP POLYCHIM HL10XF' ] #HL10XF not used after 2016, #BOREALIS virgin_polymer = [] for col in df . columns : if ( 'PP INEOS' in col ) or ( 'PP TOTAL' in col ) or ( 'PP UNI' in col ) or ( 'PP BORE' in col ) or ( 'PP POLY' in col ): virgin_polymer . append ( col ) pressures = [] for col in df . columns : if 'Press' in col or 'PRESS' in col : pressures . append ( col ) silicates = [] for col in df . columns : if 'SIL' in col : silicates . append ( col ) uvs = [] for col in df . columns : if 'UV ' in col : uvs . append ( col ) recycle = [] for col in df . columns : if 'MFR' in col : recycle . append ( col ) seconds = [] for col in df . columns : if 'Sec' in col : seconds . append ( col ) feeds = [ 'ADD ARGUS ARGUSTAT AT 31 HK' , 'ADD ARGUS ARGUTHERM AO 77 PP' , 'ADD POLYPLAST PPM AO 01 UV PP' , 'ADD SCHULMAN DTY 20 AOUV' , 'BEIGE SCHULMAN PZ 91738 1 5 ZA' , 'BLACK SCHULMAN P7381' , 'DEVAN 2SPIN PP 106' , 'GREEN POLYPLAST COLOUR MB PP 6821' , 'GREEN SCHULMAN PZ 302446 1 5 T' , 'High MFR Black' , 'GREEN SCHULMAN PZ 34198 1 5 T' , 'MFR 4 Black' , 'MFR 4 Black Brown' , 'High MFR Grey' , 'MFR 4 Green' , 'PP BOREALIS HE465FB' , 'MFR 4 Black Colour Blending' , 'PP INEOS 100 GA02' , 'PP INEOS 100 GA04' , 'PP INEOS 100 GA09' , 'MFR 4 Grey Colour Blending' , 'PP INEOS GA012' , 'PP POLYCHIM HB12XF' , 'MFR 4 White' , 'PP TOTAL 7089' , 'PP TOTAL PPH 4065' , 'MFR 4 White Colour Blending' , 'PP BOREALIS HE370FB' , 'PP UNIPETROL GB005' , 'SILIKAT PBH ABPP 05' , 'SILIKAT POLYPLUS AB 1001 PP' , 'UV ARGUS ARGUVIN UV 361 PP' , 'UV ARGUS ARX 904 11 PP' , 'UV ARGUS ARX 904 11 PP.1' , 'UV ARGUS ARX 904 11 PP.2' , 'UV POLYPLAST 6005 PP' , 'UV SCHULMAN FPPUV 38' , 'MFR 4 Grey' , 'PP POLYCHIM HL10XF' , 'MFR 4 Grey Grey' , 'PP POLYMER TEST MFI4' , 'PP TOTAL 4069' , 'RR MASTERBATCH TEST' , 'UV ARGUS ARX V 16 706 UV PP' ] master_batch = list ( set ( feeds ) - set ( recycle ) - set ( virgin_polymer )) operation = list ( set ( df . columns ) - set ( master_batch ) - set ( virgin_polymer ) - set ( recycle ) - set ( hangs ) - set ( outs )) operation . sort () A summary of the columns print ( f 'pressures: { len ( pressures ) } ' ) print ( f 'silicates: { len ( silicates ) } ' ) print ( f 'uvs: { len ( uvs ) } ' ) print ( f 'feeds: { len ( feeds ) } ' ) print ( f 'master_batch: { len ( master_batch ) } ' ) print ( f 'virgin_polymer: { len ( virgin_polymer ) } ' ) print ( f 'recycle: { len ( recycle ) } ' ) print ( f 'operation: { len ( operation ) } ' ) print () print ( f 'hangs: { len ( hangs ) } ' ) print ( f 'speeds: { len ( speeds ) } ' ) print ( f 'outs: { len ( outs ) } ' ) print ( f 'seconds: { len ( seconds ) } ' ) print () print ( 'summary target columns: TotalHangs, Total Position Out' ) print () print ( df . shape ) pressures: 8 silicates: 2 uvs: 8 feeds: 44 master_batch: 20 virgin_polymer: 13 recycle: 11 operation: 180 hangs: 9 speeds: 9 outs: 10 seconds: 9 summary target columns: TotalHangs, Total Position Out (17897, 243) all_feats = df . select_dtypes ( float ) . columns [ df . select_dtypes ( float ) . columns . isin ( feeds + operation )] . values","title":"Load Data"},{"location":"extras/X3_Truffletopia/#20-understand-the-dependent-variable","text":"","title":"2.0 Understand The Dependent Variable"},{"location":"extras/X3_Truffletopia/#what-is-the-hit-rate-in-these-upset-columns","text":"Around 7500 cases with no problem; 10000 cases with a position out target = \"Total Position Out\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 10355 False 7542 Name: Total Position Out, dtype: int64 There are 30% higher incidence of hangs target = \"TotalHangs\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 13954 False 3943 Name: TotalHangs, dtype: int64 target1 = \"TotalHangs\" target2 = \"Total Position Out\" print ( f \"both upsets: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"only hangs: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) print ( f \"only outs: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"neither upset: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) both upsets: 10161 only hangs: 3793 only outs: 194 neither upset: 3749","title":"What is the hit rate in these upset columns?"},{"location":"extras/X3_Truffletopia/#what-is-the-magnitude-of-the-outages","text":"target = \"Total Position Out\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f163a9867c0>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total Position Out Unnamed 0 Total Position Out 1.00000 0.62591 Unnamed 0 0.62591 1.00000 target = \"TotalHangs\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f163caf6190>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TotalHangs Unnamed 0 TotalHangs 1.000000 0.390949 Unnamed 0 0.390949 1.000000","title":"What is the magnitude of the outages?"},{"location":"extras/X3_Truffletopia/#distribution-around-the-target-variable-total-seconds","text":"We will certainly encounter trouble in an OLS estimate with this shape of the target variable; extreme skewness, upper tail, perhaps outliers tot_seconds = df [ seconds ] . sum ( 1 ) tot_seconds . kurt () 923.6728632470827 tot_seconds . skew () 25.09323820232841 tot_seconds . plot . kde () <AxesSubplot:ylabel='Density'> tot_seconds . describe () count 17897.000000 mean 2093.512879 std 8060.466090 min 0.000000 25% 20.000000 50% 666.000000 75% 1970.000000 max 396570.000000 dtype: float64","title":"Distribution around the target variable, total seconds"},{"location":"extras/X3_Truffletopia/#do-block-positions-have-the-same-behavior","text":"target = \"TotalHangs\" df . groupby ( target )[ hangs ] . count () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs TotalHangs 0.0 0 0 0 0 0 0 0 0 3943 1.0 510 682 745 977 510 682 745 977 2914 2.0 709 761 901 1109 709 761 901 1109 2424 3.0 721 776 930 1103 721 776 930 1103 1929 4.0 700 773 875 993 700 773 875 993 1577 5.0 609 668 744 828 609 668 745 828 1189 6.0 523 498 599 631 523 498 599 631 883 7.0 387 414 472 505 387 414 472 505 640 8.0 321 338 376 409 321 338 376 409 503 9.0 286 275 305 339 286 275 305 339 405 10.0 219 206 244 248 219 206 244 248 296 11.0 144 154 168 171 144 154 168 171 203 12.0 148 139 159 171 148 139 159 171 191 13.0 105 100 117 106 105 100 117 106 131 14.0 93 88 95 96 93 88 95 96 109 15.0 97 89 94 97 97 89 94 97 108 16.0 70 64 72 76 70 64 72 76 81 17.0 51 50 51 47 51 50 51 47 55 18.0 29 30 32 33 29 30 32 33 35 19.0 32 32 33 32 32 32 33 32 35 20.0 32 32 31 34 32 32 31 34 35 21.0 26 27 25 24 26 27 25 24 28 22.0 21 20 24 24 21 20 24 24 24 23.0 23 23 24 23 23 23 24 23 25 24.0 16 17 18 19 16 17 18 19 20 25.0 7 8 8 7 7 8 8 7 8 26.0 11 10 10 11 11 10 10 11 11 27.0 8 9 9 8 8 9 9 8 9 28.0 10 10 10 10 10 10 10 10 10 29.0 6 5 6 6 6 5 6 6 6 30.0 4 3 4 4 4 3 4 4 4 31.0 8 8 8 8 8 8 8 8 8 32.0 4 3 4 4 4 3 4 4 4 33.0 7 5 7 6 7 5 7 6 7 34.0 9 8 9 8 9 8 9 8 9 35.0 6 6 6 6 6 6 6 6 6 36.0 6 5 6 6 6 5 6 6 6 37.0 3 3 3 3 3 3 3 3 3 38.0 1 1 1 0 1 1 1 0 1 39.0 1 1 1 1 1 1 1 1 1 40.0 2 2 2 2 2 2 2 2 2 42.0 1 1 1 1 1 1 1 1 1 43.0 3 3 3 3 3 3 3 3 3 45.0 3 3 3 3 3 3 3 3 3 46.0 2 2 2 2 2 2 2 2 2 48.0 1 1 1 1 1 1 1 1 1 52.0 1 1 1 1 1 1 1 1 1 53.0 1 1 1 1 1 1 1 1 1 54.0 1 1 1 1 1 1 1 1 1 58.0 1 1 1 1 1 1 1 1 1 62.0 1 1 1 1 1 1 1 1 1 63.0 1 1 1 1 1 1 1 1 1 70.0 1 1 1 1 1 1 1 1 1 72.0 1 1 1 1 1 1 1 1 1 140.0 1 1 1 1 1 1 1 1 1 df [[ i for i in outs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:> df [[ i for i in hangs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:>","title":"Do block positions have the same behavior?"},{"location":"extras/X3_Truffletopia/#30-look-for-dependent-independent-signal","text":"","title":"3.0 Look for dependent-independent signal"},{"location":"extras/X3_Truffletopia/#are-there-linear-relationships-between-the-dependent-and-independent-variables","text":"corr = df . corr () ( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 ) Product False Roll Width False Block1Pos Out Seconds True Block2Pos Out Seconds True Block3Pos Out Seconds True ... UPEX - UWU PLOT EXTREM (UPEX) False UWG - UWU GRAVIMETRIC WITH SCALE (UWG) False UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP) False WATP - WATER PERMEABILITY (WATP) False Total Seconds Out True Length: 237, dtype: bool cor_filt = corr . loc [( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 )] . drop ( labels = outs + hangs , axis = 0 )[ outs + hangs ] cor_filt .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Pos Out Seconds Block2Pos Out Seconds Block3Pos Out Seconds Block4Pos Out Seconds Block1Pos Out Count Block2Pos Out Count Block3Pos Out Count Block4Pos Out Count Total Position Out Total Seconds Out Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs ADD ARGUS ARGUSTAT AT 31 HK -0.103999 -0.417688 -0.383222 0.202131 -0.056194 -0.412274 0.155126 0.145535 0.014216 0.029563 0.121682 -0.256723 -0.012676 0.099690 -0.069354 -0.041492 0.094150 0.041900 0.344896 PP INEOS 100 GA02 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 -1.000000 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 MFR 4 White 0.289648 -0.016268 0.451446 -0.155932 0.297256 -0.000098 0.376795 -0.003129 0.225425 0.182687 0.231764 -0.045076 0.283286 -0.119481 0.108396 -0.057188 0.317089 0.008018 0.189109 PP TOTAL 7089 0.160268 0.171023 0.210796 0.093720 0.192159 0.210942 0.206859 0.076058 0.260505 0.207522 0.087954 0.050670 0.052622 -0.050506 0.218148 0.251676 0.134881 0.121080 0.331897 SILIKAT POLYPLUS AB 1001 PP 0.015132 0.125801 0.379559 0.255662 -0.018715 0.012019 0.161698 0.067661 0.127830 0.137376 -0.016317 -0.018187 -0.018956 0.086781 0.011207 -0.020307 0.092291 0.109259 0.069217 RJET15219WG1 - Corona Charge I AVG BLK#2 -0.120204 -0.120509 -0.056377 -0.002054 -0.198138 -0.221189 -0.193393 -0.024541 -0.232863 -0.149862 -0.090896 -0.060404 -0.054169 -0.052683 -0.215325 -0.216545 -0.182990 -0.120387 -0.308217 BLFL - COLOUR BLACK FILAMENT L value (BLFL) -0.360086 -0.157845 -0.245565 -0.268837 -0.153321 -0.162697 -0.212481 -0.247721 -0.067632 -0.121690 0.097783 0.146515 -0.346923 -0.110987 0.402310 -0.274153 -0.378670 -0.116264 -0.117501 BRFA - COLOUR BROWN FILAMENT a value (BRFa) 0.159056 0.250229 -0.384880 0.109024 0.134248 0.112836 -0.380715 -0.054074 0.099040 0.166926 0.170184 -0.065344 -0.195062 0.094596 0.195404 -0.333632 -0.026209 0.177576 0.173995 BRFB - COLOUR BROWN FILAMENT b value (BRFb) 0.413876 0.047748 0.332752 0.342115 0.145794 -0.028170 0.246787 0.234624 0.216156 0.311363 0.148441 -0.210193 0.118275 0.285286 -0.027055 -0.275750 0.100830 0.328240 0.164345 BRFL - COLOUR BROWN FILAMENT L value (BRFL) 0.198864 -0.178050 0.483242 0.205796 0.017456 -0.220432 0.411110 0.090885 0.039031 0.128326 0.337130 -0.326368 0.075738 0.145878 0.195055 -0.467536 -0.075130 0.188013 -0.000575 E20M AVG - SST ELONGATION 20X20 MD (E20M) 0.106870 -0.096775 -0.249986 -0.039633 0.009518 -0.049131 -0.367065 -0.074018 -0.070058 -0.071425 -0.124456 -0.016993 -0.070312 -0.036314 -0.145788 -0.075968 -0.197153 -0.069477 -0.072872 EF1M AVG - SST ELONGATION FOLDED 10 CM MD (EF1M) 1.000000 NaN 1.000000 -0.081738 1.000000 NaN NaN NaN -0.101606 -0.178993 0.313364 -0.936781 -1.000000 -0.517401 0.306235 -0.950303 -1.000000 -0.324932 -0.112334 EF1X AVG - SST ELONGATION FOLDED 10 CM XD (EF1X) 1.000000 NaN 1.000000 0.126777 1.000000 NaN NaN NaN 0.010039 -0.063399 0.896084 -0.961383 -1.000000 -0.609904 0.892730 -0.971830 -1.000000 -0.340772 0.050483 NATM AVG - Nail Tear MD EN 13859-1 (NATM) -0.517421 -0.835147 0.511409 -0.258360 -0.296500 -0.906038 NaN -0.333754 -0.100244 0.013436 -0.338129 0.580493 0.826191 -0.322585 -0.348782 0.063309 0.929207 -0.430201 0.086484 NATX AVG - Nail Tear XD EN 13859-1 (NATX) 0.269389 -0.864128 0.542651 0.071499 -0.053990 -0.927938 NaN -0.056465 -0.084131 0.046366 -0.190830 0.541297 0.777131 -0.114506 -0.096640 0.051081 0.902927 -0.295087 0.093804 O90 - O90 EN ISO 12956 (O90E) NaN NaN NaN 0.807962 NaN NaN NaN NaN -0.265093 -0.268107 -0.280791 -0.378899 NaN -0.451610 -0.307148 -0.267261 NaN -0.269680 -0.193539 PU AVG - PUNCTURE \"US Rod\" (PU) 0.173625 -0.790779 -0.165265 0.451194 -0.127912 -0.952390 -0.184515 0.734933 -0.040323 -0.021124 0.096383 -0.998695 -0.288786 0.156274 0.066142 0.363427 -0.297118 0.479459 -0.239496 TKO4 AVG - Tack-Tear Ost 4Meter -0.170480 -0.383667 -0.326335 0.006590 -0.104058 -0.308260 -0.329544 0.047734 -0.171814 -0.195662 -0.214633 -0.440748 -0.182875 -0.027353 -0.077638 -0.447423 -0.187893 0.114764 -0.160340 TKO5 AVG - Tack - Tear Ost 5 Meter 1.000000 0.134107 0.560717 -0.070116 NaN 0.106485 0.152594 -0.525375 -0.373817 -0.306252 0.338272 0.155577 0.034557 -0.453643 0.286066 0.056270 -0.066270 -0.400941 -0.387297 TKW4 AVG - Tack-Tear West 4Meter -0.334291 -0.282220 -0.265607 -0.134467 -0.320379 -0.231417 -0.300679 -0.265049 -0.274024 -0.250617 -0.277325 -0.375559 -0.168965 -0.154119 -0.236424 -0.282687 -0.168865 -0.162801 -0.316193 TKW5 AVG - Tack - Tear West 5 Meter 1.000000 0.051583 0.100609 0.018368 NaN 0.014194 -0.167032 -0.366762 -0.279911 -0.190366 0.069184 0.108657 0.224892 -0.326866 -0.003422 0.043730 0.039670 -0.362928 -0.270898 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt , cmap = cmap ) <AxesSubplot:>","title":"Are there linear relationships between the dependent and independent variables?"},{"location":"extras/X3_Truffletopia/#filter-n-100","text":"cor_filt = cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]] fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]], cmap = cmap ) <AxesSubplot:>","title":"Filter N &gt; 100"},{"location":"extras/X3_Truffletopia/#operating-conditions","text":"fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Operations Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Operations Correlations to Hangs and Outs')","title":"Operating Conditions"},{"location":"extras/X3_Truffletopia/#feeds","text":"fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ ~ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Feeds Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Feeds Correlations to Hangs and Outs') ind_vars = list ( cor_filt . index [ ~ cor_filt . index . isin ( operation )]) ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] fig , ax = plt . subplots ( 1 , 3 , figsize = ( 30 , 10 )) indices = np . argwhere ( ax ) for index , ind_var in enumerate ( ind_vars ): df [[ ind_var ] + seconds ] . melt ( id_vars = ind_var , value_vars = None , var_name = None ,) . plot ( x = ind_var , y = 'value' , ax = ax [ index ], ls = '' , marker = '.' )","title":"Feeds"},{"location":"extras/X3_Truffletopia/#40-understand-the-independent-variable","text":"","title":"4.0 Understand the Independent Variable"},{"location":"extras/X3_Truffletopia/#descriptive-stats-on-ind-var","text":"all the feeds sum to 1 (feeds are mass or volume fractions), so we probably won't need to worry about scaling df [ feeds ] . sum ( 1 ) 0 1.0 1 1.0 2 1.0 3 1.0 4 1.0 ... 17892 1.0 17893 1.0 17894 1.0 17895 1.0 17896 1.0 Length: 17897, dtype: float64 many of the feeds are not used at all skew = df [ feeds ] . skew () kurt = df [ feeds ] . kurtosis () null = df [ feeds ] . isnull () . sum () n = df [ feeds ] . notnull () . sum () med = df [ feeds ] . median () men = df [ feeds ] . mean () dff = pd . DataFrame ([ skew , kurt , null , n , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'null cnt' , 'n' , 'median' , 'mean' ] dff . loc [ dff [ 'n' ] > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis null cnt n median mean ADD ARGUS ARGUSTAT AT 31 HK -1.122018 -0.275303 17798.0 99.0 0.006560 0.005080 ADD ARGUS ARGUTHERM AO 77 PP 14.927951 248.300227 15627.0 2270.0 0.005249 0.010705 ADD POLYPLAST PPM AO 01 UV PP 5.788514 87.051660 16769.0 1128.0 0.006397 0.006218 ADD SCHULMAN DTY 20 AOUV 6.573319 99.052816 13484.0 4413.0 0.004997 0.005411 BEIGE SCHULMAN PZ 91738 1 5 ZA -0.455858 10.108094 16975.0 922.0 0.008922 0.008708 BLACK SCHULMAN P7381 2.301415 16.313957 4471.0 13426.0 0.000317 0.002040 DEVAN 2SPIN PP 106 0.770601 11.066370 13279.0 4618.0 0.006658 0.005554 GREEN POLYPLAST COLOUR MB PP 6821 -1.856631 3.540741 16894.0 1003.0 0.007519 0.006822 GREEN SCHULMAN PZ 302446 1 5 T 0.363239 8.864015 17150.0 747.0 0.018764 0.018006 High MFR Black 0.473484 -1.518082 14862.0 3035.0 0.047883 0.146677 MFR 4 Black 1.612264 4.261601 12595.0 5302.0 0.030099 0.098792 MFR 4 Black Brown -0.086521 -0.342944 16360.0 1537.0 0.189244 0.150273 High MFR Grey -0.364616 -1.483643 15900.0 1997.0 0.226358 0.199138 MFR 4 Green 0.149918 8.244109 15985.0 1912.0 0.279622 0.262574 PP INEOS 100 GA02 NaN NaN 17895.0 2.0 0.107927 0.107927 PP INEOS 100 GA04 1.294449 5.764334 6684.0 11213.0 0.208757 0.216941 PP INEOS 100 GA09 -0.449540 -0.968103 16802.0 1095.0 0.224353 0.219208 PP POLYCHIM HB12XF -0.222658 -0.684893 16525.0 1372.0 0.275755 0.291827 MFR 4 White 1.136619 2.053071 17559.0 338.0 0.098961 0.111129 PP TOTAL 7089 -0.216726 -1.333059 16346.0 1551.0 0.276430 0.288643 PP TOTAL PPH 4065 0.353911 0.740658 611.0 17286.0 0.441009 0.424684 PP UNIPETROL GB005 0.117291 1.045121 11444.0 6453.0 0.324773 0.295906 SILIKAT PBH ABPP 05 2.690146 26.471577 15967.0 1930.0 0.005001 0.005229 SILIKAT POLYPLUS AB 1001 PP 6.874010 77.288671 17360.0 537.0 0.002506 0.002642 UV ARGUS ARX 904 11 PP 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.1 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.2 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV POLYPLAST 6005 PP 1.426790 1.634549 17506.0 391.0 0.020009 0.026245 UV SCHULMAN FPPUV 38 2.114261 14.372541 17141.0 756.0 0.029986 0.032261 MFR 4 Grey 1.718802 10.970192 8567.0 9330.0 0.269445 0.263401 MFR 4 Grey Grey 3.607370 20.342313 15865.0 2032.0 0.040035 0.068547 UV ARGUS ARX V 16 706 UV PP 1.889895 6.474782 7972.0 9925.0 0.002110 0.007994 our ind_vars feeds have some strong correlations with other feeds In particular, PP TOTAL 7089 and SILIKAT POLYPLUS AB 1001 PP have a correlation w/ eachother of .923 ind_corr = df [ feeds ] . corr ()[ ind_vars ] . dropna ( how = 'all' ) ind_corr . loc [( abs ( ind_corr ) > 0.5 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 ind_vars feeds exhibit bimodal distributions (2+ recipes) for ind in ind_vars : df [ ind ] . plot . kde () plt . title ( ind ) plt . xlim ( 0 ) plt . show ()","title":"Descriptive Stats on Ind Var"},{"location":"extras/X3_Truffletopia/#50-hypothesis-tests","text":"","title":"5.0 Hypothesis Tests"},{"location":"extras/X3_Truffletopia/#non-parametric","text":"","title":"Non-Parametric"},{"location":"extras/X3_Truffletopia/#univariate-categorical-to-categorical-chi-square","text":"ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 for ind in ind_vars : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 10182 False 7377 True True 173 False 165 Name: Total Position Out, dtype: int64 [[10182 173] [ 7377 165]] chi2 stat: 6.02 p-value: 1.41e-02 PP TOTAL 7089 Total Position Out False True 9171 False 7175 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[9171 1184] [7175 367]] chi2 stat: 237.00 p-value: 1.78e-53 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 9996 False 7364 True True 359 False 178 Name: Total Position Out, dtype: int64 [[9996 359] [7364 178]] chi2 stat: 17.99 p-value: 2.22e-05 chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 chisum = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum [ 'type' ] = None chisum . loc [ chisum [ 'feed' ] . isin ( master_batch ), 'type' ] = 'master_batch' chisum . loc [ chisum [ 'feed' ] . isin ( recycle ), 'type' ] = 'recyle' chisum . loc [ chisum [ 'feed' ] . isin ( virgin_polymer ), 'type' ] = 'virgin_polymer' chisum . loc [ chisum [ 'feed' ] . isin ( silicates ), 'type' ] = 'silicates' chisum . loc [ chisum [ 'feed' ] . isin ( uvs ), 'type' ] = 'uvs' chisum . loc [ chisum [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null type 0 PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1 PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2 High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3 SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 4 BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 5 PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 6 ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 7 DEVAN 2SPIN PP 106 35.237690 2.918209e-09 [[7511, 2844], [5768, 1774]] False True master_batch 8 High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 9 GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 10 MFR 4 Black Brown 20.494540 5.980160e-06 [[9550, 805], [6810, 732]] True True recyle 11 UV ARGUS ARX V 16 706 UV PP 20.048858 7.548859e-06 [[4465, 5890], [3507, 4035]] False True uvs 12 ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 13 SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 14 GREEN SCHULMAN PZ 302446 1 5 T 14.155700 1.682865e-04 [[9973, 382], [7177, 365]] True True master_batch 15 MFR 4 Grey Grey 9.004184 2.693622e-03 [[9464, 1141], [6401, 891]] True True recyle","title":"Univariate Categorical to Categorical (Chi-Square)"},{"location":"extras/X3_Truffletopia/#check-confounding-variables","text":"Isolate Products (product conflation with feed) Let's isolate the comparison by only products made with the feed ( ind_var ) in question We see that MFR 4 White and SILIKAT POLYPLUS AB 1001 PP are very imbalanced compared to PP TOTAL 7089 target = 'Total Position Out' for ind in ind_vars : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 3978 False 2933 True True 173 False 165 Name: Total Position Out, dtype: int64 [[3978 173] [2933 165]] chi2 stat: 5.10 p-value: 2.40e-02 PP TOTAL 7089 Total Position Out False True 2301 False 1393 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[2301 1184] [1393 367]] chi2 stat: 96.05 p-value: 1.12e-22 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 2418 False 1602 True True 359 False 178 Name: Total Position Out, dtype: int64 [[2418 359] [1602 178]] chi2 stat: 8.66 p-value: 3.25e-03 target = 'Total Position Out' chisum2 = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum2 = pd . concat ([ chisum2 , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum2 . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum2 = chisum2 . sort_values ( 'p-value' ) . reset_index ( drop = True ) tests = chisum . set_index ( 'feed' ) . join ( chisum2 . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], rsuffix = ', confound' ) tests . loc [( tests [ 'reject null' ] == True ) & ( tests [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value cTable + reject null type p-value, confound reject null, confound feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True we can compare this with our feed correlation table - there may be other explanatory variables 'hiding' ind_corr . loc [( abs ( ind_corr ) > 0.3 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 BLACK SCHULMAN P7381 0.196431 -0.470838 0.082699 DEVAN 2SPIN PP 106 NaN -0.415896 0.097590 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA04 0.342077 -0.248904 0.402788 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP TOTAL PPH 4065 -0.419569 -0.160492 0.089303 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 UV ARGUS ARX 904 11 PP NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.1 NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.2 NaN -0.461986 NaN MFR 4 Grey -0.303323 -0.381548 0.038125 MFR 4 Grey Grey 0.395658 -0.102141 -0.086338 UV ARGUS ARX V 16 706 UV PP 0.246254 -0.439913 -0.084811","title":"Check confounding variables"},{"location":"extras/X3_Truffletopia/#univariate-categorical-to-quantitative-moods-median","text":"A limitation of the previous analysis is that our threshold for counting an upset in the line was pretty low - any delay (position out > 0 seconds). A way we can naturally increase this threshold is to compare medians (of position out (secs)) across our groups. chidf = pd . DataFrame () target = 'Seconds' chidf [ target ] = df [ seconds ] . sum ( 1 ) moods = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind in feeds : try : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) grp = chidf . loc [ chidf [ ind ] == True ][ target ] . values chi2 , p , m , cTable = stats . median_test ( grp , pop , correction = False ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] < 1 moods = pd . concat ([ moods , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass moods . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] moods = moods . sort_values ( 'p-value' ) . reset_index ( drop = True ) moods . loc [ moods [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null 0 PP TOTAL 7089 284.071422 9.736103e-64 [[1093, 8620], [458, 9277]] False True 1 MFR 4 Grey Grey 161.505327 5.305934e-37 [[744, 9215], [1288, 8682]] True True 2 PP POLYCHIM HB12XF 140.079653 2.557382e-32 [[897, 8734], [475, 9163]] False True 3 High MFR Grey 134.851343 3.558380e-31 [[1244, 8697], [753, 9200]] False True 4 SILIKAT PBH ABPP 05 132.735135 1.033157e-30 [[1205, 8704], [725, 9193]] False True 5 SILIKAT POLYPLUS AB 1001 PP 40.522176 1.943972e-10 [[341, 8870], [196, 9027]] False True 6 DEVAN 2SPIN PP 106 39.379816 3.488847e-10 [[2499, 8758], [2119, 9139]] False True 7 BLACK SCHULMAN P7381 33.697655 6.437826e-09 [[6961, 8686], [6465, 9211]] False True 8 GREEN POLYPLAST COLOUR MB PP 6821 32.518968 1.180348e-08 [[589, 8854], [414, 9043]] False True 9 PP INEOS 100 GA09 26.828649 2.223170e-07 [[630, 8854], [465, 9043]] False True 10 ADD ARGUS ARGUTHERM AO 77 PP 26.802325 2.253660e-07 [[1250, 8823], [1020, 9074]] False True 11 UV ARGUS ARX V 16 706 UV PP 23.787198 1.075957e-06 [[5152, 8744], [4773, 9153]] False True 12 High MFR Black 18.389746 1.800242e-05 [[1626, 8835], [1409, 9062]] False True 13 ADD POLYPLAST PPM AO 01 UV PP 15.536764 8.091620e-05 [[499, 9000], [629, 8897]] True True 14 UV SCHULMAN FPPUV 38 12.417442 4.253424e-04 [[330, 8983], [426, 8914]] True True 15 MFR 4 White 11.470090 7.072526e-04 [[138, 8971], [200, 8926]] True True 16 GREEN SCHULMAN PZ 302446 1 5 T 9.924320 1.631091e-03 [[331, 8983], [416, 8914]] True True we see that overall the moods test is slightly less pessimistic chisum [ 'reject null' ] . value_counts () True 16 False 16 Name: reject null, dtype: int64 moods [ 'reject null' ] . value_counts () True 17 False 15 Name: reject null, dtype: int64 testsf = tests . join ( moods . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], lsuffix = ', chi' , rsuffix = ', moods' ) testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True 2.223170e-07 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True 2.253660e-07 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True 1.800242e-05 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True 1.180348e-08 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True 8.091620e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True 1.943972e-10 True","title":"Univariate Categorical to Quantitative (Moods Median)"},{"location":"extras/X3_Truffletopia/#non-parametric-conclusions","text":"problem_vars = testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] . index print ( f \"Compared with correlation \\n the dependency tests show that the top 5 problem variables are: \\n { [ f ' { i } ' for i in problem_vars ] } \" ) Compared with correlation the dependency tests show that the top 5 problem variables are: ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True","title":"Non-Parametric Conclusions"},{"location":"extras/X3_Truffletopia/#parametric","text":"","title":"Parametric"},{"location":"extras/X3_Truffletopia/#univariate-quantitative-to-quantitative-linear-regression","text":"for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () plt . plot ( x , ( y - y_pred ), ls = '' , marker = '.' ) plt . ylabel ( 'Residual' ) plt . xlabel ( 'x' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( f \"test statistic: { t } \" ) print ( f \"p-value: { stats . t . sf ( x = t , df = n - 2 ) } \" ) R2: 3.34e-02 RSE: 9.97e-03 b: -2.51e-03 m: 3.61e-01 n: 338 x_bar: 1.11e-01 SE_b: 1.51e-03 SE_m: 1.26e-02 test statistic: 28.56535956265302 p-value: 3.2661498371310494e-92 R2: 4.31e-02 RSE: 5.27e-03 b: -3.19e-03 m: 9.08e-02 n: 1551 x_bar: 2.89e-01 SE_b: 3.09e-04 SE_m: 9.66e-04 test statistic: 94.02166426512674 p-value: 0.0 R2: 1.89e-02 RSE: 5.94e-03 b: -1.36e-02 m: 1.57e+01 n: 537 x_bar: 2.64e-03 SE_b: 1.04e-03 SE_m: 3.81e-01 test statistic: 41.11483903422569 p-value: 4.981863349397906e-168 for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.033 Model: OLS Adj. R-squared: 0.030 Method: Least Squares F-statistic: 11.60 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.000739 Time: 10:02:50 Log-Likelihood: 360.28 No. Observations: 338 AIC: -716.6 Df Residuals: 336 BIC: -708.9 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0025 0.013 -0.199 0.843 -0.027 0.022 x1 0.3612 0.106 3.406 0.001 0.153 0.570 ============================================================================== Omnibus: 457.321 Durbin-Watson: 1.128 Prob(Omnibus): 0.000 Jarque-Bera (JB): 55378.392 Skew: 6.475 Prob(JB): 0.00 Kurtosis: 64.356 Cond. No. 23.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.043 Model: OLS Adj. R-squared: 0.042 Method: Least Squares F-statistic: 69.71 Date: Sat, 02 Apr 2022 Prob (F-statistic): 1.50e-16 Time: 10:02:50 Log-Likelihood: 2180.0 No. Observations: 1551 AIC: -4356. Df Residuals: 1549 BIC: -4345. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0032 0.003 -0.917 0.359 -0.010 0.004 x1 0.0908 0.011 8.349 0.000 0.069 0.112 ============================================================================== Omnibus: 2262.527 Durbin-Watson: 1.176 Prob(Omnibus): 0.000 Jarque-Bera (JB): 660230.502 Skew: 8.562 Prob(JB): 0.00 Kurtosis: 102.615 Cond. No. 7.82 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.019 Model: OLS Adj. R-squared: 0.017 Method: Least Squares F-statistic: 10.29 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.00142 Time: 10:02:50 Log-Likelihood: 622.03 No. Observations: 537 AIC: -1240. Df Residuals: 535 BIC: -1231. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0136 0.013 -1.020 0.308 -0.040 0.013 x1 15.6795 4.888 3.208 0.001 6.078 25.281 ============================================================================== Omnibus: 739.911 Durbin-Watson: 1.268 Prob(Omnibus): 0.000 Jarque-Bera (JB): 108127.158 Skew: 7.213 Prob(JB): 0.00 Kurtosis: 71.003 Cond. No. 1.49e+03 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.49e+03. This might indicate that there are strong multicollinearity or other numerical problems.","title":"Univariate Quantitative to Quantitative (Linear Regression)"},{"location":"extras/X3_Truffletopia/#feature-engineering","text":"Introducing polynomial features results in poor coefficient estimates ind = ind_vars [ 1 ] lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) features = PolynomialFeatures ( degree = 3 ) x = features . fit_transform ( x . reshape ( - 1 , 1 )) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.056 Model: OLS Adj. R-squared: 0.055 Method: Least Squares F-statistic: 30.83 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.35e-19 Time: 10:03:01 Log-Likelihood: 2190.9 No. Observations: 1551 AIC: -4374. Df Residuals: 1547 BIC: -4352. Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0030 0.009 0.334 0.739 -0.014 0.020 x1 0.1612 0.124 1.301 0.193 -0.082 0.404 x2 -0.9203 0.523 -1.761 0.078 -1.945 0.105 x3 1.6922 0.664 2.550 0.011 0.390 2.994 ============================================================================== Omnibus: 2252.329 Durbin-Watson: 1.188 Prob(Omnibus): 0.000 Jarque-Bera (JB): 651735.784 Skew: 8.487 Prob(JB): 0.00 Kurtosis: 101.979 Cond. No. 594. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. R2: 5.64e-02 RSE: 6.03e-03 b: 2.95e-03 m: 0.00e+00 n: 1551 x_bar: 3.58e-01 SE_b: 1.69e-04 SE_m: 1.96e-04 0.0 0.5","title":"Feature Engineering"},{"location":"extras/X3_Truffletopia/#multivariate-quantitative-to-quantitative-multivariate-linear-regression","text":"lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) lindf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ADD ARGUS ARGUSTAT AT 31 HK ADD ARGUS ARGUTHERM AO 77 PP ADD POLYPLAST PPM AO 01 UV PP ADD SCHULMAN DTY 20 AOUV BEIGE SCHULMAN PZ 91738 1 5 ZA BLACK SCHULMAN P7381 DEVAN 2SPIN PP 106 GREEN POLYPLAST COLOUR MB PP 6821 GREEN SCHULMAN PZ 302446 1 5 T High MFR Black ... UV POLYPLAST 6005 PP UV SCHULMAN FPPUV 38 MFR 4 Grey PP POLYCHIM HL10XF MFR 4 Grey Grey PP POLYMER TEST MFI4 PP TOTAL 4069 RR MASTERBATCH TEST UV ARGUS ARX V 16 706 UV PP seconds 0 0.0 0.0 0.0 0.005337 0.0 0.000253 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.409353 0.0 0.0 0.0 0.0 0.0 0.000000 9.0 1 0.0 0.0 0.0 0.005867 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.452007 0.0 0.0 0.0 0.0 0.0 0.000000 536.0 2 0.0 0.0 0.0 0.005895 0.0 0.000270 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.449608 0.0 0.0 0.0 0.0 0.0 0.000000 20.0 3 0.0 0.0 0.0 0.005743 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.440883 0.0 0.0 0.0 0.0 0.0 0.000000 1001.0 4 0.0 0.0 0.0 0.004553 0.0 0.000211 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.349459 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.339726 0.0 0.0 0.0 0.0 0.0 0.002062 0.0 17893 0.0 0.0 0.0 0.000000 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.342659 0.0 0.0 0.0 0.0 0.0 0.002071 0.0 17894 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345886 0.0 0.0 0.0 0.0 0.0 0.002107 0.0 17895 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.344945 0.0 0.0 0.0 0.0 0.0 0.002095 799.0 17896 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345288 0.0 0.0 0.0 0.0 0.0 0.002105 429.0 17897 rows \u00d7 45 columns lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) x = lindf [ feeds ] . values # features = PolynomialFeatures(degree=3) # x = features.fit_transform(x.reshape(-1,1)) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.050 Model: OLS Adj. R-squared: 0.049 Method: Least Squares F-statistic: 31.47 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.76e-174 Time: 10:03:19 Log-Likelihood: 44791. No. Observations: 17897 AIC: -8.952e+04 Df Residuals: 17866 BIC: -8.928e+04 Df Model: 30 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0123 0.002 5.316 0.000 0.008 0.017 x1 -1.0553 0.382 -2.760 0.006 -1.805 -0.306 x2 -0.0119 0.010 -1.241 0.215 -0.031 0.007 x3 0.0351 0.100 0.352 0.725 -0.161 0.231 x4 -0.2377 0.069 -3.438 0.001 -0.373 -0.102 x5 0.4001 0.145 2.750 0.006 0.115 0.685 x6 -0.0224 0.128 -0.175 0.861 -0.272 0.228 x7 -0.3174 0.096 -3.305 0.001 -0.506 -0.129 x8 -0.5909 0.209 -2.825 0.005 -1.001 -0.181 x9 -0.4059 0.087 -4.655 0.000 -0.577 -0.235 x10 -0.0241 0.005 -5.206 0.000 -0.033 -0.015 x11 -2.473e-14 1.17e-14 -2.107 0.035 -4.77e-14 -1.73e-15 x12 -0.0050 0.004 -1.371 0.171 -0.012 0.002 x13 -0.0288 0.007 -4.319 0.000 -0.042 -0.016 x14 -0.0268 0.005 -5.644 0.000 -0.036 -0.018 x15 -0.0200 0.006 -3.620 0.000 -0.031 -0.009 x16 -1.023e-14 3.5e-15 -2.923 0.003 -1.71e-14 -3.37e-15 x17 -9.109e-16 2.5e-16 -3.649 0.000 -1.4e-15 -4.22e-16 x18 -0.0994 0.093 -1.073 0.283 -0.281 0.082 x19 -0.0055 0.003 -2.044 0.041 -0.011 -0.000 x20 0.0003 0.005 0.058 0.954 -0.009 0.010 x21 1.325e-15 1.2e-16 11.078 0.000 1.09e-15 1.56e-15 x22 -2.494e-16 8.68e-17 -2.873 0.004 -4.2e-16 -7.93e-17 x23 0.0145 0.004 3.827 0.000 0.007 0.022 x24 -0.0100 0.010 -1.051 0.293 -0.029 0.009 x25 0.0216 0.004 5.724 0.000 0.014 0.029 x26 -0.0101 0.002 -4.198 0.000 -0.015 -0.005 x27 -1.03e-16 8.73e-17 -1.180 0.238 -2.74e-16 6.8e-17 x28 9.329e-16 7.53e-17 12.396 0.000 7.85e-16 1.08e-15 x29 -0.0074 0.003 -2.881 0.004 -0.012 -0.002 x30 1.7091 0.132 12.994 0.000 1.451 1.967 x31 4.1538 0.376 11.053 0.000 3.417 4.890 x32 0 0 nan nan 0 0 x33 0.0031 0.012 0.263 0.792 -0.020 0.026 x34 0.0031 0.012 0.263 0.792 -0.020 0.026 x35 0.0031 0.012 0.263 0.792 -0.020 0.026 x36 0.0790 0.042 1.868 0.062 -0.004 0.162 x37 0.0525 0.030 1.774 0.076 -0.006 0.111 x38 -0.0064 0.003 -2.521 0.012 -0.011 -0.001 x39 0 0 nan nan 0 0 x40 -0.0136 0.005 -2.762 0.006 -0.023 -0.004 x41 0 0 nan nan 0 0 x42 0 0 nan nan 0 0 x43 0 0 nan nan 0 0 x44 0.1990 0.048 4.119 0.000 0.104 0.294 ============================================================================== Omnibus: 43479.020 Durbin-Watson: 1.074 Prob(Omnibus): 0.000 Jarque-Bera (JB): 693938974.691 Skew: 25.434 Prob(JB): 0.00 Kurtosis: 966.322 Cond. No. 1.11e+16 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 1.79e-28. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. R2: 5.02e-02 RSE: 1.67e-03 b: 1.23e-02 m: -1.06e+00 n: 17897 x_bar: 2.26e-02 SE_b: 1.25e-05 SE_m: 2.14e-05 -49284.75476441197 1.0","title":"Multivariate Quantitative to Quantitative (Multivariate Linear Regression)"},{"location":"extras/X3_Truffletopia/#forward-selection","text":"def add_feature ( features , basemodel , data , y , r2max ): for feed in features : basemodel [ feed ] = data [ feed ] basemodel . fillna ( 0 , inplace = True ) x = basemodel [[ col for col in basemodel . columns if col != 'seconds' ]] . values ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () if ( est2 . rsquared > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = est2 . rsquared feedmax = feed bestsum = est2 . summary () newmodel = basemodel . copy () else : pass basemodel . drop ( labels = feed , axis = 1 , inplace = True ) return r2max , feedmax , bestsum , newmodel lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) r2max = 0 candidates = feeds . copy () basemodel = lindf while True : newr2max , feedmax , bestsum , newmodel = add_feature ( features = candidates , basemodel = basemodel , data = df , y = y , r2max = 0 ) if newr2max > r2max : # print(newr2max, feedmax) candidates . remove ( feedmax ) r2max = newr2max basemodel = newmodel continue else : break /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) we see some of the usual suspects from the chi-square tests. Notably some are missing: High MFR Grey , BLACK SCHULMAN P7381 , PP INEOS 100 GA09 and others. basemodel . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } seconds PP TOTAL 7089 PP POLYCHIM HB12XF SILIKAT PBH ABPP 05 SILIKAT POLYPLUS AB 1001 PP MFR 4 Green ADD SCHULMAN DTY 20 AOUV GREEN SCHULMAN PZ 302446 1 5 T MFR 4 Grey UV ARGUS ARX V 16 706 UV PP GREEN SCHULMAN PZ 34198 1 5 T 0 9.0 0.0 0.0 0.0 0.0 0.0 0.005337 0.0 0.409353 0.0 0.0 1 536.0 0.0 0.0 0.0 0.0 0.0 0.005867 0.0 0.452007 0.0 0.0 2 20.0 0.0 0.0 0.0 0.0 0.0 0.005895 0.0 0.449608 0.0 0.0 3 1001.0 0.0 0.0 0.0 0.0 0.0 0.005743 0.0 0.440883 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.004553 0.0 0.349459 0.0 0.0 forward_selection_feeds = basemodel . columns [ 1 :] bestsum OLS Regression Results Dep. Variable: y R-squared: 0.046 Model: OLS Adj. R-squared: 0.045 Method: Least Squares F-statistic: 94.99 Date: Sat, 02 Apr 2022 Prob (F-statistic): 5.51e-174 Time: 10:04:41 Log-Likelihood: 44748. No. Observations: 17897 AIC: -8.948e+04 Df Residuals: 17887 BIC: -8.940e+04 Df Model: 9 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 0.0034 0.000 11.288 0.000 0.003 0.004 x1 0.0214 0.002 11.535 0.000 0.018 0.025 x2 0.0146 0.002 7.837 0.000 0.011 0.018 x3 1.5742 0.110 14.375 0.000 1.360 1.789 x4 4.1700 0.363 11.497 0.000 3.459 4.881 x5 -0.0181 0.003 -6.276 0.000 -0.024 -0.012 x6 -0.2879 0.065 -4.450 0.000 -0.415 -0.161 x7 -0.1932 0.052 -3.698 0.000 -0.296 -0.091 x8 0.0037 0.001 3.322 0.001 0.002 0.006 x9 0.0736 0.028 2.641 0.008 0.019 0.128 x10 0 0 nan nan 0 0 x11 0 0 nan nan 0 0 Omnibus: 43415.763 Durbin-Watson: 1.068 Prob(Omnibus): 0.000 Jarque-Bera (JB): 683838684.139 Skew: 25.346 Prob(JB): 0.00 Kurtosis: 959.275 Cond. No. inf Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 0. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.","title":"Forward Selection"},{"location":"extras/X3_Truffletopia/#multivariate-conclusions","text":"y poses a problem with out of the box regression. Extreme tails will upset the residual plots. We some overlap of the parametric and non-parametric approaches: [ i for i in basemodel . columns if i in ( problem_vars )] ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05']","title":"Multivariate Conclusions"},{"location":"extras/X3_Truffletopia/#multivariate-quantitative-to-categorical-binned-output-variable","text":"Logistic regression does little better than random guessing lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) x = lindf [ ind ] . values #.reshape(-1,1) model = LogisticRegression () model . fit ( x , y ) y_pred = model . predict ( x ) print ( f \"Train Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) Train Acc: 0.37 Test Acc: 0.37 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( print ( classification_report ( y , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.35 0.80 0.49 4527 2.0 0.36 0.32 0.34 4434 3.0 0.30 0.03 0.06 4463 4.0 0.43 0.32 0.37 4473 accuracy 0.37 17897 macro avg 0.36 0.37 0.31 17897 weighted avg 0.36 0.37 0.31 17897 <AxesSubplot:> If we are careful about regularizing the random forest model, it does a little bit better lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) X = lindf [ ind ] . values #.reshape(-1,1) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 0.99 Test Acc: 0.39 y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.41 0.48 0.44 898 2.0 0.33 0.30 0.31 881 3.0 0.29 0.27 0.28 893 4.0 0.50 0.51 0.51 908 accuracy 0.39 3580 macro avg 0.38 0.39 0.38 3580 weighted avg 0.38 0.39 0.38 3580 <AxesSubplot:> # grab feature importances imp = model . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in model . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ ind , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 PP TOTAL PPH 4065 0.150387 0.008053 1 BLACK SCHULMAN P7381 0.118158 0.008725 2 PP INEOS 100 GA04 0.100635 0.005217 3 UV ARGUS ARX V 16 706 UV PP 0.090776 0.006273 4 MFR 4 Grey 0.08828 0.005656 5 PP UNIPETROL GB005 0.060558 0.003569 6 MFR 4 Black 0.052589 0.003695 7 ADD SCHULMAN DTY 20 AOUV 0.041478 0.003554 8 DEVAN 2SPIN PP 106 0.038438 0.004061 9 High MFR Black 0.028838 0.002515 10 MFR 4 Grey Grey 0.025758 0.002319 11 ADD ARGUS ARGUTHERM AO 77 PP 0.024526 0.002253 12 High MFR Grey 0.020652 0.004188 13 MFR 4 Green 0.016508 0.002725 14 SILIKAT PBH ABPP 05 0.016283 0.002291 15 PP TOTAL 7089 0.01624 0.004897 16 ADD POLYPLAST PPM AO 01 UV PP 0.013499 0.001685 17 MFR 4 Black Brown 0.013401 0.002131 18 PP POLYCHIM HB12XF 0.01245 0.002185 19 PP INEOS 100 GA09 0.010354 0.00156","title":"Multivariate Quantitative to Categorical (Binned Output Variable)"},{"location":"extras/X3_Truffletopia/#60-business-impact","text":"What is the so what? From Hyp tests - target is either 'Total Position Out' or 'Total Seconds Out' We had culprits from pearsons: ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] culprits from non parametric tests: list ( problem_vars ) ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] and culprits from linear regression forward_selection_feeds Index(['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'MFR 4 Green', 'ADD SCHULMAN DTY 20 AOUV', 'GREEN SCHULMAN PZ 302446 1 5 T', 'MFR 4 Grey', 'UV ARGUS ARX V 16 706 UV PP', 'GREEN SCHULMAN PZ 34198 1 5 T'], dtype='object') and we can check the overlap between non-parametric and linear regression (parametric): predict_vars = [ i for i in basemodel . columns if i in ( problem_vars )] predict_vars ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05'] all_vars = set ( list ( problem_vars ) + list ( forward_selection_feeds )) all_vars {'ADD SCHULMAN DTY 20 AOUV', 'BLACK SCHULMAN P7381', 'GREEN SCHULMAN PZ 302446 1 5 T', 'GREEN SCHULMAN PZ 34198 1 5 T', 'High MFR Grey', 'MFR 4 Green', 'MFR 4 Grey', 'PP POLYCHIM HB12XF', 'PP TOTAL 7089', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'UV ARGUS ARX V 16 706 UV PP'} # add to DF business = pd . DataFrame () value_line_by_day = 99000 for ind in problem_vars : try : # products, prods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique () nprods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . nunique () dff = df . loc [ df [ 'Product' ] . isin ( prods )] # total runtime, tot_runtime = dff [ 'Time Delta' ] . sum () # total runtime with feed, tot_runtime_wfeed = dff [ dff [ ind ] > 0 ][ 'Time Delta' ] . sum () #downtime w/o feed, downtime w/ feed, avg_downtime_min = dff . groupby ( dff [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 avg_downtime_min_wfeed = avg_downtime_min [ True ] avg_downtime_min_w_ofeed = avg_downtime_min [ False ] ratio = avg_downtime_min_wfeed / avg_downtime_min_w_ofeed # average runtime, avg_runtime_min = dff [ 'Time Delta' ] . mean () . total_seconds () / 60 # days saved, dollars saved days_saved = ( tot_runtime_wfeed . total_seconds () / 60 / avg_runtime_min * avg_downtime_min . diff () . values [ - 1 ]) / 60 / 24 dollars_saved_per_batch = value_line_by_day * avg_downtime_min . diff () . values [ - 1 ] / ( 60 * 24 ) business = pd . concat (( business , pd . DataFrame ([[ ind , nprods , tot_runtime , tot_runtime_wfeed , avg_downtime_min_wfeed , avg_downtime_min_w_ofeed , ratio , avg_runtime_min , days_saved , days_saved * value_line_by_day , dollars_saved_per_batch ]], columns = [ 'Feed' , 'Products' , 'Total Runtime, Products' , 'Total Runtime With Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Average Runtime' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]))) except : print ( ind ) print ( dff . groupby ( df [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 ) business = business . sort_values ( 'Dollars Saved (Per Batch)' , ascending = False ) . reset_index ( drop = True ) business # workshop downtime ratio # feed products, feed products # Dollars saved per batch .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Products Total Runtime, Products Total Runtime With Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Average Runtime Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26 409 days 20:43:21 94 days 04:36:05 26.250639 13.671063 1.920161 70.835736 16.727319 1.656005e+06 864.845823 1 High MFR Grey 36 564 days 04:13:41 94 days 12:26:59 23.623539 12.809036 1.844287 65.459164 15.615433 1.545928e+06 743.497080 2 PP POLYCHIM HB12XF 17 383 days 00:03:51 67 days 09:13:53 21.043525 15.600015 1.348943 70.908183 5.173013 5.121283e+05 374.241303 3 BLACK SCHULMAN P7381 64 751 days 20:01:57 590 days 12:21:50 14.902871 12.363943 1.205349 63.216276 23.716605 2.347944e+06 174.551301 4 PP TOTAL 7089 13 291 days 22:06:22 79 days 15:56:47 20.651050 18.269305 1.130369 80.146114 2.367430 2.343756e+05 163.744933 pd . set_option ( 'display.precision' , 2 ) display ( business [[ 'Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]], ) pd . set_option ( 'display.precision' , 6 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26.25 13.67 1.92 16.73 1.66e+06 864.85 1 High MFR Grey 23.62 12.81 1.84 15.62 1.55e+06 743.50 2 PP POLYCHIM HB12XF 21.04 15.60 1.35 5.17 5.12e+05 374.24 3 BLACK SCHULMAN P7381 14.90 12.36 1.21 23.72 2.35e+06 174.55 4 PP TOTAL 7089 20.65 18.27 1.13 2.37 2.34e+05 163.74 We see that 'SILKAT PBH ABPP 05' as well as 'High MFR Grey' have a large amount of days saved over the two years, a strong downtime multiplier effect and the largest dollars saved per batch. Since High MFR Grey is a recycle feed, we may not have the ability to remove it from the feed list. Leaving the SILKAT (Silicate) feed the prime feedstock to remove","title":"6.0 Business Impact"},{"location":"extras/X3_Truffletopia/#70-visualizations","text":"","title":"7.0 Visualizations"},{"location":"extras/X3_Truffletopia/#pp-total-7089","text":"predict_var = predict_vars [ 1 ] outcome_var = [ i for i in df . columns if 'Seconds' in i ] def myplot ( col = list ( df . loc [ df [ predict_var ] > 0 ][ 'Product' ] . unique ())): fig , ax = plt . subplots ( figsize = ( 20 , 10 )) axt = ax . twinx () # other feeds df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True ) . dropna ( axis = 1 , how = 'all' ) . plot ( ls = '' , marker = '*' , ax = ax , alpha = 0.2 ) # predict_var feed df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True )[ predict_var ] . plot ( ls = '--' , marker = '+' , ax = ax , lw = 2 , c = 'red' ) # outcome_var df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ) . plot ( ls = '-' , marker = '+' , lw = 1 , ax = axt , alpha = 1 , c = 'blue' , label = 'Total Seconds Out' ) # outliers ax . set_ylim ( 0 ) axt . set_ylim ( 0 , np . quantile ( df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ), 0.997 )) h1 , l1 = ax . get_legend_handles_labels () h2 , l2 = axt . get_legend_handles_labels () ax . legend ( h1 + h2 , l1 + l2 , loc = 0 ) interact ( myplot ) interactive(children=(Dropdown(description='col', options=(0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.\u2026 <function __main__.myplot(col=[0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.0, 136215.0, 125215.0, 110215.0, 100215.0, 80215.0, 100617.0, 90617.0, 90215.0, 90216.0, 110520.0, 80617.0])>","title":"PP TOTAL 7089"},{"location":"extras/X4_Candy_Ribbons/","text":"Causality Analysis \u00b6 TODO \u00b6 ols assumptions for univariate analyses residual plots forward selection lines up more or less with chi square tests include some colinearity measure? logistic regression (binned seconds, binned total outs) random forest does little better than random guessing business impact reorganize hypothesis tests section univariate vs multivariate cat v vat, cat v quant, quant v quant, quant v cat 1.0 Imports \u00b6 Import Libraries \u00b6 # basic packages import pandas as pd import numpy as np import random import copy # visualization packages from ipywidgets import interact import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures , MinMaxScaler from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , silhouette_score , calinski_harabasz_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score def make_patch_spines_invisible ( ax ): ax . set_frame_on ( True ) ax . patch . set_visible ( False ) for sp in ax . spines . values (): sp . set_visible ( False ) from matplotlib import colors from itertools import cycle def my_colors (): tab = cycle ( colors . TABLEAU_COLORS ) return tab Load Data \u00b6 consider only feeds consider feeds and operating conditions feeds = [ 'ADD ARGUS ARGUSTAT AT 31 HK' , 'ADD ARGUS ARGUTHERM AO 77 PP' , 'ADD POLYPLAST PPM AO 01 UV PP' , 'ADD SCHULMAN DTY 20 AOUV' , 'BEIGE SCHULMAN PZ 91738 1 5 ZA' , 'BLACK SCHULMAN P7381' , 'DEVAN 2SPIN PP 106' , 'GREEN POLYPLAST COLOUR MB PP 6821' , 'GREEN SCHULMAN PZ 302446 1 5 T' , 'High MFR Black' , 'GREEN SCHULMAN PZ 34198 1 5 T' , 'MFR 4 Black' , 'MFR 4 Black Brown' , 'High MFR Grey' , 'MFR 4 Green' , 'PP BOREALIS HE465FB' , 'MFR 4 Black Colour Blending' , 'PP INEOS 100 GA02' , 'PP INEOS 100 GA04' , 'PP INEOS 100 GA09' , 'MFR 4 Grey Colour Blending' , 'PP INEOS GA012' , 'PP POLYCHIM HB12XF' , 'MFR 4 White' , 'PP TOTAL 7089' , 'PP TOTAL PPH 4065' , 'MFR 4 White Colour Blending' , 'PP BOREALIS HE370FB' , 'PP UNIPETROL GB005' , 'SILIKAT PBH ABPP 05' , 'SILIKAT POLYPLUS AB 1001 PP' , 'UV ARGUS ARGUVIN UV 361 PP' , 'UV ARGUS ARX 904 11 PP' , 'UV ARGUS ARX 904 11 PP.1' , 'UV ARGUS ARX 904 11 PP.2' , 'UV POLYPLAST 6005 PP' , 'UV SCHULMAN FPPUV 38' , 'MFR 4 Grey' , 'PP POLYCHIM HL10XF' , 'MFR 4 Grey Grey' , 'PP POLYMER TEST MFI4' , 'PP TOTAL 4069' , 'RR MASTERBATCH TEST' , 'UV ARGUS ARX V 16 706 UV PP' ] df = pd . read_csv ( '../../../../../Dropbox/work/mfganalytic/data/dupont/TyparMasterReportWithSampleDataImprovement2017-today.csv' , header = 1 ) df = df . loc [ df [ 'Product' ] . notnull ()] df [ feeds ] = df [ feeds ] . div ( df [ feeds ] . sum ( axis = 1 ), axis = 0 ) #normalize feed rates df [ 'Time Delta' ] = pd . to_datetime ( df [ 'Doff Time' ]) . diff () df [ 'Total Seconds Out' ] = df [[ i for i in df . columns if 'Sec' in i ]] . sum ( 1 ) # df.dropna(inplace=True) hangs = [ df . columns [ i ] for i , j in enumerate ( 'Hang' in col for col in df . columns ) if j ] speeds = [ df . columns [ i ] for i , j in enumerate ( 'speed' in col for col in df . columns ) if j ] outs = [ df . columns [ i ] for i , j in enumerate ( 'Out' in col for col in df . columns ) if j ] prods = df [ 'Product' ] . unique () counts = [ i for i in df . columns if 'Count' in i ] #from LOEWEN virgin_polymer = [ 'PP INEOS 100 GA04' , 'PP POLYCHIM HB12XF' , 'PP TOTAL 7089' , #INEOS 100 GA09 until 2018 (2k rolls) 'PP TOTAL PPH 4065' , 'PP UNIPETROL GB005' , 'PP POLYCHIM HL10XF' ] #HL10XF not used after 2016, #BOREALIS virgin_polymer = [] for col in df . columns : if ( 'PP INEOS' in col ) or ( 'PP TOTAL' in col ) or ( 'PP UNI' in col ) or ( 'PP BORE' in col ) or ( 'PP POLY' in col ): virgin_polymer . append ( col ) pressures = [] for col in df . columns : if 'Press' in col or 'PRESS' in col : pressures . append ( col ) silicates = [] for col in df . columns : if 'SIL' in col : silicates . append ( col ) uvs = [] for col in df . columns : if 'UV ' in col : uvs . append ( col ) recycle = [] for col in df . columns : if 'MFR' in col : recycle . append ( col ) seconds = [] for col in df . columns : if 'Sec' in col : seconds . append ( col ) master_batch = list ( set ( feeds ) - set ( recycle ) - set ( virgin_polymer )) operation = list ( set ( df . columns ) - set ( master_batch ) - set ( virgin_polymer ) - set ( recycle ) - set ( hangs ) - set ( outs )) operation . sort () A summary of the columns print ( f 'pressures: { len ( pressures ) } ' ) print ( f 'silicates: { len ( silicates ) } ' ) print ( f 'uvs: { len ( uvs ) } ' ) print ( f 'feeds: { len ( feeds ) } ' ) print ( f 'master_batch: { len ( master_batch ) } ' ) print ( f 'virgin_polymer: { len ( virgin_polymer ) } ' ) print ( f 'recycle: { len ( recycle ) } ' ) print ( f 'operation: { len ( operation ) } ' ) print () print ( f 'hangs: { len ( hangs ) } ' ) print ( f 'speeds: { len ( speeds ) } ' ) print ( f 'outs: { len ( outs ) } ' ) print ( f 'seconds: { len ( seconds ) } ' ) print () print ( 'summary target columns: TotalHangs, Total Position Out' ) print () print ( df . shape ) pressures: 8 silicates: 2 uvs: 8 feeds: 44 master_batch: 20 virgin_polymer: 13 recycle: 11 operation: 180 hangs: 9 speeds: 9 outs: 10 seconds: 9 summary target columns: TotalHangs, Total Position Out (17897, 243) all_feats = df . select_dtypes ( float ) . columns [ df . select_dtypes ( float ) . columns . isin ( feeds + operation )] . values df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Millroll ID Product Roll Width Sample Number Doff Time Block1Pos Out Seconds Block2Pos Out Seconds Block3Pos Out Seconds Block4Pos Out Seconds Block1Pos Out Count ... TKW5 AVG - Tack - Tear West 5 Meter TPOP - NUMBER OF TPO PEEKS (TPOP) TTMD AVG - TRAP TEAR MD (TTMD) TTXD AVG - TRAP TEAR XD (TTXD) UPEX - UWU PLOT EXTREM (UPEX) UWG - UWU GRAVIMETRIC WITH SCALE (UWG) UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP) WATP - WATER PERMEABILITY (WATP) Time Delta Total Seconds Out 0 PM10022907 136215.0 5200.0 NaN 1/1/2017 12:43:57 AM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaT 9.0 1 PM10022908 136215.0 5200.0 NaN 1/1/2017 1:32:49 AM NaN 164.0 240.0 NaN NaN ... NaN NaN NaN NaN NaN 138.70 NaN NaN 0 days 00:48:52 536.0 2 PM10022909 136215.0 5200.0 NaN 1/1/2017 2:21:40 AM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:48:51 20.0 3 PM10022910 136215.0 5200.0 NaN 1/1/2017 3:10:34 AM NaN 320.0 264.0 NaN NaN ... NaN NaN 36.96 33.59 NaN 135.98 NaN NaN 0 days 00:48:54 1001.0 4 PM10022911 136215.0 5200.0 NaN 1/1/2017 3:59:24 AM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:48:50 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 PM10040811 90215.0 4250.0 P113099,P113100,P113101,P113102,P113103 2/25/2019 7:56:16 PM NaN NaN NaN NaN NaN ... NaN NaN 21.30 22.10 NaN 89.60 NaN NaN 0 days 00:57:24 0.0 17893 PM10040812 90215.0 4250.0 P113104 2/25/2019 8:53:41 PM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:57:25 0.0 17894 PM10040813 90215.0 4250.0 P113105,P113106,P113107 2/25/2019 9:51:06 PM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 89.30 NaN NaN 0 days 00:57:25 0.0 17895 PM10040814 90215.0 4250.0 P113108 2/25/2019 10:48:32 PM NaN NaN 576.0 NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:57:26 799.0 17896 PM10040815 90215.0 4250.0 P113109,P113110,P113111,P113112 2/25/2019 11:45:55 PM NaN NaN 429.0 NaN NaN ... NaN NaN 23.20 22.60 NaN 91.40 NaN NaN 0 days 00:57:23 429.0 17897 rows \u00d7 243 columns 2.0 Understand The Dependent Variable \u00b6 What is the hit rate in these upset columns? \u00b6 Around 7500 cases with no problem; 10000 cases with a position out target = \"Total Position Out\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 10355 False 7542 Name: Total Position Out, dtype: int64 There are 30% higher incidence of hangs target = \"TotalHangs\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 13954 False 3943 Name: TotalHangs, dtype: int64 target1 = \"TotalHangs\" target2 = \"Total Position Out\" print ( f \"both upsets: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"only hangs: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) print ( f \"only outs: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"neither upset: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) both upsets: 10161 only hangs: 3793 only outs: 194 neither upset: 3749 What is the magnitude of the outages? \u00b6 target = \"Total Position Out\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f0ad7207b80>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total Position Out Unnamed 0 Total Position Out 1.00000 0.62591 Unnamed 0 0.62591 1.00000 target = \"TotalHangs\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f0ad3eff460>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TotalHangs Unnamed 0 TotalHangs 1.000000 0.390949 Unnamed 0 0.390949 1.000000 Distribution around the target variable, total seconds \u00b6 We will certainly encounter trouble in an OLS estimate with this shape of the target variable; extreme skewness, upper tail, perhaps outliers tot_seconds = df [ seconds ] . sum ( 1 ) tot_seconds . kurt () 923.6728632470827 tot_seconds . skew () 25.09323820232841 tot_seconds . plot . kde () <AxesSubplot:ylabel='Density'> tot_seconds . describe () count 17897.000000 mean 2093.512879 std 8060.466090 min 0.000000 25% 20.000000 50% 666.000000 75% 1970.000000 max 396570.000000 dtype: float64 Do block positions have the same behavior? \u00b6 target = \"TotalHangs\" df . groupby ( target )[ hangs ] . count () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs TotalHangs 0.0 0 0 0 0 0 0 0 0 3943 1.0 510 682 745 977 510 682 745 977 2914 2.0 709 761 901 1109 709 761 901 1109 2424 3.0 721 776 930 1103 721 776 930 1103 1929 4.0 700 773 875 993 700 773 875 993 1577 5.0 609 668 744 828 609 668 745 828 1189 6.0 523 498 599 631 523 498 599 631 883 7.0 387 414 472 505 387 414 472 505 640 8.0 321 338 376 409 321 338 376 409 503 9.0 286 275 305 339 286 275 305 339 405 10.0 219 206 244 248 219 206 244 248 296 11.0 144 154 168 171 144 154 168 171 203 12.0 148 139 159 171 148 139 159 171 191 13.0 105 100 117 106 105 100 117 106 131 14.0 93 88 95 96 93 88 95 96 109 15.0 97 89 94 97 97 89 94 97 108 16.0 70 64 72 76 70 64 72 76 81 17.0 51 50 51 47 51 50 51 47 55 18.0 29 30 32 33 29 30 32 33 35 19.0 32 32 33 32 32 32 33 32 35 20.0 32 32 31 34 32 32 31 34 35 21.0 26 27 25 24 26 27 25 24 28 22.0 21 20 24 24 21 20 24 24 24 23.0 23 23 24 23 23 23 24 23 25 24.0 16 17 18 19 16 17 18 19 20 25.0 7 8 8 7 7 8 8 7 8 26.0 11 10 10 11 11 10 10 11 11 27.0 8 9 9 8 8 9 9 8 9 28.0 10 10 10 10 10 10 10 10 10 29.0 6 5 6 6 6 5 6 6 6 30.0 4 3 4 4 4 3 4 4 4 31.0 8 8 8 8 8 8 8 8 8 32.0 4 3 4 4 4 3 4 4 4 33.0 7 5 7 6 7 5 7 6 7 34.0 9 8 9 8 9 8 9 8 9 35.0 6 6 6 6 6 6 6 6 6 36.0 6 5 6 6 6 5 6 6 6 37.0 3 3 3 3 3 3 3 3 3 38.0 1 1 1 0 1 1 1 0 1 39.0 1 1 1 1 1 1 1 1 1 40.0 2 2 2 2 2 2 2 2 2 42.0 1 1 1 1 1 1 1 1 1 43.0 3 3 3 3 3 3 3 3 3 45.0 3 3 3 3 3 3 3 3 3 46.0 2 2 2 2 2 2 2 2 2 48.0 1 1 1 1 1 1 1 1 1 52.0 1 1 1 1 1 1 1 1 1 53.0 1 1 1 1 1 1 1 1 1 54.0 1 1 1 1 1 1 1 1 1 58.0 1 1 1 1 1 1 1 1 1 62.0 1 1 1 1 1 1 1 1 1 63.0 1 1 1 1 1 1 1 1 1 70.0 1 1 1 1 1 1 1 1 1 72.0 1 1 1 1 1 1 1 1 1 140.0 1 1 1 1 1 1 1 1 1 df [[ i for i in outs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:> df [[ i for i in hangs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:> 3.0 Look for dependent-independent signal \u00b6 Are there linear relationships between the dependent and independent variables? \u00b6 corr = df . corr () ( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 ) Product False Roll Width False Block1Pos Out Seconds True Block2Pos Out Seconds True Block3Pos Out Seconds True ... UPEX - UWU PLOT EXTREM (UPEX) False UWG - UWU GRAVIMETRIC WITH SCALE (UWG) False UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP) False WATP - WATER PERMEABILITY (WATP) False Total Seconds Out True Length: 237, dtype: bool cor_filt = corr . loc [( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 )] . drop ( labels = outs + hangs , axis = 0 )[ outs + hangs ] cor_filt .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Pos Out Seconds Block2Pos Out Seconds Block3Pos Out Seconds Block4Pos Out Seconds Block1Pos Out Count Block2Pos Out Count Block3Pos Out Count Block4Pos Out Count Total Position Out Total Seconds Out Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs ADD ARGUS ARGUSTAT AT 31 HK -0.103999 -0.417688 -0.383222 0.202131 -0.056194 -0.412274 0.155126 0.145535 0.014216 0.029563 0.121682 -0.256723 -0.012676 0.099690 -0.069354 -0.041492 0.094150 0.041900 0.344896 PP INEOS 100 GA02 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 -1.000000 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 MFR 4 White 0.289648 -0.016268 0.451446 -0.155932 0.297256 -0.000098 0.376795 -0.003129 0.225425 0.182687 0.231764 -0.045076 0.283286 -0.119481 0.108396 -0.057188 0.317089 0.008018 0.189109 PP TOTAL 7089 0.160268 0.171023 0.210796 0.093720 0.192159 0.210942 0.206859 0.076058 0.260505 0.207522 0.087954 0.050670 0.052622 -0.050506 0.218148 0.251676 0.134881 0.121080 0.331897 SILIKAT POLYPLUS AB 1001 PP 0.015132 0.125801 0.379559 0.255662 -0.018715 0.012019 0.161698 0.067661 0.127830 0.137376 -0.016317 -0.018187 -0.018956 0.086781 0.011207 -0.020307 0.092291 0.109259 0.069217 RJET15219WG1 - Corona Charge I AVG BLK#2 -0.120204 -0.120509 -0.056377 -0.002054 -0.198138 -0.221189 -0.193393 -0.024541 -0.232863 -0.149862 -0.090896 -0.060404 -0.054169 -0.052683 -0.215325 -0.216545 -0.182990 -0.120387 -0.308217 BLFL - COLOUR BLACK FILAMENT L value (BLFL) -0.360086 -0.157845 -0.245565 -0.268837 -0.153321 -0.162697 -0.212481 -0.247721 -0.067632 -0.121690 0.097783 0.146515 -0.346923 -0.110987 0.402310 -0.274153 -0.378670 -0.116264 -0.117501 BRFA - COLOUR BROWN FILAMENT a value (BRFa) 0.159056 0.250229 -0.384880 0.109024 0.134248 0.112836 -0.380715 -0.054074 0.099040 0.166926 0.170184 -0.065344 -0.195062 0.094596 0.195404 -0.333632 -0.026209 0.177576 0.173995 BRFB - COLOUR BROWN FILAMENT b value (BRFb) 0.413876 0.047748 0.332752 0.342115 0.145794 -0.028170 0.246787 0.234624 0.216156 0.311363 0.148441 -0.210193 0.118275 0.285286 -0.027055 -0.275750 0.100830 0.328240 0.164345 BRFL - COLOUR BROWN FILAMENT L value (BRFL) 0.198864 -0.178050 0.483242 0.205796 0.017456 -0.220432 0.411110 0.090885 0.039031 0.128326 0.337130 -0.326368 0.075738 0.145878 0.195055 -0.467536 -0.075130 0.188013 -0.000575 E20M AVG - SST ELONGATION 20X20 MD (E20M) 0.106870 -0.096775 -0.249986 -0.039633 0.009518 -0.049131 -0.367065 -0.074018 -0.070058 -0.071425 -0.124456 -0.016993 -0.070312 -0.036314 -0.145788 -0.075968 -0.197153 -0.069477 -0.072872 EF1M AVG - SST ELONGATION FOLDED 10 CM MD (EF1M) 1.000000 NaN 1.000000 -0.081738 1.000000 NaN NaN NaN -0.101606 -0.178993 0.313364 -0.936781 -1.000000 -0.517401 0.306235 -0.950303 -1.000000 -0.324932 -0.112334 EF1X AVG - SST ELONGATION FOLDED 10 CM XD (EF1X) 1.000000 NaN 1.000000 0.126777 1.000000 NaN NaN NaN 0.010039 -0.063399 0.896084 -0.961383 -1.000000 -0.609904 0.892730 -0.971830 -1.000000 -0.340772 0.050483 NATM AVG - Nail Tear MD EN 13859-1 (NATM) -0.517421 -0.835147 0.511409 -0.258360 -0.296500 -0.906038 NaN -0.333754 -0.100244 0.013436 -0.338129 0.580493 0.826191 -0.322585 -0.348782 0.063309 0.929207 -0.430201 0.086484 NATX AVG - Nail Tear XD EN 13859-1 (NATX) 0.269389 -0.864128 0.542651 0.071499 -0.053990 -0.927938 NaN -0.056465 -0.084131 0.046366 -0.190830 0.541297 0.777131 -0.114506 -0.096640 0.051081 0.902927 -0.295087 0.093804 O90 - O90 EN ISO 12956 (O90E) NaN NaN NaN 0.807962 NaN NaN NaN NaN -0.265093 -0.268107 -0.280791 -0.378899 NaN -0.451610 -0.307148 -0.267261 NaN -0.269680 -0.193539 PU AVG - PUNCTURE \"US Rod\" (PU) 0.173625 -0.790779 -0.165265 0.451194 -0.127912 -0.952390 -0.184515 0.734933 -0.040323 -0.021124 0.096383 -0.998695 -0.288786 0.156274 0.066142 0.363427 -0.297118 0.479459 -0.239496 TKO4 AVG - Tack-Tear Ost 4Meter -0.170480 -0.383667 -0.326335 0.006590 -0.104058 -0.308260 -0.329544 0.047734 -0.171814 -0.195662 -0.214633 -0.440748 -0.182875 -0.027353 -0.077638 -0.447423 -0.187893 0.114764 -0.160340 TKO5 AVG - Tack - Tear Ost 5 Meter 1.000000 0.134107 0.560717 -0.070116 NaN 0.106485 0.152594 -0.525375 -0.373817 -0.306252 0.338272 0.155577 0.034557 -0.453643 0.286066 0.056270 -0.066270 -0.400941 -0.387297 TKW4 AVG - Tack-Tear West 4Meter -0.334291 -0.282220 -0.265607 -0.134467 -0.320379 -0.231417 -0.300679 -0.265049 -0.274024 -0.250617 -0.277325 -0.375559 -0.168965 -0.154119 -0.236424 -0.282687 -0.168865 -0.162801 -0.316193 TKW5 AVG - Tack - Tear West 5 Meter 1.000000 0.051583 0.100609 0.018368 NaN 0.014194 -0.167032 -0.366762 -0.279911 -0.190366 0.069184 0.108657 0.224892 -0.326866 -0.003422 0.043730 0.039670 -0.362928 -0.270898 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt , cmap = cmap ) <AxesSubplot:> Filter N > 100 \u00b6 cor_filt = cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]] fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]], cmap = cmap ) <AxesSubplot:> Operating Conditions \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Operations Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Operations Correlations to Hangs and Outs') Feeds \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ ~ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Feeds Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Feeds Correlations to Hangs and Outs') ind_vars = list ( cor_filt . index [ ~ cor_filt . index . isin ( operation )]) ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] fig , ax = plt . subplots ( 1 , 3 , figsize = ( 30 , 10 )) indices = np . argwhere ( ax ) for index , ind_var in enumerate ( ind_vars ): df [[ ind_var ] + seconds ] . melt ( id_vars = ind_var , value_vars = None , var_name = None ,) . plot ( x = ind_var , y = 'value' , ax = ax [ index ], ls = '' , marker = '.' ) 4.0 Understand the Independent Variable \u00b6 Descriptive Stats on Ind Var \u00b6 all the feeds sum to 1 (feeds are mass or volume fractions), so we probably won't need to worry about scaling df [ feeds ] . sum ( 1 ) 0 1.0 1 1.0 2 1.0 3 1.0 4 1.0 ... 17892 1.0 17893 1.0 17894 1.0 17895 1.0 17896 1.0 Length: 17897, dtype: float64 many of the feeds are not used at all skew = df [ feeds ] . skew () kurt = df [ feeds ] . kurtosis () null = df [ feeds ] . isnull () . sum () n = df [ feeds ] . notnull () . sum () med = df [ feeds ] . median () men = df [ feeds ] . mean () dff = pd . DataFrame ([ skew , kurt , null , n , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'null cnt' , 'n' , 'median' , 'mean' ] dff . loc [ dff [ 'n' ] > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis null cnt n median mean ADD ARGUS ARGUSTAT AT 31 HK -1.122018 -0.275303 17798.0 99.0 0.006560 0.005080 ADD ARGUS ARGUTHERM AO 77 PP 14.927951 248.300227 15627.0 2270.0 0.005249 0.010705 ADD POLYPLAST PPM AO 01 UV PP 5.788514 87.051660 16769.0 1128.0 0.006397 0.006218 ADD SCHULMAN DTY 20 AOUV 6.573319 99.052816 13484.0 4413.0 0.004997 0.005411 BEIGE SCHULMAN PZ 91738 1 5 ZA -0.455858 10.108094 16975.0 922.0 0.008922 0.008708 BLACK SCHULMAN P7381 2.301415 16.313957 4471.0 13426.0 0.000317 0.002040 DEVAN 2SPIN PP 106 0.770601 11.066370 13279.0 4618.0 0.006658 0.005554 GREEN POLYPLAST COLOUR MB PP 6821 -1.856631 3.540741 16894.0 1003.0 0.007519 0.006822 GREEN SCHULMAN PZ 302446 1 5 T 0.363239 8.864015 17150.0 747.0 0.018764 0.018006 High MFR Black 0.473484 -1.518082 14862.0 3035.0 0.047883 0.146677 MFR 4 Black 1.612264 4.261601 12595.0 5302.0 0.030099 0.098792 MFR 4 Black Brown -0.086521 -0.342944 16360.0 1537.0 0.189244 0.150273 High MFR Grey -0.364616 -1.483643 15900.0 1997.0 0.226358 0.199138 MFR 4 Green 0.149918 8.244109 15985.0 1912.0 0.279622 0.262574 PP INEOS 100 GA02 NaN NaN 17895.0 2.0 0.107927 0.107927 PP INEOS 100 GA04 1.294449 5.764334 6684.0 11213.0 0.208757 0.216941 PP INEOS 100 GA09 -0.449540 -0.968103 16802.0 1095.0 0.224353 0.219208 PP POLYCHIM HB12XF -0.222658 -0.684893 16525.0 1372.0 0.275755 0.291827 MFR 4 White 1.136619 2.053071 17559.0 338.0 0.098961 0.111129 PP TOTAL 7089 -0.216726 -1.333059 16346.0 1551.0 0.276430 0.288643 PP TOTAL PPH 4065 0.353911 0.740658 611.0 17286.0 0.441009 0.424684 PP UNIPETROL GB005 0.117291 1.045121 11444.0 6453.0 0.324773 0.295906 SILIKAT PBH ABPP 05 2.690146 26.471577 15967.0 1930.0 0.005001 0.005229 SILIKAT POLYPLUS AB 1001 PP 6.874010 77.288671 17360.0 537.0 0.002506 0.002642 UV ARGUS ARX 904 11 PP 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.1 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.2 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV POLYPLAST 6005 PP 1.426790 1.634549 17506.0 391.0 0.020009 0.026245 UV SCHULMAN FPPUV 38 2.114261 14.372541 17141.0 756.0 0.029986 0.032261 MFR 4 Grey 1.718802 10.970192 8567.0 9330.0 0.269445 0.263401 MFR 4 Grey Grey 3.607370 20.342313 15865.0 2032.0 0.040035 0.068547 UV ARGUS ARX V 16 706 UV PP 1.889895 6.474782 7972.0 9925.0 0.002110 0.007994 our ind_vars feeds have some strong correlations with other feeds In particular, PP TOTAL 7089 and SILIKAT POLYPLUS AB 1001 PP have a correlation w/ eachother of .923 ind_corr = df [ feeds ] . corr ()[ ind_vars ] . dropna ( how = 'all' ) ind_corr . loc [( abs ( ind_corr ) > 0.5 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 ind_vars feeds exhibit bimodal distributions (2+ recipes) for ind in ind_vars : df [ ind ] . plot . kde () plt . title ( ind ) plt . xlim ( 0 ) plt . show () 5.0 Hypothesis Tests \u00b6 Non-Parametric \u00b6 Univariate Categorical to Categorical (Chi-Square) \u00b6 ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 for ind in ind_vars : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 10182 False 7377 True True 173 False 165 Name: Total Position Out, dtype: int64 [[10182 173] [ 7377 165]] chi2 stat: 6.02 p-value: 1.41e-02 PP TOTAL 7089 Total Position Out False True 9171 False 7175 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[9171 1184] [7175 367]] chi2 stat: 237.00 p-value: 1.78e-53 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 9996 False 7364 True True 359 False 178 Name: Total Position Out, dtype: int64 [[9996 359] [7364 178]] chi2 stat: 17.99 p-value: 2.22e-05 chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 chisum = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum [ 'type' ] = None chisum . loc [ chisum [ 'feed' ] . isin ( master_batch ), 'type' ] = 'master_batch' chisum . loc [ chisum [ 'feed' ] . isin ( recycle ), 'type' ] = 'recyle' chisum . loc [ chisum [ 'feed' ] . isin ( virgin_polymer ), 'type' ] = 'virgin_polymer' chisum . loc [ chisum [ 'feed' ] . isin ( silicates ), 'type' ] = 'silicates' chisum . loc [ chisum [ 'feed' ] . isin ( uvs ), 'type' ] = 'uvs' chisum . loc [ chisum [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null type 0 PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1 PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2 High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3 SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 4 BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 5 PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 6 ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 7 DEVAN 2SPIN PP 106 35.237690 2.918209e-09 [[7511, 2844], [5768, 1774]] False True master_batch 8 High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 9 GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 10 MFR 4 Black Brown 20.494540 5.980160e-06 [[9550, 805], [6810, 732]] True True recyle 11 UV ARGUS ARX V 16 706 UV PP 20.048858 7.548859e-06 [[4465, 5890], [3507, 4035]] False True uvs 12 ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 13 SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 14 GREEN SCHULMAN PZ 302446 1 5 T 14.155700 1.682865e-04 [[9973, 382], [7177, 365]] True True master_batch 15 MFR 4 Grey Grey 9.004184 2.693622e-03 [[9464, 1141], [6401, 891]] True True recyle Check confounding variables \u00b6 Isolate Products (product conflation with feed) Let's isolate the comparison by only products made with the feed ( ind_var ) in question We see that MFR 4 White and SILIKAT POLYPLUS AB 1001 PP are very imbalanced compared to PP TOTAL 7089 target = 'Total Position Out' for ind in ind_vars : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 3978 False 2933 True True 173 False 165 Name: Total Position Out, dtype: int64 [[3978 173] [2933 165]] chi2 stat: 5.10 p-value: 2.40e-02 PP TOTAL 7089 Total Position Out False True 2301 False 1393 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[2301 1184] [1393 367]] chi2 stat: 96.05 p-value: 1.12e-22 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 2418 False 1602 True True 359 False 178 Name: Total Position Out, dtype: int64 [[2418 359] [1602 178]] chi2 stat: 8.66 p-value: 3.25e-03 target = 'Total Position Out' chisum2 = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum2 = pd . concat ([ chisum2 , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum2 . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum2 = chisum2 . sort_values ( 'p-value' ) . reset_index ( drop = True ) tests = chisum . set_index ( 'feed' ) . join ( chisum2 . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], rsuffix = ', confound' ) tests . loc [( tests [ 'reject null' ] == True ) & ( tests [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value cTable + reject null type p-value, confound reject null, confound feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True we can compare this with our feed correlation table - there may be other explanatory variables 'hiding' ind_corr . loc [( abs ( ind_corr ) > 0.3 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 BLACK SCHULMAN P7381 0.196431 -0.470838 0.082699 DEVAN 2SPIN PP 106 NaN -0.415896 0.097590 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA04 0.342077 -0.248904 0.402788 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP TOTAL PPH 4065 -0.419569 -0.160492 0.089303 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 UV ARGUS ARX 904 11 PP NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.1 NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.2 NaN -0.461986 NaN MFR 4 Grey -0.303323 -0.381548 0.038125 MFR 4 Grey Grey 0.395658 -0.102141 -0.086338 UV ARGUS ARX V 16 706 UV PP 0.246254 -0.439913 -0.084811 Univariate Categorical to Quantitative (Moods Median) \u00b6 A limitation of the previous analysis is that our threshold for counting an upset in the line was pretty low - any delay (position out > 0 seconds). A way we can naturally increase this threshold is to compare medians (of position out (secs)) across our groups. chidf = pd . DataFrame () target = 'Seconds' chidf [ target ] = df [ seconds ] . sum ( 1 ) moods = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind in feeds : try : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) grp = chidf . loc [ chidf [ ind ] == True ][ target ] . values chi2 , p , m , cTable = stats . median_test ( grp , pop , correction = False ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] < 1 moods = pd . concat ([ moods , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass moods . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] moods = moods . sort_values ( 'p-value' ) . reset_index ( drop = True ) moods . loc [ moods [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null 0 PP TOTAL 7089 284.071422 9.736103e-64 [[1093, 8620], [458, 9277]] False True 1 MFR 4 Grey Grey 161.505327 5.305934e-37 [[744, 9215], [1288, 8682]] True True 2 PP POLYCHIM HB12XF 140.079653 2.557382e-32 [[897, 8734], [475, 9163]] False True 3 High MFR Grey 134.851343 3.558380e-31 [[1244, 8697], [753, 9200]] False True 4 SILIKAT PBH ABPP 05 132.735135 1.033157e-30 [[1205, 8704], [725, 9193]] False True 5 SILIKAT POLYPLUS AB 1001 PP 40.522176 1.943972e-10 [[341, 8870], [196, 9027]] False True 6 DEVAN 2SPIN PP 106 39.379816 3.488847e-10 [[2499, 8758], [2119, 9139]] False True 7 BLACK SCHULMAN P7381 33.697655 6.437826e-09 [[6961, 8686], [6465, 9211]] False True 8 GREEN POLYPLAST COLOUR MB PP 6821 32.518968 1.180348e-08 [[589, 8854], [414, 9043]] False True 9 PP INEOS 100 GA09 26.828649 2.223170e-07 [[630, 8854], [465, 9043]] False True 10 ADD ARGUS ARGUTHERM AO 77 PP 26.802325 2.253660e-07 [[1250, 8823], [1020, 9074]] False True 11 UV ARGUS ARX V 16 706 UV PP 23.787198 1.075957e-06 [[5152, 8744], [4773, 9153]] False True 12 High MFR Black 18.389746 1.800242e-05 [[1626, 8835], [1409, 9062]] False True 13 ADD POLYPLAST PPM AO 01 UV PP 15.536764 8.091620e-05 [[499, 9000], [629, 8897]] True True 14 UV SCHULMAN FPPUV 38 12.417442 4.253424e-04 [[330, 8983], [426, 8914]] True True 15 MFR 4 White 11.470090 7.072526e-04 [[138, 8971], [200, 8926]] True True 16 GREEN SCHULMAN PZ 302446 1 5 T 9.924320 1.631091e-03 [[331, 8983], [416, 8914]] True True we see that overall the moods test is slightly less pessimistic chisum [ 'reject null' ] . value_counts () True 16 False 16 Name: reject null, dtype: int64 moods [ 'reject null' ] . value_counts () True 17 False 15 Name: reject null, dtype: int64 testsf = tests . join ( moods . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], lsuffix = ', chi' , rsuffix = ', moods' ) testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True 2.223170e-07 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True 2.253660e-07 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True 1.800242e-05 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True 1.180348e-08 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True 8.091620e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True 1.943972e-10 True Non-Parametric Conclusions \u00b6 problem_vars = testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] . index print ( f \"Compared with correlation \\n the dependency tests show that the top 5 problem variables are: \\n { [ f ' { i } ' for i in problem_vars ] } \" ) Compared with correlation the dependency tests show that the top 5 problem variables are: ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True Parametric \u00b6 Univariate Quantitative to Quantitative (Linear Regression) \u00b6 for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () plt . plot ( x , ( y - y_pred ), ls = '' , marker = '.' ) plt . ylabel ( 'Residual' ) plt . xlabel ( 'x' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( f \"test statistic: { t } \" ) print ( f \"p-value: { stats . t . sf ( x = t , df = n - 2 ) } \" ) R2: 3.34e-02 RSE: 9.97e-03 b: -2.51e-03 m: 3.61e-01 n: 338 x_bar: 1.11e-01 SE_b: 1.51e-03 SE_m: 1.26e-02 test statistic: 28.56535956265302 p-value: 3.2661498371310494e-92 R2: 4.31e-02 RSE: 5.27e-03 b: -3.19e-03 m: 9.08e-02 n: 1551 x_bar: 2.89e-01 SE_b: 3.09e-04 SE_m: 9.66e-04 test statistic: 94.02166426512674 p-value: 0.0 R2: 1.89e-02 RSE: 5.94e-03 b: -1.36e-02 m: 1.57e+01 n: 537 x_bar: 2.64e-03 SE_b: 1.04e-03 SE_m: 3.81e-01 test statistic: 41.11483903422569 p-value: 4.981863349397906e-168 for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.033 Model: OLS Adj. R-squared: 0.030 Method: Least Squares F-statistic: 11.60 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.000739 Time: 10:02:50 Log-Likelihood: 360.28 No. Observations: 338 AIC: -716.6 Df Residuals: 336 BIC: -708.9 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0025 0.013 -0.199 0.843 -0.027 0.022 x1 0.3612 0.106 3.406 0.001 0.153 0.570 ============================================================================== Omnibus: 457.321 Durbin-Watson: 1.128 Prob(Omnibus): 0.000 Jarque-Bera (JB): 55378.392 Skew: 6.475 Prob(JB): 0.00 Kurtosis: 64.356 Cond. No. 23.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.043 Model: OLS Adj. R-squared: 0.042 Method: Least Squares F-statistic: 69.71 Date: Sat, 02 Apr 2022 Prob (F-statistic): 1.50e-16 Time: 10:02:50 Log-Likelihood: 2180.0 No. Observations: 1551 AIC: -4356. Df Residuals: 1549 BIC: -4345. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0032 0.003 -0.917 0.359 -0.010 0.004 x1 0.0908 0.011 8.349 0.000 0.069 0.112 ============================================================================== Omnibus: 2262.527 Durbin-Watson: 1.176 Prob(Omnibus): 0.000 Jarque-Bera (JB): 660230.502 Skew: 8.562 Prob(JB): 0.00 Kurtosis: 102.615 Cond. No. 7.82 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.019 Model: OLS Adj. R-squared: 0.017 Method: Least Squares F-statistic: 10.29 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.00142 Time: 10:02:50 Log-Likelihood: 622.03 No. Observations: 537 AIC: -1240. Df Residuals: 535 BIC: -1231. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0136 0.013 -1.020 0.308 -0.040 0.013 x1 15.6795 4.888 3.208 0.001 6.078 25.281 ============================================================================== Omnibus: 739.911 Durbin-Watson: 1.268 Prob(Omnibus): 0.000 Jarque-Bera (JB): 108127.158 Skew: 7.213 Prob(JB): 0.00 Kurtosis: 71.003 Cond. No. 1.49e+03 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.49e+03. This might indicate that there are strong multicollinearity or other numerical problems. Feature Engineering \u00b6 Introducing polynomial features results in poor coefficient estimates ind = ind_vars [ 1 ] lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) features = PolynomialFeatures ( degree = 3 ) x = features . fit_transform ( x . reshape ( - 1 , 1 )) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.056 Model: OLS Adj. R-squared: 0.055 Method: Least Squares F-statistic: 30.83 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.35e-19 Time: 10:03:01 Log-Likelihood: 2190.9 No. Observations: 1551 AIC: -4374. Df Residuals: 1547 BIC: -4352. Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0030 0.009 0.334 0.739 -0.014 0.020 x1 0.1612 0.124 1.301 0.193 -0.082 0.404 x2 -0.9203 0.523 -1.761 0.078 -1.945 0.105 x3 1.6922 0.664 2.550 0.011 0.390 2.994 ============================================================================== Omnibus: 2252.329 Durbin-Watson: 1.188 Prob(Omnibus): 0.000 Jarque-Bera (JB): 651735.784 Skew: 8.487 Prob(JB): 0.00 Kurtosis: 101.979 Cond. No. 594. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. R2: 5.64e-02 RSE: 6.03e-03 b: 2.95e-03 m: 0.00e+00 n: 1551 x_bar: 3.58e-01 SE_b: 1.69e-04 SE_m: 1.96e-04 0.0 0.5 Multivariate Quantitative to Quantitative (Multivariate Linear Regression) \u00b6 lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) lindf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ADD ARGUS ARGUSTAT AT 31 HK ADD ARGUS ARGUTHERM AO 77 PP ADD POLYPLAST PPM AO 01 UV PP ADD SCHULMAN DTY 20 AOUV BEIGE SCHULMAN PZ 91738 1 5 ZA BLACK SCHULMAN P7381 DEVAN 2SPIN PP 106 GREEN POLYPLAST COLOUR MB PP 6821 GREEN SCHULMAN PZ 302446 1 5 T High MFR Black ... UV POLYPLAST 6005 PP UV SCHULMAN FPPUV 38 MFR 4 Grey PP POLYCHIM HL10XF MFR 4 Grey Grey PP POLYMER TEST MFI4 PP TOTAL 4069 RR MASTERBATCH TEST UV ARGUS ARX V 16 706 UV PP seconds 0 0.0 0.0 0.0 0.005337 0.0 0.000253 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.409353 0.0 0.0 0.0 0.0 0.0 0.000000 9.0 1 0.0 0.0 0.0 0.005867 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.452007 0.0 0.0 0.0 0.0 0.0 0.000000 536.0 2 0.0 0.0 0.0 0.005895 0.0 0.000270 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.449608 0.0 0.0 0.0 0.0 0.0 0.000000 20.0 3 0.0 0.0 0.0 0.005743 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.440883 0.0 0.0 0.0 0.0 0.0 0.000000 1001.0 4 0.0 0.0 0.0 0.004553 0.0 0.000211 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.349459 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.339726 0.0 0.0 0.0 0.0 0.0 0.002062 0.0 17893 0.0 0.0 0.0 0.000000 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.342659 0.0 0.0 0.0 0.0 0.0 0.002071 0.0 17894 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345886 0.0 0.0 0.0 0.0 0.0 0.002107 0.0 17895 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.344945 0.0 0.0 0.0 0.0 0.0 0.002095 799.0 17896 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345288 0.0 0.0 0.0 0.0 0.0 0.002105 429.0 17897 rows \u00d7 45 columns lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) x = lindf [ feeds ] . values # features = PolynomialFeatures(degree=3) # x = features.fit_transform(x.reshape(-1,1)) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.050 Model: OLS Adj. R-squared: 0.049 Method: Least Squares F-statistic: 31.47 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.76e-174 Time: 10:03:19 Log-Likelihood: 44791. No. Observations: 17897 AIC: -8.952e+04 Df Residuals: 17866 BIC: -8.928e+04 Df Model: 30 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0123 0.002 5.316 0.000 0.008 0.017 x1 -1.0553 0.382 -2.760 0.006 -1.805 -0.306 x2 -0.0119 0.010 -1.241 0.215 -0.031 0.007 x3 0.0351 0.100 0.352 0.725 -0.161 0.231 x4 -0.2377 0.069 -3.438 0.001 -0.373 -0.102 x5 0.4001 0.145 2.750 0.006 0.115 0.685 x6 -0.0224 0.128 -0.175 0.861 -0.272 0.228 x7 -0.3174 0.096 -3.305 0.001 -0.506 -0.129 x8 -0.5909 0.209 -2.825 0.005 -1.001 -0.181 x9 -0.4059 0.087 -4.655 0.000 -0.577 -0.235 x10 -0.0241 0.005 -5.206 0.000 -0.033 -0.015 x11 -2.473e-14 1.17e-14 -2.107 0.035 -4.77e-14 -1.73e-15 x12 -0.0050 0.004 -1.371 0.171 -0.012 0.002 x13 -0.0288 0.007 -4.319 0.000 -0.042 -0.016 x14 -0.0268 0.005 -5.644 0.000 -0.036 -0.018 x15 -0.0200 0.006 -3.620 0.000 -0.031 -0.009 x16 -1.023e-14 3.5e-15 -2.923 0.003 -1.71e-14 -3.37e-15 x17 -9.109e-16 2.5e-16 -3.649 0.000 -1.4e-15 -4.22e-16 x18 -0.0994 0.093 -1.073 0.283 -0.281 0.082 x19 -0.0055 0.003 -2.044 0.041 -0.011 -0.000 x20 0.0003 0.005 0.058 0.954 -0.009 0.010 x21 1.325e-15 1.2e-16 11.078 0.000 1.09e-15 1.56e-15 x22 -2.494e-16 8.68e-17 -2.873 0.004 -4.2e-16 -7.93e-17 x23 0.0145 0.004 3.827 0.000 0.007 0.022 x24 -0.0100 0.010 -1.051 0.293 -0.029 0.009 x25 0.0216 0.004 5.724 0.000 0.014 0.029 x26 -0.0101 0.002 -4.198 0.000 -0.015 -0.005 x27 -1.03e-16 8.73e-17 -1.180 0.238 -2.74e-16 6.8e-17 x28 9.329e-16 7.53e-17 12.396 0.000 7.85e-16 1.08e-15 x29 -0.0074 0.003 -2.881 0.004 -0.012 -0.002 x30 1.7091 0.132 12.994 0.000 1.451 1.967 x31 4.1538 0.376 11.053 0.000 3.417 4.890 x32 0 0 nan nan 0 0 x33 0.0031 0.012 0.263 0.792 -0.020 0.026 x34 0.0031 0.012 0.263 0.792 -0.020 0.026 x35 0.0031 0.012 0.263 0.792 -0.020 0.026 x36 0.0790 0.042 1.868 0.062 -0.004 0.162 x37 0.0525 0.030 1.774 0.076 -0.006 0.111 x38 -0.0064 0.003 -2.521 0.012 -0.011 -0.001 x39 0 0 nan nan 0 0 x40 -0.0136 0.005 -2.762 0.006 -0.023 -0.004 x41 0 0 nan nan 0 0 x42 0 0 nan nan 0 0 x43 0 0 nan nan 0 0 x44 0.1990 0.048 4.119 0.000 0.104 0.294 ============================================================================== Omnibus: 43479.020 Durbin-Watson: 1.074 Prob(Omnibus): 0.000 Jarque-Bera (JB): 693938974.691 Skew: 25.434 Prob(JB): 0.00 Kurtosis: 966.322 Cond. No. 1.11e+16 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 1.79e-28. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. R2: 5.02e-02 RSE: 1.67e-03 b: 1.23e-02 m: -1.06e+00 n: 17897 x_bar: 2.26e-02 SE_b: 1.25e-05 SE_m: 2.14e-05 -49284.75476441197 1.0 Forward Selection \u00b6 def add_feature ( features , basemodel , data , y , r2max ): for feed in features : basemodel [ feed ] = data [ feed ] basemodel . fillna ( 0 , inplace = True ) x = basemodel [[ col for col in basemodel . columns if col != 'seconds' ]] . values ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () if ( est2 . rsquared > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = est2 . rsquared feedmax = feed bestsum = est2 . summary () newmodel = basemodel . copy () else : pass basemodel . drop ( labels = feed , axis = 1 , inplace = True ) return r2max , feedmax , bestsum , newmodel lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) r2max = 0 candidates = feeds . copy () basemodel = lindf while True : newr2max , feedmax , bestsum , newmodel = add_feature ( features = candidates , basemodel = basemodel , data = df , y = y , r2max = 0 ) if newr2max > r2max : # print(newr2max, feedmax) candidates . remove ( feedmax ) r2max = newr2max basemodel = newmodel continue else : break /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) we see some of the usual suspects from the chi-square tests. Notably some are missing: High MFR Grey , BLACK SCHULMAN P7381 , PP INEOS 100 GA09 and others. basemodel . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } seconds PP TOTAL 7089 PP POLYCHIM HB12XF SILIKAT PBH ABPP 05 SILIKAT POLYPLUS AB 1001 PP MFR 4 Green ADD SCHULMAN DTY 20 AOUV GREEN SCHULMAN PZ 302446 1 5 T MFR 4 Grey UV ARGUS ARX V 16 706 UV PP GREEN SCHULMAN PZ 34198 1 5 T 0 9.0 0.0 0.0 0.0 0.0 0.0 0.005337 0.0 0.409353 0.0 0.0 1 536.0 0.0 0.0 0.0 0.0 0.0 0.005867 0.0 0.452007 0.0 0.0 2 20.0 0.0 0.0 0.0 0.0 0.0 0.005895 0.0 0.449608 0.0 0.0 3 1001.0 0.0 0.0 0.0 0.0 0.0 0.005743 0.0 0.440883 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.004553 0.0 0.349459 0.0 0.0 forward_selection_feeds = basemodel . columns [ 1 :] bestsum OLS Regression Results Dep. Variable: y R-squared: 0.046 Model: OLS Adj. R-squared: 0.045 Method: Least Squares F-statistic: 94.99 Date: Sat, 02 Apr 2022 Prob (F-statistic): 5.51e-174 Time: 10:04:41 Log-Likelihood: 44748. No. Observations: 17897 AIC: -8.948e+04 Df Residuals: 17887 BIC: -8.940e+04 Df Model: 9 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 0.0034 0.000 11.288 0.000 0.003 0.004 x1 0.0214 0.002 11.535 0.000 0.018 0.025 x2 0.0146 0.002 7.837 0.000 0.011 0.018 x3 1.5742 0.110 14.375 0.000 1.360 1.789 x4 4.1700 0.363 11.497 0.000 3.459 4.881 x5 -0.0181 0.003 -6.276 0.000 -0.024 -0.012 x6 -0.2879 0.065 -4.450 0.000 -0.415 -0.161 x7 -0.1932 0.052 -3.698 0.000 -0.296 -0.091 x8 0.0037 0.001 3.322 0.001 0.002 0.006 x9 0.0736 0.028 2.641 0.008 0.019 0.128 x10 0 0 nan nan 0 0 x11 0 0 nan nan 0 0 Omnibus: 43415.763 Durbin-Watson: 1.068 Prob(Omnibus): 0.000 Jarque-Bera (JB): 683838684.139 Skew: 25.346 Prob(JB): 0.00 Kurtosis: 959.275 Cond. No. inf Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 0. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular. Multivariate Conclusions \u00b6 y poses a problem with out of the box regression. Extreme tails will upset the residual plots. We some overlap of the parametric and non-parametric approaches: [ i for i in basemodel . columns if i in ( problem_vars )] ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05'] Multivariate Quantitative to Categorical (Binned Output Variable) \u00b6 Logistic regression does little better than random guessing lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) x = lindf [ ind ] . values #.reshape(-1,1) model = LogisticRegression () model . fit ( x , y ) y_pred = model . predict ( x ) print ( f \"Train Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) Train Acc: 0.37 Test Acc: 0.37 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( print ( classification_report ( y , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.35 0.80 0.49 4527 2.0 0.36 0.32 0.34 4434 3.0 0.30 0.03 0.06 4463 4.0 0.43 0.32 0.37 4473 accuracy 0.37 17897 macro avg 0.36 0.37 0.31 17897 weighted avg 0.36 0.37 0.31 17897 <AxesSubplot:> If we are careful about regularizing the random forest model, it does a little bit better lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) X = lindf [ ind ] . values #.reshape(-1,1) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 0.99 Test Acc: 0.39 y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.41 0.48 0.44 898 2.0 0.33 0.30 0.31 881 3.0 0.29 0.27 0.28 893 4.0 0.50 0.51 0.51 908 accuracy 0.39 3580 macro avg 0.38 0.39 0.38 3580 weighted avg 0.38 0.39 0.38 3580 <AxesSubplot:> # grab feature importances imp = model . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in model . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ ind , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 PP TOTAL PPH 4065 0.150387 0.008053 1 BLACK SCHULMAN P7381 0.118158 0.008725 2 PP INEOS 100 GA04 0.100635 0.005217 3 UV ARGUS ARX V 16 706 UV PP 0.090776 0.006273 4 MFR 4 Grey 0.08828 0.005656 5 PP UNIPETROL GB005 0.060558 0.003569 6 MFR 4 Black 0.052589 0.003695 7 ADD SCHULMAN DTY 20 AOUV 0.041478 0.003554 8 DEVAN 2SPIN PP 106 0.038438 0.004061 9 High MFR Black 0.028838 0.002515 10 MFR 4 Grey Grey 0.025758 0.002319 11 ADD ARGUS ARGUTHERM AO 77 PP 0.024526 0.002253 12 High MFR Grey 0.020652 0.004188 13 MFR 4 Green 0.016508 0.002725 14 SILIKAT PBH ABPP 05 0.016283 0.002291 15 PP TOTAL 7089 0.01624 0.004897 16 ADD POLYPLAST PPM AO 01 UV PP 0.013499 0.001685 17 MFR 4 Black Brown 0.013401 0.002131 18 PP POLYCHIM HB12XF 0.01245 0.002185 19 PP INEOS 100 GA09 0.010354 0.00156 6.0 Business Impact \u00b6 What is the so what? From Hyp tests - target is either 'Total Position Out' or 'Total Seconds Out' We had culprits from pearsons: ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] culprits from non parametric tests: list ( problem_vars ) ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] and culprits from linear regression forward_selection_feeds Index(['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'MFR 4 Green', 'ADD SCHULMAN DTY 20 AOUV', 'GREEN SCHULMAN PZ 302446 1 5 T', 'MFR 4 Grey', 'UV ARGUS ARX V 16 706 UV PP', 'GREEN SCHULMAN PZ 34198 1 5 T'], dtype='object') and we can check the overlap between non-parametric and linear regression (parametric): predict_vars = [ i for i in basemodel . columns if i in ( problem_vars )] predict_vars ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05'] all_vars = set ( list ( problem_vars ) + list ( forward_selection_feeds )) all_vars {'ADD SCHULMAN DTY 20 AOUV', 'BLACK SCHULMAN P7381', 'GREEN SCHULMAN PZ 302446 1 5 T', 'GREEN SCHULMAN PZ 34198 1 5 T', 'High MFR Grey', 'MFR 4 Green', 'MFR 4 Grey', 'PP POLYCHIM HB12XF', 'PP TOTAL 7089', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'UV ARGUS ARX V 16 706 UV PP'} # add to DF business = pd . DataFrame () value_line_by_day = 99000 for ind in problem_vars : try : # products, prods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique () nprods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . nunique () dff = df . loc [ df [ 'Product' ] . isin ( prods )] # total runtime, tot_runtime = dff [ 'Time Delta' ] . sum () # total runtime with feed, tot_runtime_wfeed = dff [ dff [ ind ] > 0 ][ 'Time Delta' ] . sum () #downtime w/o feed, downtime w/ feed, avg_downtime_min = dff . groupby ( dff [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 avg_downtime_min_wfeed = avg_downtime_min [ True ] avg_downtime_min_w_ofeed = avg_downtime_min [ False ] ratio = avg_downtime_min_wfeed / avg_downtime_min_w_ofeed # average runtime, avg_runtime_min = dff [ 'Time Delta' ] . mean () . total_seconds () / 60 # days saved, dollars saved days_saved = ( tot_runtime_wfeed . total_seconds () / 60 / avg_runtime_min * avg_downtime_min . diff () . values [ - 1 ]) / 60 / 24 dollars_saved_per_batch = value_line_by_day * avg_downtime_min . diff () . values [ - 1 ] / ( 60 * 24 ) business = pd . concat (( business , pd . DataFrame ([[ ind , nprods , tot_runtime , tot_runtime_wfeed , avg_downtime_min_wfeed , avg_downtime_min_w_ofeed , ratio , avg_runtime_min , days_saved , days_saved * value_line_by_day , dollars_saved_per_batch ]], columns = [ 'Feed' , 'Products' , 'Total Runtime, Products' , 'Total Runtime With Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Average Runtime' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]))) except : print ( ind ) print ( dff . groupby ( df [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 ) business = business . sort_values ( 'Dollars Saved (Per Batch)' , ascending = False ) . reset_index ( drop = True ) business # workshop downtime ratio # feed products, feed products # Dollars saved per batch .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Products Total Runtime, Products Total Runtime With Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Average Runtime Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26 409 days 20:43:21 94 days 04:36:05 26.250639 13.671063 1.920161 70.835736 16.727319 1.656005e+06 864.845823 1 High MFR Grey 36 564 days 04:13:41 94 days 12:26:59 23.623539 12.809036 1.844287 65.459164 15.615433 1.545928e+06 743.497080 2 PP POLYCHIM HB12XF 17 383 days 00:03:51 67 days 09:13:53 21.043525 15.600015 1.348943 70.908183 5.173013 5.121283e+05 374.241303 3 BLACK SCHULMAN P7381 64 751 days 20:01:57 590 days 12:21:50 14.902871 12.363943 1.205349 63.216276 23.716605 2.347944e+06 174.551301 4 PP TOTAL 7089 13 291 days 22:06:22 79 days 15:56:47 20.651050 18.269305 1.130369 80.146114 2.367430 2.343756e+05 163.744933 pd . set_option ( 'display.precision' , 2 ) display ( business [[ 'Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]], ) pd . set_option ( 'display.precision' , 6 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26.25 13.67 1.92 16.73 1.66e+06 864.85 1 High MFR Grey 23.62 12.81 1.84 15.62 1.55e+06 743.50 2 PP POLYCHIM HB12XF 21.04 15.60 1.35 5.17 5.12e+05 374.24 3 BLACK SCHULMAN P7381 14.90 12.36 1.21 23.72 2.35e+06 174.55 4 PP TOTAL 7089 20.65 18.27 1.13 2.37 2.34e+05 163.74 We see that 'SILKAT PBH ABPP 05' as well as 'High MFR Grey' have a large amount of days saved over the two years, a strong downtime multiplier effect and the largest dollars saved per batch. Since High MFR Grey is a recycle feed, we may not have the ability to remove it from the feed list. Leaving the SILKAT (Silicate) feed the prime feedstock to remove 7.0 Visualizations \u00b6 PP TOTAL 7089 \u00b6 predict_var = predict_vars [ 1 ] outcome_var = [ i for i in df . columns if 'Seconds' in i ] def myplot ( col = list ( df . loc [ df [ predict_var ] > 0 ][ 'Product' ] . unique ())): fig , ax = plt . subplots ( figsize = ( 20 , 10 )) axt = ax . twinx () # other feeds df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True ) . dropna ( axis = 1 , how = 'all' ) . plot ( ls = '' , marker = '*' , ax = ax , alpha = 0.2 ) # predict_var feed df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True )[ predict_var ] . plot ( ls = '--' , marker = '+' , ax = ax , lw = 2 , c = 'red' ) # outcome_var df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ) . plot ( ls = '-' , marker = '+' , lw = 1 , ax = axt , alpha = 1 , c = 'blue' , label = 'Total Seconds Out' ) # outliers ax . set_ylim ( 0 ) axt . set_ylim ( 0 , np . quantile ( df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ), 0.997 )) h1 , l1 = ax . get_legend_handles_labels () h2 , l2 = axt . get_legend_handles_labels () ax . legend ( h1 + h2 , l1 + l2 , loc = 0 ) interact ( myplot ) interactive(children=(Dropdown(description='col', options=(0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.\u2026 <function __main__.myplot(col=[0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.0, 136215.0, 125215.0, 110215.0, 100215.0, 80215.0, 100617.0, 90617.0, 90215.0, 90216.0, 110520.0, 80617.0])>","title":"X4 Candy Ribbons"},{"location":"extras/X4_Candy_Ribbons/#causality-analysis","text":"","title":"Causality Analysis"},{"location":"extras/X4_Candy_Ribbons/#todo","text":"ols assumptions for univariate analyses residual plots forward selection lines up more or less with chi square tests include some colinearity measure? logistic regression (binned seconds, binned total outs) random forest does little better than random guessing business impact reorganize hypothesis tests section univariate vs multivariate cat v vat, cat v quant, quant v quant, quant v cat","title":"TODO"},{"location":"extras/X4_Candy_Ribbons/#10-imports","text":"","title":"1.0 Imports"},{"location":"extras/X4_Candy_Ribbons/#import-libraries","text":"# basic packages import pandas as pd import numpy as np import random import copy # visualization packages from ipywidgets import interact import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures , MinMaxScaler from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , silhouette_score , calinski_harabasz_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score def make_patch_spines_invisible ( ax ): ax . set_frame_on ( True ) ax . patch . set_visible ( False ) for sp in ax . spines . values (): sp . set_visible ( False ) from matplotlib import colors from itertools import cycle def my_colors (): tab = cycle ( colors . TABLEAU_COLORS ) return tab","title":"Import Libraries"},{"location":"extras/X4_Candy_Ribbons/#load-data","text":"consider only feeds consider feeds and operating conditions feeds = [ 'ADD ARGUS ARGUSTAT AT 31 HK' , 'ADD ARGUS ARGUTHERM AO 77 PP' , 'ADD POLYPLAST PPM AO 01 UV PP' , 'ADD SCHULMAN DTY 20 AOUV' , 'BEIGE SCHULMAN PZ 91738 1 5 ZA' , 'BLACK SCHULMAN P7381' , 'DEVAN 2SPIN PP 106' , 'GREEN POLYPLAST COLOUR MB PP 6821' , 'GREEN SCHULMAN PZ 302446 1 5 T' , 'High MFR Black' , 'GREEN SCHULMAN PZ 34198 1 5 T' , 'MFR 4 Black' , 'MFR 4 Black Brown' , 'High MFR Grey' , 'MFR 4 Green' , 'PP BOREALIS HE465FB' , 'MFR 4 Black Colour Blending' , 'PP INEOS 100 GA02' , 'PP INEOS 100 GA04' , 'PP INEOS 100 GA09' , 'MFR 4 Grey Colour Blending' , 'PP INEOS GA012' , 'PP POLYCHIM HB12XF' , 'MFR 4 White' , 'PP TOTAL 7089' , 'PP TOTAL PPH 4065' , 'MFR 4 White Colour Blending' , 'PP BOREALIS HE370FB' , 'PP UNIPETROL GB005' , 'SILIKAT PBH ABPP 05' , 'SILIKAT POLYPLUS AB 1001 PP' , 'UV ARGUS ARGUVIN UV 361 PP' , 'UV ARGUS ARX 904 11 PP' , 'UV ARGUS ARX 904 11 PP.1' , 'UV ARGUS ARX 904 11 PP.2' , 'UV POLYPLAST 6005 PP' , 'UV SCHULMAN FPPUV 38' , 'MFR 4 Grey' , 'PP POLYCHIM HL10XF' , 'MFR 4 Grey Grey' , 'PP POLYMER TEST MFI4' , 'PP TOTAL 4069' , 'RR MASTERBATCH TEST' , 'UV ARGUS ARX V 16 706 UV PP' ] df = pd . read_csv ( '../../../../../Dropbox/work/mfganalytic/data/dupont/TyparMasterReportWithSampleDataImprovement2017-today.csv' , header = 1 ) df = df . loc [ df [ 'Product' ] . notnull ()] df [ feeds ] = df [ feeds ] . div ( df [ feeds ] . sum ( axis = 1 ), axis = 0 ) #normalize feed rates df [ 'Time Delta' ] = pd . to_datetime ( df [ 'Doff Time' ]) . diff () df [ 'Total Seconds Out' ] = df [[ i for i in df . columns if 'Sec' in i ]] . sum ( 1 ) # df.dropna(inplace=True) hangs = [ df . columns [ i ] for i , j in enumerate ( 'Hang' in col for col in df . columns ) if j ] speeds = [ df . columns [ i ] for i , j in enumerate ( 'speed' in col for col in df . columns ) if j ] outs = [ df . columns [ i ] for i , j in enumerate ( 'Out' in col for col in df . columns ) if j ] prods = df [ 'Product' ] . unique () counts = [ i for i in df . columns if 'Count' in i ] #from LOEWEN virgin_polymer = [ 'PP INEOS 100 GA04' , 'PP POLYCHIM HB12XF' , 'PP TOTAL 7089' , #INEOS 100 GA09 until 2018 (2k rolls) 'PP TOTAL PPH 4065' , 'PP UNIPETROL GB005' , 'PP POLYCHIM HL10XF' ] #HL10XF not used after 2016, #BOREALIS virgin_polymer = [] for col in df . columns : if ( 'PP INEOS' in col ) or ( 'PP TOTAL' in col ) or ( 'PP UNI' in col ) or ( 'PP BORE' in col ) or ( 'PP POLY' in col ): virgin_polymer . append ( col ) pressures = [] for col in df . columns : if 'Press' in col or 'PRESS' in col : pressures . append ( col ) silicates = [] for col in df . columns : if 'SIL' in col : silicates . append ( col ) uvs = [] for col in df . columns : if 'UV ' in col : uvs . append ( col ) recycle = [] for col in df . columns : if 'MFR' in col : recycle . append ( col ) seconds = [] for col in df . columns : if 'Sec' in col : seconds . append ( col ) master_batch = list ( set ( feeds ) - set ( recycle ) - set ( virgin_polymer )) operation = list ( set ( df . columns ) - set ( master_batch ) - set ( virgin_polymer ) - set ( recycle ) - set ( hangs ) - set ( outs )) operation . sort () A summary of the columns print ( f 'pressures: { len ( pressures ) } ' ) print ( f 'silicates: { len ( silicates ) } ' ) print ( f 'uvs: { len ( uvs ) } ' ) print ( f 'feeds: { len ( feeds ) } ' ) print ( f 'master_batch: { len ( master_batch ) } ' ) print ( f 'virgin_polymer: { len ( virgin_polymer ) } ' ) print ( f 'recycle: { len ( recycle ) } ' ) print ( f 'operation: { len ( operation ) } ' ) print () print ( f 'hangs: { len ( hangs ) } ' ) print ( f 'speeds: { len ( speeds ) } ' ) print ( f 'outs: { len ( outs ) } ' ) print ( f 'seconds: { len ( seconds ) } ' ) print () print ( 'summary target columns: TotalHangs, Total Position Out' ) print () print ( df . shape ) pressures: 8 silicates: 2 uvs: 8 feeds: 44 master_batch: 20 virgin_polymer: 13 recycle: 11 operation: 180 hangs: 9 speeds: 9 outs: 10 seconds: 9 summary target columns: TotalHangs, Total Position Out (17897, 243) all_feats = df . select_dtypes ( float ) . columns [ df . select_dtypes ( float ) . columns . isin ( feeds + operation )] . values df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Millroll ID Product Roll Width Sample Number Doff Time Block1Pos Out Seconds Block2Pos Out Seconds Block3Pos Out Seconds Block4Pos Out Seconds Block1Pos Out Count ... TKW5 AVG - Tack - Tear West 5 Meter TPOP - NUMBER OF TPO PEEKS (TPOP) TTMD AVG - TRAP TEAR MD (TTMD) TTXD AVG - TRAP TEAR XD (TTXD) UPEX - UWU PLOT EXTREM (UPEX) UWG - UWU GRAVIMETRIC WITH SCALE (UWG) UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP) WATP - WATER PERMEABILITY (WATP) Time Delta Total Seconds Out 0 PM10022907 136215.0 5200.0 NaN 1/1/2017 12:43:57 AM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaT 9.0 1 PM10022908 136215.0 5200.0 NaN 1/1/2017 1:32:49 AM NaN 164.0 240.0 NaN NaN ... NaN NaN NaN NaN NaN 138.70 NaN NaN 0 days 00:48:52 536.0 2 PM10022909 136215.0 5200.0 NaN 1/1/2017 2:21:40 AM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:48:51 20.0 3 PM10022910 136215.0 5200.0 NaN 1/1/2017 3:10:34 AM NaN 320.0 264.0 NaN NaN ... NaN NaN 36.96 33.59 NaN 135.98 NaN NaN 0 days 00:48:54 1001.0 4 PM10022911 136215.0 5200.0 NaN 1/1/2017 3:59:24 AM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:48:50 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 PM10040811 90215.0 4250.0 P113099,P113100,P113101,P113102,P113103 2/25/2019 7:56:16 PM NaN NaN NaN NaN NaN ... NaN NaN 21.30 22.10 NaN 89.60 NaN NaN 0 days 00:57:24 0.0 17893 PM10040812 90215.0 4250.0 P113104 2/25/2019 8:53:41 PM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:57:25 0.0 17894 PM10040813 90215.0 4250.0 P113105,P113106,P113107 2/25/2019 9:51:06 PM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 89.30 NaN NaN 0 days 00:57:25 0.0 17895 PM10040814 90215.0 4250.0 P113108 2/25/2019 10:48:32 PM NaN NaN 576.0 NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:57:26 799.0 17896 PM10040815 90215.0 4250.0 P113109,P113110,P113111,P113112 2/25/2019 11:45:55 PM NaN NaN 429.0 NaN NaN ... NaN NaN 23.20 22.60 NaN 91.40 NaN NaN 0 days 00:57:23 429.0 17897 rows \u00d7 243 columns","title":"Load Data"},{"location":"extras/X4_Candy_Ribbons/#20-understand-the-dependent-variable","text":"","title":"2.0 Understand The Dependent Variable"},{"location":"extras/X4_Candy_Ribbons/#what-is-the-hit-rate-in-these-upset-columns","text":"Around 7500 cases with no problem; 10000 cases with a position out target = \"Total Position Out\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 10355 False 7542 Name: Total Position Out, dtype: int64 There are 30% higher incidence of hangs target = \"TotalHangs\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 13954 False 3943 Name: TotalHangs, dtype: int64 target1 = \"TotalHangs\" target2 = \"Total Position Out\" print ( f \"both upsets: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"only hangs: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) print ( f \"only outs: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"neither upset: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) both upsets: 10161 only hangs: 3793 only outs: 194 neither upset: 3749","title":"What is the hit rate in these upset columns?"},{"location":"extras/X4_Candy_Ribbons/#what-is-the-magnitude-of-the-outages","text":"target = \"Total Position Out\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f0ad7207b80>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total Position Out Unnamed 0 Total Position Out 1.00000 0.62591 Unnamed 0 0.62591 1.00000 target = \"TotalHangs\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f0ad3eff460>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TotalHangs Unnamed 0 TotalHangs 1.000000 0.390949 Unnamed 0 0.390949 1.000000","title":"What is the magnitude of the outages?"},{"location":"extras/X4_Candy_Ribbons/#distribution-around-the-target-variable-total-seconds","text":"We will certainly encounter trouble in an OLS estimate with this shape of the target variable; extreme skewness, upper tail, perhaps outliers tot_seconds = df [ seconds ] . sum ( 1 ) tot_seconds . kurt () 923.6728632470827 tot_seconds . skew () 25.09323820232841 tot_seconds . plot . kde () <AxesSubplot:ylabel='Density'> tot_seconds . describe () count 17897.000000 mean 2093.512879 std 8060.466090 min 0.000000 25% 20.000000 50% 666.000000 75% 1970.000000 max 396570.000000 dtype: float64","title":"Distribution around the target variable, total seconds"},{"location":"extras/X4_Candy_Ribbons/#do-block-positions-have-the-same-behavior","text":"target = \"TotalHangs\" df . groupby ( target )[ hangs ] . count () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs TotalHangs 0.0 0 0 0 0 0 0 0 0 3943 1.0 510 682 745 977 510 682 745 977 2914 2.0 709 761 901 1109 709 761 901 1109 2424 3.0 721 776 930 1103 721 776 930 1103 1929 4.0 700 773 875 993 700 773 875 993 1577 5.0 609 668 744 828 609 668 745 828 1189 6.0 523 498 599 631 523 498 599 631 883 7.0 387 414 472 505 387 414 472 505 640 8.0 321 338 376 409 321 338 376 409 503 9.0 286 275 305 339 286 275 305 339 405 10.0 219 206 244 248 219 206 244 248 296 11.0 144 154 168 171 144 154 168 171 203 12.0 148 139 159 171 148 139 159 171 191 13.0 105 100 117 106 105 100 117 106 131 14.0 93 88 95 96 93 88 95 96 109 15.0 97 89 94 97 97 89 94 97 108 16.0 70 64 72 76 70 64 72 76 81 17.0 51 50 51 47 51 50 51 47 55 18.0 29 30 32 33 29 30 32 33 35 19.0 32 32 33 32 32 32 33 32 35 20.0 32 32 31 34 32 32 31 34 35 21.0 26 27 25 24 26 27 25 24 28 22.0 21 20 24 24 21 20 24 24 24 23.0 23 23 24 23 23 23 24 23 25 24.0 16 17 18 19 16 17 18 19 20 25.0 7 8 8 7 7 8 8 7 8 26.0 11 10 10 11 11 10 10 11 11 27.0 8 9 9 8 8 9 9 8 9 28.0 10 10 10 10 10 10 10 10 10 29.0 6 5 6 6 6 5 6 6 6 30.0 4 3 4 4 4 3 4 4 4 31.0 8 8 8 8 8 8 8 8 8 32.0 4 3 4 4 4 3 4 4 4 33.0 7 5 7 6 7 5 7 6 7 34.0 9 8 9 8 9 8 9 8 9 35.0 6 6 6 6 6 6 6 6 6 36.0 6 5 6 6 6 5 6 6 6 37.0 3 3 3 3 3 3 3 3 3 38.0 1 1 1 0 1 1 1 0 1 39.0 1 1 1 1 1 1 1 1 1 40.0 2 2 2 2 2 2 2 2 2 42.0 1 1 1 1 1 1 1 1 1 43.0 3 3 3 3 3 3 3 3 3 45.0 3 3 3 3 3 3 3 3 3 46.0 2 2 2 2 2 2 2 2 2 48.0 1 1 1 1 1 1 1 1 1 52.0 1 1 1 1 1 1 1 1 1 53.0 1 1 1 1 1 1 1 1 1 54.0 1 1 1 1 1 1 1 1 1 58.0 1 1 1 1 1 1 1 1 1 62.0 1 1 1 1 1 1 1 1 1 63.0 1 1 1 1 1 1 1 1 1 70.0 1 1 1 1 1 1 1 1 1 72.0 1 1 1 1 1 1 1 1 1 140.0 1 1 1 1 1 1 1 1 1 df [[ i for i in outs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:> df [[ i for i in hangs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:>","title":"Do block positions have the same behavior?"},{"location":"extras/X4_Candy_Ribbons/#30-look-for-dependent-independent-signal","text":"","title":"3.0 Look for dependent-independent signal"},{"location":"extras/X4_Candy_Ribbons/#are-there-linear-relationships-between-the-dependent-and-independent-variables","text":"corr = df . corr () ( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 ) Product False Roll Width False Block1Pos Out Seconds True Block2Pos Out Seconds True Block3Pos Out Seconds True ... UPEX - UWU PLOT EXTREM (UPEX) False UWG - UWU GRAVIMETRIC WITH SCALE (UWG) False UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP) False WATP - WATER PERMEABILITY (WATP) False Total Seconds Out True Length: 237, dtype: bool cor_filt = corr . loc [( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 )] . drop ( labels = outs + hangs , axis = 0 )[ outs + hangs ] cor_filt .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Pos Out Seconds Block2Pos Out Seconds Block3Pos Out Seconds Block4Pos Out Seconds Block1Pos Out Count Block2Pos Out Count Block3Pos Out Count Block4Pos Out Count Total Position Out Total Seconds Out Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs ADD ARGUS ARGUSTAT AT 31 HK -0.103999 -0.417688 -0.383222 0.202131 -0.056194 -0.412274 0.155126 0.145535 0.014216 0.029563 0.121682 -0.256723 -0.012676 0.099690 -0.069354 -0.041492 0.094150 0.041900 0.344896 PP INEOS 100 GA02 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 -1.000000 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 MFR 4 White 0.289648 -0.016268 0.451446 -0.155932 0.297256 -0.000098 0.376795 -0.003129 0.225425 0.182687 0.231764 -0.045076 0.283286 -0.119481 0.108396 -0.057188 0.317089 0.008018 0.189109 PP TOTAL 7089 0.160268 0.171023 0.210796 0.093720 0.192159 0.210942 0.206859 0.076058 0.260505 0.207522 0.087954 0.050670 0.052622 -0.050506 0.218148 0.251676 0.134881 0.121080 0.331897 SILIKAT POLYPLUS AB 1001 PP 0.015132 0.125801 0.379559 0.255662 -0.018715 0.012019 0.161698 0.067661 0.127830 0.137376 -0.016317 -0.018187 -0.018956 0.086781 0.011207 -0.020307 0.092291 0.109259 0.069217 RJET15219WG1 - Corona Charge I AVG BLK#2 -0.120204 -0.120509 -0.056377 -0.002054 -0.198138 -0.221189 -0.193393 -0.024541 -0.232863 -0.149862 -0.090896 -0.060404 -0.054169 -0.052683 -0.215325 -0.216545 -0.182990 -0.120387 -0.308217 BLFL - COLOUR BLACK FILAMENT L value (BLFL) -0.360086 -0.157845 -0.245565 -0.268837 -0.153321 -0.162697 -0.212481 -0.247721 -0.067632 -0.121690 0.097783 0.146515 -0.346923 -0.110987 0.402310 -0.274153 -0.378670 -0.116264 -0.117501 BRFA - COLOUR BROWN FILAMENT a value (BRFa) 0.159056 0.250229 -0.384880 0.109024 0.134248 0.112836 -0.380715 -0.054074 0.099040 0.166926 0.170184 -0.065344 -0.195062 0.094596 0.195404 -0.333632 -0.026209 0.177576 0.173995 BRFB - COLOUR BROWN FILAMENT b value (BRFb) 0.413876 0.047748 0.332752 0.342115 0.145794 -0.028170 0.246787 0.234624 0.216156 0.311363 0.148441 -0.210193 0.118275 0.285286 -0.027055 -0.275750 0.100830 0.328240 0.164345 BRFL - COLOUR BROWN FILAMENT L value (BRFL) 0.198864 -0.178050 0.483242 0.205796 0.017456 -0.220432 0.411110 0.090885 0.039031 0.128326 0.337130 -0.326368 0.075738 0.145878 0.195055 -0.467536 -0.075130 0.188013 -0.000575 E20M AVG - SST ELONGATION 20X20 MD (E20M) 0.106870 -0.096775 -0.249986 -0.039633 0.009518 -0.049131 -0.367065 -0.074018 -0.070058 -0.071425 -0.124456 -0.016993 -0.070312 -0.036314 -0.145788 -0.075968 -0.197153 -0.069477 -0.072872 EF1M AVG - SST ELONGATION FOLDED 10 CM MD (EF1M) 1.000000 NaN 1.000000 -0.081738 1.000000 NaN NaN NaN -0.101606 -0.178993 0.313364 -0.936781 -1.000000 -0.517401 0.306235 -0.950303 -1.000000 -0.324932 -0.112334 EF1X AVG - SST ELONGATION FOLDED 10 CM XD (EF1X) 1.000000 NaN 1.000000 0.126777 1.000000 NaN NaN NaN 0.010039 -0.063399 0.896084 -0.961383 -1.000000 -0.609904 0.892730 -0.971830 -1.000000 -0.340772 0.050483 NATM AVG - Nail Tear MD EN 13859-1 (NATM) -0.517421 -0.835147 0.511409 -0.258360 -0.296500 -0.906038 NaN -0.333754 -0.100244 0.013436 -0.338129 0.580493 0.826191 -0.322585 -0.348782 0.063309 0.929207 -0.430201 0.086484 NATX AVG - Nail Tear XD EN 13859-1 (NATX) 0.269389 -0.864128 0.542651 0.071499 -0.053990 -0.927938 NaN -0.056465 -0.084131 0.046366 -0.190830 0.541297 0.777131 -0.114506 -0.096640 0.051081 0.902927 -0.295087 0.093804 O90 - O90 EN ISO 12956 (O90E) NaN NaN NaN 0.807962 NaN NaN NaN NaN -0.265093 -0.268107 -0.280791 -0.378899 NaN -0.451610 -0.307148 -0.267261 NaN -0.269680 -0.193539 PU AVG - PUNCTURE \"US Rod\" (PU) 0.173625 -0.790779 -0.165265 0.451194 -0.127912 -0.952390 -0.184515 0.734933 -0.040323 -0.021124 0.096383 -0.998695 -0.288786 0.156274 0.066142 0.363427 -0.297118 0.479459 -0.239496 TKO4 AVG - Tack-Tear Ost 4Meter -0.170480 -0.383667 -0.326335 0.006590 -0.104058 -0.308260 -0.329544 0.047734 -0.171814 -0.195662 -0.214633 -0.440748 -0.182875 -0.027353 -0.077638 -0.447423 -0.187893 0.114764 -0.160340 TKO5 AVG - Tack - Tear Ost 5 Meter 1.000000 0.134107 0.560717 -0.070116 NaN 0.106485 0.152594 -0.525375 -0.373817 -0.306252 0.338272 0.155577 0.034557 -0.453643 0.286066 0.056270 -0.066270 -0.400941 -0.387297 TKW4 AVG - Tack-Tear West 4Meter -0.334291 -0.282220 -0.265607 -0.134467 -0.320379 -0.231417 -0.300679 -0.265049 -0.274024 -0.250617 -0.277325 -0.375559 -0.168965 -0.154119 -0.236424 -0.282687 -0.168865 -0.162801 -0.316193 TKW5 AVG - Tack - Tear West 5 Meter 1.000000 0.051583 0.100609 0.018368 NaN 0.014194 -0.167032 -0.366762 -0.279911 -0.190366 0.069184 0.108657 0.224892 -0.326866 -0.003422 0.043730 0.039670 -0.362928 -0.270898 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt , cmap = cmap ) <AxesSubplot:>","title":"Are there linear relationships between the dependent and independent variables?"},{"location":"extras/X4_Candy_Ribbons/#filter-n-100","text":"cor_filt = cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]] fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]], cmap = cmap ) <AxesSubplot:>","title":"Filter N &gt; 100"},{"location":"extras/X4_Candy_Ribbons/#operating-conditions","text":"fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Operations Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Operations Correlations to Hangs and Outs')","title":"Operating Conditions"},{"location":"extras/X4_Candy_Ribbons/#feeds","text":"fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ ~ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Feeds Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Feeds Correlations to Hangs and Outs') ind_vars = list ( cor_filt . index [ ~ cor_filt . index . isin ( operation )]) ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] fig , ax = plt . subplots ( 1 , 3 , figsize = ( 30 , 10 )) indices = np . argwhere ( ax ) for index , ind_var in enumerate ( ind_vars ): df [[ ind_var ] + seconds ] . melt ( id_vars = ind_var , value_vars = None , var_name = None ,) . plot ( x = ind_var , y = 'value' , ax = ax [ index ], ls = '' , marker = '.' )","title":"Feeds"},{"location":"extras/X4_Candy_Ribbons/#40-understand-the-independent-variable","text":"","title":"4.0 Understand the Independent Variable"},{"location":"extras/X4_Candy_Ribbons/#descriptive-stats-on-ind-var","text":"all the feeds sum to 1 (feeds are mass or volume fractions), so we probably won't need to worry about scaling df [ feeds ] . sum ( 1 ) 0 1.0 1 1.0 2 1.0 3 1.0 4 1.0 ... 17892 1.0 17893 1.0 17894 1.0 17895 1.0 17896 1.0 Length: 17897, dtype: float64 many of the feeds are not used at all skew = df [ feeds ] . skew () kurt = df [ feeds ] . kurtosis () null = df [ feeds ] . isnull () . sum () n = df [ feeds ] . notnull () . sum () med = df [ feeds ] . median () men = df [ feeds ] . mean () dff = pd . DataFrame ([ skew , kurt , null , n , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'null cnt' , 'n' , 'median' , 'mean' ] dff . loc [ dff [ 'n' ] > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis null cnt n median mean ADD ARGUS ARGUSTAT AT 31 HK -1.122018 -0.275303 17798.0 99.0 0.006560 0.005080 ADD ARGUS ARGUTHERM AO 77 PP 14.927951 248.300227 15627.0 2270.0 0.005249 0.010705 ADD POLYPLAST PPM AO 01 UV PP 5.788514 87.051660 16769.0 1128.0 0.006397 0.006218 ADD SCHULMAN DTY 20 AOUV 6.573319 99.052816 13484.0 4413.0 0.004997 0.005411 BEIGE SCHULMAN PZ 91738 1 5 ZA -0.455858 10.108094 16975.0 922.0 0.008922 0.008708 BLACK SCHULMAN P7381 2.301415 16.313957 4471.0 13426.0 0.000317 0.002040 DEVAN 2SPIN PP 106 0.770601 11.066370 13279.0 4618.0 0.006658 0.005554 GREEN POLYPLAST COLOUR MB PP 6821 -1.856631 3.540741 16894.0 1003.0 0.007519 0.006822 GREEN SCHULMAN PZ 302446 1 5 T 0.363239 8.864015 17150.0 747.0 0.018764 0.018006 High MFR Black 0.473484 -1.518082 14862.0 3035.0 0.047883 0.146677 MFR 4 Black 1.612264 4.261601 12595.0 5302.0 0.030099 0.098792 MFR 4 Black Brown -0.086521 -0.342944 16360.0 1537.0 0.189244 0.150273 High MFR Grey -0.364616 -1.483643 15900.0 1997.0 0.226358 0.199138 MFR 4 Green 0.149918 8.244109 15985.0 1912.0 0.279622 0.262574 PP INEOS 100 GA02 NaN NaN 17895.0 2.0 0.107927 0.107927 PP INEOS 100 GA04 1.294449 5.764334 6684.0 11213.0 0.208757 0.216941 PP INEOS 100 GA09 -0.449540 -0.968103 16802.0 1095.0 0.224353 0.219208 PP POLYCHIM HB12XF -0.222658 -0.684893 16525.0 1372.0 0.275755 0.291827 MFR 4 White 1.136619 2.053071 17559.0 338.0 0.098961 0.111129 PP TOTAL 7089 -0.216726 -1.333059 16346.0 1551.0 0.276430 0.288643 PP TOTAL PPH 4065 0.353911 0.740658 611.0 17286.0 0.441009 0.424684 PP UNIPETROL GB005 0.117291 1.045121 11444.0 6453.0 0.324773 0.295906 SILIKAT PBH ABPP 05 2.690146 26.471577 15967.0 1930.0 0.005001 0.005229 SILIKAT POLYPLUS AB 1001 PP 6.874010 77.288671 17360.0 537.0 0.002506 0.002642 UV ARGUS ARX 904 11 PP 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.1 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.2 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV POLYPLAST 6005 PP 1.426790 1.634549 17506.0 391.0 0.020009 0.026245 UV SCHULMAN FPPUV 38 2.114261 14.372541 17141.0 756.0 0.029986 0.032261 MFR 4 Grey 1.718802 10.970192 8567.0 9330.0 0.269445 0.263401 MFR 4 Grey Grey 3.607370 20.342313 15865.0 2032.0 0.040035 0.068547 UV ARGUS ARX V 16 706 UV PP 1.889895 6.474782 7972.0 9925.0 0.002110 0.007994 our ind_vars feeds have some strong correlations with other feeds In particular, PP TOTAL 7089 and SILIKAT POLYPLUS AB 1001 PP have a correlation w/ eachother of .923 ind_corr = df [ feeds ] . corr ()[ ind_vars ] . dropna ( how = 'all' ) ind_corr . loc [( abs ( ind_corr ) > 0.5 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 ind_vars feeds exhibit bimodal distributions (2+ recipes) for ind in ind_vars : df [ ind ] . plot . kde () plt . title ( ind ) plt . xlim ( 0 ) plt . show ()","title":"Descriptive Stats on Ind Var"},{"location":"extras/X4_Candy_Ribbons/#50-hypothesis-tests","text":"","title":"5.0 Hypothesis Tests"},{"location":"extras/X4_Candy_Ribbons/#non-parametric","text":"","title":"Non-Parametric"},{"location":"extras/X4_Candy_Ribbons/#univariate-categorical-to-categorical-chi-square","text":"ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 for ind in ind_vars : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 10182 False 7377 True True 173 False 165 Name: Total Position Out, dtype: int64 [[10182 173] [ 7377 165]] chi2 stat: 6.02 p-value: 1.41e-02 PP TOTAL 7089 Total Position Out False True 9171 False 7175 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[9171 1184] [7175 367]] chi2 stat: 237.00 p-value: 1.78e-53 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 9996 False 7364 True True 359 False 178 Name: Total Position Out, dtype: int64 [[9996 359] [7364 178]] chi2 stat: 17.99 p-value: 2.22e-05 chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 chisum = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum [ 'type' ] = None chisum . loc [ chisum [ 'feed' ] . isin ( master_batch ), 'type' ] = 'master_batch' chisum . loc [ chisum [ 'feed' ] . isin ( recycle ), 'type' ] = 'recyle' chisum . loc [ chisum [ 'feed' ] . isin ( virgin_polymer ), 'type' ] = 'virgin_polymer' chisum . loc [ chisum [ 'feed' ] . isin ( silicates ), 'type' ] = 'silicates' chisum . loc [ chisum [ 'feed' ] . isin ( uvs ), 'type' ] = 'uvs' chisum . loc [ chisum [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null type 0 PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1 PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2 High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3 SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 4 BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 5 PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 6 ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 7 DEVAN 2SPIN PP 106 35.237690 2.918209e-09 [[7511, 2844], [5768, 1774]] False True master_batch 8 High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 9 GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 10 MFR 4 Black Brown 20.494540 5.980160e-06 [[9550, 805], [6810, 732]] True True recyle 11 UV ARGUS ARX V 16 706 UV PP 20.048858 7.548859e-06 [[4465, 5890], [3507, 4035]] False True uvs 12 ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 13 SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 14 GREEN SCHULMAN PZ 302446 1 5 T 14.155700 1.682865e-04 [[9973, 382], [7177, 365]] True True master_batch 15 MFR 4 Grey Grey 9.004184 2.693622e-03 [[9464, 1141], [6401, 891]] True True recyle","title":"Univariate Categorical to Categorical (Chi-Square)"},{"location":"extras/X4_Candy_Ribbons/#check-confounding-variables","text":"Isolate Products (product conflation with feed) Let's isolate the comparison by only products made with the feed ( ind_var ) in question We see that MFR 4 White and SILIKAT POLYPLUS AB 1001 PP are very imbalanced compared to PP TOTAL 7089 target = 'Total Position Out' for ind in ind_vars : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 3978 False 2933 True True 173 False 165 Name: Total Position Out, dtype: int64 [[3978 173] [2933 165]] chi2 stat: 5.10 p-value: 2.40e-02 PP TOTAL 7089 Total Position Out False True 2301 False 1393 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[2301 1184] [1393 367]] chi2 stat: 96.05 p-value: 1.12e-22 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 2418 False 1602 True True 359 False 178 Name: Total Position Out, dtype: int64 [[2418 359] [1602 178]] chi2 stat: 8.66 p-value: 3.25e-03 target = 'Total Position Out' chisum2 = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum2 = pd . concat ([ chisum2 , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum2 . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum2 = chisum2 . sort_values ( 'p-value' ) . reset_index ( drop = True ) tests = chisum . set_index ( 'feed' ) . join ( chisum2 . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], rsuffix = ', confound' ) tests . loc [( tests [ 'reject null' ] == True ) & ( tests [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value cTable + reject null type p-value, confound reject null, confound feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True we can compare this with our feed correlation table - there may be other explanatory variables 'hiding' ind_corr . loc [( abs ( ind_corr ) > 0.3 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 BLACK SCHULMAN P7381 0.196431 -0.470838 0.082699 DEVAN 2SPIN PP 106 NaN -0.415896 0.097590 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA04 0.342077 -0.248904 0.402788 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP TOTAL PPH 4065 -0.419569 -0.160492 0.089303 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 UV ARGUS ARX 904 11 PP NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.1 NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.2 NaN -0.461986 NaN MFR 4 Grey -0.303323 -0.381548 0.038125 MFR 4 Grey Grey 0.395658 -0.102141 -0.086338 UV ARGUS ARX V 16 706 UV PP 0.246254 -0.439913 -0.084811","title":"Check confounding variables"},{"location":"extras/X4_Candy_Ribbons/#univariate-categorical-to-quantitative-moods-median","text":"A limitation of the previous analysis is that our threshold for counting an upset in the line was pretty low - any delay (position out > 0 seconds). A way we can naturally increase this threshold is to compare medians (of position out (secs)) across our groups. chidf = pd . DataFrame () target = 'Seconds' chidf [ target ] = df [ seconds ] . sum ( 1 ) moods = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind in feeds : try : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) grp = chidf . loc [ chidf [ ind ] == True ][ target ] . values chi2 , p , m , cTable = stats . median_test ( grp , pop , correction = False ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] < 1 moods = pd . concat ([ moods , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass moods . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] moods = moods . sort_values ( 'p-value' ) . reset_index ( drop = True ) moods . loc [ moods [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null 0 PP TOTAL 7089 284.071422 9.736103e-64 [[1093, 8620], [458, 9277]] False True 1 MFR 4 Grey Grey 161.505327 5.305934e-37 [[744, 9215], [1288, 8682]] True True 2 PP POLYCHIM HB12XF 140.079653 2.557382e-32 [[897, 8734], [475, 9163]] False True 3 High MFR Grey 134.851343 3.558380e-31 [[1244, 8697], [753, 9200]] False True 4 SILIKAT PBH ABPP 05 132.735135 1.033157e-30 [[1205, 8704], [725, 9193]] False True 5 SILIKAT POLYPLUS AB 1001 PP 40.522176 1.943972e-10 [[341, 8870], [196, 9027]] False True 6 DEVAN 2SPIN PP 106 39.379816 3.488847e-10 [[2499, 8758], [2119, 9139]] False True 7 BLACK SCHULMAN P7381 33.697655 6.437826e-09 [[6961, 8686], [6465, 9211]] False True 8 GREEN POLYPLAST COLOUR MB PP 6821 32.518968 1.180348e-08 [[589, 8854], [414, 9043]] False True 9 PP INEOS 100 GA09 26.828649 2.223170e-07 [[630, 8854], [465, 9043]] False True 10 ADD ARGUS ARGUTHERM AO 77 PP 26.802325 2.253660e-07 [[1250, 8823], [1020, 9074]] False True 11 UV ARGUS ARX V 16 706 UV PP 23.787198 1.075957e-06 [[5152, 8744], [4773, 9153]] False True 12 High MFR Black 18.389746 1.800242e-05 [[1626, 8835], [1409, 9062]] False True 13 ADD POLYPLAST PPM AO 01 UV PP 15.536764 8.091620e-05 [[499, 9000], [629, 8897]] True True 14 UV SCHULMAN FPPUV 38 12.417442 4.253424e-04 [[330, 8983], [426, 8914]] True True 15 MFR 4 White 11.470090 7.072526e-04 [[138, 8971], [200, 8926]] True True 16 GREEN SCHULMAN PZ 302446 1 5 T 9.924320 1.631091e-03 [[331, 8983], [416, 8914]] True True we see that overall the moods test is slightly less pessimistic chisum [ 'reject null' ] . value_counts () True 16 False 16 Name: reject null, dtype: int64 moods [ 'reject null' ] . value_counts () True 17 False 15 Name: reject null, dtype: int64 testsf = tests . join ( moods . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], lsuffix = ', chi' , rsuffix = ', moods' ) testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True 2.223170e-07 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True 2.253660e-07 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True 1.800242e-05 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True 1.180348e-08 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True 8.091620e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True 1.943972e-10 True","title":"Univariate Categorical to Quantitative (Moods Median)"},{"location":"extras/X4_Candy_Ribbons/#non-parametric-conclusions","text":"problem_vars = testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] . index print ( f \"Compared with correlation \\n the dependency tests show that the top 5 problem variables are: \\n { [ f ' { i } ' for i in problem_vars ] } \" ) Compared with correlation the dependency tests show that the top 5 problem variables are: ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True","title":"Non-Parametric Conclusions"},{"location":"extras/X4_Candy_Ribbons/#parametric","text":"","title":"Parametric"},{"location":"extras/X4_Candy_Ribbons/#univariate-quantitative-to-quantitative-linear-regression","text":"for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () plt . plot ( x , ( y - y_pred ), ls = '' , marker = '.' ) plt . ylabel ( 'Residual' ) plt . xlabel ( 'x' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( f \"test statistic: { t } \" ) print ( f \"p-value: { stats . t . sf ( x = t , df = n - 2 ) } \" ) R2: 3.34e-02 RSE: 9.97e-03 b: -2.51e-03 m: 3.61e-01 n: 338 x_bar: 1.11e-01 SE_b: 1.51e-03 SE_m: 1.26e-02 test statistic: 28.56535956265302 p-value: 3.2661498371310494e-92 R2: 4.31e-02 RSE: 5.27e-03 b: -3.19e-03 m: 9.08e-02 n: 1551 x_bar: 2.89e-01 SE_b: 3.09e-04 SE_m: 9.66e-04 test statistic: 94.02166426512674 p-value: 0.0 R2: 1.89e-02 RSE: 5.94e-03 b: -1.36e-02 m: 1.57e+01 n: 537 x_bar: 2.64e-03 SE_b: 1.04e-03 SE_m: 3.81e-01 test statistic: 41.11483903422569 p-value: 4.981863349397906e-168 for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.033 Model: OLS Adj. R-squared: 0.030 Method: Least Squares F-statistic: 11.60 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.000739 Time: 10:02:50 Log-Likelihood: 360.28 No. Observations: 338 AIC: -716.6 Df Residuals: 336 BIC: -708.9 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0025 0.013 -0.199 0.843 -0.027 0.022 x1 0.3612 0.106 3.406 0.001 0.153 0.570 ============================================================================== Omnibus: 457.321 Durbin-Watson: 1.128 Prob(Omnibus): 0.000 Jarque-Bera (JB): 55378.392 Skew: 6.475 Prob(JB): 0.00 Kurtosis: 64.356 Cond. No. 23.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.043 Model: OLS Adj. R-squared: 0.042 Method: Least Squares F-statistic: 69.71 Date: Sat, 02 Apr 2022 Prob (F-statistic): 1.50e-16 Time: 10:02:50 Log-Likelihood: 2180.0 No. Observations: 1551 AIC: -4356. Df Residuals: 1549 BIC: -4345. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0032 0.003 -0.917 0.359 -0.010 0.004 x1 0.0908 0.011 8.349 0.000 0.069 0.112 ============================================================================== Omnibus: 2262.527 Durbin-Watson: 1.176 Prob(Omnibus): 0.000 Jarque-Bera (JB): 660230.502 Skew: 8.562 Prob(JB): 0.00 Kurtosis: 102.615 Cond. No. 7.82 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.019 Model: OLS Adj. R-squared: 0.017 Method: Least Squares F-statistic: 10.29 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.00142 Time: 10:02:50 Log-Likelihood: 622.03 No. Observations: 537 AIC: -1240. Df Residuals: 535 BIC: -1231. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0136 0.013 -1.020 0.308 -0.040 0.013 x1 15.6795 4.888 3.208 0.001 6.078 25.281 ============================================================================== Omnibus: 739.911 Durbin-Watson: 1.268 Prob(Omnibus): 0.000 Jarque-Bera (JB): 108127.158 Skew: 7.213 Prob(JB): 0.00 Kurtosis: 71.003 Cond. No. 1.49e+03 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.49e+03. This might indicate that there are strong multicollinearity or other numerical problems.","title":"Univariate Quantitative to Quantitative (Linear Regression)"},{"location":"extras/X4_Candy_Ribbons/#feature-engineering","text":"Introducing polynomial features results in poor coefficient estimates ind = ind_vars [ 1 ] lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) features = PolynomialFeatures ( degree = 3 ) x = features . fit_transform ( x . reshape ( - 1 , 1 )) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.056 Model: OLS Adj. R-squared: 0.055 Method: Least Squares F-statistic: 30.83 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.35e-19 Time: 10:03:01 Log-Likelihood: 2190.9 No. Observations: 1551 AIC: -4374. Df Residuals: 1547 BIC: -4352. Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0030 0.009 0.334 0.739 -0.014 0.020 x1 0.1612 0.124 1.301 0.193 -0.082 0.404 x2 -0.9203 0.523 -1.761 0.078 -1.945 0.105 x3 1.6922 0.664 2.550 0.011 0.390 2.994 ============================================================================== Omnibus: 2252.329 Durbin-Watson: 1.188 Prob(Omnibus): 0.000 Jarque-Bera (JB): 651735.784 Skew: 8.487 Prob(JB): 0.00 Kurtosis: 101.979 Cond. No. 594. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. R2: 5.64e-02 RSE: 6.03e-03 b: 2.95e-03 m: 0.00e+00 n: 1551 x_bar: 3.58e-01 SE_b: 1.69e-04 SE_m: 1.96e-04 0.0 0.5","title":"Feature Engineering"},{"location":"extras/X4_Candy_Ribbons/#multivariate-quantitative-to-quantitative-multivariate-linear-regression","text":"lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) lindf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ADD ARGUS ARGUSTAT AT 31 HK ADD ARGUS ARGUTHERM AO 77 PP ADD POLYPLAST PPM AO 01 UV PP ADD SCHULMAN DTY 20 AOUV BEIGE SCHULMAN PZ 91738 1 5 ZA BLACK SCHULMAN P7381 DEVAN 2SPIN PP 106 GREEN POLYPLAST COLOUR MB PP 6821 GREEN SCHULMAN PZ 302446 1 5 T High MFR Black ... UV POLYPLAST 6005 PP UV SCHULMAN FPPUV 38 MFR 4 Grey PP POLYCHIM HL10XF MFR 4 Grey Grey PP POLYMER TEST MFI4 PP TOTAL 4069 RR MASTERBATCH TEST UV ARGUS ARX V 16 706 UV PP seconds 0 0.0 0.0 0.0 0.005337 0.0 0.000253 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.409353 0.0 0.0 0.0 0.0 0.0 0.000000 9.0 1 0.0 0.0 0.0 0.005867 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.452007 0.0 0.0 0.0 0.0 0.0 0.000000 536.0 2 0.0 0.0 0.0 0.005895 0.0 0.000270 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.449608 0.0 0.0 0.0 0.0 0.0 0.000000 20.0 3 0.0 0.0 0.0 0.005743 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.440883 0.0 0.0 0.0 0.0 0.0 0.000000 1001.0 4 0.0 0.0 0.0 0.004553 0.0 0.000211 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.349459 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.339726 0.0 0.0 0.0 0.0 0.0 0.002062 0.0 17893 0.0 0.0 0.0 0.000000 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.342659 0.0 0.0 0.0 0.0 0.0 0.002071 0.0 17894 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345886 0.0 0.0 0.0 0.0 0.0 0.002107 0.0 17895 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.344945 0.0 0.0 0.0 0.0 0.0 0.002095 799.0 17896 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345288 0.0 0.0 0.0 0.0 0.0 0.002105 429.0 17897 rows \u00d7 45 columns lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) x = lindf [ feeds ] . values # features = PolynomialFeatures(degree=3) # x = features.fit_transform(x.reshape(-1,1)) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.050 Model: OLS Adj. R-squared: 0.049 Method: Least Squares F-statistic: 31.47 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.76e-174 Time: 10:03:19 Log-Likelihood: 44791. No. Observations: 17897 AIC: -8.952e+04 Df Residuals: 17866 BIC: -8.928e+04 Df Model: 30 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0123 0.002 5.316 0.000 0.008 0.017 x1 -1.0553 0.382 -2.760 0.006 -1.805 -0.306 x2 -0.0119 0.010 -1.241 0.215 -0.031 0.007 x3 0.0351 0.100 0.352 0.725 -0.161 0.231 x4 -0.2377 0.069 -3.438 0.001 -0.373 -0.102 x5 0.4001 0.145 2.750 0.006 0.115 0.685 x6 -0.0224 0.128 -0.175 0.861 -0.272 0.228 x7 -0.3174 0.096 -3.305 0.001 -0.506 -0.129 x8 -0.5909 0.209 -2.825 0.005 -1.001 -0.181 x9 -0.4059 0.087 -4.655 0.000 -0.577 -0.235 x10 -0.0241 0.005 -5.206 0.000 -0.033 -0.015 x11 -2.473e-14 1.17e-14 -2.107 0.035 -4.77e-14 -1.73e-15 x12 -0.0050 0.004 -1.371 0.171 -0.012 0.002 x13 -0.0288 0.007 -4.319 0.000 -0.042 -0.016 x14 -0.0268 0.005 -5.644 0.000 -0.036 -0.018 x15 -0.0200 0.006 -3.620 0.000 -0.031 -0.009 x16 -1.023e-14 3.5e-15 -2.923 0.003 -1.71e-14 -3.37e-15 x17 -9.109e-16 2.5e-16 -3.649 0.000 -1.4e-15 -4.22e-16 x18 -0.0994 0.093 -1.073 0.283 -0.281 0.082 x19 -0.0055 0.003 -2.044 0.041 -0.011 -0.000 x20 0.0003 0.005 0.058 0.954 -0.009 0.010 x21 1.325e-15 1.2e-16 11.078 0.000 1.09e-15 1.56e-15 x22 -2.494e-16 8.68e-17 -2.873 0.004 -4.2e-16 -7.93e-17 x23 0.0145 0.004 3.827 0.000 0.007 0.022 x24 -0.0100 0.010 -1.051 0.293 -0.029 0.009 x25 0.0216 0.004 5.724 0.000 0.014 0.029 x26 -0.0101 0.002 -4.198 0.000 -0.015 -0.005 x27 -1.03e-16 8.73e-17 -1.180 0.238 -2.74e-16 6.8e-17 x28 9.329e-16 7.53e-17 12.396 0.000 7.85e-16 1.08e-15 x29 -0.0074 0.003 -2.881 0.004 -0.012 -0.002 x30 1.7091 0.132 12.994 0.000 1.451 1.967 x31 4.1538 0.376 11.053 0.000 3.417 4.890 x32 0 0 nan nan 0 0 x33 0.0031 0.012 0.263 0.792 -0.020 0.026 x34 0.0031 0.012 0.263 0.792 -0.020 0.026 x35 0.0031 0.012 0.263 0.792 -0.020 0.026 x36 0.0790 0.042 1.868 0.062 -0.004 0.162 x37 0.0525 0.030 1.774 0.076 -0.006 0.111 x38 -0.0064 0.003 -2.521 0.012 -0.011 -0.001 x39 0 0 nan nan 0 0 x40 -0.0136 0.005 -2.762 0.006 -0.023 -0.004 x41 0 0 nan nan 0 0 x42 0 0 nan nan 0 0 x43 0 0 nan nan 0 0 x44 0.1990 0.048 4.119 0.000 0.104 0.294 ============================================================================== Omnibus: 43479.020 Durbin-Watson: 1.074 Prob(Omnibus): 0.000 Jarque-Bera (JB): 693938974.691 Skew: 25.434 Prob(JB): 0.00 Kurtosis: 966.322 Cond. No. 1.11e+16 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 1.79e-28. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. R2: 5.02e-02 RSE: 1.67e-03 b: 1.23e-02 m: -1.06e+00 n: 17897 x_bar: 2.26e-02 SE_b: 1.25e-05 SE_m: 2.14e-05 -49284.75476441197 1.0","title":"Multivariate Quantitative to Quantitative (Multivariate Linear Regression)"},{"location":"extras/X4_Candy_Ribbons/#forward-selection","text":"def add_feature ( features , basemodel , data , y , r2max ): for feed in features : basemodel [ feed ] = data [ feed ] basemodel . fillna ( 0 , inplace = True ) x = basemodel [[ col for col in basemodel . columns if col != 'seconds' ]] . values ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () if ( est2 . rsquared > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = est2 . rsquared feedmax = feed bestsum = est2 . summary () newmodel = basemodel . copy () else : pass basemodel . drop ( labels = feed , axis = 1 , inplace = True ) return r2max , feedmax , bestsum , newmodel lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) r2max = 0 candidates = feeds . copy () basemodel = lindf while True : newr2max , feedmax , bestsum , newmodel = add_feature ( features = candidates , basemodel = basemodel , data = df , y = y , r2max = 0 ) if newr2max > r2max : # print(newr2max, feedmax) candidates . remove ( feedmax ) r2max = newr2max basemodel = newmodel continue else : break /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) we see some of the usual suspects from the chi-square tests. Notably some are missing: High MFR Grey , BLACK SCHULMAN P7381 , PP INEOS 100 GA09 and others. basemodel . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } seconds PP TOTAL 7089 PP POLYCHIM HB12XF SILIKAT PBH ABPP 05 SILIKAT POLYPLUS AB 1001 PP MFR 4 Green ADD SCHULMAN DTY 20 AOUV GREEN SCHULMAN PZ 302446 1 5 T MFR 4 Grey UV ARGUS ARX V 16 706 UV PP GREEN SCHULMAN PZ 34198 1 5 T 0 9.0 0.0 0.0 0.0 0.0 0.0 0.005337 0.0 0.409353 0.0 0.0 1 536.0 0.0 0.0 0.0 0.0 0.0 0.005867 0.0 0.452007 0.0 0.0 2 20.0 0.0 0.0 0.0 0.0 0.0 0.005895 0.0 0.449608 0.0 0.0 3 1001.0 0.0 0.0 0.0 0.0 0.0 0.005743 0.0 0.440883 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.004553 0.0 0.349459 0.0 0.0 forward_selection_feeds = basemodel . columns [ 1 :] bestsum OLS Regression Results Dep. Variable: y R-squared: 0.046 Model: OLS Adj. R-squared: 0.045 Method: Least Squares F-statistic: 94.99 Date: Sat, 02 Apr 2022 Prob (F-statistic): 5.51e-174 Time: 10:04:41 Log-Likelihood: 44748. No. Observations: 17897 AIC: -8.948e+04 Df Residuals: 17887 BIC: -8.940e+04 Df Model: 9 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 0.0034 0.000 11.288 0.000 0.003 0.004 x1 0.0214 0.002 11.535 0.000 0.018 0.025 x2 0.0146 0.002 7.837 0.000 0.011 0.018 x3 1.5742 0.110 14.375 0.000 1.360 1.789 x4 4.1700 0.363 11.497 0.000 3.459 4.881 x5 -0.0181 0.003 -6.276 0.000 -0.024 -0.012 x6 -0.2879 0.065 -4.450 0.000 -0.415 -0.161 x7 -0.1932 0.052 -3.698 0.000 -0.296 -0.091 x8 0.0037 0.001 3.322 0.001 0.002 0.006 x9 0.0736 0.028 2.641 0.008 0.019 0.128 x10 0 0 nan nan 0 0 x11 0 0 nan nan 0 0 Omnibus: 43415.763 Durbin-Watson: 1.068 Prob(Omnibus): 0.000 Jarque-Bera (JB): 683838684.139 Skew: 25.346 Prob(JB): 0.00 Kurtosis: 959.275 Cond. No. inf Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 0. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.","title":"Forward Selection"},{"location":"extras/X4_Candy_Ribbons/#multivariate-conclusions","text":"y poses a problem with out of the box regression. Extreme tails will upset the residual plots. We some overlap of the parametric and non-parametric approaches: [ i for i in basemodel . columns if i in ( problem_vars )] ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05']","title":"Multivariate Conclusions"},{"location":"extras/X4_Candy_Ribbons/#multivariate-quantitative-to-categorical-binned-output-variable","text":"Logistic regression does little better than random guessing lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) x = lindf [ ind ] . values #.reshape(-1,1) model = LogisticRegression () model . fit ( x , y ) y_pred = model . predict ( x ) print ( f \"Train Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) Train Acc: 0.37 Test Acc: 0.37 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( print ( classification_report ( y , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.35 0.80 0.49 4527 2.0 0.36 0.32 0.34 4434 3.0 0.30 0.03 0.06 4463 4.0 0.43 0.32 0.37 4473 accuracy 0.37 17897 macro avg 0.36 0.37 0.31 17897 weighted avg 0.36 0.37 0.31 17897 <AxesSubplot:> If we are careful about regularizing the random forest model, it does a little bit better lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) X = lindf [ ind ] . values #.reshape(-1,1) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 0.99 Test Acc: 0.39 y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.41 0.48 0.44 898 2.0 0.33 0.30 0.31 881 3.0 0.29 0.27 0.28 893 4.0 0.50 0.51 0.51 908 accuracy 0.39 3580 macro avg 0.38 0.39 0.38 3580 weighted avg 0.38 0.39 0.38 3580 <AxesSubplot:> # grab feature importances imp = model . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in model . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ ind , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 PP TOTAL PPH 4065 0.150387 0.008053 1 BLACK SCHULMAN P7381 0.118158 0.008725 2 PP INEOS 100 GA04 0.100635 0.005217 3 UV ARGUS ARX V 16 706 UV PP 0.090776 0.006273 4 MFR 4 Grey 0.08828 0.005656 5 PP UNIPETROL GB005 0.060558 0.003569 6 MFR 4 Black 0.052589 0.003695 7 ADD SCHULMAN DTY 20 AOUV 0.041478 0.003554 8 DEVAN 2SPIN PP 106 0.038438 0.004061 9 High MFR Black 0.028838 0.002515 10 MFR 4 Grey Grey 0.025758 0.002319 11 ADD ARGUS ARGUTHERM AO 77 PP 0.024526 0.002253 12 High MFR Grey 0.020652 0.004188 13 MFR 4 Green 0.016508 0.002725 14 SILIKAT PBH ABPP 05 0.016283 0.002291 15 PP TOTAL 7089 0.01624 0.004897 16 ADD POLYPLAST PPM AO 01 UV PP 0.013499 0.001685 17 MFR 4 Black Brown 0.013401 0.002131 18 PP POLYCHIM HB12XF 0.01245 0.002185 19 PP INEOS 100 GA09 0.010354 0.00156","title":"Multivariate Quantitative to Categorical (Binned Output Variable)"},{"location":"extras/X4_Candy_Ribbons/#60-business-impact","text":"What is the so what? From Hyp tests - target is either 'Total Position Out' or 'Total Seconds Out' We had culprits from pearsons: ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] culprits from non parametric tests: list ( problem_vars ) ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] and culprits from linear regression forward_selection_feeds Index(['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'MFR 4 Green', 'ADD SCHULMAN DTY 20 AOUV', 'GREEN SCHULMAN PZ 302446 1 5 T', 'MFR 4 Grey', 'UV ARGUS ARX V 16 706 UV PP', 'GREEN SCHULMAN PZ 34198 1 5 T'], dtype='object') and we can check the overlap between non-parametric and linear regression (parametric): predict_vars = [ i for i in basemodel . columns if i in ( problem_vars )] predict_vars ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05'] all_vars = set ( list ( problem_vars ) + list ( forward_selection_feeds )) all_vars {'ADD SCHULMAN DTY 20 AOUV', 'BLACK SCHULMAN P7381', 'GREEN SCHULMAN PZ 302446 1 5 T', 'GREEN SCHULMAN PZ 34198 1 5 T', 'High MFR Grey', 'MFR 4 Green', 'MFR 4 Grey', 'PP POLYCHIM HB12XF', 'PP TOTAL 7089', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'UV ARGUS ARX V 16 706 UV PP'} # add to DF business = pd . DataFrame () value_line_by_day = 99000 for ind in problem_vars : try : # products, prods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique () nprods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . nunique () dff = df . loc [ df [ 'Product' ] . isin ( prods )] # total runtime, tot_runtime = dff [ 'Time Delta' ] . sum () # total runtime with feed, tot_runtime_wfeed = dff [ dff [ ind ] > 0 ][ 'Time Delta' ] . sum () #downtime w/o feed, downtime w/ feed, avg_downtime_min = dff . groupby ( dff [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 avg_downtime_min_wfeed = avg_downtime_min [ True ] avg_downtime_min_w_ofeed = avg_downtime_min [ False ] ratio = avg_downtime_min_wfeed / avg_downtime_min_w_ofeed # average runtime, avg_runtime_min = dff [ 'Time Delta' ] . mean () . total_seconds () / 60 # days saved, dollars saved days_saved = ( tot_runtime_wfeed . total_seconds () / 60 / avg_runtime_min * avg_downtime_min . diff () . values [ - 1 ]) / 60 / 24 dollars_saved_per_batch = value_line_by_day * avg_downtime_min . diff () . values [ - 1 ] / ( 60 * 24 ) business = pd . concat (( business , pd . DataFrame ([[ ind , nprods , tot_runtime , tot_runtime_wfeed , avg_downtime_min_wfeed , avg_downtime_min_w_ofeed , ratio , avg_runtime_min , days_saved , days_saved * value_line_by_day , dollars_saved_per_batch ]], columns = [ 'Feed' , 'Products' , 'Total Runtime, Products' , 'Total Runtime With Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Average Runtime' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]))) except : print ( ind ) print ( dff . groupby ( df [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 ) business = business . sort_values ( 'Dollars Saved (Per Batch)' , ascending = False ) . reset_index ( drop = True ) business # workshop downtime ratio # feed products, feed products # Dollars saved per batch .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Products Total Runtime, Products Total Runtime With Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Average Runtime Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26 409 days 20:43:21 94 days 04:36:05 26.250639 13.671063 1.920161 70.835736 16.727319 1.656005e+06 864.845823 1 High MFR Grey 36 564 days 04:13:41 94 days 12:26:59 23.623539 12.809036 1.844287 65.459164 15.615433 1.545928e+06 743.497080 2 PP POLYCHIM HB12XF 17 383 days 00:03:51 67 days 09:13:53 21.043525 15.600015 1.348943 70.908183 5.173013 5.121283e+05 374.241303 3 BLACK SCHULMAN P7381 64 751 days 20:01:57 590 days 12:21:50 14.902871 12.363943 1.205349 63.216276 23.716605 2.347944e+06 174.551301 4 PP TOTAL 7089 13 291 days 22:06:22 79 days 15:56:47 20.651050 18.269305 1.130369 80.146114 2.367430 2.343756e+05 163.744933 pd . set_option ( 'display.precision' , 2 ) display ( business [[ 'Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]], ) pd . set_option ( 'display.precision' , 6 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26.25 13.67 1.92 16.73 1.66e+06 864.85 1 High MFR Grey 23.62 12.81 1.84 15.62 1.55e+06 743.50 2 PP POLYCHIM HB12XF 21.04 15.60 1.35 5.17 5.12e+05 374.24 3 BLACK SCHULMAN P7381 14.90 12.36 1.21 23.72 2.35e+06 174.55 4 PP TOTAL 7089 20.65 18.27 1.13 2.37 2.34e+05 163.74 We see that 'SILKAT PBH ABPP 05' as well as 'High MFR Grey' have a large amount of days saved over the two years, a strong downtime multiplier effect and the largest dollars saved per batch. Since High MFR Grey is a recycle feed, we may not have the ability to remove it from the feed list. Leaving the SILKAT (Silicate) feed the prime feedstock to remove","title":"6.0 Business Impact"},{"location":"extras/X4_Candy_Ribbons/#70-visualizations","text":"","title":"7.0 Visualizations"},{"location":"extras/X4_Candy_Ribbons/#pp-total-7089","text":"predict_var = predict_vars [ 1 ] outcome_var = [ i for i in df . columns if 'Seconds' in i ] def myplot ( col = list ( df . loc [ df [ predict_var ] > 0 ][ 'Product' ] . unique ())): fig , ax = plt . subplots ( figsize = ( 20 , 10 )) axt = ax . twinx () # other feeds df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True ) . dropna ( axis = 1 , how = 'all' ) . plot ( ls = '' , marker = '*' , ax = ax , alpha = 0.2 ) # predict_var feed df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True )[ predict_var ] . plot ( ls = '--' , marker = '+' , ax = ax , lw = 2 , c = 'red' ) # outcome_var df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ) . plot ( ls = '-' , marker = '+' , lw = 1 , ax = axt , alpha = 1 , c = 'blue' , label = 'Total Seconds Out' ) # outliers ax . set_ylim ( 0 ) axt . set_ylim ( 0 , np . quantile ( df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ), 0.997 )) h1 , l1 = ax . get_legend_handles_labels () h2 , l2 = axt . get_legend_handles_labels () ax . legend ( h1 + h2 , l1 + l2 , loc = 0 ) interact ( myplot ) interactive(children=(Dropdown(description='col', options=(0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.\u2026 <function __main__.myplot(col=[0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.0, 136215.0, 125215.0, 110215.0, 100215.0, 80215.0, 100617.0, 90617.0, 90215.0, 90216.0, 110520.0, 80617.0])>","title":"PP TOTAL 7089"},{"location":"extras/X4_Candy_Ribbons_Make_Data/","text":"Causality Analysis \u00b6 TODO \u00b6 ols assumptions for univariate analyses residual plots forward selection lines up more or less with chi square tests include some colinearity measure? logistic regression (binned seconds, binned total outs) random forest does little better than random guessing business impact reorganize hypothesis tests section univariate vs multivariate cat v vat, cat v quant, quant v quant, quant v cat 1.0 Imports \u00b6 Import Libraries \u00b6 # basic packages import pandas as pd import numpy as np import random import copy # visualization packages from ipywidgets import interact import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures , MinMaxScaler from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , silhouette_score , calinski_harabasz_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score def make_patch_spines_invisible ( ax ): ax . set_frame_on ( True ) ax . patch . set_visible ( False ) for sp in ax . spines . values (): sp . set_visible ( False ) from matplotlib import colors from itertools import cycle def my_colors (): tab = cycle ( colors . TABLEAU_COLORS ) return tab Load Data \u00b6 consider only feeds consider feeds and operating conditions feeds = [ 'ADD ARGUS ARGUSTAT AT 31 HK' , 'ADD ARGUS ARGUTHERM AO 77 PP' , 'ADD POLYPLAST PPM AO 01 UV PP' , 'ADD SCHULMAN DTY 20 AOUV' , 'BEIGE SCHULMAN PZ 91738 1 5 ZA' , 'BLACK SCHULMAN P7381' , 'DEVAN 2SPIN PP 106' , 'GREEN POLYPLAST COLOUR MB PP 6821' , 'GREEN SCHULMAN PZ 302446 1 5 T' , 'High MFR Black' , 'GREEN SCHULMAN PZ 34198 1 5 T' , 'MFR 4 Black' , 'MFR 4 Black Brown' , 'High MFR Grey' , 'MFR 4 Green' , 'PP BOREALIS HE465FB' , 'MFR 4 Black Colour Blending' , 'PP INEOS 100 GA02' , 'PP INEOS 100 GA04' , 'PP INEOS 100 GA09' , 'MFR 4 Grey Colour Blending' , 'PP INEOS GA012' , 'PP POLYCHIM HB12XF' , 'MFR 4 White' , 'PP TOTAL 7089' , 'PP TOTAL PPH 4065' , 'MFR 4 White Colour Blending' , 'PP BOREALIS HE370FB' , 'PP UNIPETROL GB005' , 'SILIKAT PBH ABPP 05' , 'SILIKAT POLYPLUS AB 1001 PP' , 'UV ARGUS ARGUVIN UV 361 PP' , 'UV ARGUS ARX 904 11 PP' , 'UV ARGUS ARX 904 11 PP.1' , 'UV ARGUS ARX 904 11 PP.2' , 'UV POLYPLAST 6005 PP' , 'UV SCHULMAN FPPUV 38' , 'MFR 4 Grey' , 'PP POLYCHIM HL10XF' , 'MFR 4 Grey Grey' , 'PP POLYMER TEST MFI4' , 'PP TOTAL 4069' , 'RR MASTERBATCH TEST' , 'UV ARGUS ARX V 16 706 UV PP' ] df = pd . read_csv ( '../../../../../Dropbox/work/mfganalytic/data/dupont/TyparMasterReportWithSampleDataImprovement2017-today.csv' , header = 1 ) df = df . loc [ df [ 'Product' ] . notnull ()] df [ feeds ] = df [ feeds ] . div ( df [ feeds ] . sum ( axis = 1 ), axis = 0 ) #normalize feed rates df [ 'Time Delta' ] = pd . to_datetime ( df [ 'Doff Time' ]) . diff () df [ 'Total Seconds Out' ] = df [[ i for i in df . columns if 'Sec' in i ]] . sum ( 1 ) # df.dropna(inplace=True) hangs = [ df . columns [ i ] for i , j in enumerate ( 'Hang' in col for col in df . columns ) if j ] speeds = [ df . columns [ i ] for i , j in enumerate ( 'speed' in col for col in df . columns ) if j ] outs = [ df . columns [ i ] for i , j in enumerate ( 'Out' in col for col in df . columns ) if j ] prods = df [ 'Product' ] . unique () counts = [ i for i in df . columns if 'Count' in i ] #from LOEWEN virgin_polymer = [ 'PP INEOS 100 GA04' , 'PP POLYCHIM HB12XF' , 'PP TOTAL 7089' , #INEOS 100 GA09 until 2018 (2k rolls) 'PP TOTAL PPH 4065' , 'PP UNIPETROL GB005' , 'PP POLYCHIM HL10XF' ] #HL10XF not used after 2016, #BOREALIS virgin_polymer = [] for col in df . columns : if ( 'PP INEOS' in col ) or ( 'PP TOTAL' in col ) or ( 'PP UNI' in col ) or ( 'PP BORE' in col ) or ( 'PP POLY' in col ): virgin_polymer . append ( col ) pressures = [] for col in df . columns : if 'Press' in col or 'PRESS' in col : pressures . append ( col ) silicates = [] for col in df . columns : if 'SIL' in col : silicates . append ( col ) uvs = [] for col in df . columns : if 'UV ' in col : uvs . append ( col ) recycle = [] for col in df . columns : if 'MFR' in col : recycle . append ( col ) seconds = [] for col in df . columns : if 'Sec' in col : seconds . append ( col ) master_batch = list ( set ( feeds ) - set ( recycle ) - set ( virgin_polymer )) operation = list ( set ( df . columns ) - set ( master_batch ) - set ( virgin_polymer ) - set ( recycle ) - set ( hangs ) - set ( outs )) operation . sort () A summary of the columns print ( f 'pressures: { len ( pressures ) } ' ) print ( f 'silicates: { len ( silicates ) } ' ) print ( f 'uvs: { len ( uvs ) } ' ) print ( f 'feeds: { len ( feeds ) } ' ) print ( f 'master_batch: { len ( master_batch ) } ' ) print ( f 'virgin_polymer: { len ( virgin_polymer ) } ' ) print ( f 'recycle: { len ( recycle ) } ' ) print ( f 'operation: { len ( operation ) } ' ) print () print ( f 'hangs: { len ( hangs ) } ' ) print ( f 'speeds: { len ( speeds ) } ' ) print ( f 'outs: { len ( outs ) } ' ) print ( f 'seconds: { len ( seconds ) } ' ) print () print ( 'summary target columns: TotalHangs, Total Position Out' ) print () print ( df . shape ) pressures: 8 silicates: 2 uvs: 8 feeds: 44 master_batch: 20 virgin_polymer: 13 recycle: 11 operation: 180 hangs: 9 speeds: 9 outs: 10 seconds: 9 summary target columns: TotalHangs, Total Position Out (17897, 243) import datetime df . columns Index(['Millroll ID', 'Product', 'Roll Width', 'Sample Number', 'Doff Time', 'Block1Pos Out Seconds', 'Block2Pos Out Seconds', 'Block3Pos Out Seconds', 'Block4Pos Out Seconds', 'Block1Pos Out Count', ... 'TKW5 AVG - Tack - Tear West 5 Meter', 'TPOP - NUMBER OF TPO PEEKS (TPOP)', 'TTMD AVG - TRAP TEAR MD (TTMD)', 'TTXD AVG - TRAP TEAR XD (TTXD)', 'UPEX - UWU PLOT EXTREM (UPEX)', 'UWG - UWU GRAVIMETRIC WITH SCALE (UWG)', 'UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP)', 'WATP - WATER PERMEABILITY (WATP)', 'Time Delta', 'Total Seconds Out'], dtype='object', length=243) df [ 'Product' ] . name 'Product' df [( pd . to_datetime ( df [ 'Doff Time' ]) < datetime . datetime ( 2018 , 1 , 1 )) & ( pd . to_datetime ( df [ 'Doff Time' ]) >= datetime . datetime ( 2017 , 12 , 1 ))] . groupby ( 'Product' )[ 'Product' , 'Roll Width' ] . apply ( lambda x : x . count () if ( x . name == 'Product' ) else ( x . sum ())) /tmp/ipykernel_301/568605750.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead. df[(pd.to_datetime(df['Doff Time']) < datetime.datetime(2018,1,1)) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Product Roll Width Product 0.0 0.0 20800.0 45235.0 1266580.0 147000.0 55235.0 110470.0 10400.0 55626.0 11458956.0 986560.0 62235.0 1244700.0 95500.0 68235.0 614115.0 37800.0 68236.0 341180.0 22800.0 90234.0 721872.0 42000.0 90617.0 2084191.0 119600.0 100015.0 800120.0 33760.0 100175.0 1702975.0 88400.0 100617.0 1408638.0 72800.0 103115.0 412460.0 20600.0 110617.0 12057253.0 566800.0 130010.0 1820140.0 59080.0 130030.0 520120.0 16600.0 135010.0 2025150.0 63300.0 136215.0 4767525.0 157500.0 136448.0 1500928.0 57200.0 150010.0 450030.0 12660.0 160030.0 800150.0 22100.0 190226.0 8940622.0 244400.0 240448.0 12503296.0 265200.0 260607.0 1563642.0 31200.0 294120.0 2647080.0 45000.0 350448.0 11915232.0 173400.0 # uvs will be color # recycle will be recycle # silicates will be sprinkles # virgin_polymer will be taffy_base colors = [ 'red' , 'green' , 'white' , 'blue' , 'magenta' , 'yellow' , 'cyan' , 'purple' ] taffr = [ 'r213' , 'r345' , 'r093' , 'r103' , 'r194' , 'r093' , 'r298' , 'r124' , 'r000' , 'r889' , 'r299' ] sprinkles = [ 'sprinkle1' , 'sprinkle2' ] taffy_base = [ 'taffy1' , 'taffy2' , 'taffy3' , 'taffy4' , 'taffy5' , 'taffy6' , 'taffy7' , 'taffy8' , 'taffy9' , 'taffy10' , 'taffy11' , 'taffy12' , 'taffy13' ] all_feats = df . select_dtypes ( float ) . columns [ df . select_dtypes ( float ) . columns . isin ( feeds + operation )] . values hangs2 = [ i . lower () for i in [ 'Block 1 Rips Seconds' , 'Block 2 Rips Seconds' , 'Block 3 Rips Seconds' , 'Block 4 Rips Seconds' , 'Block 1 Rips Count' , 'Block 2 Rips Count' , 'Block 3 Rips Count' , 'Block 4 Rips Count' , 'Total Rips' ]] outs2 = [ i . lower () for i in [ 'Block 1 Bumps Seconds' , 'Block 2 Bumps Seconds' , 'Block 3 Bumps Seconds' , 'Block 4 Bumps Seconds' , 'Block 1 Bumps Count' , 'Block 2 Bumps Count' , 'Block 3 Bumps Count' , 'Block 4 Bumps Count' , 'Total Bumps' , 'Total Seconds Out' ]] taffy = pd . DataFrame () taffy [ 'product' ] = df [ 'Product' ] taffy [ 'date' ] = df [ 'Doff Time' ] taffy [ hangs2 ] = df [ hangs ] taffy [ outs2 ] = df [ outs ] taffy [ colors ] = df [ uvs ] taffy [ taffr ] = df [ recycle ] taffy [ sprinkles ] = df [ silicates ] taffy [ taffy_base ] = df [ virgin_polymer ] taffy .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } product date block 1 rips seconds block 2 rips seconds block 3 rips seconds block 4 rips seconds block 1 rips count block 2 rips count block 3 rips count block 4 rips count ... taffy4 taffy5 taffy6 taffy7 taffy8 taffy9 taffy10 taffy11 taffy12 taffy13 0 136215.0 1/1/2017 12:43:57 AM 9.0 NaN NaN NaN 1.0 NaN NaN NaN ... NaN NaN NaN NaN 0.357711 NaN NaN NaN NaN NaN 1 136215.0 1/1/2017 1:32:49 AM NaN 29.0 103.0 NaN NaN 2.0 1.0 NaN ... NaN NaN NaN NaN 0.291769 NaN NaN NaN NaN NaN 2 136215.0 1/1/2017 2:21:40 AM 11.0 NaN 9.0 NaN 1.0 NaN 1.0 NaN ... NaN NaN NaN NaN 0.293044 NaN NaN NaN NaN NaN 3 136215.0 1/1/2017 3:10:34 AM NaN 198.0 219.0 NaN NaN 4.0 2.0 NaN ... NaN NaN NaN NaN 0.308707 NaN NaN NaN NaN NaN 4 136215.0 1/1/2017 3:59:24 AM NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN 0.451992 NaN NaN NaN NaN NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 90215.0 2/25/2019 7:56:16 PM NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN 0.382452 NaN 0.254966 NaN NaN NaN 17893 90215.0 2/25/2019 8:53:41 PM NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN 0.380653 NaN 0.253770 NaN NaN NaN 17894 90215.0 2/25/2019 9:51:06 PM NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN 0.378461 NaN 0.252311 NaN NaN NaN 17895 90215.0 2/25/2019 10:48:32 PM NaN NaN 223.0 NaN NaN NaN 3.0 NaN ... NaN NaN NaN NaN 0.379047 NaN 0.252599 NaN NaN NaN 17896 90215.0 2/25/2019 11:45:55 PM NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN 0.378854 NaN 0.252431 NaN NaN NaN 17897 rows \u00d7 54 columns 2.0 Understand The Dependent Variable \u00b6 What is the hit rate in these upset columns? \u00b6 Around 7500 cases with no problem; 10000 cases with a position out target = \"Total Position Out\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 10355 False 7542 Name: Total Position Out, dtype: int64 There are 30% higher incidence of hangs target = \"TotalHangs\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 13954 False 3943 Name: TotalHangs, dtype: int64 target1 = \"TotalHangs\" target2 = \"Total Position Out\" print ( f \"both upsets: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"only hangs: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) print ( f \"only outs: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"neither upset: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) both upsets: 10161 only hangs: 3793 only outs: 194 neither upset: 3749 What is the magnitude of the outages? \u00b6 target = \"Total Position Out\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f0ad7207b80>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total Position Out Unnamed 0 Total Position Out 1.00000 0.62591 Unnamed 0 0.62591 1.00000 target = \"TotalHangs\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f0ad3eff460>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TotalHangs Unnamed 0 TotalHangs 1.000000 0.390949 Unnamed 0 0.390949 1.000000 Distribution around the target variable, total seconds \u00b6 We will certainly encounter trouble in an OLS estimate with this shape of the target variable; extreme skewness, upper tail, perhaps outliers tot_seconds = df [ seconds ] . sum ( 1 ) tot_seconds . kurt () 923.6728632470827 tot_seconds . skew () 25.09323820232841 tot_seconds . plot . kde () <AxesSubplot:ylabel='Density'> tot_seconds . describe () count 17897.000000 mean 2093.512879 std 8060.466090 min 0.000000 25% 20.000000 50% 666.000000 75% 1970.000000 max 396570.000000 dtype: float64 Do block positions have the same behavior? \u00b6 target = \"TotalHangs\" df . groupby ( target )[ hangs ] . count () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs TotalHangs 0.0 0 0 0 0 0 0 0 0 3943 1.0 510 682 745 977 510 682 745 977 2914 2.0 709 761 901 1109 709 761 901 1109 2424 3.0 721 776 930 1103 721 776 930 1103 1929 4.0 700 773 875 993 700 773 875 993 1577 5.0 609 668 744 828 609 668 745 828 1189 6.0 523 498 599 631 523 498 599 631 883 7.0 387 414 472 505 387 414 472 505 640 8.0 321 338 376 409 321 338 376 409 503 9.0 286 275 305 339 286 275 305 339 405 10.0 219 206 244 248 219 206 244 248 296 11.0 144 154 168 171 144 154 168 171 203 12.0 148 139 159 171 148 139 159 171 191 13.0 105 100 117 106 105 100 117 106 131 14.0 93 88 95 96 93 88 95 96 109 15.0 97 89 94 97 97 89 94 97 108 16.0 70 64 72 76 70 64 72 76 81 17.0 51 50 51 47 51 50 51 47 55 18.0 29 30 32 33 29 30 32 33 35 19.0 32 32 33 32 32 32 33 32 35 20.0 32 32 31 34 32 32 31 34 35 21.0 26 27 25 24 26 27 25 24 28 22.0 21 20 24 24 21 20 24 24 24 23.0 23 23 24 23 23 23 24 23 25 24.0 16 17 18 19 16 17 18 19 20 25.0 7 8 8 7 7 8 8 7 8 26.0 11 10 10 11 11 10 10 11 11 27.0 8 9 9 8 8 9 9 8 9 28.0 10 10 10 10 10 10 10 10 10 29.0 6 5 6 6 6 5 6 6 6 30.0 4 3 4 4 4 3 4 4 4 31.0 8 8 8 8 8 8 8 8 8 32.0 4 3 4 4 4 3 4 4 4 33.0 7 5 7 6 7 5 7 6 7 34.0 9 8 9 8 9 8 9 8 9 35.0 6 6 6 6 6 6 6 6 6 36.0 6 5 6 6 6 5 6 6 6 37.0 3 3 3 3 3 3 3 3 3 38.0 1 1 1 0 1 1 1 0 1 39.0 1 1 1 1 1 1 1 1 1 40.0 2 2 2 2 2 2 2 2 2 42.0 1 1 1 1 1 1 1 1 1 43.0 3 3 3 3 3 3 3 3 3 45.0 3 3 3 3 3 3 3 3 3 46.0 2 2 2 2 2 2 2 2 2 48.0 1 1 1 1 1 1 1 1 1 52.0 1 1 1 1 1 1 1 1 1 53.0 1 1 1 1 1 1 1 1 1 54.0 1 1 1 1 1 1 1 1 1 58.0 1 1 1 1 1 1 1 1 1 62.0 1 1 1 1 1 1 1 1 1 63.0 1 1 1 1 1 1 1 1 1 70.0 1 1 1 1 1 1 1 1 1 72.0 1 1 1 1 1 1 1 1 1 140.0 1 1 1 1 1 1 1 1 1 df [[ i for i in outs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:> df [[ i for i in hangs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:> 3.0 Look for dependent-independent signal \u00b6 Are there linear relationships between the dependent and independent variables? \u00b6 corr = df . corr () ( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 ) Product False Roll Width False Block1Pos Out Seconds True Block2Pos Out Seconds True Block3Pos Out Seconds True ... UPEX - UWU PLOT EXTREM (UPEX) False UWG - UWU GRAVIMETRIC WITH SCALE (UWG) False UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP) False WATP - WATER PERMEABILITY (WATP) False Total Seconds Out True Length: 237, dtype: bool cor_filt = corr . loc [( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 )] . drop ( labels = outs + hangs , axis = 0 )[ outs + hangs ] cor_filt .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Pos Out Seconds Block2Pos Out Seconds Block3Pos Out Seconds Block4Pos Out Seconds Block1Pos Out Count Block2Pos Out Count Block3Pos Out Count Block4Pos Out Count Total Position Out Total Seconds Out Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs ADD ARGUS ARGUSTAT AT 31 HK -0.103999 -0.417688 -0.383222 0.202131 -0.056194 -0.412274 0.155126 0.145535 0.014216 0.029563 0.121682 -0.256723 -0.012676 0.099690 -0.069354 -0.041492 0.094150 0.041900 0.344896 PP INEOS 100 GA02 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 -1.000000 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 MFR 4 White 0.289648 -0.016268 0.451446 -0.155932 0.297256 -0.000098 0.376795 -0.003129 0.225425 0.182687 0.231764 -0.045076 0.283286 -0.119481 0.108396 -0.057188 0.317089 0.008018 0.189109 PP TOTAL 7089 0.160268 0.171023 0.210796 0.093720 0.192159 0.210942 0.206859 0.076058 0.260505 0.207522 0.087954 0.050670 0.052622 -0.050506 0.218148 0.251676 0.134881 0.121080 0.331897 SILIKAT POLYPLUS AB 1001 PP 0.015132 0.125801 0.379559 0.255662 -0.018715 0.012019 0.161698 0.067661 0.127830 0.137376 -0.016317 -0.018187 -0.018956 0.086781 0.011207 -0.020307 0.092291 0.109259 0.069217 RJET15219WG1 - Corona Charge I AVG BLK#2 -0.120204 -0.120509 -0.056377 -0.002054 -0.198138 -0.221189 -0.193393 -0.024541 -0.232863 -0.149862 -0.090896 -0.060404 -0.054169 -0.052683 -0.215325 -0.216545 -0.182990 -0.120387 -0.308217 BLFL - COLOUR BLACK FILAMENT L value (BLFL) -0.360086 -0.157845 -0.245565 -0.268837 -0.153321 -0.162697 -0.212481 -0.247721 -0.067632 -0.121690 0.097783 0.146515 -0.346923 -0.110987 0.402310 -0.274153 -0.378670 -0.116264 -0.117501 BRFA - COLOUR BROWN FILAMENT a value (BRFa) 0.159056 0.250229 -0.384880 0.109024 0.134248 0.112836 -0.380715 -0.054074 0.099040 0.166926 0.170184 -0.065344 -0.195062 0.094596 0.195404 -0.333632 -0.026209 0.177576 0.173995 BRFB - COLOUR BROWN FILAMENT b value (BRFb) 0.413876 0.047748 0.332752 0.342115 0.145794 -0.028170 0.246787 0.234624 0.216156 0.311363 0.148441 -0.210193 0.118275 0.285286 -0.027055 -0.275750 0.100830 0.328240 0.164345 BRFL - COLOUR BROWN FILAMENT L value (BRFL) 0.198864 -0.178050 0.483242 0.205796 0.017456 -0.220432 0.411110 0.090885 0.039031 0.128326 0.337130 -0.326368 0.075738 0.145878 0.195055 -0.467536 -0.075130 0.188013 -0.000575 E20M AVG - SST ELONGATION 20X20 MD (E20M) 0.106870 -0.096775 -0.249986 -0.039633 0.009518 -0.049131 -0.367065 -0.074018 -0.070058 -0.071425 -0.124456 -0.016993 -0.070312 -0.036314 -0.145788 -0.075968 -0.197153 -0.069477 -0.072872 EF1M AVG - SST ELONGATION FOLDED 10 CM MD (EF1M) 1.000000 NaN 1.000000 -0.081738 1.000000 NaN NaN NaN -0.101606 -0.178993 0.313364 -0.936781 -1.000000 -0.517401 0.306235 -0.950303 -1.000000 -0.324932 -0.112334 EF1X AVG - SST ELONGATION FOLDED 10 CM XD (EF1X) 1.000000 NaN 1.000000 0.126777 1.000000 NaN NaN NaN 0.010039 -0.063399 0.896084 -0.961383 -1.000000 -0.609904 0.892730 -0.971830 -1.000000 -0.340772 0.050483 NATM AVG - Nail Tear MD EN 13859-1 (NATM) -0.517421 -0.835147 0.511409 -0.258360 -0.296500 -0.906038 NaN -0.333754 -0.100244 0.013436 -0.338129 0.580493 0.826191 -0.322585 -0.348782 0.063309 0.929207 -0.430201 0.086484 NATX AVG - Nail Tear XD EN 13859-1 (NATX) 0.269389 -0.864128 0.542651 0.071499 -0.053990 -0.927938 NaN -0.056465 -0.084131 0.046366 -0.190830 0.541297 0.777131 -0.114506 -0.096640 0.051081 0.902927 -0.295087 0.093804 O90 - O90 EN ISO 12956 (O90E) NaN NaN NaN 0.807962 NaN NaN NaN NaN -0.265093 -0.268107 -0.280791 -0.378899 NaN -0.451610 -0.307148 -0.267261 NaN -0.269680 -0.193539 PU AVG - PUNCTURE \"US Rod\" (PU) 0.173625 -0.790779 -0.165265 0.451194 -0.127912 -0.952390 -0.184515 0.734933 -0.040323 -0.021124 0.096383 -0.998695 -0.288786 0.156274 0.066142 0.363427 -0.297118 0.479459 -0.239496 TKO4 AVG - Tack-Tear Ost 4Meter -0.170480 -0.383667 -0.326335 0.006590 -0.104058 -0.308260 -0.329544 0.047734 -0.171814 -0.195662 -0.214633 -0.440748 -0.182875 -0.027353 -0.077638 -0.447423 -0.187893 0.114764 -0.160340 TKO5 AVG - Tack - Tear Ost 5 Meter 1.000000 0.134107 0.560717 -0.070116 NaN 0.106485 0.152594 -0.525375 -0.373817 -0.306252 0.338272 0.155577 0.034557 -0.453643 0.286066 0.056270 -0.066270 -0.400941 -0.387297 TKW4 AVG - Tack-Tear West 4Meter -0.334291 -0.282220 -0.265607 -0.134467 -0.320379 -0.231417 -0.300679 -0.265049 -0.274024 -0.250617 -0.277325 -0.375559 -0.168965 -0.154119 -0.236424 -0.282687 -0.168865 -0.162801 -0.316193 TKW5 AVG - Tack - Tear West 5 Meter 1.000000 0.051583 0.100609 0.018368 NaN 0.014194 -0.167032 -0.366762 -0.279911 -0.190366 0.069184 0.108657 0.224892 -0.326866 -0.003422 0.043730 0.039670 -0.362928 -0.270898 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt , cmap = cmap ) <AxesSubplot:> Filter N > 100 \u00b6 cor_filt = cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]] fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]], cmap = cmap ) <AxesSubplot:> Operating Conditions \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Operations Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Operations Correlations to Hangs and Outs') Feeds \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ ~ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Feeds Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Feeds Correlations to Hangs and Outs') ind_vars = list ( cor_filt . index [ ~ cor_filt . index . isin ( operation )]) ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] fig , ax = plt . subplots ( 1 , 3 , figsize = ( 30 , 10 )) indices = np . argwhere ( ax ) for index , ind_var in enumerate ( ind_vars ): df [[ ind_var ] + seconds ] . melt ( id_vars = ind_var , value_vars = None , var_name = None ,) . plot ( x = ind_var , y = 'value' , ax = ax [ index ], ls = '' , marker = '.' ) 4.0 Understand the Independent Variable \u00b6 Descriptive Stats on Ind Var \u00b6 all the feeds sum to 1 (feeds are mass or volume fractions), so we probably won't need to worry about scaling df [ feeds ] . sum ( 1 ) 0 1.0 1 1.0 2 1.0 3 1.0 4 1.0 ... 17892 1.0 17893 1.0 17894 1.0 17895 1.0 17896 1.0 Length: 17897, dtype: float64 many of the feeds are not used at all skew = df [ feeds ] . skew () kurt = df [ feeds ] . kurtosis () null = df [ feeds ] . isnull () . sum () n = df [ feeds ] . notnull () . sum () med = df [ feeds ] . median () men = df [ feeds ] . mean () dff = pd . DataFrame ([ skew , kurt , null , n , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'null cnt' , 'n' , 'median' , 'mean' ] dff . loc [ dff [ 'n' ] > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis null cnt n median mean ADD ARGUS ARGUSTAT AT 31 HK -1.122018 -0.275303 17798.0 99.0 0.006560 0.005080 ADD ARGUS ARGUTHERM AO 77 PP 14.927951 248.300227 15627.0 2270.0 0.005249 0.010705 ADD POLYPLAST PPM AO 01 UV PP 5.788514 87.051660 16769.0 1128.0 0.006397 0.006218 ADD SCHULMAN DTY 20 AOUV 6.573319 99.052816 13484.0 4413.0 0.004997 0.005411 BEIGE SCHULMAN PZ 91738 1 5 ZA -0.455858 10.108094 16975.0 922.0 0.008922 0.008708 BLACK SCHULMAN P7381 2.301415 16.313957 4471.0 13426.0 0.000317 0.002040 DEVAN 2SPIN PP 106 0.770601 11.066370 13279.0 4618.0 0.006658 0.005554 GREEN POLYPLAST COLOUR MB PP 6821 -1.856631 3.540741 16894.0 1003.0 0.007519 0.006822 GREEN SCHULMAN PZ 302446 1 5 T 0.363239 8.864015 17150.0 747.0 0.018764 0.018006 High MFR Black 0.473484 -1.518082 14862.0 3035.0 0.047883 0.146677 MFR 4 Black 1.612264 4.261601 12595.0 5302.0 0.030099 0.098792 MFR 4 Black Brown -0.086521 -0.342944 16360.0 1537.0 0.189244 0.150273 High MFR Grey -0.364616 -1.483643 15900.0 1997.0 0.226358 0.199138 MFR 4 Green 0.149918 8.244109 15985.0 1912.0 0.279622 0.262574 PP INEOS 100 GA02 NaN NaN 17895.0 2.0 0.107927 0.107927 PP INEOS 100 GA04 1.294449 5.764334 6684.0 11213.0 0.208757 0.216941 PP INEOS 100 GA09 -0.449540 -0.968103 16802.0 1095.0 0.224353 0.219208 PP POLYCHIM HB12XF -0.222658 -0.684893 16525.0 1372.0 0.275755 0.291827 MFR 4 White 1.136619 2.053071 17559.0 338.0 0.098961 0.111129 PP TOTAL 7089 -0.216726 -1.333059 16346.0 1551.0 0.276430 0.288643 PP TOTAL PPH 4065 0.353911 0.740658 611.0 17286.0 0.441009 0.424684 PP UNIPETROL GB005 0.117291 1.045121 11444.0 6453.0 0.324773 0.295906 SILIKAT PBH ABPP 05 2.690146 26.471577 15967.0 1930.0 0.005001 0.005229 SILIKAT POLYPLUS AB 1001 PP 6.874010 77.288671 17360.0 537.0 0.002506 0.002642 UV ARGUS ARX 904 11 PP 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.1 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.2 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV POLYPLAST 6005 PP 1.426790 1.634549 17506.0 391.0 0.020009 0.026245 UV SCHULMAN FPPUV 38 2.114261 14.372541 17141.0 756.0 0.029986 0.032261 MFR 4 Grey 1.718802 10.970192 8567.0 9330.0 0.269445 0.263401 MFR 4 Grey Grey 3.607370 20.342313 15865.0 2032.0 0.040035 0.068547 UV ARGUS ARX V 16 706 UV PP 1.889895 6.474782 7972.0 9925.0 0.002110 0.007994 our ind_vars feeds have some strong correlations with other feeds In particular, PP TOTAL 7089 and SILIKAT POLYPLUS AB 1001 PP have a correlation w/ eachother of .923 ind_corr = df [ feeds ] . corr ()[ ind_vars ] . dropna ( how = 'all' ) ind_corr . loc [( abs ( ind_corr ) > 0.5 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 ind_vars feeds exhibit bimodal distributions (2+ recipes) for ind in ind_vars : df [ ind ] . plot . kde () plt . title ( ind ) plt . xlim ( 0 ) plt . show () 5.0 Hypothesis Tests \u00b6 Non-Parametric \u00b6 Univariate Categorical to Categorical (Chi-Square) \u00b6 ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 for ind in ind_vars : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 10182 False 7377 True True 173 False 165 Name: Total Position Out, dtype: int64 [[10182 173] [ 7377 165]] chi2 stat: 6.02 p-value: 1.41e-02 PP TOTAL 7089 Total Position Out False True 9171 False 7175 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[9171 1184] [7175 367]] chi2 stat: 237.00 p-value: 1.78e-53 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 9996 False 7364 True True 359 False 178 Name: Total Position Out, dtype: int64 [[9996 359] [7364 178]] chi2 stat: 17.99 p-value: 2.22e-05 chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 chisum = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum [ 'type' ] = None chisum . loc [ chisum [ 'feed' ] . isin ( master_batch ), 'type' ] = 'master_batch' chisum . loc [ chisum [ 'feed' ] . isin ( recycle ), 'type' ] = 'recyle' chisum . loc [ chisum [ 'feed' ] . isin ( virgin_polymer ), 'type' ] = 'virgin_polymer' chisum . loc [ chisum [ 'feed' ] . isin ( silicates ), 'type' ] = 'silicates' chisum . loc [ chisum [ 'feed' ] . isin ( uvs ), 'type' ] = 'uvs' chisum . loc [ chisum [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null type 0 PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1 PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2 High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3 SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 4 BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 5 PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 6 ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 7 DEVAN 2SPIN PP 106 35.237690 2.918209e-09 [[7511, 2844], [5768, 1774]] False True master_batch 8 High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 9 GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 10 MFR 4 Black Brown 20.494540 5.980160e-06 [[9550, 805], [6810, 732]] True True recyle 11 UV ARGUS ARX V 16 706 UV PP 20.048858 7.548859e-06 [[4465, 5890], [3507, 4035]] False True uvs 12 ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 13 SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 14 GREEN SCHULMAN PZ 302446 1 5 T 14.155700 1.682865e-04 [[9973, 382], [7177, 365]] True True master_batch 15 MFR 4 Grey Grey 9.004184 2.693622e-03 [[9464, 1141], [6401, 891]] True True recyle Check confounding variables \u00b6 Isolate Products (product conflation with feed) Let's isolate the comparison by only products made with the feed ( ind_var ) in question We see that MFR 4 White and SILIKAT POLYPLUS AB 1001 PP are very imbalanced compared to PP TOTAL 7089 target = 'Total Position Out' for ind in ind_vars : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 3978 False 2933 True True 173 False 165 Name: Total Position Out, dtype: int64 [[3978 173] [2933 165]] chi2 stat: 5.10 p-value: 2.40e-02 PP TOTAL 7089 Total Position Out False True 2301 False 1393 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[2301 1184] [1393 367]] chi2 stat: 96.05 p-value: 1.12e-22 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 2418 False 1602 True True 359 False 178 Name: Total Position Out, dtype: int64 [[2418 359] [1602 178]] chi2 stat: 8.66 p-value: 3.25e-03 target = 'Total Position Out' chisum2 = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum2 = pd . concat ([ chisum2 , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum2 . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum2 = chisum2 . sort_values ( 'p-value' ) . reset_index ( drop = True ) tests = chisum . set_index ( 'feed' ) . join ( chisum2 . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], rsuffix = ', confound' ) tests . loc [( tests [ 'reject null' ] == True ) & ( tests [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value cTable + reject null type p-value, confound reject null, confound feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True we can compare this with our feed correlation table - there may be other explanatory variables 'hiding' ind_corr . loc [( abs ( ind_corr ) > 0.3 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 BLACK SCHULMAN P7381 0.196431 -0.470838 0.082699 DEVAN 2SPIN PP 106 NaN -0.415896 0.097590 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA04 0.342077 -0.248904 0.402788 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP TOTAL PPH 4065 -0.419569 -0.160492 0.089303 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 UV ARGUS ARX 904 11 PP NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.1 NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.2 NaN -0.461986 NaN MFR 4 Grey -0.303323 -0.381548 0.038125 MFR 4 Grey Grey 0.395658 -0.102141 -0.086338 UV ARGUS ARX V 16 706 UV PP 0.246254 -0.439913 -0.084811 Univariate Categorical to Quantitative (Moods Median) \u00b6 A limitation of the previous analysis is that our threshold for counting an upset in the line was pretty low - any delay (position out > 0 seconds). A way we can naturally increase this threshold is to compare medians (of position out (secs)) across our groups. chidf = pd . DataFrame () target = 'Seconds' chidf [ target ] = df [ seconds ] . sum ( 1 ) moods = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind in feeds : try : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) grp = chidf . loc [ chidf [ ind ] == True ][ target ] . values chi2 , p , m , cTable = stats . median_test ( grp , pop , correction = False ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] < 1 moods = pd . concat ([ moods , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass moods . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] moods = moods . sort_values ( 'p-value' ) . reset_index ( drop = True ) moods . loc [ moods [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null 0 PP TOTAL 7089 284.071422 9.736103e-64 [[1093, 8620], [458, 9277]] False True 1 MFR 4 Grey Grey 161.505327 5.305934e-37 [[744, 9215], [1288, 8682]] True True 2 PP POLYCHIM HB12XF 140.079653 2.557382e-32 [[897, 8734], [475, 9163]] False True 3 High MFR Grey 134.851343 3.558380e-31 [[1244, 8697], [753, 9200]] False True 4 SILIKAT PBH ABPP 05 132.735135 1.033157e-30 [[1205, 8704], [725, 9193]] False True 5 SILIKAT POLYPLUS AB 1001 PP 40.522176 1.943972e-10 [[341, 8870], [196, 9027]] False True 6 DEVAN 2SPIN PP 106 39.379816 3.488847e-10 [[2499, 8758], [2119, 9139]] False True 7 BLACK SCHULMAN P7381 33.697655 6.437826e-09 [[6961, 8686], [6465, 9211]] False True 8 GREEN POLYPLAST COLOUR MB PP 6821 32.518968 1.180348e-08 [[589, 8854], [414, 9043]] False True 9 PP INEOS 100 GA09 26.828649 2.223170e-07 [[630, 8854], [465, 9043]] False True 10 ADD ARGUS ARGUTHERM AO 77 PP 26.802325 2.253660e-07 [[1250, 8823], [1020, 9074]] False True 11 UV ARGUS ARX V 16 706 UV PP 23.787198 1.075957e-06 [[5152, 8744], [4773, 9153]] False True 12 High MFR Black 18.389746 1.800242e-05 [[1626, 8835], [1409, 9062]] False True 13 ADD POLYPLAST PPM AO 01 UV PP 15.536764 8.091620e-05 [[499, 9000], [629, 8897]] True True 14 UV SCHULMAN FPPUV 38 12.417442 4.253424e-04 [[330, 8983], [426, 8914]] True True 15 MFR 4 White 11.470090 7.072526e-04 [[138, 8971], [200, 8926]] True True 16 GREEN SCHULMAN PZ 302446 1 5 T 9.924320 1.631091e-03 [[331, 8983], [416, 8914]] True True we see that overall the moods test is slightly less pessimistic chisum [ 'reject null' ] . value_counts () True 16 False 16 Name: reject null, dtype: int64 moods [ 'reject null' ] . value_counts () True 17 False 15 Name: reject null, dtype: int64 testsf = tests . join ( moods . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], lsuffix = ', chi' , rsuffix = ', moods' ) testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True 2.223170e-07 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True 2.253660e-07 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True 1.800242e-05 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True 1.180348e-08 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True 8.091620e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True 1.943972e-10 True Non-Parametric Conclusions \u00b6 problem_vars = testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] . index print ( f \"Compared with correlation \\n the dependency tests show that the top 5 problem variables are: \\n { [ f ' { i } ' for i in problem_vars ] } \" ) Compared with correlation the dependency tests show that the top 5 problem variables are: ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True Parametric \u00b6 Univariate Quantitative to Quantitative (Linear Regression) \u00b6 for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () plt . plot ( x , ( y - y_pred ), ls = '' , marker = '.' ) plt . ylabel ( 'Residual' ) plt . xlabel ( 'x' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( f \"test statistic: { t } \" ) print ( f \"p-value: { stats . t . sf ( x = t , df = n - 2 ) } \" ) R2: 3.34e-02 RSE: 9.97e-03 b: -2.51e-03 m: 3.61e-01 n: 338 x_bar: 1.11e-01 SE_b: 1.51e-03 SE_m: 1.26e-02 test statistic: 28.56535956265302 p-value: 3.2661498371310494e-92 R2: 4.31e-02 RSE: 5.27e-03 b: -3.19e-03 m: 9.08e-02 n: 1551 x_bar: 2.89e-01 SE_b: 3.09e-04 SE_m: 9.66e-04 test statistic: 94.02166426512674 p-value: 0.0 R2: 1.89e-02 RSE: 5.94e-03 b: -1.36e-02 m: 1.57e+01 n: 537 x_bar: 2.64e-03 SE_b: 1.04e-03 SE_m: 3.81e-01 test statistic: 41.11483903422569 p-value: 4.981863349397906e-168 for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.033 Model: OLS Adj. R-squared: 0.030 Method: Least Squares F-statistic: 11.60 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.000739 Time: 10:02:50 Log-Likelihood: 360.28 No. Observations: 338 AIC: -716.6 Df Residuals: 336 BIC: -708.9 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0025 0.013 -0.199 0.843 -0.027 0.022 x1 0.3612 0.106 3.406 0.001 0.153 0.570 ============================================================================== Omnibus: 457.321 Durbin-Watson: 1.128 Prob(Omnibus): 0.000 Jarque-Bera (JB): 55378.392 Skew: 6.475 Prob(JB): 0.00 Kurtosis: 64.356 Cond. No. 23.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.043 Model: OLS Adj. R-squared: 0.042 Method: Least Squares F-statistic: 69.71 Date: Sat, 02 Apr 2022 Prob (F-statistic): 1.50e-16 Time: 10:02:50 Log-Likelihood: 2180.0 No. Observations: 1551 AIC: -4356. Df Residuals: 1549 BIC: -4345. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0032 0.003 -0.917 0.359 -0.010 0.004 x1 0.0908 0.011 8.349 0.000 0.069 0.112 ============================================================================== Omnibus: 2262.527 Durbin-Watson: 1.176 Prob(Omnibus): 0.000 Jarque-Bera (JB): 660230.502 Skew: 8.562 Prob(JB): 0.00 Kurtosis: 102.615 Cond. No. 7.82 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.019 Model: OLS Adj. R-squared: 0.017 Method: Least Squares F-statistic: 10.29 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.00142 Time: 10:02:50 Log-Likelihood: 622.03 No. Observations: 537 AIC: -1240. Df Residuals: 535 BIC: -1231. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0136 0.013 -1.020 0.308 -0.040 0.013 x1 15.6795 4.888 3.208 0.001 6.078 25.281 ============================================================================== Omnibus: 739.911 Durbin-Watson: 1.268 Prob(Omnibus): 0.000 Jarque-Bera (JB): 108127.158 Skew: 7.213 Prob(JB): 0.00 Kurtosis: 71.003 Cond. No. 1.49e+03 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.49e+03. This might indicate that there are strong multicollinearity or other numerical problems. Feature Engineering \u00b6 Introducing polynomial features results in poor coefficient estimates ind = ind_vars [ 1 ] lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) features = PolynomialFeatures ( degree = 3 ) x = features . fit_transform ( x . reshape ( - 1 , 1 )) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.056 Model: OLS Adj. R-squared: 0.055 Method: Least Squares F-statistic: 30.83 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.35e-19 Time: 10:03:01 Log-Likelihood: 2190.9 No. Observations: 1551 AIC: -4374. Df Residuals: 1547 BIC: -4352. Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0030 0.009 0.334 0.739 -0.014 0.020 x1 0.1612 0.124 1.301 0.193 -0.082 0.404 x2 -0.9203 0.523 -1.761 0.078 -1.945 0.105 x3 1.6922 0.664 2.550 0.011 0.390 2.994 ============================================================================== Omnibus: 2252.329 Durbin-Watson: 1.188 Prob(Omnibus): 0.000 Jarque-Bera (JB): 651735.784 Skew: 8.487 Prob(JB): 0.00 Kurtosis: 101.979 Cond. No. 594. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. R2: 5.64e-02 RSE: 6.03e-03 b: 2.95e-03 m: 0.00e+00 n: 1551 x_bar: 3.58e-01 SE_b: 1.69e-04 SE_m: 1.96e-04 0.0 0.5 Multivariate Quantitative to Quantitative (Multivariate Linear Regression) \u00b6 lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) lindf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ADD ARGUS ARGUSTAT AT 31 HK ADD ARGUS ARGUTHERM AO 77 PP ADD POLYPLAST PPM AO 01 UV PP ADD SCHULMAN DTY 20 AOUV BEIGE SCHULMAN PZ 91738 1 5 ZA BLACK SCHULMAN P7381 DEVAN 2SPIN PP 106 GREEN POLYPLAST COLOUR MB PP 6821 GREEN SCHULMAN PZ 302446 1 5 T High MFR Black ... UV POLYPLAST 6005 PP UV SCHULMAN FPPUV 38 MFR 4 Grey PP POLYCHIM HL10XF MFR 4 Grey Grey PP POLYMER TEST MFI4 PP TOTAL 4069 RR MASTERBATCH TEST UV ARGUS ARX V 16 706 UV PP seconds 0 0.0 0.0 0.0 0.005337 0.0 0.000253 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.409353 0.0 0.0 0.0 0.0 0.0 0.000000 9.0 1 0.0 0.0 0.0 0.005867 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.452007 0.0 0.0 0.0 0.0 0.0 0.000000 536.0 2 0.0 0.0 0.0 0.005895 0.0 0.000270 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.449608 0.0 0.0 0.0 0.0 0.0 0.000000 20.0 3 0.0 0.0 0.0 0.005743 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.440883 0.0 0.0 0.0 0.0 0.0 0.000000 1001.0 4 0.0 0.0 0.0 0.004553 0.0 0.000211 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.349459 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.339726 0.0 0.0 0.0 0.0 0.0 0.002062 0.0 17893 0.0 0.0 0.0 0.000000 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.342659 0.0 0.0 0.0 0.0 0.0 0.002071 0.0 17894 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345886 0.0 0.0 0.0 0.0 0.0 0.002107 0.0 17895 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.344945 0.0 0.0 0.0 0.0 0.0 0.002095 799.0 17896 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345288 0.0 0.0 0.0 0.0 0.0 0.002105 429.0 17897 rows \u00d7 45 columns lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) x = lindf [ feeds ] . values # features = PolynomialFeatures(degree=3) # x = features.fit_transform(x.reshape(-1,1)) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.050 Model: OLS Adj. R-squared: 0.049 Method: Least Squares F-statistic: 31.47 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.76e-174 Time: 10:03:19 Log-Likelihood: 44791. No. Observations: 17897 AIC: -8.952e+04 Df Residuals: 17866 BIC: -8.928e+04 Df Model: 30 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0123 0.002 5.316 0.000 0.008 0.017 x1 -1.0553 0.382 -2.760 0.006 -1.805 -0.306 x2 -0.0119 0.010 -1.241 0.215 -0.031 0.007 x3 0.0351 0.100 0.352 0.725 -0.161 0.231 x4 -0.2377 0.069 -3.438 0.001 -0.373 -0.102 x5 0.4001 0.145 2.750 0.006 0.115 0.685 x6 -0.0224 0.128 -0.175 0.861 -0.272 0.228 x7 -0.3174 0.096 -3.305 0.001 -0.506 -0.129 x8 -0.5909 0.209 -2.825 0.005 -1.001 -0.181 x9 -0.4059 0.087 -4.655 0.000 -0.577 -0.235 x10 -0.0241 0.005 -5.206 0.000 -0.033 -0.015 x11 -2.473e-14 1.17e-14 -2.107 0.035 -4.77e-14 -1.73e-15 x12 -0.0050 0.004 -1.371 0.171 -0.012 0.002 x13 -0.0288 0.007 -4.319 0.000 -0.042 -0.016 x14 -0.0268 0.005 -5.644 0.000 -0.036 -0.018 x15 -0.0200 0.006 -3.620 0.000 -0.031 -0.009 x16 -1.023e-14 3.5e-15 -2.923 0.003 -1.71e-14 -3.37e-15 x17 -9.109e-16 2.5e-16 -3.649 0.000 -1.4e-15 -4.22e-16 x18 -0.0994 0.093 -1.073 0.283 -0.281 0.082 x19 -0.0055 0.003 -2.044 0.041 -0.011 -0.000 x20 0.0003 0.005 0.058 0.954 -0.009 0.010 x21 1.325e-15 1.2e-16 11.078 0.000 1.09e-15 1.56e-15 x22 -2.494e-16 8.68e-17 -2.873 0.004 -4.2e-16 -7.93e-17 x23 0.0145 0.004 3.827 0.000 0.007 0.022 x24 -0.0100 0.010 -1.051 0.293 -0.029 0.009 x25 0.0216 0.004 5.724 0.000 0.014 0.029 x26 -0.0101 0.002 -4.198 0.000 -0.015 -0.005 x27 -1.03e-16 8.73e-17 -1.180 0.238 -2.74e-16 6.8e-17 x28 9.329e-16 7.53e-17 12.396 0.000 7.85e-16 1.08e-15 x29 -0.0074 0.003 -2.881 0.004 -0.012 -0.002 x30 1.7091 0.132 12.994 0.000 1.451 1.967 x31 4.1538 0.376 11.053 0.000 3.417 4.890 x32 0 0 nan nan 0 0 x33 0.0031 0.012 0.263 0.792 -0.020 0.026 x34 0.0031 0.012 0.263 0.792 -0.020 0.026 x35 0.0031 0.012 0.263 0.792 -0.020 0.026 x36 0.0790 0.042 1.868 0.062 -0.004 0.162 x37 0.0525 0.030 1.774 0.076 -0.006 0.111 x38 -0.0064 0.003 -2.521 0.012 -0.011 -0.001 x39 0 0 nan nan 0 0 x40 -0.0136 0.005 -2.762 0.006 -0.023 -0.004 x41 0 0 nan nan 0 0 x42 0 0 nan nan 0 0 x43 0 0 nan nan 0 0 x44 0.1990 0.048 4.119 0.000 0.104 0.294 ============================================================================== Omnibus: 43479.020 Durbin-Watson: 1.074 Prob(Omnibus): 0.000 Jarque-Bera (JB): 693938974.691 Skew: 25.434 Prob(JB): 0.00 Kurtosis: 966.322 Cond. No. 1.11e+16 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 1.79e-28. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. R2: 5.02e-02 RSE: 1.67e-03 b: 1.23e-02 m: -1.06e+00 n: 17897 x_bar: 2.26e-02 SE_b: 1.25e-05 SE_m: 2.14e-05 -49284.75476441197 1.0 Forward Selection \u00b6 def add_feature ( features , basemodel , data , y , r2max ): for feed in features : basemodel [ feed ] = data [ feed ] basemodel . fillna ( 0 , inplace = True ) x = basemodel [[ col for col in basemodel . columns if col != 'seconds' ]] . values ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () if ( est2 . rsquared > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = est2 . rsquared feedmax = feed bestsum = est2 . summary () newmodel = basemodel . copy () else : pass basemodel . drop ( labels = feed , axis = 1 , inplace = True ) return r2max , feedmax , bestsum , newmodel lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) r2max = 0 candidates = feeds . copy () basemodel = lindf while True : newr2max , feedmax , bestsum , newmodel = add_feature ( features = candidates , basemodel = basemodel , data = df , y = y , r2max = 0 ) if newr2max > r2max : # print(newr2max, feedmax) candidates . remove ( feedmax ) r2max = newr2max basemodel = newmodel continue else : break /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) we see some of the usual suspects from the chi-square tests. Notably some are missing: High MFR Grey , BLACK SCHULMAN P7381 , PP INEOS 100 GA09 and others. basemodel . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } seconds PP TOTAL 7089 PP POLYCHIM HB12XF SILIKAT PBH ABPP 05 SILIKAT POLYPLUS AB 1001 PP MFR 4 Green ADD SCHULMAN DTY 20 AOUV GREEN SCHULMAN PZ 302446 1 5 T MFR 4 Grey UV ARGUS ARX V 16 706 UV PP GREEN SCHULMAN PZ 34198 1 5 T 0 9.0 0.0 0.0 0.0 0.0 0.0 0.005337 0.0 0.409353 0.0 0.0 1 536.0 0.0 0.0 0.0 0.0 0.0 0.005867 0.0 0.452007 0.0 0.0 2 20.0 0.0 0.0 0.0 0.0 0.0 0.005895 0.0 0.449608 0.0 0.0 3 1001.0 0.0 0.0 0.0 0.0 0.0 0.005743 0.0 0.440883 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.004553 0.0 0.349459 0.0 0.0 forward_selection_feeds = basemodel . columns [ 1 :] bestsum OLS Regression Results Dep. Variable: y R-squared: 0.046 Model: OLS Adj. R-squared: 0.045 Method: Least Squares F-statistic: 94.99 Date: Sat, 02 Apr 2022 Prob (F-statistic): 5.51e-174 Time: 10:04:41 Log-Likelihood: 44748. No. Observations: 17897 AIC: -8.948e+04 Df Residuals: 17887 BIC: -8.940e+04 Df Model: 9 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 0.0034 0.000 11.288 0.000 0.003 0.004 x1 0.0214 0.002 11.535 0.000 0.018 0.025 x2 0.0146 0.002 7.837 0.000 0.011 0.018 x3 1.5742 0.110 14.375 0.000 1.360 1.789 x4 4.1700 0.363 11.497 0.000 3.459 4.881 x5 -0.0181 0.003 -6.276 0.000 -0.024 -0.012 x6 -0.2879 0.065 -4.450 0.000 -0.415 -0.161 x7 -0.1932 0.052 -3.698 0.000 -0.296 -0.091 x8 0.0037 0.001 3.322 0.001 0.002 0.006 x9 0.0736 0.028 2.641 0.008 0.019 0.128 x10 0 0 nan nan 0 0 x11 0 0 nan nan 0 0 Omnibus: 43415.763 Durbin-Watson: 1.068 Prob(Omnibus): 0.000 Jarque-Bera (JB): 683838684.139 Skew: 25.346 Prob(JB): 0.00 Kurtosis: 959.275 Cond. No. inf Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 0. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular. Multivariate Conclusions \u00b6 y poses a problem with out of the box regression. Extreme tails will upset the residual plots. We some overlap of the parametric and non-parametric approaches: [ i for i in basemodel . columns if i in ( problem_vars )] ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05'] Multivariate Quantitative to Categorical (Binned Output Variable) \u00b6 Logistic regression does little better than random guessing lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) x = lindf [ ind ] . values #.reshape(-1,1) model = LogisticRegression () model . fit ( x , y ) y_pred = model . predict ( x ) print ( f \"Train Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) Train Acc: 0.37 Test Acc: 0.37 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( print ( classification_report ( y , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.35 0.80 0.49 4527 2.0 0.36 0.32 0.34 4434 3.0 0.30 0.03 0.06 4463 4.0 0.43 0.32 0.37 4473 accuracy 0.37 17897 macro avg 0.36 0.37 0.31 17897 weighted avg 0.36 0.37 0.31 17897 <AxesSubplot:> If we are careful about regularizing the random forest model, it does a little bit better lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) X = lindf [ ind ] . values #.reshape(-1,1) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 0.99 Test Acc: 0.39 y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.41 0.48 0.44 898 2.0 0.33 0.30 0.31 881 3.0 0.29 0.27 0.28 893 4.0 0.50 0.51 0.51 908 accuracy 0.39 3580 macro avg 0.38 0.39 0.38 3580 weighted avg 0.38 0.39 0.38 3580 <AxesSubplot:> # grab feature importances imp = model . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in model . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ ind , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 PP TOTAL PPH 4065 0.150387 0.008053 1 BLACK SCHULMAN P7381 0.118158 0.008725 2 PP INEOS 100 GA04 0.100635 0.005217 3 UV ARGUS ARX V 16 706 UV PP 0.090776 0.006273 4 MFR 4 Grey 0.08828 0.005656 5 PP UNIPETROL GB005 0.060558 0.003569 6 MFR 4 Black 0.052589 0.003695 7 ADD SCHULMAN DTY 20 AOUV 0.041478 0.003554 8 DEVAN 2SPIN PP 106 0.038438 0.004061 9 High MFR Black 0.028838 0.002515 10 MFR 4 Grey Grey 0.025758 0.002319 11 ADD ARGUS ARGUTHERM AO 77 PP 0.024526 0.002253 12 High MFR Grey 0.020652 0.004188 13 MFR 4 Green 0.016508 0.002725 14 SILIKAT PBH ABPP 05 0.016283 0.002291 15 PP TOTAL 7089 0.01624 0.004897 16 ADD POLYPLAST PPM AO 01 UV PP 0.013499 0.001685 17 MFR 4 Black Brown 0.013401 0.002131 18 PP POLYCHIM HB12XF 0.01245 0.002185 19 PP INEOS 100 GA09 0.010354 0.00156 6.0 Business Impact \u00b6 What is the so what? From Hyp tests - target is either 'Total Position Out' or 'Total Seconds Out' We had culprits from pearsons: ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] culprits from non parametric tests: list ( problem_vars ) ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] and culprits from linear regression forward_selection_feeds Index(['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'MFR 4 Green', 'ADD SCHULMAN DTY 20 AOUV', 'GREEN SCHULMAN PZ 302446 1 5 T', 'MFR 4 Grey', 'UV ARGUS ARX V 16 706 UV PP', 'GREEN SCHULMAN PZ 34198 1 5 T'], dtype='object') and we can check the overlap between non-parametric and linear regression (parametric): predict_vars = [ i for i in basemodel . columns if i in ( problem_vars )] predict_vars ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05'] all_vars = set ( list ( problem_vars ) + list ( forward_selection_feeds )) all_vars {'ADD SCHULMAN DTY 20 AOUV', 'BLACK SCHULMAN P7381', 'GREEN SCHULMAN PZ 302446 1 5 T', 'GREEN SCHULMAN PZ 34198 1 5 T', 'High MFR Grey', 'MFR 4 Green', 'MFR 4 Grey', 'PP POLYCHIM HB12XF', 'PP TOTAL 7089', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'UV ARGUS ARX V 16 706 UV PP'} # add to DF business = pd . DataFrame () value_line_by_day = 99000 for ind in problem_vars : try : # products, prods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique () nprods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . nunique () dff = df . loc [ df [ 'Product' ] . isin ( prods )] # total runtime, tot_runtime = dff [ 'Time Delta' ] . sum () # total runtime with feed, tot_runtime_wfeed = dff [ dff [ ind ] > 0 ][ 'Time Delta' ] . sum () #downtime w/o feed, downtime w/ feed, avg_downtime_min = dff . groupby ( dff [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 avg_downtime_min_wfeed = avg_downtime_min [ True ] avg_downtime_min_w_ofeed = avg_downtime_min [ False ] ratio = avg_downtime_min_wfeed / avg_downtime_min_w_ofeed # average runtime, avg_runtime_min = dff [ 'Time Delta' ] . mean () . total_seconds () / 60 # days saved, dollars saved days_saved = ( tot_runtime_wfeed . total_seconds () / 60 / avg_runtime_min * avg_downtime_min . diff () . values [ - 1 ]) / 60 / 24 dollars_saved_per_batch = value_line_by_day * avg_downtime_min . diff () . values [ - 1 ] / ( 60 * 24 ) business = pd . concat (( business , pd . DataFrame ([[ ind , nprods , tot_runtime , tot_runtime_wfeed , avg_downtime_min_wfeed , avg_downtime_min_w_ofeed , ratio , avg_runtime_min , days_saved , days_saved * value_line_by_day , dollars_saved_per_batch ]], columns = [ 'Feed' , 'Products' , 'Total Runtime, Products' , 'Total Runtime With Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Average Runtime' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]))) except : print ( ind ) print ( dff . groupby ( df [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 ) business = business . sort_values ( 'Dollars Saved (Per Batch)' , ascending = False ) . reset_index ( drop = True ) business # workshop downtime ratio # feed products, feed products # Dollars saved per batch .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Products Total Runtime, Products Total Runtime With Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Average Runtime Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26 409 days 20:43:21 94 days 04:36:05 26.250639 13.671063 1.920161 70.835736 16.727319 1.656005e+06 864.845823 1 High MFR Grey 36 564 days 04:13:41 94 days 12:26:59 23.623539 12.809036 1.844287 65.459164 15.615433 1.545928e+06 743.497080 2 PP POLYCHIM HB12XF 17 383 days 00:03:51 67 days 09:13:53 21.043525 15.600015 1.348943 70.908183 5.173013 5.121283e+05 374.241303 3 BLACK SCHULMAN P7381 64 751 days 20:01:57 590 days 12:21:50 14.902871 12.363943 1.205349 63.216276 23.716605 2.347944e+06 174.551301 4 PP TOTAL 7089 13 291 days 22:06:22 79 days 15:56:47 20.651050 18.269305 1.130369 80.146114 2.367430 2.343756e+05 163.744933 pd . set_option ( 'display.precision' , 2 ) display ( business [[ 'Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]], ) pd . set_option ( 'display.precision' , 6 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26.25 13.67 1.92 16.73 1.66e+06 864.85 1 High MFR Grey 23.62 12.81 1.84 15.62 1.55e+06 743.50 2 PP POLYCHIM HB12XF 21.04 15.60 1.35 5.17 5.12e+05 374.24 3 BLACK SCHULMAN P7381 14.90 12.36 1.21 23.72 2.35e+06 174.55 4 PP TOTAL 7089 20.65 18.27 1.13 2.37 2.34e+05 163.74 We see that 'SILKAT PBH ABPP 05' as well as 'High MFR Grey' have a large amount of days saved over the two years, a strong downtime multiplier effect and the largest dollars saved per batch. Since High MFR Grey is a recycle feed, we may not have the ability to remove it from the feed list. Leaving the SILKAT (Silicate) feed the prime feedstock to remove 7.0 Visualizations \u00b6 PP TOTAL 7089 \u00b6 predict_var = predict_vars [ 1 ] outcome_var = [ i for i in df . columns if 'Seconds' in i ] def myplot ( col = list ( df . loc [ df [ predict_var ] > 0 ][ 'Product' ] . unique ())): fig , ax = plt . subplots ( figsize = ( 20 , 10 )) axt = ax . twinx () # other feeds df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True ) . dropna ( axis = 1 , how = 'all' ) . plot ( ls = '' , marker = '*' , ax = ax , alpha = 0.2 ) # predict_var feed df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True )[ predict_var ] . plot ( ls = '--' , marker = '+' , ax = ax , lw = 2 , c = 'red' ) # outcome_var df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ) . plot ( ls = '-' , marker = '+' , lw = 1 , ax = axt , alpha = 1 , c = 'blue' , label = 'Total Seconds Out' ) # outliers ax . set_ylim ( 0 ) axt . set_ylim ( 0 , np . quantile ( df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ), 0.997 )) h1 , l1 = ax . get_legend_handles_labels () h2 , l2 = axt . get_legend_handles_labels () ax . legend ( h1 + h2 , l1 + l2 , loc = 0 ) interact ( myplot ) interactive(children=(Dropdown(description='col', options=(0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.\u2026 <function __main__.myplot(col=[0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.0, 136215.0, 125215.0, 110215.0, 100215.0, 80215.0, 100617.0, 90617.0, 90215.0, 90216.0, 110520.0, 80617.0])>","title":"X4 Candy Ribbons Make Data"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#causality-analysis","text":"","title":"Causality Analysis"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#todo","text":"ols assumptions for univariate analyses residual plots forward selection lines up more or less with chi square tests include some colinearity measure? logistic regression (binned seconds, binned total outs) random forest does little better than random guessing business impact reorganize hypothesis tests section univariate vs multivariate cat v vat, cat v quant, quant v quant, quant v cat","title":"TODO"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#10-imports","text":"","title":"1.0 Imports"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#import-libraries","text":"# basic packages import pandas as pd import numpy as np import random import copy # visualization packages from ipywidgets import interact import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures , MinMaxScaler from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , silhouette_score , calinski_harabasz_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score def make_patch_spines_invisible ( ax ): ax . set_frame_on ( True ) ax . patch . set_visible ( False ) for sp in ax . spines . values (): sp . set_visible ( False ) from matplotlib import colors from itertools import cycle def my_colors (): tab = cycle ( colors . TABLEAU_COLORS ) return tab","title":"Import Libraries"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#load-data","text":"consider only feeds consider feeds and operating conditions feeds = [ 'ADD ARGUS ARGUSTAT AT 31 HK' , 'ADD ARGUS ARGUTHERM AO 77 PP' , 'ADD POLYPLAST PPM AO 01 UV PP' , 'ADD SCHULMAN DTY 20 AOUV' , 'BEIGE SCHULMAN PZ 91738 1 5 ZA' , 'BLACK SCHULMAN P7381' , 'DEVAN 2SPIN PP 106' , 'GREEN POLYPLAST COLOUR MB PP 6821' , 'GREEN SCHULMAN PZ 302446 1 5 T' , 'High MFR Black' , 'GREEN SCHULMAN PZ 34198 1 5 T' , 'MFR 4 Black' , 'MFR 4 Black Brown' , 'High MFR Grey' , 'MFR 4 Green' , 'PP BOREALIS HE465FB' , 'MFR 4 Black Colour Blending' , 'PP INEOS 100 GA02' , 'PP INEOS 100 GA04' , 'PP INEOS 100 GA09' , 'MFR 4 Grey Colour Blending' , 'PP INEOS GA012' , 'PP POLYCHIM HB12XF' , 'MFR 4 White' , 'PP TOTAL 7089' , 'PP TOTAL PPH 4065' , 'MFR 4 White Colour Blending' , 'PP BOREALIS HE370FB' , 'PP UNIPETROL GB005' , 'SILIKAT PBH ABPP 05' , 'SILIKAT POLYPLUS AB 1001 PP' , 'UV ARGUS ARGUVIN UV 361 PP' , 'UV ARGUS ARX 904 11 PP' , 'UV ARGUS ARX 904 11 PP.1' , 'UV ARGUS ARX 904 11 PP.2' , 'UV POLYPLAST 6005 PP' , 'UV SCHULMAN FPPUV 38' , 'MFR 4 Grey' , 'PP POLYCHIM HL10XF' , 'MFR 4 Grey Grey' , 'PP POLYMER TEST MFI4' , 'PP TOTAL 4069' , 'RR MASTERBATCH TEST' , 'UV ARGUS ARX V 16 706 UV PP' ] df = pd . read_csv ( '../../../../../Dropbox/work/mfganalytic/data/dupont/TyparMasterReportWithSampleDataImprovement2017-today.csv' , header = 1 ) df = df . loc [ df [ 'Product' ] . notnull ()] df [ feeds ] = df [ feeds ] . div ( df [ feeds ] . sum ( axis = 1 ), axis = 0 ) #normalize feed rates df [ 'Time Delta' ] = pd . to_datetime ( df [ 'Doff Time' ]) . diff () df [ 'Total Seconds Out' ] = df [[ i for i in df . columns if 'Sec' in i ]] . sum ( 1 ) # df.dropna(inplace=True) hangs = [ df . columns [ i ] for i , j in enumerate ( 'Hang' in col for col in df . columns ) if j ] speeds = [ df . columns [ i ] for i , j in enumerate ( 'speed' in col for col in df . columns ) if j ] outs = [ df . columns [ i ] for i , j in enumerate ( 'Out' in col for col in df . columns ) if j ] prods = df [ 'Product' ] . unique () counts = [ i for i in df . columns if 'Count' in i ] #from LOEWEN virgin_polymer = [ 'PP INEOS 100 GA04' , 'PP POLYCHIM HB12XF' , 'PP TOTAL 7089' , #INEOS 100 GA09 until 2018 (2k rolls) 'PP TOTAL PPH 4065' , 'PP UNIPETROL GB005' , 'PP POLYCHIM HL10XF' ] #HL10XF not used after 2016, #BOREALIS virgin_polymer = [] for col in df . columns : if ( 'PP INEOS' in col ) or ( 'PP TOTAL' in col ) or ( 'PP UNI' in col ) or ( 'PP BORE' in col ) or ( 'PP POLY' in col ): virgin_polymer . append ( col ) pressures = [] for col in df . columns : if 'Press' in col or 'PRESS' in col : pressures . append ( col ) silicates = [] for col in df . columns : if 'SIL' in col : silicates . append ( col ) uvs = [] for col in df . columns : if 'UV ' in col : uvs . append ( col ) recycle = [] for col in df . columns : if 'MFR' in col : recycle . append ( col ) seconds = [] for col in df . columns : if 'Sec' in col : seconds . append ( col ) master_batch = list ( set ( feeds ) - set ( recycle ) - set ( virgin_polymer )) operation = list ( set ( df . columns ) - set ( master_batch ) - set ( virgin_polymer ) - set ( recycle ) - set ( hangs ) - set ( outs )) operation . sort () A summary of the columns print ( f 'pressures: { len ( pressures ) } ' ) print ( f 'silicates: { len ( silicates ) } ' ) print ( f 'uvs: { len ( uvs ) } ' ) print ( f 'feeds: { len ( feeds ) } ' ) print ( f 'master_batch: { len ( master_batch ) } ' ) print ( f 'virgin_polymer: { len ( virgin_polymer ) } ' ) print ( f 'recycle: { len ( recycle ) } ' ) print ( f 'operation: { len ( operation ) } ' ) print () print ( f 'hangs: { len ( hangs ) } ' ) print ( f 'speeds: { len ( speeds ) } ' ) print ( f 'outs: { len ( outs ) } ' ) print ( f 'seconds: { len ( seconds ) } ' ) print () print ( 'summary target columns: TotalHangs, Total Position Out' ) print () print ( df . shape ) pressures: 8 silicates: 2 uvs: 8 feeds: 44 master_batch: 20 virgin_polymer: 13 recycle: 11 operation: 180 hangs: 9 speeds: 9 outs: 10 seconds: 9 summary target columns: TotalHangs, Total Position Out (17897, 243) import datetime df . columns Index(['Millroll ID', 'Product', 'Roll Width', 'Sample Number', 'Doff Time', 'Block1Pos Out Seconds', 'Block2Pos Out Seconds', 'Block3Pos Out Seconds', 'Block4Pos Out Seconds', 'Block1Pos Out Count', ... 'TKW5 AVG - Tack - Tear West 5 Meter', 'TPOP - NUMBER OF TPO PEEKS (TPOP)', 'TTMD AVG - TRAP TEAR MD (TTMD)', 'TTXD AVG - TRAP TEAR XD (TTXD)', 'UPEX - UWU PLOT EXTREM (UPEX)', 'UWG - UWU GRAVIMETRIC WITH SCALE (UWG)', 'UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP)', 'WATP - WATER PERMEABILITY (WATP)', 'Time Delta', 'Total Seconds Out'], dtype='object', length=243) df [ 'Product' ] . name 'Product' df [( pd . to_datetime ( df [ 'Doff Time' ]) < datetime . datetime ( 2018 , 1 , 1 )) & ( pd . to_datetime ( df [ 'Doff Time' ]) >= datetime . datetime ( 2017 , 12 , 1 ))] . groupby ( 'Product' )[ 'Product' , 'Roll Width' ] . apply ( lambda x : x . count () if ( x . name == 'Product' ) else ( x . sum ())) /tmp/ipykernel_301/568605750.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead. df[(pd.to_datetime(df['Doff Time']) < datetime.datetime(2018,1,1)) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Product Roll Width Product 0.0 0.0 20800.0 45235.0 1266580.0 147000.0 55235.0 110470.0 10400.0 55626.0 11458956.0 986560.0 62235.0 1244700.0 95500.0 68235.0 614115.0 37800.0 68236.0 341180.0 22800.0 90234.0 721872.0 42000.0 90617.0 2084191.0 119600.0 100015.0 800120.0 33760.0 100175.0 1702975.0 88400.0 100617.0 1408638.0 72800.0 103115.0 412460.0 20600.0 110617.0 12057253.0 566800.0 130010.0 1820140.0 59080.0 130030.0 520120.0 16600.0 135010.0 2025150.0 63300.0 136215.0 4767525.0 157500.0 136448.0 1500928.0 57200.0 150010.0 450030.0 12660.0 160030.0 800150.0 22100.0 190226.0 8940622.0 244400.0 240448.0 12503296.0 265200.0 260607.0 1563642.0 31200.0 294120.0 2647080.0 45000.0 350448.0 11915232.0 173400.0 # uvs will be color # recycle will be recycle # silicates will be sprinkles # virgin_polymer will be taffy_base colors = [ 'red' , 'green' , 'white' , 'blue' , 'magenta' , 'yellow' , 'cyan' , 'purple' ] taffr = [ 'r213' , 'r345' , 'r093' , 'r103' , 'r194' , 'r093' , 'r298' , 'r124' , 'r000' , 'r889' , 'r299' ] sprinkles = [ 'sprinkle1' , 'sprinkle2' ] taffy_base = [ 'taffy1' , 'taffy2' , 'taffy3' , 'taffy4' , 'taffy5' , 'taffy6' , 'taffy7' , 'taffy8' , 'taffy9' , 'taffy10' , 'taffy11' , 'taffy12' , 'taffy13' ] all_feats = df . select_dtypes ( float ) . columns [ df . select_dtypes ( float ) . columns . isin ( feeds + operation )] . values hangs2 = [ i . lower () for i in [ 'Block 1 Rips Seconds' , 'Block 2 Rips Seconds' , 'Block 3 Rips Seconds' , 'Block 4 Rips Seconds' , 'Block 1 Rips Count' , 'Block 2 Rips Count' , 'Block 3 Rips Count' , 'Block 4 Rips Count' , 'Total Rips' ]] outs2 = [ i . lower () for i in [ 'Block 1 Bumps Seconds' , 'Block 2 Bumps Seconds' , 'Block 3 Bumps Seconds' , 'Block 4 Bumps Seconds' , 'Block 1 Bumps Count' , 'Block 2 Bumps Count' , 'Block 3 Bumps Count' , 'Block 4 Bumps Count' , 'Total Bumps' , 'Total Seconds Out' ]] taffy = pd . DataFrame () taffy [ 'product' ] = df [ 'Product' ] taffy [ 'date' ] = df [ 'Doff Time' ] taffy [ hangs2 ] = df [ hangs ] taffy [ outs2 ] = df [ outs ] taffy [ colors ] = df [ uvs ] taffy [ taffr ] = df [ recycle ] taffy [ sprinkles ] = df [ silicates ] taffy [ taffy_base ] = df [ virgin_polymer ] taffy .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } product date block 1 rips seconds block 2 rips seconds block 3 rips seconds block 4 rips seconds block 1 rips count block 2 rips count block 3 rips count block 4 rips count ... taffy4 taffy5 taffy6 taffy7 taffy8 taffy9 taffy10 taffy11 taffy12 taffy13 0 136215.0 1/1/2017 12:43:57 AM 9.0 NaN NaN NaN 1.0 NaN NaN NaN ... NaN NaN NaN NaN 0.357711 NaN NaN NaN NaN NaN 1 136215.0 1/1/2017 1:32:49 AM NaN 29.0 103.0 NaN NaN 2.0 1.0 NaN ... NaN NaN NaN NaN 0.291769 NaN NaN NaN NaN NaN 2 136215.0 1/1/2017 2:21:40 AM 11.0 NaN 9.0 NaN 1.0 NaN 1.0 NaN ... NaN NaN NaN NaN 0.293044 NaN NaN NaN NaN NaN 3 136215.0 1/1/2017 3:10:34 AM NaN 198.0 219.0 NaN NaN 4.0 2.0 NaN ... NaN NaN NaN NaN 0.308707 NaN NaN NaN NaN NaN 4 136215.0 1/1/2017 3:59:24 AM NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN 0.451992 NaN NaN NaN NaN NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 90215.0 2/25/2019 7:56:16 PM NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN 0.382452 NaN 0.254966 NaN NaN NaN 17893 90215.0 2/25/2019 8:53:41 PM NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN 0.380653 NaN 0.253770 NaN NaN NaN 17894 90215.0 2/25/2019 9:51:06 PM NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN 0.378461 NaN 0.252311 NaN NaN NaN 17895 90215.0 2/25/2019 10:48:32 PM NaN NaN 223.0 NaN NaN NaN 3.0 NaN ... NaN NaN NaN NaN 0.379047 NaN 0.252599 NaN NaN NaN 17896 90215.0 2/25/2019 11:45:55 PM NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN 0.378854 NaN 0.252431 NaN NaN NaN 17897 rows \u00d7 54 columns","title":"Load Data"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#20-understand-the-dependent-variable","text":"","title":"2.0 Understand The Dependent Variable"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#what-is-the-hit-rate-in-these-upset-columns","text":"Around 7500 cases with no problem; 10000 cases with a position out target = \"Total Position Out\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 10355 False 7542 Name: Total Position Out, dtype: int64 There are 30% higher incidence of hangs target = \"TotalHangs\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 13954 False 3943 Name: TotalHangs, dtype: int64 target1 = \"TotalHangs\" target2 = \"Total Position Out\" print ( f \"both upsets: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"only hangs: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) print ( f \"only outs: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"neither upset: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) both upsets: 10161 only hangs: 3793 only outs: 194 neither upset: 3749","title":"What is the hit rate in these upset columns?"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#what-is-the-magnitude-of-the-outages","text":"target = \"Total Position Out\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f0ad7207b80>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total Position Out Unnamed 0 Total Position Out 1.00000 0.62591 Unnamed 0 0.62591 1.00000 target = \"TotalHangs\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f0ad3eff460>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TotalHangs Unnamed 0 TotalHangs 1.000000 0.390949 Unnamed 0 0.390949 1.000000","title":"What is the magnitude of the outages?"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#distribution-around-the-target-variable-total-seconds","text":"We will certainly encounter trouble in an OLS estimate with this shape of the target variable; extreme skewness, upper tail, perhaps outliers tot_seconds = df [ seconds ] . sum ( 1 ) tot_seconds . kurt () 923.6728632470827 tot_seconds . skew () 25.09323820232841 tot_seconds . plot . kde () <AxesSubplot:ylabel='Density'> tot_seconds . describe () count 17897.000000 mean 2093.512879 std 8060.466090 min 0.000000 25% 20.000000 50% 666.000000 75% 1970.000000 max 396570.000000 dtype: float64","title":"Distribution around the target variable, total seconds"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#do-block-positions-have-the-same-behavior","text":"target = \"TotalHangs\" df . groupby ( target )[ hangs ] . count () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs TotalHangs 0.0 0 0 0 0 0 0 0 0 3943 1.0 510 682 745 977 510 682 745 977 2914 2.0 709 761 901 1109 709 761 901 1109 2424 3.0 721 776 930 1103 721 776 930 1103 1929 4.0 700 773 875 993 700 773 875 993 1577 5.0 609 668 744 828 609 668 745 828 1189 6.0 523 498 599 631 523 498 599 631 883 7.0 387 414 472 505 387 414 472 505 640 8.0 321 338 376 409 321 338 376 409 503 9.0 286 275 305 339 286 275 305 339 405 10.0 219 206 244 248 219 206 244 248 296 11.0 144 154 168 171 144 154 168 171 203 12.0 148 139 159 171 148 139 159 171 191 13.0 105 100 117 106 105 100 117 106 131 14.0 93 88 95 96 93 88 95 96 109 15.0 97 89 94 97 97 89 94 97 108 16.0 70 64 72 76 70 64 72 76 81 17.0 51 50 51 47 51 50 51 47 55 18.0 29 30 32 33 29 30 32 33 35 19.0 32 32 33 32 32 32 33 32 35 20.0 32 32 31 34 32 32 31 34 35 21.0 26 27 25 24 26 27 25 24 28 22.0 21 20 24 24 21 20 24 24 24 23.0 23 23 24 23 23 23 24 23 25 24.0 16 17 18 19 16 17 18 19 20 25.0 7 8 8 7 7 8 8 7 8 26.0 11 10 10 11 11 10 10 11 11 27.0 8 9 9 8 8 9 9 8 9 28.0 10 10 10 10 10 10 10 10 10 29.0 6 5 6 6 6 5 6 6 6 30.0 4 3 4 4 4 3 4 4 4 31.0 8 8 8 8 8 8 8 8 8 32.0 4 3 4 4 4 3 4 4 4 33.0 7 5 7 6 7 5 7 6 7 34.0 9 8 9 8 9 8 9 8 9 35.0 6 6 6 6 6 6 6 6 6 36.0 6 5 6 6 6 5 6 6 6 37.0 3 3 3 3 3 3 3 3 3 38.0 1 1 1 0 1 1 1 0 1 39.0 1 1 1 1 1 1 1 1 1 40.0 2 2 2 2 2 2 2 2 2 42.0 1 1 1 1 1 1 1 1 1 43.0 3 3 3 3 3 3 3 3 3 45.0 3 3 3 3 3 3 3 3 3 46.0 2 2 2 2 2 2 2 2 2 48.0 1 1 1 1 1 1 1 1 1 52.0 1 1 1 1 1 1 1 1 1 53.0 1 1 1 1 1 1 1 1 1 54.0 1 1 1 1 1 1 1 1 1 58.0 1 1 1 1 1 1 1 1 1 62.0 1 1 1 1 1 1 1 1 1 63.0 1 1 1 1 1 1 1 1 1 70.0 1 1 1 1 1 1 1 1 1 72.0 1 1 1 1 1 1 1 1 1 140.0 1 1 1 1 1 1 1 1 1 df [[ i for i in outs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:> df [[ i for i in hangs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:>","title":"Do block positions have the same behavior?"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#30-look-for-dependent-independent-signal","text":"","title":"3.0 Look for dependent-independent signal"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#are-there-linear-relationships-between-the-dependent-and-independent-variables","text":"corr = df . corr () ( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 ) Product False Roll Width False Block1Pos Out Seconds True Block2Pos Out Seconds True Block3Pos Out Seconds True ... UPEX - UWU PLOT EXTREM (UPEX) False UWG - UWU GRAVIMETRIC WITH SCALE (UWG) False UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP) False WATP - WATER PERMEABILITY (WATP) False Total Seconds Out True Length: 237, dtype: bool cor_filt = corr . loc [( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 )] . drop ( labels = outs + hangs , axis = 0 )[ outs + hangs ] cor_filt .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Pos Out Seconds Block2Pos Out Seconds Block3Pos Out Seconds Block4Pos Out Seconds Block1Pos Out Count Block2Pos Out Count Block3Pos Out Count Block4Pos Out Count Total Position Out Total Seconds Out Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs ADD ARGUS ARGUSTAT AT 31 HK -0.103999 -0.417688 -0.383222 0.202131 -0.056194 -0.412274 0.155126 0.145535 0.014216 0.029563 0.121682 -0.256723 -0.012676 0.099690 -0.069354 -0.041492 0.094150 0.041900 0.344896 PP INEOS 100 GA02 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 -1.000000 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 MFR 4 White 0.289648 -0.016268 0.451446 -0.155932 0.297256 -0.000098 0.376795 -0.003129 0.225425 0.182687 0.231764 -0.045076 0.283286 -0.119481 0.108396 -0.057188 0.317089 0.008018 0.189109 PP TOTAL 7089 0.160268 0.171023 0.210796 0.093720 0.192159 0.210942 0.206859 0.076058 0.260505 0.207522 0.087954 0.050670 0.052622 -0.050506 0.218148 0.251676 0.134881 0.121080 0.331897 SILIKAT POLYPLUS AB 1001 PP 0.015132 0.125801 0.379559 0.255662 -0.018715 0.012019 0.161698 0.067661 0.127830 0.137376 -0.016317 -0.018187 -0.018956 0.086781 0.011207 -0.020307 0.092291 0.109259 0.069217 RJET15219WG1 - Corona Charge I AVG BLK#2 -0.120204 -0.120509 -0.056377 -0.002054 -0.198138 -0.221189 -0.193393 -0.024541 -0.232863 -0.149862 -0.090896 -0.060404 -0.054169 -0.052683 -0.215325 -0.216545 -0.182990 -0.120387 -0.308217 BLFL - COLOUR BLACK FILAMENT L value (BLFL) -0.360086 -0.157845 -0.245565 -0.268837 -0.153321 -0.162697 -0.212481 -0.247721 -0.067632 -0.121690 0.097783 0.146515 -0.346923 -0.110987 0.402310 -0.274153 -0.378670 -0.116264 -0.117501 BRFA - COLOUR BROWN FILAMENT a value (BRFa) 0.159056 0.250229 -0.384880 0.109024 0.134248 0.112836 -0.380715 -0.054074 0.099040 0.166926 0.170184 -0.065344 -0.195062 0.094596 0.195404 -0.333632 -0.026209 0.177576 0.173995 BRFB - COLOUR BROWN FILAMENT b value (BRFb) 0.413876 0.047748 0.332752 0.342115 0.145794 -0.028170 0.246787 0.234624 0.216156 0.311363 0.148441 -0.210193 0.118275 0.285286 -0.027055 -0.275750 0.100830 0.328240 0.164345 BRFL - COLOUR BROWN FILAMENT L value (BRFL) 0.198864 -0.178050 0.483242 0.205796 0.017456 -0.220432 0.411110 0.090885 0.039031 0.128326 0.337130 -0.326368 0.075738 0.145878 0.195055 -0.467536 -0.075130 0.188013 -0.000575 E20M AVG - SST ELONGATION 20X20 MD (E20M) 0.106870 -0.096775 -0.249986 -0.039633 0.009518 -0.049131 -0.367065 -0.074018 -0.070058 -0.071425 -0.124456 -0.016993 -0.070312 -0.036314 -0.145788 -0.075968 -0.197153 -0.069477 -0.072872 EF1M AVG - SST ELONGATION FOLDED 10 CM MD (EF1M) 1.000000 NaN 1.000000 -0.081738 1.000000 NaN NaN NaN -0.101606 -0.178993 0.313364 -0.936781 -1.000000 -0.517401 0.306235 -0.950303 -1.000000 -0.324932 -0.112334 EF1X AVG - SST ELONGATION FOLDED 10 CM XD (EF1X) 1.000000 NaN 1.000000 0.126777 1.000000 NaN NaN NaN 0.010039 -0.063399 0.896084 -0.961383 -1.000000 -0.609904 0.892730 -0.971830 -1.000000 -0.340772 0.050483 NATM AVG - Nail Tear MD EN 13859-1 (NATM) -0.517421 -0.835147 0.511409 -0.258360 -0.296500 -0.906038 NaN -0.333754 -0.100244 0.013436 -0.338129 0.580493 0.826191 -0.322585 -0.348782 0.063309 0.929207 -0.430201 0.086484 NATX AVG - Nail Tear XD EN 13859-1 (NATX) 0.269389 -0.864128 0.542651 0.071499 -0.053990 -0.927938 NaN -0.056465 -0.084131 0.046366 -0.190830 0.541297 0.777131 -0.114506 -0.096640 0.051081 0.902927 -0.295087 0.093804 O90 - O90 EN ISO 12956 (O90E) NaN NaN NaN 0.807962 NaN NaN NaN NaN -0.265093 -0.268107 -0.280791 -0.378899 NaN -0.451610 -0.307148 -0.267261 NaN -0.269680 -0.193539 PU AVG - PUNCTURE \"US Rod\" (PU) 0.173625 -0.790779 -0.165265 0.451194 -0.127912 -0.952390 -0.184515 0.734933 -0.040323 -0.021124 0.096383 -0.998695 -0.288786 0.156274 0.066142 0.363427 -0.297118 0.479459 -0.239496 TKO4 AVG - Tack-Tear Ost 4Meter -0.170480 -0.383667 -0.326335 0.006590 -0.104058 -0.308260 -0.329544 0.047734 -0.171814 -0.195662 -0.214633 -0.440748 -0.182875 -0.027353 -0.077638 -0.447423 -0.187893 0.114764 -0.160340 TKO5 AVG - Tack - Tear Ost 5 Meter 1.000000 0.134107 0.560717 -0.070116 NaN 0.106485 0.152594 -0.525375 -0.373817 -0.306252 0.338272 0.155577 0.034557 -0.453643 0.286066 0.056270 -0.066270 -0.400941 -0.387297 TKW4 AVG - Tack-Tear West 4Meter -0.334291 -0.282220 -0.265607 -0.134467 -0.320379 -0.231417 -0.300679 -0.265049 -0.274024 -0.250617 -0.277325 -0.375559 -0.168965 -0.154119 -0.236424 -0.282687 -0.168865 -0.162801 -0.316193 TKW5 AVG - Tack - Tear West 5 Meter 1.000000 0.051583 0.100609 0.018368 NaN 0.014194 -0.167032 -0.366762 -0.279911 -0.190366 0.069184 0.108657 0.224892 -0.326866 -0.003422 0.043730 0.039670 -0.362928 -0.270898 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt , cmap = cmap ) <AxesSubplot:>","title":"Are there linear relationships between the dependent and independent variables?"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#filter-n-100","text":"cor_filt = cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]] fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]], cmap = cmap ) <AxesSubplot:>","title":"Filter N &gt; 100"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#operating-conditions","text":"fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Operations Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Operations Correlations to Hangs and Outs')","title":"Operating Conditions"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#feeds","text":"fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ ~ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Feeds Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Feeds Correlations to Hangs and Outs') ind_vars = list ( cor_filt . index [ ~ cor_filt . index . isin ( operation )]) ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] fig , ax = plt . subplots ( 1 , 3 , figsize = ( 30 , 10 )) indices = np . argwhere ( ax ) for index , ind_var in enumerate ( ind_vars ): df [[ ind_var ] + seconds ] . melt ( id_vars = ind_var , value_vars = None , var_name = None ,) . plot ( x = ind_var , y = 'value' , ax = ax [ index ], ls = '' , marker = '.' )","title":"Feeds"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#40-understand-the-independent-variable","text":"","title":"4.0 Understand the Independent Variable"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#descriptive-stats-on-ind-var","text":"all the feeds sum to 1 (feeds are mass or volume fractions), so we probably won't need to worry about scaling df [ feeds ] . sum ( 1 ) 0 1.0 1 1.0 2 1.0 3 1.0 4 1.0 ... 17892 1.0 17893 1.0 17894 1.0 17895 1.0 17896 1.0 Length: 17897, dtype: float64 many of the feeds are not used at all skew = df [ feeds ] . skew () kurt = df [ feeds ] . kurtosis () null = df [ feeds ] . isnull () . sum () n = df [ feeds ] . notnull () . sum () med = df [ feeds ] . median () men = df [ feeds ] . mean () dff = pd . DataFrame ([ skew , kurt , null , n , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'null cnt' , 'n' , 'median' , 'mean' ] dff . loc [ dff [ 'n' ] > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis null cnt n median mean ADD ARGUS ARGUSTAT AT 31 HK -1.122018 -0.275303 17798.0 99.0 0.006560 0.005080 ADD ARGUS ARGUTHERM AO 77 PP 14.927951 248.300227 15627.0 2270.0 0.005249 0.010705 ADD POLYPLAST PPM AO 01 UV PP 5.788514 87.051660 16769.0 1128.0 0.006397 0.006218 ADD SCHULMAN DTY 20 AOUV 6.573319 99.052816 13484.0 4413.0 0.004997 0.005411 BEIGE SCHULMAN PZ 91738 1 5 ZA -0.455858 10.108094 16975.0 922.0 0.008922 0.008708 BLACK SCHULMAN P7381 2.301415 16.313957 4471.0 13426.0 0.000317 0.002040 DEVAN 2SPIN PP 106 0.770601 11.066370 13279.0 4618.0 0.006658 0.005554 GREEN POLYPLAST COLOUR MB PP 6821 -1.856631 3.540741 16894.0 1003.0 0.007519 0.006822 GREEN SCHULMAN PZ 302446 1 5 T 0.363239 8.864015 17150.0 747.0 0.018764 0.018006 High MFR Black 0.473484 -1.518082 14862.0 3035.0 0.047883 0.146677 MFR 4 Black 1.612264 4.261601 12595.0 5302.0 0.030099 0.098792 MFR 4 Black Brown -0.086521 -0.342944 16360.0 1537.0 0.189244 0.150273 High MFR Grey -0.364616 -1.483643 15900.0 1997.0 0.226358 0.199138 MFR 4 Green 0.149918 8.244109 15985.0 1912.0 0.279622 0.262574 PP INEOS 100 GA02 NaN NaN 17895.0 2.0 0.107927 0.107927 PP INEOS 100 GA04 1.294449 5.764334 6684.0 11213.0 0.208757 0.216941 PP INEOS 100 GA09 -0.449540 -0.968103 16802.0 1095.0 0.224353 0.219208 PP POLYCHIM HB12XF -0.222658 -0.684893 16525.0 1372.0 0.275755 0.291827 MFR 4 White 1.136619 2.053071 17559.0 338.0 0.098961 0.111129 PP TOTAL 7089 -0.216726 -1.333059 16346.0 1551.0 0.276430 0.288643 PP TOTAL PPH 4065 0.353911 0.740658 611.0 17286.0 0.441009 0.424684 PP UNIPETROL GB005 0.117291 1.045121 11444.0 6453.0 0.324773 0.295906 SILIKAT PBH ABPP 05 2.690146 26.471577 15967.0 1930.0 0.005001 0.005229 SILIKAT POLYPLUS AB 1001 PP 6.874010 77.288671 17360.0 537.0 0.002506 0.002642 UV ARGUS ARX 904 11 PP 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.1 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.2 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV POLYPLAST 6005 PP 1.426790 1.634549 17506.0 391.0 0.020009 0.026245 UV SCHULMAN FPPUV 38 2.114261 14.372541 17141.0 756.0 0.029986 0.032261 MFR 4 Grey 1.718802 10.970192 8567.0 9330.0 0.269445 0.263401 MFR 4 Grey Grey 3.607370 20.342313 15865.0 2032.0 0.040035 0.068547 UV ARGUS ARX V 16 706 UV PP 1.889895 6.474782 7972.0 9925.0 0.002110 0.007994 our ind_vars feeds have some strong correlations with other feeds In particular, PP TOTAL 7089 and SILIKAT POLYPLUS AB 1001 PP have a correlation w/ eachother of .923 ind_corr = df [ feeds ] . corr ()[ ind_vars ] . dropna ( how = 'all' ) ind_corr . loc [( abs ( ind_corr ) > 0.5 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 ind_vars feeds exhibit bimodal distributions (2+ recipes) for ind in ind_vars : df [ ind ] . plot . kde () plt . title ( ind ) plt . xlim ( 0 ) plt . show ()","title":"Descriptive Stats on Ind Var"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#50-hypothesis-tests","text":"","title":"5.0 Hypothesis Tests"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#non-parametric","text":"","title":"Non-Parametric"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#univariate-categorical-to-categorical-chi-square","text":"ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 for ind in ind_vars : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 10182 False 7377 True True 173 False 165 Name: Total Position Out, dtype: int64 [[10182 173] [ 7377 165]] chi2 stat: 6.02 p-value: 1.41e-02 PP TOTAL 7089 Total Position Out False True 9171 False 7175 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[9171 1184] [7175 367]] chi2 stat: 237.00 p-value: 1.78e-53 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 9996 False 7364 True True 359 False 178 Name: Total Position Out, dtype: int64 [[9996 359] [7364 178]] chi2 stat: 17.99 p-value: 2.22e-05 chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 chisum = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum [ 'type' ] = None chisum . loc [ chisum [ 'feed' ] . isin ( master_batch ), 'type' ] = 'master_batch' chisum . loc [ chisum [ 'feed' ] . isin ( recycle ), 'type' ] = 'recyle' chisum . loc [ chisum [ 'feed' ] . isin ( virgin_polymer ), 'type' ] = 'virgin_polymer' chisum . loc [ chisum [ 'feed' ] . isin ( silicates ), 'type' ] = 'silicates' chisum . loc [ chisum [ 'feed' ] . isin ( uvs ), 'type' ] = 'uvs' chisum . loc [ chisum [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null type 0 PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1 PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2 High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3 SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 4 BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 5 PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 6 ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 7 DEVAN 2SPIN PP 106 35.237690 2.918209e-09 [[7511, 2844], [5768, 1774]] False True master_batch 8 High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 9 GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 10 MFR 4 Black Brown 20.494540 5.980160e-06 [[9550, 805], [6810, 732]] True True recyle 11 UV ARGUS ARX V 16 706 UV PP 20.048858 7.548859e-06 [[4465, 5890], [3507, 4035]] False True uvs 12 ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 13 SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 14 GREEN SCHULMAN PZ 302446 1 5 T 14.155700 1.682865e-04 [[9973, 382], [7177, 365]] True True master_batch 15 MFR 4 Grey Grey 9.004184 2.693622e-03 [[9464, 1141], [6401, 891]] True True recyle","title":"Univariate Categorical to Categorical (Chi-Square)"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#check-confounding-variables","text":"Isolate Products (product conflation with feed) Let's isolate the comparison by only products made with the feed ( ind_var ) in question We see that MFR 4 White and SILIKAT POLYPLUS AB 1001 PP are very imbalanced compared to PP TOTAL 7089 target = 'Total Position Out' for ind in ind_vars : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 3978 False 2933 True True 173 False 165 Name: Total Position Out, dtype: int64 [[3978 173] [2933 165]] chi2 stat: 5.10 p-value: 2.40e-02 PP TOTAL 7089 Total Position Out False True 2301 False 1393 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[2301 1184] [1393 367]] chi2 stat: 96.05 p-value: 1.12e-22 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 2418 False 1602 True True 359 False 178 Name: Total Position Out, dtype: int64 [[2418 359] [1602 178]] chi2 stat: 8.66 p-value: 3.25e-03 target = 'Total Position Out' chisum2 = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum2 = pd . concat ([ chisum2 , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum2 . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum2 = chisum2 . sort_values ( 'p-value' ) . reset_index ( drop = True ) tests = chisum . set_index ( 'feed' ) . join ( chisum2 . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], rsuffix = ', confound' ) tests . loc [( tests [ 'reject null' ] == True ) & ( tests [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value cTable + reject null type p-value, confound reject null, confound feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True we can compare this with our feed correlation table - there may be other explanatory variables 'hiding' ind_corr . loc [( abs ( ind_corr ) > 0.3 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 BLACK SCHULMAN P7381 0.196431 -0.470838 0.082699 DEVAN 2SPIN PP 106 NaN -0.415896 0.097590 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA04 0.342077 -0.248904 0.402788 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP TOTAL PPH 4065 -0.419569 -0.160492 0.089303 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 UV ARGUS ARX 904 11 PP NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.1 NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.2 NaN -0.461986 NaN MFR 4 Grey -0.303323 -0.381548 0.038125 MFR 4 Grey Grey 0.395658 -0.102141 -0.086338 UV ARGUS ARX V 16 706 UV PP 0.246254 -0.439913 -0.084811","title":"Check confounding variables"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#univariate-categorical-to-quantitative-moods-median","text":"A limitation of the previous analysis is that our threshold for counting an upset in the line was pretty low - any delay (position out > 0 seconds). A way we can naturally increase this threshold is to compare medians (of position out (secs)) across our groups. chidf = pd . DataFrame () target = 'Seconds' chidf [ target ] = df [ seconds ] . sum ( 1 ) moods = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind in feeds : try : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) grp = chidf . loc [ chidf [ ind ] == True ][ target ] . values chi2 , p , m , cTable = stats . median_test ( grp , pop , correction = False ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] < 1 moods = pd . concat ([ moods , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass moods . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] moods = moods . sort_values ( 'p-value' ) . reset_index ( drop = True ) moods . loc [ moods [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null 0 PP TOTAL 7089 284.071422 9.736103e-64 [[1093, 8620], [458, 9277]] False True 1 MFR 4 Grey Grey 161.505327 5.305934e-37 [[744, 9215], [1288, 8682]] True True 2 PP POLYCHIM HB12XF 140.079653 2.557382e-32 [[897, 8734], [475, 9163]] False True 3 High MFR Grey 134.851343 3.558380e-31 [[1244, 8697], [753, 9200]] False True 4 SILIKAT PBH ABPP 05 132.735135 1.033157e-30 [[1205, 8704], [725, 9193]] False True 5 SILIKAT POLYPLUS AB 1001 PP 40.522176 1.943972e-10 [[341, 8870], [196, 9027]] False True 6 DEVAN 2SPIN PP 106 39.379816 3.488847e-10 [[2499, 8758], [2119, 9139]] False True 7 BLACK SCHULMAN P7381 33.697655 6.437826e-09 [[6961, 8686], [6465, 9211]] False True 8 GREEN POLYPLAST COLOUR MB PP 6821 32.518968 1.180348e-08 [[589, 8854], [414, 9043]] False True 9 PP INEOS 100 GA09 26.828649 2.223170e-07 [[630, 8854], [465, 9043]] False True 10 ADD ARGUS ARGUTHERM AO 77 PP 26.802325 2.253660e-07 [[1250, 8823], [1020, 9074]] False True 11 UV ARGUS ARX V 16 706 UV PP 23.787198 1.075957e-06 [[5152, 8744], [4773, 9153]] False True 12 High MFR Black 18.389746 1.800242e-05 [[1626, 8835], [1409, 9062]] False True 13 ADD POLYPLAST PPM AO 01 UV PP 15.536764 8.091620e-05 [[499, 9000], [629, 8897]] True True 14 UV SCHULMAN FPPUV 38 12.417442 4.253424e-04 [[330, 8983], [426, 8914]] True True 15 MFR 4 White 11.470090 7.072526e-04 [[138, 8971], [200, 8926]] True True 16 GREEN SCHULMAN PZ 302446 1 5 T 9.924320 1.631091e-03 [[331, 8983], [416, 8914]] True True we see that overall the moods test is slightly less pessimistic chisum [ 'reject null' ] . value_counts () True 16 False 16 Name: reject null, dtype: int64 moods [ 'reject null' ] . value_counts () True 17 False 15 Name: reject null, dtype: int64 testsf = tests . join ( moods . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], lsuffix = ', chi' , rsuffix = ', moods' ) testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True 2.223170e-07 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True 2.253660e-07 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True 1.800242e-05 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True 1.180348e-08 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True 8.091620e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True 1.943972e-10 True","title":"Univariate Categorical to Quantitative (Moods Median)"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#non-parametric-conclusions","text":"problem_vars = testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] . index print ( f \"Compared with correlation \\n the dependency tests show that the top 5 problem variables are: \\n { [ f ' { i } ' for i in problem_vars ] } \" ) Compared with correlation the dependency tests show that the top 5 problem variables are: ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True","title":"Non-Parametric Conclusions"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#parametric","text":"","title":"Parametric"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#univariate-quantitative-to-quantitative-linear-regression","text":"for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () plt . plot ( x , ( y - y_pred ), ls = '' , marker = '.' ) plt . ylabel ( 'Residual' ) plt . xlabel ( 'x' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( f \"test statistic: { t } \" ) print ( f \"p-value: { stats . t . sf ( x = t , df = n - 2 ) } \" ) R2: 3.34e-02 RSE: 9.97e-03 b: -2.51e-03 m: 3.61e-01 n: 338 x_bar: 1.11e-01 SE_b: 1.51e-03 SE_m: 1.26e-02 test statistic: 28.56535956265302 p-value: 3.2661498371310494e-92 R2: 4.31e-02 RSE: 5.27e-03 b: -3.19e-03 m: 9.08e-02 n: 1551 x_bar: 2.89e-01 SE_b: 3.09e-04 SE_m: 9.66e-04 test statistic: 94.02166426512674 p-value: 0.0 R2: 1.89e-02 RSE: 5.94e-03 b: -1.36e-02 m: 1.57e+01 n: 537 x_bar: 2.64e-03 SE_b: 1.04e-03 SE_m: 3.81e-01 test statistic: 41.11483903422569 p-value: 4.981863349397906e-168 for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.033 Model: OLS Adj. R-squared: 0.030 Method: Least Squares F-statistic: 11.60 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.000739 Time: 10:02:50 Log-Likelihood: 360.28 No. Observations: 338 AIC: -716.6 Df Residuals: 336 BIC: -708.9 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0025 0.013 -0.199 0.843 -0.027 0.022 x1 0.3612 0.106 3.406 0.001 0.153 0.570 ============================================================================== Omnibus: 457.321 Durbin-Watson: 1.128 Prob(Omnibus): 0.000 Jarque-Bera (JB): 55378.392 Skew: 6.475 Prob(JB): 0.00 Kurtosis: 64.356 Cond. No. 23.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.043 Model: OLS Adj. R-squared: 0.042 Method: Least Squares F-statistic: 69.71 Date: Sat, 02 Apr 2022 Prob (F-statistic): 1.50e-16 Time: 10:02:50 Log-Likelihood: 2180.0 No. Observations: 1551 AIC: -4356. Df Residuals: 1549 BIC: -4345. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0032 0.003 -0.917 0.359 -0.010 0.004 x1 0.0908 0.011 8.349 0.000 0.069 0.112 ============================================================================== Omnibus: 2262.527 Durbin-Watson: 1.176 Prob(Omnibus): 0.000 Jarque-Bera (JB): 660230.502 Skew: 8.562 Prob(JB): 0.00 Kurtosis: 102.615 Cond. No. 7.82 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.019 Model: OLS Adj. R-squared: 0.017 Method: Least Squares F-statistic: 10.29 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.00142 Time: 10:02:50 Log-Likelihood: 622.03 No. Observations: 537 AIC: -1240. Df Residuals: 535 BIC: -1231. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0136 0.013 -1.020 0.308 -0.040 0.013 x1 15.6795 4.888 3.208 0.001 6.078 25.281 ============================================================================== Omnibus: 739.911 Durbin-Watson: 1.268 Prob(Omnibus): 0.000 Jarque-Bera (JB): 108127.158 Skew: 7.213 Prob(JB): 0.00 Kurtosis: 71.003 Cond. No. 1.49e+03 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.49e+03. This might indicate that there are strong multicollinearity or other numerical problems.","title":"Univariate Quantitative to Quantitative (Linear Regression)"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#feature-engineering","text":"Introducing polynomial features results in poor coefficient estimates ind = ind_vars [ 1 ] lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) features = PolynomialFeatures ( degree = 3 ) x = features . fit_transform ( x . reshape ( - 1 , 1 )) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.056 Model: OLS Adj. R-squared: 0.055 Method: Least Squares F-statistic: 30.83 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.35e-19 Time: 10:03:01 Log-Likelihood: 2190.9 No. Observations: 1551 AIC: -4374. Df Residuals: 1547 BIC: -4352. Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0030 0.009 0.334 0.739 -0.014 0.020 x1 0.1612 0.124 1.301 0.193 -0.082 0.404 x2 -0.9203 0.523 -1.761 0.078 -1.945 0.105 x3 1.6922 0.664 2.550 0.011 0.390 2.994 ============================================================================== Omnibus: 2252.329 Durbin-Watson: 1.188 Prob(Omnibus): 0.000 Jarque-Bera (JB): 651735.784 Skew: 8.487 Prob(JB): 0.00 Kurtosis: 101.979 Cond. No. 594. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. R2: 5.64e-02 RSE: 6.03e-03 b: 2.95e-03 m: 0.00e+00 n: 1551 x_bar: 3.58e-01 SE_b: 1.69e-04 SE_m: 1.96e-04 0.0 0.5","title":"Feature Engineering"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#multivariate-quantitative-to-quantitative-multivariate-linear-regression","text":"lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) lindf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ADD ARGUS ARGUSTAT AT 31 HK ADD ARGUS ARGUTHERM AO 77 PP ADD POLYPLAST PPM AO 01 UV PP ADD SCHULMAN DTY 20 AOUV BEIGE SCHULMAN PZ 91738 1 5 ZA BLACK SCHULMAN P7381 DEVAN 2SPIN PP 106 GREEN POLYPLAST COLOUR MB PP 6821 GREEN SCHULMAN PZ 302446 1 5 T High MFR Black ... UV POLYPLAST 6005 PP UV SCHULMAN FPPUV 38 MFR 4 Grey PP POLYCHIM HL10XF MFR 4 Grey Grey PP POLYMER TEST MFI4 PP TOTAL 4069 RR MASTERBATCH TEST UV ARGUS ARX V 16 706 UV PP seconds 0 0.0 0.0 0.0 0.005337 0.0 0.000253 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.409353 0.0 0.0 0.0 0.0 0.0 0.000000 9.0 1 0.0 0.0 0.0 0.005867 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.452007 0.0 0.0 0.0 0.0 0.0 0.000000 536.0 2 0.0 0.0 0.0 0.005895 0.0 0.000270 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.449608 0.0 0.0 0.0 0.0 0.0 0.000000 20.0 3 0.0 0.0 0.0 0.005743 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.440883 0.0 0.0 0.0 0.0 0.0 0.000000 1001.0 4 0.0 0.0 0.0 0.004553 0.0 0.000211 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.349459 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.339726 0.0 0.0 0.0 0.0 0.0 0.002062 0.0 17893 0.0 0.0 0.0 0.000000 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.342659 0.0 0.0 0.0 0.0 0.0 0.002071 0.0 17894 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345886 0.0 0.0 0.0 0.0 0.0 0.002107 0.0 17895 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.344945 0.0 0.0 0.0 0.0 0.0 0.002095 799.0 17896 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345288 0.0 0.0 0.0 0.0 0.0 0.002105 429.0 17897 rows \u00d7 45 columns lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) x = lindf [ feeds ] . values # features = PolynomialFeatures(degree=3) # x = features.fit_transform(x.reshape(-1,1)) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.050 Model: OLS Adj. R-squared: 0.049 Method: Least Squares F-statistic: 31.47 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.76e-174 Time: 10:03:19 Log-Likelihood: 44791. No. Observations: 17897 AIC: -8.952e+04 Df Residuals: 17866 BIC: -8.928e+04 Df Model: 30 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0123 0.002 5.316 0.000 0.008 0.017 x1 -1.0553 0.382 -2.760 0.006 -1.805 -0.306 x2 -0.0119 0.010 -1.241 0.215 -0.031 0.007 x3 0.0351 0.100 0.352 0.725 -0.161 0.231 x4 -0.2377 0.069 -3.438 0.001 -0.373 -0.102 x5 0.4001 0.145 2.750 0.006 0.115 0.685 x6 -0.0224 0.128 -0.175 0.861 -0.272 0.228 x7 -0.3174 0.096 -3.305 0.001 -0.506 -0.129 x8 -0.5909 0.209 -2.825 0.005 -1.001 -0.181 x9 -0.4059 0.087 -4.655 0.000 -0.577 -0.235 x10 -0.0241 0.005 -5.206 0.000 -0.033 -0.015 x11 -2.473e-14 1.17e-14 -2.107 0.035 -4.77e-14 -1.73e-15 x12 -0.0050 0.004 -1.371 0.171 -0.012 0.002 x13 -0.0288 0.007 -4.319 0.000 -0.042 -0.016 x14 -0.0268 0.005 -5.644 0.000 -0.036 -0.018 x15 -0.0200 0.006 -3.620 0.000 -0.031 -0.009 x16 -1.023e-14 3.5e-15 -2.923 0.003 -1.71e-14 -3.37e-15 x17 -9.109e-16 2.5e-16 -3.649 0.000 -1.4e-15 -4.22e-16 x18 -0.0994 0.093 -1.073 0.283 -0.281 0.082 x19 -0.0055 0.003 -2.044 0.041 -0.011 -0.000 x20 0.0003 0.005 0.058 0.954 -0.009 0.010 x21 1.325e-15 1.2e-16 11.078 0.000 1.09e-15 1.56e-15 x22 -2.494e-16 8.68e-17 -2.873 0.004 -4.2e-16 -7.93e-17 x23 0.0145 0.004 3.827 0.000 0.007 0.022 x24 -0.0100 0.010 -1.051 0.293 -0.029 0.009 x25 0.0216 0.004 5.724 0.000 0.014 0.029 x26 -0.0101 0.002 -4.198 0.000 -0.015 -0.005 x27 -1.03e-16 8.73e-17 -1.180 0.238 -2.74e-16 6.8e-17 x28 9.329e-16 7.53e-17 12.396 0.000 7.85e-16 1.08e-15 x29 -0.0074 0.003 -2.881 0.004 -0.012 -0.002 x30 1.7091 0.132 12.994 0.000 1.451 1.967 x31 4.1538 0.376 11.053 0.000 3.417 4.890 x32 0 0 nan nan 0 0 x33 0.0031 0.012 0.263 0.792 -0.020 0.026 x34 0.0031 0.012 0.263 0.792 -0.020 0.026 x35 0.0031 0.012 0.263 0.792 -0.020 0.026 x36 0.0790 0.042 1.868 0.062 -0.004 0.162 x37 0.0525 0.030 1.774 0.076 -0.006 0.111 x38 -0.0064 0.003 -2.521 0.012 -0.011 -0.001 x39 0 0 nan nan 0 0 x40 -0.0136 0.005 -2.762 0.006 -0.023 -0.004 x41 0 0 nan nan 0 0 x42 0 0 nan nan 0 0 x43 0 0 nan nan 0 0 x44 0.1990 0.048 4.119 0.000 0.104 0.294 ============================================================================== Omnibus: 43479.020 Durbin-Watson: 1.074 Prob(Omnibus): 0.000 Jarque-Bera (JB): 693938974.691 Skew: 25.434 Prob(JB): 0.00 Kurtosis: 966.322 Cond. No. 1.11e+16 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 1.79e-28. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. R2: 5.02e-02 RSE: 1.67e-03 b: 1.23e-02 m: -1.06e+00 n: 17897 x_bar: 2.26e-02 SE_b: 1.25e-05 SE_m: 2.14e-05 -49284.75476441197 1.0","title":"Multivariate Quantitative to Quantitative (Multivariate Linear Regression)"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#forward-selection","text":"def add_feature ( features , basemodel , data , y , r2max ): for feed in features : basemodel [ feed ] = data [ feed ] basemodel . fillna ( 0 , inplace = True ) x = basemodel [[ col for col in basemodel . columns if col != 'seconds' ]] . values ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () if ( est2 . rsquared > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = est2 . rsquared feedmax = feed bestsum = est2 . summary () newmodel = basemodel . copy () else : pass basemodel . drop ( labels = feed , axis = 1 , inplace = True ) return r2max , feedmax , bestsum , newmodel lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) r2max = 0 candidates = feeds . copy () basemodel = lindf while True : newr2max , feedmax , bestsum , newmodel = add_feature ( features = candidates , basemodel = basemodel , data = df , y = y , r2max = 0 ) if newr2max > r2max : # print(newr2max, feedmax) candidates . remove ( feedmax ) r2max = newr2max basemodel = newmodel continue else : break /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) we see some of the usual suspects from the chi-square tests. Notably some are missing: High MFR Grey , BLACK SCHULMAN P7381 , PP INEOS 100 GA09 and others. basemodel . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } seconds PP TOTAL 7089 PP POLYCHIM HB12XF SILIKAT PBH ABPP 05 SILIKAT POLYPLUS AB 1001 PP MFR 4 Green ADD SCHULMAN DTY 20 AOUV GREEN SCHULMAN PZ 302446 1 5 T MFR 4 Grey UV ARGUS ARX V 16 706 UV PP GREEN SCHULMAN PZ 34198 1 5 T 0 9.0 0.0 0.0 0.0 0.0 0.0 0.005337 0.0 0.409353 0.0 0.0 1 536.0 0.0 0.0 0.0 0.0 0.0 0.005867 0.0 0.452007 0.0 0.0 2 20.0 0.0 0.0 0.0 0.0 0.0 0.005895 0.0 0.449608 0.0 0.0 3 1001.0 0.0 0.0 0.0 0.0 0.0 0.005743 0.0 0.440883 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.004553 0.0 0.349459 0.0 0.0 forward_selection_feeds = basemodel . columns [ 1 :] bestsum OLS Regression Results Dep. Variable: y R-squared: 0.046 Model: OLS Adj. R-squared: 0.045 Method: Least Squares F-statistic: 94.99 Date: Sat, 02 Apr 2022 Prob (F-statistic): 5.51e-174 Time: 10:04:41 Log-Likelihood: 44748. No. Observations: 17897 AIC: -8.948e+04 Df Residuals: 17887 BIC: -8.940e+04 Df Model: 9 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 0.0034 0.000 11.288 0.000 0.003 0.004 x1 0.0214 0.002 11.535 0.000 0.018 0.025 x2 0.0146 0.002 7.837 0.000 0.011 0.018 x3 1.5742 0.110 14.375 0.000 1.360 1.789 x4 4.1700 0.363 11.497 0.000 3.459 4.881 x5 -0.0181 0.003 -6.276 0.000 -0.024 -0.012 x6 -0.2879 0.065 -4.450 0.000 -0.415 -0.161 x7 -0.1932 0.052 -3.698 0.000 -0.296 -0.091 x8 0.0037 0.001 3.322 0.001 0.002 0.006 x9 0.0736 0.028 2.641 0.008 0.019 0.128 x10 0 0 nan nan 0 0 x11 0 0 nan nan 0 0 Omnibus: 43415.763 Durbin-Watson: 1.068 Prob(Omnibus): 0.000 Jarque-Bera (JB): 683838684.139 Skew: 25.346 Prob(JB): 0.00 Kurtosis: 959.275 Cond. No. inf Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 0. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.","title":"Forward Selection"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#multivariate-conclusions","text":"y poses a problem with out of the box regression. Extreme tails will upset the residual plots. We some overlap of the parametric and non-parametric approaches: [ i for i in basemodel . columns if i in ( problem_vars )] ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05']","title":"Multivariate Conclusions"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#multivariate-quantitative-to-categorical-binned-output-variable","text":"Logistic regression does little better than random guessing lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) x = lindf [ ind ] . values #.reshape(-1,1) model = LogisticRegression () model . fit ( x , y ) y_pred = model . predict ( x ) print ( f \"Train Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) Train Acc: 0.37 Test Acc: 0.37 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( print ( classification_report ( y , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.35 0.80 0.49 4527 2.0 0.36 0.32 0.34 4434 3.0 0.30 0.03 0.06 4463 4.0 0.43 0.32 0.37 4473 accuracy 0.37 17897 macro avg 0.36 0.37 0.31 17897 weighted avg 0.36 0.37 0.31 17897 <AxesSubplot:> If we are careful about regularizing the random forest model, it does a little bit better lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) X = lindf [ ind ] . values #.reshape(-1,1) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 0.99 Test Acc: 0.39 y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.41 0.48 0.44 898 2.0 0.33 0.30 0.31 881 3.0 0.29 0.27 0.28 893 4.0 0.50 0.51 0.51 908 accuracy 0.39 3580 macro avg 0.38 0.39 0.38 3580 weighted avg 0.38 0.39 0.38 3580 <AxesSubplot:> # grab feature importances imp = model . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in model . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ ind , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 PP TOTAL PPH 4065 0.150387 0.008053 1 BLACK SCHULMAN P7381 0.118158 0.008725 2 PP INEOS 100 GA04 0.100635 0.005217 3 UV ARGUS ARX V 16 706 UV PP 0.090776 0.006273 4 MFR 4 Grey 0.08828 0.005656 5 PP UNIPETROL GB005 0.060558 0.003569 6 MFR 4 Black 0.052589 0.003695 7 ADD SCHULMAN DTY 20 AOUV 0.041478 0.003554 8 DEVAN 2SPIN PP 106 0.038438 0.004061 9 High MFR Black 0.028838 0.002515 10 MFR 4 Grey Grey 0.025758 0.002319 11 ADD ARGUS ARGUTHERM AO 77 PP 0.024526 0.002253 12 High MFR Grey 0.020652 0.004188 13 MFR 4 Green 0.016508 0.002725 14 SILIKAT PBH ABPP 05 0.016283 0.002291 15 PP TOTAL 7089 0.01624 0.004897 16 ADD POLYPLAST PPM AO 01 UV PP 0.013499 0.001685 17 MFR 4 Black Brown 0.013401 0.002131 18 PP POLYCHIM HB12XF 0.01245 0.002185 19 PP INEOS 100 GA09 0.010354 0.00156","title":"Multivariate Quantitative to Categorical (Binned Output Variable)"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#60-business-impact","text":"What is the so what? From Hyp tests - target is either 'Total Position Out' or 'Total Seconds Out' We had culprits from pearsons: ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] culprits from non parametric tests: list ( problem_vars ) ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] and culprits from linear regression forward_selection_feeds Index(['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'MFR 4 Green', 'ADD SCHULMAN DTY 20 AOUV', 'GREEN SCHULMAN PZ 302446 1 5 T', 'MFR 4 Grey', 'UV ARGUS ARX V 16 706 UV PP', 'GREEN SCHULMAN PZ 34198 1 5 T'], dtype='object') and we can check the overlap between non-parametric and linear regression (parametric): predict_vars = [ i for i in basemodel . columns if i in ( problem_vars )] predict_vars ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05'] all_vars = set ( list ( problem_vars ) + list ( forward_selection_feeds )) all_vars {'ADD SCHULMAN DTY 20 AOUV', 'BLACK SCHULMAN P7381', 'GREEN SCHULMAN PZ 302446 1 5 T', 'GREEN SCHULMAN PZ 34198 1 5 T', 'High MFR Grey', 'MFR 4 Green', 'MFR 4 Grey', 'PP POLYCHIM HB12XF', 'PP TOTAL 7089', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'UV ARGUS ARX V 16 706 UV PP'} # add to DF business = pd . DataFrame () value_line_by_day = 99000 for ind in problem_vars : try : # products, prods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique () nprods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . nunique () dff = df . loc [ df [ 'Product' ] . isin ( prods )] # total runtime, tot_runtime = dff [ 'Time Delta' ] . sum () # total runtime with feed, tot_runtime_wfeed = dff [ dff [ ind ] > 0 ][ 'Time Delta' ] . sum () #downtime w/o feed, downtime w/ feed, avg_downtime_min = dff . groupby ( dff [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 avg_downtime_min_wfeed = avg_downtime_min [ True ] avg_downtime_min_w_ofeed = avg_downtime_min [ False ] ratio = avg_downtime_min_wfeed / avg_downtime_min_w_ofeed # average runtime, avg_runtime_min = dff [ 'Time Delta' ] . mean () . total_seconds () / 60 # days saved, dollars saved days_saved = ( tot_runtime_wfeed . total_seconds () / 60 / avg_runtime_min * avg_downtime_min . diff () . values [ - 1 ]) / 60 / 24 dollars_saved_per_batch = value_line_by_day * avg_downtime_min . diff () . values [ - 1 ] / ( 60 * 24 ) business = pd . concat (( business , pd . DataFrame ([[ ind , nprods , tot_runtime , tot_runtime_wfeed , avg_downtime_min_wfeed , avg_downtime_min_w_ofeed , ratio , avg_runtime_min , days_saved , days_saved * value_line_by_day , dollars_saved_per_batch ]], columns = [ 'Feed' , 'Products' , 'Total Runtime, Products' , 'Total Runtime With Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Average Runtime' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]))) except : print ( ind ) print ( dff . groupby ( df [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 ) business = business . sort_values ( 'Dollars Saved (Per Batch)' , ascending = False ) . reset_index ( drop = True ) business # workshop downtime ratio # feed products, feed products # Dollars saved per batch .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Products Total Runtime, Products Total Runtime With Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Average Runtime Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26 409 days 20:43:21 94 days 04:36:05 26.250639 13.671063 1.920161 70.835736 16.727319 1.656005e+06 864.845823 1 High MFR Grey 36 564 days 04:13:41 94 days 12:26:59 23.623539 12.809036 1.844287 65.459164 15.615433 1.545928e+06 743.497080 2 PP POLYCHIM HB12XF 17 383 days 00:03:51 67 days 09:13:53 21.043525 15.600015 1.348943 70.908183 5.173013 5.121283e+05 374.241303 3 BLACK SCHULMAN P7381 64 751 days 20:01:57 590 days 12:21:50 14.902871 12.363943 1.205349 63.216276 23.716605 2.347944e+06 174.551301 4 PP TOTAL 7089 13 291 days 22:06:22 79 days 15:56:47 20.651050 18.269305 1.130369 80.146114 2.367430 2.343756e+05 163.744933 pd . set_option ( 'display.precision' , 2 ) display ( business [[ 'Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]], ) pd . set_option ( 'display.precision' , 6 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26.25 13.67 1.92 16.73 1.66e+06 864.85 1 High MFR Grey 23.62 12.81 1.84 15.62 1.55e+06 743.50 2 PP POLYCHIM HB12XF 21.04 15.60 1.35 5.17 5.12e+05 374.24 3 BLACK SCHULMAN P7381 14.90 12.36 1.21 23.72 2.35e+06 174.55 4 PP TOTAL 7089 20.65 18.27 1.13 2.37 2.34e+05 163.74 We see that 'SILKAT PBH ABPP 05' as well as 'High MFR Grey' have a large amount of days saved over the two years, a strong downtime multiplier effect and the largest dollars saved per batch. Since High MFR Grey is a recycle feed, we may not have the ability to remove it from the feed list. Leaving the SILKAT (Silicate) feed the prime feedstock to remove","title":"6.0 Business Impact"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#70-visualizations","text":"","title":"7.0 Visualizations"},{"location":"extras/X4_Candy_Ribbons_Make_Data/#pp-total-7089","text":"predict_var = predict_vars [ 1 ] outcome_var = [ i for i in df . columns if 'Seconds' in i ] def myplot ( col = list ( df . loc [ df [ predict_var ] > 0 ][ 'Product' ] . unique ())): fig , ax = plt . subplots ( figsize = ( 20 , 10 )) axt = ax . twinx () # other feeds df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True ) . dropna ( axis = 1 , how = 'all' ) . plot ( ls = '' , marker = '*' , ax = ax , alpha = 0.2 ) # predict_var feed df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True )[ predict_var ] . plot ( ls = '--' , marker = '+' , ax = ax , lw = 2 , c = 'red' ) # outcome_var df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ) . plot ( ls = '-' , marker = '+' , lw = 1 , ax = axt , alpha = 1 , c = 'blue' , label = 'Total Seconds Out' ) # outliers ax . set_ylim ( 0 ) axt . set_ylim ( 0 , np . quantile ( df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ), 0.997 )) h1 , l1 = ax . get_legend_handles_labels () h2 , l2 = axt . get_legend_handles_labels () ax . legend ( h1 + h2 , l1 + l2 , loc = 0 ) interact ( myplot ) interactive(children=(Dropdown(description='col', options=(0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.\u2026 <function __main__.myplot(col=[0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.0, 136215.0, 125215.0, 110215.0, 100215.0, 80215.0, 100617.0, 90617.0, 90215.0, 90216.0, 110520.0, 80617.0])>","title":"PP TOTAL 7089"},{"location":"extras/X4_Spotify/","text":"Data Science Foundations X4: Spotify \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Prompt: What makes a playlist successful? Data Description \u00b6 Column Name Description playlist_uri The key, Spotify uri of the playlist owner Playlist owner, Spotify username streams Number of streams from the playlist today stream30s Number of streams over 30 seconds from playlist today dau Number of Daily Active Users, i.e. users with a stream over 30 seconds from playlist today wau Number of Weekly Active Users, i.e. users with a stream over 30 seconds from playlist in past week mau Number of Monthly Active Users, i.e. users with a stream over 30 seconds from playlist in the past month mau_previous_months Number of Monthly Active users in the month prior to this one mau_both_months Number of users that were active on the playlist both this and the previous month users Number of users streaming (all streams) from this playlist this month skippers Number of users who skipped more than 90 percent of their streams today owner_country Country of the playlist owner n_tracks Number of tracks in playlist n_local_tracks Change in number of tracks on playlist since yesterday n_artists Number of unique artists in playlist n_albums Number of unique albums in playlist monthly_stream30s Number of streams over 30 seconds this month monthly_owner_stream30s Number of streams over 30 seconds by playlist owner this month tokens List of playlist title tokens, stopwords and punctuation removed genre_1 No. 1 Genre by weight of playlist tracks, from Gracenote metadata genre_2 No. 2 Genre by weight of playlist tracks, from Gracenote metadata genre_3 No. 3 Genre by weight of playlist tracks, from Gracenote metadata mood_1 No. 1 Mood by weight of playlist tracks, from Gracenote metadata mood_2 No. 2 Mood by weight of playlist tracks, from Gracenote metadata mood_3 No. 3 Mood by weight of playlist tracks, from Gracenote metadata Imports \u00b6 # basic packages import pandas as pd pd . set_option ( 'display.max_columns' , 500 ) import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.discrete.discrete_model import Logit from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.utils.class_weight import compute_class_weight # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score df = pd . read_csv ( \"../../data/playlist_summary_external-4.txt\" , delimiter = ' \\t ' ) Dependency \u00b6 sub_targets = [ 'mau_previous_month' , 'mau_both_months' , 'monthly_stream30s' , 'stream30s' ] # sub_targets = ['mau', 'dau', 'monthly_stream30s', 'stream30s'] des_features = [ 'mood_1' , 'mood_2' , 'mood_3' , 'genre_1' , 'genre_2' , 'genre_3' ] con_features = [ 'n_albums' , 'n_artists' , 'n_tracks' , 'n_local_tracks' ] df [ 'success' ] = df [ sub_targets ] . apply ( lambda x : x > np . quantile ( x , 0.75 )) . all ( axis = 1 ) df [ 'listen_conversions' ] = df [ 'stream30s' ] / df [ 'streams' ] df [ 'listen_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'user_retention' ] = df [ 'mau_both_months' ] / df [ 'mau_previous_month' ] df [ 'user_retention' ] . fillna ( value = 0 , inplace = True ) df [ 'user_conversions' ] = df [ 'mau' ] / df [ 'users' ] df [ 'user_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growing' ] = df [ 'mau' ] > df [ 'mau_previous_month' ] df [ 'mau_growth' ] = df [ 'mau' ] / df [ 'mau_previous_month' ] df [ 'mau_growth' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growth' ] . replace ([ np . inf , - np . inf ], 1 , inplace = True ) new_metrics = [ 'listen_conversions' , 'user_conversions' , 'user_retention' , 'mau_growth' ] df [ 'new_success' ] = df [ new_metrics ] . apply ( lambda x : ( x > 0.5 ) if ( max ( x ) == 1 ) else ( x > 1 )) . all ( axis = 1 ) Discrete \u00b6 fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 10 )) dff = pd . DataFrame ( df [ des_features [ 0 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 1 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 2 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'mood' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'mood' , x = 'count' , orient = 'h' , ax = ax [ 0 ]) dff = pd . DataFrame ( df [ des_features [ 3 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 4 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 5 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'genre' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'genre' , x = 'count' , orient = 'h' , ax = ax [ 1 ]) plt . tight_layout () fig . savefig ( \"discrete_rank_bar_plot.svg\" ) def make_chisum ( target = 'success' ): chidf = pd . DataFrame () chidf [ target ] = df [ target ] chisum = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind in des_features : chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) return chisum def make_cat_plots ( target = 'success' , ind_feature = 'genre_1' ): fig , ax = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 ), sharex = 'col' , sharey = 'row' ) genre_list = chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False )[ 'group' ] . values dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'percent' ] dff = dff . reset_index () dff . loc [ dff [ target ] == True , 'percent' ] = dff . loc [ dff [ target ] == True , 'percent' ] / dff . loc [ dff [ target ] == True , 'percent' ] . sum () dff . loc [ dff [ target ] == False , 'percent' ] = dff . loc [ dff [ target ] == False , 'percent' ] / dff . loc [ dff [ target ] == False , 'percent' ] . sum () dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 0 , 0 ]) ax [ 0 , 0 ] . set_title ( 'Best and Worst Genres, Percent' ) ax [ 0 , 0 ] . set_ylabel ( '' ) ax [ 0 , 0 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 1 , 0 ]) ax [ 1 , 0 ] . set_ylabel ( '' ) dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'count' ] dff = dff . reset_index () dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 0 , 1 ]) ax [ 0 , 1 ] . set_title ( 'Best and Worst Genres, Count' ) ax [ 0 , 1 ] . set_ylabel ( '' ) ax [ 0 , 1 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 1 , 1 ]) ax [ 1 , 1 ] . set_ylabel ( '' ) plt . tight_layout () ax [ 0 , 0 ] . get_legend () . remove () ax [ 1 , 1 ] . get_legend () . remove () ax [ 1 , 0 ] . get_legend () . remove () ax [ 0 , 1 ] . legend ( framealpha = 1 , facecolor = 'white' , title = \"Success\" ) return fig ind_feature = 'genre_1' target = 'success' chisum = make_chisum ( target ) fig = make_cat_plots ( target , ind_feature ) chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False ) . to_excel ( f ' { target } _ { ind_feature } _categorical.xlsx' ) fig . savefig ( f \" { target } _ { ind_feature } _categorical.svg\" ) ind_feature = 'mood_1' target = 'success' chisum = make_chisum ( target ) fig = make_cat_plots ( target , ind_feature ) chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False ) . to_excel ( f ' { target } _ { ind_feature } _categorical.xlsx' ) fig . savefig ( f \" { target } _ { ind_feature } _categorical.svg\" ) Continuous \u00b6 def make_con_plots ( target , con_features ): fig , (( ax1 , ax2 ),( ax3 , ax4 )) = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) chidf = pd . DataFrame () chidf [ target ] = df [ target ] welchsum = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind , ax in zip ( con_features , [ ax1 , ax2 , ax3 , ax4 ]): chidf [ ind ] = df [ ind ] a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } == True\" , ax = ax ) sns . histplot ( b , label = f \" { target } == False\" , ax = ax ) ax . set_title ( ind ) welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) ax . legend () return fig , welchsum target = 'new_success' fig , welchsum = make_con_plots ( target , con_features ) welchsum . to_excel ( f \" { target } _continuous.xlsx\" ) fig . savefig ( f \" { target } _ttest.svg\" ) Models \u00b6 Logistic Regression \u00b6 ### y target = \"success\" print ( target ) y = df [ target ] . values #### X X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( np . ones (( X_con . shape [ 0 ], 1 )), X_con , X_cat )) feature_names = [ 'intercept' ] + con_features + list ( enc . get_feature_names_out ()) data = pd . DataFrame ( X , columns = feature_names ) success def add_feature ( feature_names , basemodel , data , y , r2max = 0 , model = 'linear' , disp = 0 ): feature_max = None bestsum = None newmodel = None for feature in feature_names : basemodel [ feature ] = data [ feature ] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () score = float ( str ( pd . DataFrame ( summ . tables [ 0 ]) . loc [ 3 , 3 ])) if ( score > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = score feature_max = feature bestsum = est2 . summary () newmodel = basemodel . copy () if disp == 1 : print ( f \"new r2max, { feature_max } , { r2max } \" ) basemodel . drop ( labels = feature , axis = 1 , inplace = True ) return r2max , feature_max , bestsum , newmodel candidates = feature_names . copy () basemodel = pd . DataFrame () r2max = 0 while True : newr2max , feature_max , bestsum , newmodel = add_feature ( feature_names = candidates , basemodel = basemodel , data = data , y = y , r2max = r2max ) if newr2max > r2max : r2max = newr2max print ( f \"new r2max, { feature_max } , { r2max } \" ) with open ( \"success_fwd_selection_results.txt\" , \"a+\" ) as f : file_data = f . read () f . write ( f \"new r2max, { feature_max } , { r2max } \\n \" ) candidates . remove ( feature_max ) with open ( \"success_canidates.txt\" , \"w+\" ) as f : file_data = f . read () for i in candidates : f . write ( f \" { i } \\n \" ) basemodel = newmodel basemodel . to_csv ( \"success_basemodel.csv\" ) continue else : break basemodel = pd . read_csv ( \"success_basemodel.csv\" , index_col = 0 ) X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns display ( res_df ) res_df . to_excel ( \"success_log.xlsx\" ) ### y target = \"monthly_stream30s\" print ( target ) y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = 0 names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = 1 weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels basemodel = pd . read_csv ( \"../../scripts/basemodel.csv\" , index_col = 0 ) X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns display ( res_df ) res_df . to_excel ( \"log_model_monthly_stream30s.xlsx\" ) monthly_stream30s .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] intercept -2.0737 0.016 -133.150 0.000 -2.104 -2.043 n_albums 0.2656 0.012 21.287 0.000 0.241 0.290 genre_1_Latin 0.5408 0.027 19.906 0.000 0.488 0.594 genre_1_Indie Rock -0.5832 0.020 -28.964 0.000 -0.623 -0.544 genre_1_Rap -0.3259 0.020 -16.697 0.000 -0.364 -0.288 genre_1_Dance & House -0.3034 0.027 -11.069 0.000 -0.357 -0.250 genre_1_Rock -0.4226 0.025 -16.996 0.000 -0.471 -0.374 mood_1_Energizing -0.2844 0.027 -10.670 0.000 -0.337 -0.232 genre_1_Children's 0.7845 0.062 12.608 0.000 0.663 0.906 mood_1_Tender 0.4943 0.055 9.032 0.000 0.387 0.602 mood_1_Other 0.6206 0.074 8.413 0.000 0.476 0.765 n_tracks 0.0462 0.006 7.613 0.000 0.034 0.058 mood_1_Peaceful 0.6294 0.060 10.426 0.000 0.511 0.748 mood_1_Romantic 0.2981 0.033 9.038 0.000 0.233 0.363 genre_1_Electronica -0.2326 0.034 -6.792 0.000 -0.300 -0.165 genre_2_Indie Rock -0.2050 0.023 -8.998 0.000 -0.250 -0.160 mood_2_Energizing -0.1384 0.019 -7.421 0.000 -0.175 -0.102 genre_1_R&B -0.2335 0.030 -7.696 0.000 -0.293 -0.174 genre_3_Indie Rock -0.2540 0.024 -10.792 0.000 -0.300 -0.208 genre_1_Classical -0.5126 0.060 -8.609 0.000 -0.629 -0.396 genre_2_Alternative 0.1769 0.019 9.542 0.000 0.141 0.213 genre_2_Metal 0.4257 0.040 10.738 0.000 0.348 0.503 mood_2_Peaceful 0.3761 0.055 6.856 0.000 0.269 0.484 mood_2_Romantic 0.2300 0.031 7.414 0.000 0.169 0.291 mood_3_Romantic 0.2597 0.031 8.252 0.000 0.198 0.321 genre_3_Alternative 0.0482 0.019 2.529 0.011 0.011 0.086 n_artists 0.0954 0.013 7.464 0.000 0.070 0.120 genre_1_Metal 0.4049 0.042 9.680 0.000 0.323 0.487 mood_1_Aggressive -0.2660 0.042 -6.275 0.000 -0.349 -0.183 mood_3_Peaceful 0.2912 0.058 4.983 0.000 0.177 0.406 mood_1_Empowering 0.1197 0.021 5.789 0.000 0.079 0.160 genre_1_Religious -0.2328 0.033 -7.154 0.000 -0.297 -0.169 genre_3_Metal 0.1978 0.044 4.527 0.000 0.112 0.283 genre_3_R&B -0.1897 0.024 -8.057 0.000 -0.236 -0.144 mood_3_Yearning 0.1176 0.019 6.096 0.000 0.080 0.155 mood_2_- 0.4272 0.074 5.772 0.000 0.282 0.572 genre_3_Electronica -0.1893 0.026 -7.408 0.000 -0.239 -0.139 genre_2_Latin 0.3700 0.062 5.959 0.000 0.248 0.492 mood_3_Empowering 0.0909 0.021 4.386 0.000 0.050 0.132 genre_3_- -0.1084 0.021 -5.104 0.000 -0.150 -0.067 genre_1_Spoken & Audio 0.4897 0.089 5.489 0.000 0.315 0.665 genre_2_New Age 0.3718 0.067 5.546 0.000 0.240 0.503 genre_3_New Age 0.3384 0.067 5.053 0.000 0.207 0.470 genre_3_Rap -0.1484 0.026 -5.791 0.000 -0.199 -0.098 mood_1_Rowdy -0.2223 0.051 -4.373 0.000 -0.322 -0.123 mood_2_Rowdy -0.1655 0.039 -4.267 0.000 -0.242 -0.089 mood_2_Aggressive -0.1323 0.030 -4.345 0.000 -0.192 -0.073 genre_2_Spoken & Audio 0.3211 0.068 4.717 0.000 0.188 0.455 genre_1_New Age 0.2391 0.062 3.863 0.000 0.118 0.360 genre_2_Jazz 0.1958 0.043 4.533 0.000 0.111 0.280 genre_2_Pop 0.0819 0.016 4.999 0.000 0.050 0.114 genre_3_Rock -0.0849 0.020 -4.290 0.000 -0.124 -0.046 mood_1_Cool -0.1212 0.035 -3.464 0.001 -0.190 -0.053 mood_1_Gritty -0.1494 0.044 -3.386 0.001 -0.236 -0.063 mood_1_Easygoing -0.2261 0.074 -3.056 0.002 -0.371 -0.081 genre_3_Dance & House -0.0910 0.025 -3.595 0.000 -0.141 -0.041 mood_1_Excited 0.0583 0.018 3.248 0.001 0.023 0.093 summ . tables [ 0 ] Logit Regression Results Dep. Variable: y No. Observations: 403366 Model: Logit Df Residuals: 403309 Method: MLE Df Model: 56 Date: Sun, 24 Apr 2022 Pseudo R-squ.: 0.04795 Time: 18:07:32 Log-Likelihood: -1.2475e+05 converged: True LL-Null: -1.3104e+05 Covariance Type: nonrobust LLR p-value: 0.000 basemodel = pd . read_csv ( \"../../scripts/new_basemodel.csv\" , index_col = 0 ) y = df [ 'new_success' ] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns display ( res_df ) res_df . to_excel ( \"new_success_log_model.xlsx\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] intercept -2.4336 0.012 -201.725 0.000 -2.457 -2.410 genre_3_- -0.6766 0.025 -27.158 0.000 -0.725 -0.628 n_albums 0.1399 0.015 9.597 0.000 0.111 0.169 genre_1_Indie Rock 0.2702 0.016 17.240 0.000 0.240 0.301 mood_1_Defiant 0.2505 0.018 14.035 0.000 0.215 0.285 genre_1_Dance & House 0.3042 0.021 14.388 0.000 0.263 0.346 mood_1_Excited 0.1917 0.017 11.607 0.000 0.159 0.224 mood_1_Upbeat 0.2698 0.028 9.713 0.000 0.215 0.324 genre_2_Indie Rock 0.1527 0.019 7.854 0.000 0.115 0.191 genre_1_Rap 0.1876 0.019 9.843 0.000 0.150 0.225 genre_1_Religious 0.2676 0.030 8.877 0.000 0.209 0.327 mood_2_Romantic -0.2858 0.044 -6.533 0.000 -0.372 -0.200 mood_1_Yearning 0.1965 0.020 9.809 0.000 0.157 0.236 mood_1_Romantic -0.2540 0.045 -5.620 0.000 -0.343 -0.165 mood_3_Romantic -0.2249 0.042 -5.304 0.000 -0.308 -0.142 mood_1_Other -0.6658 0.134 -4.954 0.000 -0.929 -0.402 mood_2_Yearning 0.1714 0.019 9.044 0.000 0.134 0.209 mood_3_Yearning 0.1290 0.019 6.682 0.000 0.091 0.167 mood_2_Defiant 0.1263 0.019 6.645 0.000 0.089 0.164 mood_2_Excited 0.1043 0.018 5.871 0.000 0.069 0.139 genre_1_Electronica 0.1490 0.030 5.018 0.000 0.091 0.207 n_artists -0.0723 0.015 -4.776 0.000 -0.102 -0.043 mood_3_Urgent -0.1036 0.022 -4.766 0.000 -0.146 -0.061","title":"Data Science Foundations <br> X4: Spotify"},{"location":"extras/X4_Spotify/#data-science-foundations-x4-spotify","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Prompt: What makes a playlist successful?","title":"Data Science Foundations  X4: Spotify"},{"location":"extras/X4_Spotify/#data-description","text":"Column Name Description playlist_uri The key, Spotify uri of the playlist owner Playlist owner, Spotify username streams Number of streams from the playlist today stream30s Number of streams over 30 seconds from playlist today dau Number of Daily Active Users, i.e. users with a stream over 30 seconds from playlist today wau Number of Weekly Active Users, i.e. users with a stream over 30 seconds from playlist in past week mau Number of Monthly Active Users, i.e. users with a stream over 30 seconds from playlist in the past month mau_previous_months Number of Monthly Active users in the month prior to this one mau_both_months Number of users that were active on the playlist both this and the previous month users Number of users streaming (all streams) from this playlist this month skippers Number of users who skipped more than 90 percent of their streams today owner_country Country of the playlist owner n_tracks Number of tracks in playlist n_local_tracks Change in number of tracks on playlist since yesterday n_artists Number of unique artists in playlist n_albums Number of unique albums in playlist monthly_stream30s Number of streams over 30 seconds this month monthly_owner_stream30s Number of streams over 30 seconds by playlist owner this month tokens List of playlist title tokens, stopwords and punctuation removed genre_1 No. 1 Genre by weight of playlist tracks, from Gracenote metadata genre_2 No. 2 Genre by weight of playlist tracks, from Gracenote metadata genre_3 No. 3 Genre by weight of playlist tracks, from Gracenote metadata mood_1 No. 1 Mood by weight of playlist tracks, from Gracenote metadata mood_2 No. 2 Mood by weight of playlist tracks, from Gracenote metadata mood_3 No. 3 Mood by weight of playlist tracks, from Gracenote metadata","title":"Data Description"},{"location":"extras/X4_Spotify/#imports","text":"# basic packages import pandas as pd pd . set_option ( 'display.max_columns' , 500 ) import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.discrete.discrete_model import Logit from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.utils.class_weight import compute_class_weight # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score df = pd . read_csv ( \"../../data/playlist_summary_external-4.txt\" , delimiter = ' \\t ' )","title":"Imports"},{"location":"extras/X4_Spotify/#dependency","text":"sub_targets = [ 'mau_previous_month' , 'mau_both_months' , 'monthly_stream30s' , 'stream30s' ] # sub_targets = ['mau', 'dau', 'monthly_stream30s', 'stream30s'] des_features = [ 'mood_1' , 'mood_2' , 'mood_3' , 'genre_1' , 'genre_2' , 'genre_3' ] con_features = [ 'n_albums' , 'n_artists' , 'n_tracks' , 'n_local_tracks' ] df [ 'success' ] = df [ sub_targets ] . apply ( lambda x : x > np . quantile ( x , 0.75 )) . all ( axis = 1 ) df [ 'listen_conversions' ] = df [ 'stream30s' ] / df [ 'streams' ] df [ 'listen_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'user_retention' ] = df [ 'mau_both_months' ] / df [ 'mau_previous_month' ] df [ 'user_retention' ] . fillna ( value = 0 , inplace = True ) df [ 'user_conversions' ] = df [ 'mau' ] / df [ 'users' ] df [ 'user_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growing' ] = df [ 'mau' ] > df [ 'mau_previous_month' ] df [ 'mau_growth' ] = df [ 'mau' ] / df [ 'mau_previous_month' ] df [ 'mau_growth' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growth' ] . replace ([ np . inf , - np . inf ], 1 , inplace = True ) new_metrics = [ 'listen_conversions' , 'user_conversions' , 'user_retention' , 'mau_growth' ] df [ 'new_success' ] = df [ new_metrics ] . apply ( lambda x : ( x > 0.5 ) if ( max ( x ) == 1 ) else ( x > 1 )) . all ( axis = 1 )","title":"Dependency"},{"location":"extras/X4_Spotify/#discrete","text":"fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 10 )) dff = pd . DataFrame ( df [ des_features [ 0 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 1 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 2 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'mood' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'mood' , x = 'count' , orient = 'h' , ax = ax [ 0 ]) dff = pd . DataFrame ( df [ des_features [ 3 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 4 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 5 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'genre' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'genre' , x = 'count' , orient = 'h' , ax = ax [ 1 ]) plt . tight_layout () fig . savefig ( \"discrete_rank_bar_plot.svg\" ) def make_chisum ( target = 'success' ): chidf = pd . DataFrame () chidf [ target ] = df [ target ] chisum = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind in des_features : chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) return chisum def make_cat_plots ( target = 'success' , ind_feature = 'genre_1' ): fig , ax = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 ), sharex = 'col' , sharey = 'row' ) genre_list = chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False )[ 'group' ] . values dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'percent' ] dff = dff . reset_index () dff . loc [ dff [ target ] == True , 'percent' ] = dff . loc [ dff [ target ] == True , 'percent' ] / dff . loc [ dff [ target ] == True , 'percent' ] . sum () dff . loc [ dff [ target ] == False , 'percent' ] = dff . loc [ dff [ target ] == False , 'percent' ] / dff . loc [ dff [ target ] == False , 'percent' ] . sum () dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 0 , 0 ]) ax [ 0 , 0 ] . set_title ( 'Best and Worst Genres, Percent' ) ax [ 0 , 0 ] . set_ylabel ( '' ) ax [ 0 , 0 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 1 , 0 ]) ax [ 1 , 0 ] . set_ylabel ( '' ) dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'count' ] dff = dff . reset_index () dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 0 , 1 ]) ax [ 0 , 1 ] . set_title ( 'Best and Worst Genres, Count' ) ax [ 0 , 1 ] . set_ylabel ( '' ) ax [ 0 , 1 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 1 , 1 ]) ax [ 1 , 1 ] . set_ylabel ( '' ) plt . tight_layout () ax [ 0 , 0 ] . get_legend () . remove () ax [ 1 , 1 ] . get_legend () . remove () ax [ 1 , 0 ] . get_legend () . remove () ax [ 0 , 1 ] . legend ( framealpha = 1 , facecolor = 'white' , title = \"Success\" ) return fig ind_feature = 'genre_1' target = 'success' chisum = make_chisum ( target ) fig = make_cat_plots ( target , ind_feature ) chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False ) . to_excel ( f ' { target } _ { ind_feature } _categorical.xlsx' ) fig . savefig ( f \" { target } _ { ind_feature } _categorical.svg\" ) ind_feature = 'mood_1' target = 'success' chisum = make_chisum ( target ) fig = make_cat_plots ( target , ind_feature ) chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False ) . to_excel ( f ' { target } _ { ind_feature } _categorical.xlsx' ) fig . savefig ( f \" { target } _ { ind_feature } _categorical.svg\" )","title":"Discrete"},{"location":"extras/X4_Spotify/#continuous","text":"def make_con_plots ( target , con_features ): fig , (( ax1 , ax2 ),( ax3 , ax4 )) = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) chidf = pd . DataFrame () chidf [ target ] = df [ target ] welchsum = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind , ax in zip ( con_features , [ ax1 , ax2 , ax3 , ax4 ]): chidf [ ind ] = df [ ind ] a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } == True\" , ax = ax ) sns . histplot ( b , label = f \" { target } == False\" , ax = ax ) ax . set_title ( ind ) welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) ax . legend () return fig , welchsum target = 'new_success' fig , welchsum = make_con_plots ( target , con_features ) welchsum . to_excel ( f \" { target } _continuous.xlsx\" ) fig . savefig ( f \" { target } _ttest.svg\" )","title":"Continuous"},{"location":"extras/X4_Spotify/#models","text":"","title":"Models"},{"location":"extras/X4_Spotify/#logistic-regression","text":"### y target = \"success\" print ( target ) y = df [ target ] . values #### X X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( np . ones (( X_con . shape [ 0 ], 1 )), X_con , X_cat )) feature_names = [ 'intercept' ] + con_features + list ( enc . get_feature_names_out ()) data = pd . DataFrame ( X , columns = feature_names ) success def add_feature ( feature_names , basemodel , data , y , r2max = 0 , model = 'linear' , disp = 0 ): feature_max = None bestsum = None newmodel = None for feature in feature_names : basemodel [ feature ] = data [ feature ] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () score = float ( str ( pd . DataFrame ( summ . tables [ 0 ]) . loc [ 3 , 3 ])) if ( score > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = score feature_max = feature bestsum = est2 . summary () newmodel = basemodel . copy () if disp == 1 : print ( f \"new r2max, { feature_max } , { r2max } \" ) basemodel . drop ( labels = feature , axis = 1 , inplace = True ) return r2max , feature_max , bestsum , newmodel candidates = feature_names . copy () basemodel = pd . DataFrame () r2max = 0 while True : newr2max , feature_max , bestsum , newmodel = add_feature ( feature_names = candidates , basemodel = basemodel , data = data , y = y , r2max = r2max ) if newr2max > r2max : r2max = newr2max print ( f \"new r2max, { feature_max } , { r2max } \" ) with open ( \"success_fwd_selection_results.txt\" , \"a+\" ) as f : file_data = f . read () f . write ( f \"new r2max, { feature_max } , { r2max } \\n \" ) candidates . remove ( feature_max ) with open ( \"success_canidates.txt\" , \"w+\" ) as f : file_data = f . read () for i in candidates : f . write ( f \" { i } \\n \" ) basemodel = newmodel basemodel . to_csv ( \"success_basemodel.csv\" ) continue else : break basemodel = pd . read_csv ( \"success_basemodel.csv\" , index_col = 0 ) X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns display ( res_df ) res_df . to_excel ( \"success_log.xlsx\" ) ### y target = \"monthly_stream30s\" print ( target ) y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = 0 names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = 1 weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels basemodel = pd . read_csv ( \"../../scripts/basemodel.csv\" , index_col = 0 ) X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns display ( res_df ) res_df . to_excel ( \"log_model_monthly_stream30s.xlsx\" ) monthly_stream30s .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] intercept -2.0737 0.016 -133.150 0.000 -2.104 -2.043 n_albums 0.2656 0.012 21.287 0.000 0.241 0.290 genre_1_Latin 0.5408 0.027 19.906 0.000 0.488 0.594 genre_1_Indie Rock -0.5832 0.020 -28.964 0.000 -0.623 -0.544 genre_1_Rap -0.3259 0.020 -16.697 0.000 -0.364 -0.288 genre_1_Dance & House -0.3034 0.027 -11.069 0.000 -0.357 -0.250 genre_1_Rock -0.4226 0.025 -16.996 0.000 -0.471 -0.374 mood_1_Energizing -0.2844 0.027 -10.670 0.000 -0.337 -0.232 genre_1_Children's 0.7845 0.062 12.608 0.000 0.663 0.906 mood_1_Tender 0.4943 0.055 9.032 0.000 0.387 0.602 mood_1_Other 0.6206 0.074 8.413 0.000 0.476 0.765 n_tracks 0.0462 0.006 7.613 0.000 0.034 0.058 mood_1_Peaceful 0.6294 0.060 10.426 0.000 0.511 0.748 mood_1_Romantic 0.2981 0.033 9.038 0.000 0.233 0.363 genre_1_Electronica -0.2326 0.034 -6.792 0.000 -0.300 -0.165 genre_2_Indie Rock -0.2050 0.023 -8.998 0.000 -0.250 -0.160 mood_2_Energizing -0.1384 0.019 -7.421 0.000 -0.175 -0.102 genre_1_R&B -0.2335 0.030 -7.696 0.000 -0.293 -0.174 genre_3_Indie Rock -0.2540 0.024 -10.792 0.000 -0.300 -0.208 genre_1_Classical -0.5126 0.060 -8.609 0.000 -0.629 -0.396 genre_2_Alternative 0.1769 0.019 9.542 0.000 0.141 0.213 genre_2_Metal 0.4257 0.040 10.738 0.000 0.348 0.503 mood_2_Peaceful 0.3761 0.055 6.856 0.000 0.269 0.484 mood_2_Romantic 0.2300 0.031 7.414 0.000 0.169 0.291 mood_3_Romantic 0.2597 0.031 8.252 0.000 0.198 0.321 genre_3_Alternative 0.0482 0.019 2.529 0.011 0.011 0.086 n_artists 0.0954 0.013 7.464 0.000 0.070 0.120 genre_1_Metal 0.4049 0.042 9.680 0.000 0.323 0.487 mood_1_Aggressive -0.2660 0.042 -6.275 0.000 -0.349 -0.183 mood_3_Peaceful 0.2912 0.058 4.983 0.000 0.177 0.406 mood_1_Empowering 0.1197 0.021 5.789 0.000 0.079 0.160 genre_1_Religious -0.2328 0.033 -7.154 0.000 -0.297 -0.169 genre_3_Metal 0.1978 0.044 4.527 0.000 0.112 0.283 genre_3_R&B -0.1897 0.024 -8.057 0.000 -0.236 -0.144 mood_3_Yearning 0.1176 0.019 6.096 0.000 0.080 0.155 mood_2_- 0.4272 0.074 5.772 0.000 0.282 0.572 genre_3_Electronica -0.1893 0.026 -7.408 0.000 -0.239 -0.139 genre_2_Latin 0.3700 0.062 5.959 0.000 0.248 0.492 mood_3_Empowering 0.0909 0.021 4.386 0.000 0.050 0.132 genre_3_- -0.1084 0.021 -5.104 0.000 -0.150 -0.067 genre_1_Spoken & Audio 0.4897 0.089 5.489 0.000 0.315 0.665 genre_2_New Age 0.3718 0.067 5.546 0.000 0.240 0.503 genre_3_New Age 0.3384 0.067 5.053 0.000 0.207 0.470 genre_3_Rap -0.1484 0.026 -5.791 0.000 -0.199 -0.098 mood_1_Rowdy -0.2223 0.051 -4.373 0.000 -0.322 -0.123 mood_2_Rowdy -0.1655 0.039 -4.267 0.000 -0.242 -0.089 mood_2_Aggressive -0.1323 0.030 -4.345 0.000 -0.192 -0.073 genre_2_Spoken & Audio 0.3211 0.068 4.717 0.000 0.188 0.455 genre_1_New Age 0.2391 0.062 3.863 0.000 0.118 0.360 genre_2_Jazz 0.1958 0.043 4.533 0.000 0.111 0.280 genre_2_Pop 0.0819 0.016 4.999 0.000 0.050 0.114 genre_3_Rock -0.0849 0.020 -4.290 0.000 -0.124 -0.046 mood_1_Cool -0.1212 0.035 -3.464 0.001 -0.190 -0.053 mood_1_Gritty -0.1494 0.044 -3.386 0.001 -0.236 -0.063 mood_1_Easygoing -0.2261 0.074 -3.056 0.002 -0.371 -0.081 genre_3_Dance & House -0.0910 0.025 -3.595 0.000 -0.141 -0.041 mood_1_Excited 0.0583 0.018 3.248 0.001 0.023 0.093 summ . tables [ 0 ] Logit Regression Results Dep. Variable: y No. Observations: 403366 Model: Logit Df Residuals: 403309 Method: MLE Df Model: 56 Date: Sun, 24 Apr 2022 Pseudo R-squ.: 0.04795 Time: 18:07:32 Log-Likelihood: -1.2475e+05 converged: True LL-Null: -1.3104e+05 Covariance Type: nonrobust LLR p-value: 0.000 basemodel = pd . read_csv ( \"../../scripts/new_basemodel.csv\" , index_col = 0 ) y = df [ 'new_success' ] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns display ( res_df ) res_df . to_excel ( \"new_success_log_model.xlsx\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] intercept -2.4336 0.012 -201.725 0.000 -2.457 -2.410 genre_3_- -0.6766 0.025 -27.158 0.000 -0.725 -0.628 n_albums 0.1399 0.015 9.597 0.000 0.111 0.169 genre_1_Indie Rock 0.2702 0.016 17.240 0.000 0.240 0.301 mood_1_Defiant 0.2505 0.018 14.035 0.000 0.215 0.285 genre_1_Dance & House 0.3042 0.021 14.388 0.000 0.263 0.346 mood_1_Excited 0.1917 0.017 11.607 0.000 0.159 0.224 mood_1_Upbeat 0.2698 0.028 9.713 0.000 0.215 0.324 genre_2_Indie Rock 0.1527 0.019 7.854 0.000 0.115 0.191 genre_1_Rap 0.1876 0.019 9.843 0.000 0.150 0.225 genre_1_Religious 0.2676 0.030 8.877 0.000 0.209 0.327 mood_2_Romantic -0.2858 0.044 -6.533 0.000 -0.372 -0.200 mood_1_Yearning 0.1965 0.020 9.809 0.000 0.157 0.236 mood_1_Romantic -0.2540 0.045 -5.620 0.000 -0.343 -0.165 mood_3_Romantic -0.2249 0.042 -5.304 0.000 -0.308 -0.142 mood_1_Other -0.6658 0.134 -4.954 0.000 -0.929 -0.402 mood_2_Yearning 0.1714 0.019 9.044 0.000 0.134 0.209 mood_3_Yearning 0.1290 0.019 6.682 0.000 0.091 0.167 mood_2_Defiant 0.1263 0.019 6.645 0.000 0.089 0.164 mood_2_Excited 0.1043 0.018 5.871 0.000 0.069 0.139 genre_1_Electronica 0.1490 0.030 5.018 0.000 0.091 0.207 n_artists -0.0723 0.015 -4.776 0.000 -0.102 -0.043 mood_3_Urgent -0.1036 0.022 -4.766 0.000 -0.146 -0.061","title":"Logistic Regression"},{"location":"extras/X4_Spotify_Appendix/","text":"Data Science Foundations X4: Spotify \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Prompt: What makes a playlist successful? Deck: PDF What makes a playlist successful? \u00b6 Analysis Simple metric (dependent variable) mau_previous_month mau_both_months monthly_stream30s stream30s Design metric (dependent variable) 30s listens/tot listens (listen conversions) Users both months/users prev month (user conversions) Best small time performers (less than X total monthly listens + high conversion) Best new user playlist (owner has only 1 popular playlist) Define \"top\" Top 10% mau_previous_month: 9.0 mau_both_months: 2.0 mau: 9.0 monthly_stream30s: 432.0 stream30s: 17.0 Top 1% mau_previous_month: 130.0 mau_both_months: 19.0 mau: 143.0 monthly_stream30s: 2843.0 stream30s: 113.0 Independent variables moods and genres (categorical) number of tracks, albums, artists, and local tracks (continuous) The analysis will consist of: understand the distribution characteristics of the dependent and independent variables quantify the dependency of the dependent/independent variables for each of the simple and design metrics chi-square test bootstrap/t-test Key Conclusions for the simple metrics, what I define as \"popularlity\" key genres and moods were Romantic, Latin, Children's, Lively, Traditional, and Jazz . Playlists that included these genres/moods had a positive multiplier effect (usually in the vicinicty of 2x more likely) on the key simple metric (i.e. playlists with latin as a primary genre were 2.5x more likely to be in the top 10% of streams longer than 30 seconds) for the design metrics, what I define as \"trendiness\" some of the key genres and moods become flipped in comparison to the relationship with popular playlists. In particular, Dance & House, Indie Rock, and Defiant rise to the top as labels that push a playlist into the trendy category Column Name Description playlist_uri The key, Spotify uri of the playlist owner Playlist owner, Spotify username streams Number of streams from the playlist today stream30s Number of streams over 30 seconds from playlist today dau Number of Daily Active Users, i.e. users with a stream over 30 seconds from playlist today wau Number of Weekly Active Users, i.e. users with a stream over 30 seconds from playlist in past week mau Number of Monthly Active Users, i.e. users with a stream over 30 seconds from playlist in the past month mau_previous_months Number of Monthly Active users in the month prior to this one mau_both_months Number of users that were active on the playlist both this and the previous month users Number of users streaming (all streams) from this playlist this month skippers Number of users who skipped more than 90 percent of their streams today owner_country Country of the playlist owner n_tracks Number of tracks in playlist n_local_tracks Change in number of tracks on playlist since yesterday n_artists Number of unique artists in playlist n_albums Number of unique albums in playlist monthly_stream30s Number of streams over 30 seconds this month monthly_owner_stream30s Number of streams over 30 seconds by playlist owner this month tokens List of playlist title tokens, stopwords and punctuation removed genre_1 No. 1 Genre by weight of playlist tracks, from Gracenote metadata genre_2 No. 2 Genre by weight of playlist tracks, from Gracenote metadata genre_3 No. 3 Genre by weight of playlist tracks, from Gracenote metadata mood_1 No. 1 Mood by weight of playlist tracks, from Gracenote metadata mood_2 No. 2 Mood by weight of playlist tracks, from Gracenote metadata mood_3 No. 3 Mood by weight of playlist tracks, from Gracenote metadata Imports \u00b6 # basic packages import pandas as pd pd . set_option ( 'display.max_columns' , 500 ) import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.discrete.discrete_model import Logit from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.utils.class_weight import compute_class_weight # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score df = pd . read_csv ( \"../../data/playlist_summary_external-4.txt\" , delimiter = ' \\t ' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playlist_uri owner streams stream30s dau wau mau mau_previous_month mau_both_months users skippers owner_country n_tracks n_local_tracks n_artists n_albums monthly_stream30s monthly_owner_stream30s tokens genre_1 genre_2 genre_3 mood_1 mood_2 mood_3 0 spotify:user:36069af6af076ccd9e597184a67b68c9:... 36069af6af076ccd9e597184a67b68c9 27 27 1 1 3 3 0 8 0 US 52 0 4 7 30 27 [\"ambient\", \"music\", \"therapy\", \"binaural\", \"b... Dance & House New Age Country & Folk Peaceful Romantic Somber 1 spotify:user:d1144a65b1c31c5f9f56b94f831124d5:... d1144a65b1c31c5f9f56b94f831124d5 0 0 0 1 2 1 1 3 0 US 131 0 112 113 112 94 [\"good\", \"living\"] Pop Indie Rock Alternative Excited Yearning Defiant 2 spotify:user:6b7fbed9edd6418ddd3b555bba441536:... 6b7fbed9edd6418ddd3b555bba441536 4 2 1 1 7 5 0 15 0 US 43 0 35 36 63 0 [\"norte\\u00f1a\"] Latin - - Lively Upbeat Romantic 3 spotify:user:580b98725077a94c3c8d01d07390426b:... 580b98725077a94c3c8d01d07390426b 12 12 1 1 4 6 1 10 0 US 27 1 27 26 154 108 [] Dance & House Electronica Pop Excited Aggressive Defiant 4 spotify:user:1305d39070c95d161cc502e15014897d:... 1305d39070c95d161cc502e15014897d 20 4 1 1 2 1 1 2 1 US 52 0 47 51 230 0 [\"cheesy\", \"pants\"] Indie Rock Alternative Electronica Excited Defiant Yearning df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playlist_uri owner streams stream30s dau wau mau mau_previous_month mau_both_months users skippers owner_country n_tracks n_local_tracks n_artists n_albums monthly_stream30s monthly_owner_stream30s tokens genre_1 genre_2 genre_3 mood_1 mood_2 mood_3 403361 spotify:user:4672952d42bdd93b9215ce9a40394ea6:... 4672952d42bdd93b9215ce9a40394ea6 18 6 2 6 13 12 8 20 1 US 48 0 44 48 464 43 [\"discover\", \"mix\"] Indie Rock Alternative Dance & House Excited Yearning Energizing 403362 spotify:user:28c4378e099b4843f5dd42bb848c78ea:... 28c4378e099b4843f5dd42bb848c78ea 0 0 0 0 2 1 1 3 0 US 182 27 114 129 44 14 [\"ambient\", \"study\", \"music\"] Electronica Dance & House Rap Sensual Excited Brooding 403363 spotify:user:1c54302dc7e610a10c51eed81e26a168:... 1c54302dc7e610a10c51eed81e26a168 0 0 0 2 2 0 0 2 0 US 36 0 16 15 82 80 [\"october\"] Rap Indie Rock Alternative Brooding Defiant Sophisticated 403364 spotify:user:adc973443cdf1abecdfb4244e530d451:... adc973443cdf1abecdfb4244e530d451 0 0 0 0 2 0 0 2 0 US 50 0 25 25 2 0 [] Rap R&B Latin Defiant Energizing Aggressive 403365 spotify:user:b3752c94e387192b7950b687453bcf45:... b3752c94e387192b7950b687453bcf45 74 16 1 1 2 1 1 3 1 US 348 10 281 290 216 178 [\"eclecticism\"] Rap Rock Alternative Defiant Energizing Cool df . sort_values ( 'users' , ascending = False ) . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playlist_uri owner streams stream30s dau wau mau mau_previous_month mau_both_months users skippers owner_country n_tracks n_local_tracks n_artists n_albums monthly_stream30s monthly_owner_stream30s tokens genre_1 genre_2 genre_3 mood_1 mood_2 mood_3 152032 spotify:user:spotify:playlist:5FJXhjdILmRA2z5b... spotify 2527075 1461324 152929 669966 1944150 1478684 578391 3455406 86162 US 51 0 51 51 42497334 22 [\"top\", \"hits\"] Pop R&B Dance & House Excited Cool Brooding 163726 spotify:user:spotify:playlist:4hOKQuZbraPDIfaG... spotify 2629715 1513237 122005 514627 1453097 970905 364140 2448881 56707 US 100 0 93 86 40722305 0 [\"top\", \"tracks\", \"currently\", \"spotify\"] Pop Dance & House Indie Rock Excited Defiant Energizing 216752 spotify:user:spotify:playlist:3ZgmfR6lsnCwdffZ... spotify 735281 348391 43498 219817 688999 365968 109688 1233952 34145 US 100 0 100 99 9879201 0 [\"top\", \"pop\", \"tracks\", \"spotify\"] Pop R&B Rap Excited Defiant Empowering 401060 spotify:user:spotify:playlist:3MlpudZs4HT3i0yG... spotify 505876 245377 33152 121173 430129 339921 79443 973788 23846 US 43 0 41 42 5567649 44 [\"teen\", \"party\"] Pop R&B Rap Excited Yearning Urgent 307283 spotify:user:spotify:playlist:04MJzJlzOoy5bTyt... spotify 252309 124903 16480 68518 278966 448102 75371 917174 11888 US 296 0 1 1 4178965 8 [\"dance\", \"mega\", \"mix\"] Dance & House Electronica Pop Excited Aggressive Energizing df . iloc [ 403361 , 0 ] 'spotify:user:4672952d42bdd93b9215ce9a40394ea6:playlist:6W45lqDBZ1TKma71Uu2F5x' df . columns Index(['playlist_uri', 'owner', 'streams', 'stream30s', 'dau', 'wau', 'mau', 'mau_previous_month', 'mau_both_months', 'users', 'skippers', 'owner_country', 'n_tracks', 'n_local_tracks', 'n_artists', 'n_albums', 'monthly_stream30s', 'monthly_owner_stream30s', 'tokens', 'genre_1', 'genre_2', 'genre_3', 'mood_1', 'mood_2', 'mood_3'], dtype='object') id = [ df . columns [ 0 ], df . columns [ 1 ]] targets = list ( df . columns [ 2 : 11 ]) + [ \"monthly_stream30s\" , \"monthly_owner_stream30s\" ] features = set ( df . columns ) - set ( targets ) - set ( id ) features = list ( features ) print ( f \"id columns: { id } \" ) print ( f \"target columns: { targets } \" ) print ( f \"feature columns: { features } \" ) id columns: ['playlist_uri', 'owner'] target columns: ['streams', 'stream30s', 'dau', 'wau', 'mau', 'mau_previous_month', 'mau_both_months', 'users', 'skippers', 'monthly_stream30s', 'monthly_owner_stream30s'] feature columns: ['n_albums', 'n_artists', 'mood_1', 'n_tracks', 'mood_3', 'genre_1', 'genre_2', 'genre_3', 'tokens', 'owner_country', 'n_local_tracks', 'mood_2'] stream30s , dau , wau , mau , monthly_stream30s , monthly_owner_stream30s , mau_previous_months and mau_both_months are all specifically for users who have streamed the playlist for over 30 seconds Let's make the north star metric mau_previous_month - tells us how many users have streamed over 30 seconds from the playlist this past month downside : since we don't know when the playlist was created, we may falsely label some playlists as having low rate of success Let's make a guardrail metric mau_both_months - tells us if the playlist has replay value downside : since we don't know when the playlist was created, we don't know at what stage the playlist is in its lifetime, i.e. do users fall off from months 1-2 or months 10-11 stream30s - number of streams over 30 seconds today; tells us demand of playlist unormalized by number of users accessing the stream (i.e. some users will stream multiple times) downside - a few users can dominate the overall number of listens monthly_stream30s - number of streams over 30 seconds for the month; will give us a longer term comparison between streams downside - playlists created at some point in the month will be compared unequally Secondary metric monthly_owner_stream30s - tells us if the owner or the playlist is significant in making a successful playlist; semi-feature column sub_targets = [ 'mau_previous_month' , 'mau_both_months' , 'mau' , 'monthly_stream30s' , 'stream30s' ] Depenent Variable \u00b6 it looks like mau may be from an incomplete month (comparing the frequency to mau_previous_months ) df [ targets ] . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } streams stream30s dau wau mau mau_previous_month mau_both_months users skippers monthly_stream30s monthly_owner_stream30s streams 1.000000 0.994380 0.988381 0.967860 0.958000 0.905523 0.957998 0.911023 0.948062 0.984383 -0.001338 stream30s 0.994380 1.000000 0.985062 0.968307 0.957810 0.908967 0.956223 0.912391 0.937712 0.992060 -0.000767 dau 0.988381 0.985062 1.000000 0.986290 0.981306 0.938572 0.975665 0.946317 0.980372 0.980044 -0.003330 wau 0.967860 0.968307 0.986290 1.000000 0.995568 0.957752 0.974101 0.970788 0.976330 0.978300 -0.004150 mau 0.958000 0.957810 0.981306 0.995568 1.000000 0.969613 0.969983 0.983961 0.980052 0.970658 -0.004432 mau_previous_month 0.905523 0.908967 0.938572 0.957752 0.969613 1.000000 0.954992 0.990228 0.943692 0.931162 -0.004802 mau_both_months 0.957998 0.956223 0.975665 0.974101 0.969983 0.954992 1.000000 0.942426 0.951045 0.971727 -0.003219 users 0.911023 0.912391 0.946317 0.970788 0.983961 0.990228 0.942426 1.000000 0.963877 0.931219 -0.005115 skippers 0.948062 0.937712 0.980372 0.976330 0.980052 0.943692 0.951045 0.963877 1.000000 0.935228 -0.004150 monthly_stream30s 0.984383 0.992060 0.980044 0.978300 0.970658 0.931162 0.971727 0.931219 0.935228 1.000000 -0.000519 monthly_owner_stream30s -0.001338 -0.000767 -0.003330 -0.004150 -0.004432 -0.004802 -0.003219 -0.005115 -0.004150 -0.000519 1.000000 df . plot ( x = 'mau' , y = 'mau_previous_month' , ls = '' , marker = '.' ) <AxesSubplot:xlabel='mau'> df . plot ( x = 'mau' , y = 'dau' , ls = '' , marker = '.' ) <AxesSubplot:xlabel='mau'> df . plot ( x = 'mau' , y = 'wau' , ls = '' , marker = '.' ) <AxesSubplot:xlabel='mau'> df . plot ( x = 'mau' , y = 'stream30s' , ls = '' , marker = '.' ) <AxesSubplot:xlabel='mau'> df . plot ( x = 'stream30s' , y = 'monthly_owner_stream30s' , ls = '' , marker = '.' ) <AxesSubplot:xlabel='stream30s'> df . plot ( x = 'stream30s' , y = 'skippers' , ls = '' , marker = '.' ) <AxesSubplot:xlabel='stream30s'> quant = 0.99 for target in targets : cutoff = np . quantile ( df [ target ], quant ) y = df . loc [ df [ target ] < cutoff ] y . plot ( kind = 'hist' , y = target , bins = 100 ) quant = 0.997 for target in sub_targets : cutoff = np . quantile ( df [ target ], quant ) y = df . loc [ df [ target ] < cutoff ] removed = df . loc [ ~ ( df [ target ] < cutoff )] print ( f \"removed items: { removed . shape [ 0 ] } \" ) y . plot ( kind = 'hist' , y = target , bins = 100 ) plt . show () removed items: 1212 removed items: 1216 removed items: 1211 removed items: 1211 df [ sub_targets ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mau_previous_month mau_both_months monthly_stream30s stream30s count 4.033660e+05 403366.000000 4.033660e+05 4.033660e+05 mean 5.819009e+01 12.937065 1.260489e+03 4.283333e+01 std 3.827248e+03 1240.912979 1.062463e+05 3.772412e+03 min 0.000000e+00 0.000000 2.000000e+00 0.000000e+00 25% 1.000000e+00 1.000000 3.100000e+01 0.000000e+00 50% 2.000000e+00 1.000000 7.900000e+01 0.000000e+00 75% 3.000000e+00 2.000000 1.930000e+02 5.000000e+00 max 1.478684e+06 578391.000000 4.249733e+07 1.513237e+06 Independent Variable \u00b6 features ['n_albums', 'n_artists', 'mood_1', 'n_tracks', 'mood_3', 'genre_1', 'genre_2', 'genre_3', 'tokens', 'owner_country', 'n_local_tracks', 'mood_2'] df [ features ] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_albums n_artists mood_1 n_tracks mood_3 genre_1 genre_2 genre_3 tokens owner_country n_local_tracks mood_2 0 7 4 Peaceful 52 Somber Dance & House New Age Country & Folk [\"ambient\", \"music\", \"therapy\", \"binaural\", \"b... US 0 Romantic 1 113 112 Excited 131 Defiant Pop Indie Rock Alternative [\"good\", \"living\"] US 0 Yearning 2 36 35 Lively 43 Romantic Latin - - [\"norte\\u00f1a\"] US 0 Upbeat 3 26 27 Excited 27 Defiant Dance & House Electronica Pop [] US 1 Aggressive 4 51 47 Excited 52 Yearning Indie Rock Alternative Electronica [\"cheesy\", \"pants\"] US 0 Defiant con_features = list ( df [ features ] . select_dtypes ( 'number' ) . columns ) print ( con_features ) des_features = list ( df [ features ] . select_dtypes ( 'object' ) . columns ) print ( des_features ) ['n_albums', 'n_artists', 'n_tracks', 'n_local_tracks'] ['mood_1', 'mood_3', 'genre_1', 'genre_2', 'genre_3', 'tokens', 'owner_country', 'mood_2'] df [ des_features ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mood_1 mood_3 genre_1 genre_2 genre_3 tokens owner_country mood_2 count 403366 403366 403366 403366 403366 403366 403366 403366 unique 27 27 26 26 26 192107 1 27 top Defiant Energizing Indie Rock Alternative Pop [] US Energizing freq 81079 56450 70571 66252 78758 32568 403366 51643 we will go ahead and remove owner_country (1 unique), owner , and tokens (cardinal) from our feature analysis id = [ df . columns [ 0 ]] targets = list ( df . columns [ 2 : 11 ]) + [ \"monthly_stream30s\" , \"monthly_owner_stream30s\" ] features = set ( df . columns ) - set ( targets ) - set ( id ) - set ([ \"owner_country\" , \"owner\" , \"tokens\" ]) features = list ( features ) print ( f \"id columns: { id } \" ) print ( f \"target columns: { targets } \" ) print ( f \"feature columns: { features } \" ) con_features = list ( df [ features ] . select_dtypes ( 'number' ) . columns ) print ( con_features ) des_features = [ 'mood_1' , 'mood_2' , 'mood_3' , 'genre_1' , 'genre_2' , 'genre_3' ] print ( des_features ) id columns: ['playlist_uri'] target columns: ['streams', 'stream30s', 'dau', 'wau', 'mau', 'mau_previous_month', 'mau_both_months', 'users', 'skippers', 'monthly_stream30s', 'monthly_owner_stream30s'] feature columns: ['n_albums', 'mood_1', 'n_artists', 'n_tracks', 'mood_3', 'genre_1', 'genre_2', 'genre_3', 'n_local_tracks', 'mood_2'] ['n_albums', 'n_artists', 'n_tracks', 'n_local_tracks'] ['mood_1', 'mood_2', 'mood_3', 'genre_1', 'genre_2', 'genre_3'] Discrete Features \u00b6 df [ des_features ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mood_1 mood_2 mood_3 genre_1 genre_2 genre_3 count 403366 403366 403366 403366 403366 403366 unique 27 27 27 26 26 26 top Defiant Energizing Energizing Indie Rock Alternative Pop freq 81079 51643 56450 70571 66252 78758 df . value_counts ( des_features ) mood_1 mood_2 mood_3 genre_1 genre_2 genre_3 Excited Aggressive Energizing Dance & House Electronica Pop 4824 Defiant Cool Energizing Rap R&B Pop 4458 Energizing Cool Rap R&B Pop 4003 Pop R&B 1803 Excited Rap Pop R&B 1225 ... Excited Aggressive Urgent Alternative Electronica Metal 1 Dance & House Pop 1 Upbeat Pop Soundtrack - 1 Indie Rock Alternative - 1 Yearning Urgent Upbeat Soundtrack Pop Rap 1 Length: 138379, dtype: int64 df [ des_features [: 3 ]] . value_counts () mood_1 mood_2 mood_3 Defiant Cool Energizing 15125 Energizing Cool 12278 Excited Aggressive Energizing 7564 Defiant Energizing Excited 6672 Excited Energizing 6179 ... Peaceful Urgent Yearning 1 Yearning Cool 1 Excited 1 Fiery 1 Other Urgent Aggressive 1 Length: 9326, dtype: int64 df [ des_features [ 3 :]] . value_counts () genre_1 genre_2 genre_3 Rap R&B Pop 15477 Indie Rock Alternative Rock 13102 Dance & House Electronica Pop 10800 Indie Rock Alternative Pop 9981 Electronica 7233 ... New Age Country & Folk Rock 1 Dance & House R&B 1 Rock 1 Soundtrack 1 Traditional Spoken & Audio Religious 1 Length: 6664, dtype: int64 fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 10 )) dff = pd . DataFrame ( df [ des_features [ 0 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 1 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 2 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'mood' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'mood' , x = 'count' , orient = 'h' , ax = ax [ 0 ]) dff = pd . DataFrame ( df [ des_features [ 3 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 4 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 5 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'genre' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'genre' , x = 'count' , orient = 'h' , ax = ax [ 1 ]) plt . tight_layout () Continuous Features \u00b6 df [ con_features ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_albums n_artists n_tracks n_local_tracks count 403366.000000 403366.000000 403366.000000 403366.000000 mean 88.224250 83.852050 201.483432 3.084035 std 133.193118 128.152488 584.077765 40.330266 min 1.000000 1.000000 1.000000 0.000000 25% 19.000000 18.000000 38.000000 0.000000 50% 48.000000 46.000000 84.000000 0.000000 75% 106.000000 100.000000 192.000000 0.000000 max 6397.000000 5226.000000 79984.000000 9117.000000 quant = 0.999 for target in con_features : cutoff = np . quantile ( df [ target ], quant ) y = df . loc [ df [ target ] < cutoff ] removed = df . loc [ ~ ( df [ target ] < cutoff )] print ( f \"removed items: { removed . shape [ 0 ] } \" ) y . plot ( kind = 'hist' , y = target , bins = 100 , density = True ) plt . show () removed items: 404 removed items: 405 removed items: 404 removed items: 406 Bootstrapping \u00b6 An example of how we will bootstrap to perform hypothesis tests later on means = [] ind = con_features [ 0 ] for i in range ( 100 ): boot = random . sample ( list ( df . loc [ ( df [ ind ] > 9 ) & ( df [ ind ] < 999 ) ][ ind ] . values ), k = 1000 ) means . append ( np . mean ( boot )) stuff = plt . hist ( means , bins = 100 , density = True ) Dependency \u00b6 Categorical Target \u00b6 sub_targets ['mau_previous_month', 'mau_both_months', 'mau', 'monthly_stream30s', 'stream30s'] for target in sub_targets : print ( f \"p99 { target } : { np . quantile ( df [ target ], 0.99 ) } \" ) p99 mau_previous_month: 130.0 p99 mau_both_months: 19.0 p99 mau: 143.0 p99 monthly_stream30s: 2843.0 p99 stream30s: 113.0 des_features ['mood_1', 'mood_2', 'mood_3', 'genre_1', 'genre_2', 'genre_3'] Categorical Feature \u00b6 Moods \u00b6 chidf = pd . DataFrame () target = sub_targets [ 2 ] chidf [ target ] = df [ target ] print ( chidf [ target ] . median ()) moods = pd . DataFrame () cutoff = 0.001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] grp = chidf . loc [ chidf [ ind ] == grp_label ][ target ] . values chi2 , p , m , cTable = stats . median_test ( grp , pop , correction = True ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 moods = pd . concat ([ moods , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) moods . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] moods = moods . sort_values ( 'p-value' ) . reset_index ( drop = True ) 79.0 moods . loc [ moods [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable + reject null 0 genre_3 - 1725.882036 0.000000e+00 [[16033, 205049], [24090, 198317]] False True 1 genre_2 - 1104.759466 3.051013e-242 [[8216, 203517], [12990, 199849]] False True 2 genre_1 Latin 651.374931 1.122254e-143 [[9000, 199027], [6012, 204339]] True True 3 mood_1 Energizing 611.189037 6.167816e-135 [[10316, 203517], [14071, 199849]] False True 4 genre_1 Rock 315.827189 1.174487e-70 [[12514, 201911], [15563, 201455]] False True ... ... ... ... ... ... ... ... 93 mood_1 Stirring 12.333846 4.448190e-04 [[877, 200454], [1044, 202912]] False True 94 mood_1 Serious 12.316512 4.489689e-04 [[778, 200454], [935, 202912]] False True 95 mood_2 Lively 12.161071 4.879735e-04 [[2588, 200454], [2882, 202912]] False True 96 mood_2 Somber 11.618507 6.529880e-04 [[792, 200454], [946, 202912]] False True 97 genre_2 Dance & House 10.834697 9.961560e-04 [[12678, 201911], [13196, 201455]] False True 98 rows \u00d7 7 columns Chi-Square \u00b6 chidf = pd . DataFrame () target = sub_targets [ 2 ] chidf [ target ] = df [ target ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) chidf [ target ] = chidf [ target ] > tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) Categorical-Categorical Conclusions \u00b6 increasing quant_value will render additional features; as the population performance worsens, new feature/group pairs have an opportunity to increase the multiplier Best Groups chisum . loc [( chisum [ 'reject null' ] == True ) & ( chisum [ 'multiplier' ] > 2 )] . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 6 genre_1 Children's 262.624693 4.596280e-59 [[361785, 1286], [39933, 362]] 2.550270 True 11 mood_1 Other 197.598843 6.979647e-45 [[361719, 1352], [39952, 343]] 2.296943 True 19 genre_1 Spoken & Audio 120.508309 4.896128e-28 [[362147, 924], [40068, 227]] 2.220451 True 0 genre_1 Latin 1150.625294 3.280867e-252 [[350782, 12289], [37572, 2723]] 2.068731 True 12 genre_1 New Age 166.484617 4.335181e-38 [[361286, 1785], [39896, 399]] 2.024214 True Worst Groups chisum . loc [( chisum [ 'reject null' ] == True ) & ( chisum [ 'multiplier' ] < 0.8 )] . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 28 mood_2 Sensual 85.309680 2.551113e-20 [[343873, 19198], [38598, 1697]] 0.787516 True 40 genre_1 Electronica 65.249731 6.598320e-16 [[350162, 12909], [39176, 1119]] 0.774794 True 2 genre_1 Indie Rock 366.567076 1.046303e-81 [[298164, 64907], [34631, 5664]] 0.751315 True 13 genre_3 Electronica 163.908151 1.584260e-37 [[337501, 25570], [38143, 2152]] 0.744684 True 21 mood_1 Brooding 109.456909 1.288759e-25 [[346296, 16775], [38893, 1402]] 0.744152 True 48 mood_1 Gritty 49.741710 1.753777e-12 [[355800, 7271], [39695, 600]] 0.739652 True 14 mood_1 Energizing 162.542129 3.149562e-37 [[340541, 22530], [38438, 1857]] 0.730229 True 68 mood_3 Other 27.407286 1.648091e-07 [[361541, 1530], [40196, 99]] 0.581994 True We would recommend would-be superstar playlist maker construct a playlist with the following attributes: Genre 1: Children's 2.6x more likely to be in the 90 th percentile 4.8x more likely to be in the 99 th percentile Mood 1: Other 2.3x more likely to be in the 90 th percentile 2.4x more likely to be in the 99 th percentile Continuous Feature \u00b6 targets ['streams', 'stream30s', 'dau', 'wau', 'mau', 'mau_previous_month', 'mau_both_months', 'users', 'skippers', 'monthly_stream30s', 'monthly_owner_stream30s'] con_features ['n_albums', 'n_artists', 'n_tracks', 'n_local_tracks'] target = \"monthly_stream30s\" print ( target ) chidf [ target ] = df [ target ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) df . loc [ df [ target ] > tar_value ] . groupby ( 'n_albums' )[[ 'wau' ]] . mean () . plot ( ls = '' , marker = '.' , ax = ax ) ax . set_xlim ( 0 , 200 ) # ax.set_ylim(0, 100) monthly_stream30s (0.0, 200.0) t-Test \u00b6 For t tests we need to deal with the long tails in the distributions along the independent variable df [ targets ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } streams stream30s dau wau mau mau_previous_month mau_both_months users skippers monthly_stream30s monthly_owner_stream30s count 4.033660e+05 4.033660e+05 403366.000000 403366.000000 4.033660e+05 4.033660e+05 403366.000000 4.033660e+05 403366.000000 4.033660e+05 403366.000000 mean 7.101375e+01 4.283333e+01 4.418265 21.784446 6.614290e+01 5.819009e+01 12.937065 1.493085e+02 2.827749 1.260489e+03 93.556621 std 6.492014e+03 3.772412e+03 358.855685 1614.650805 4.732580e+03 3.827248e+03 1240.912979 9.247484e+03 205.059728 1.062463e+05 226.250189 min 0.000000e+00 0.000000e+00 0.000000 0.000000 2.000000e+00 0.000000e+00 0.000000 2.000000e+00 0.000000 2.000000e+00 0.000000 25% 0.000000e+00 0.000000e+00 0.000000 1.000000 2.000000e+00 1.000000e+00 1.000000 2.000000e+00 0.000000 3.100000e+01 6.000000 50% 1.000000e+00 0.000000e+00 0.000000 1.000000 2.000000e+00 2.000000e+00 1.000000 3.000000e+00 0.000000 7.900000e+01 30.000000 75% 8.000000e+00 5.000000e+00 1.000000 2.000000 4.000000e+00 3.000000e+00 2.000000 7.000000e+00 0.000000 1.930000e+02 96.000000 max 2.629715e+06 1.513237e+06 152929.000000 669966.000000 1.944150e+06 1.478684e+06 578391.000000 3.455406e+06 86162.000000 4.249733e+07 25904.000000 df . loc [ df [ 'owner' ] != 'spotify' ][ targets ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } streams stream30s dau wau mau mau_previous_month mau_both_months users skippers monthly_stream30s monthly_owner_stream30s count 402967.000000 402967.000000 402967.000000 402967.000000 402967.000000 402967.000000 402967.000000 402967.000000 402967.000000 4.029670e+05 402967.000000 mean 20.968960 11.990945 1.232421 5.275308 14.860487 13.483665 3.029327 32.824100 0.728640 3.543268e+02 93.647783 std 766.262668 404.190477 41.227771 185.706612 504.704081 548.731437 129.629183 1157.601711 27.054367 1.093559e+04 226.343585 min 0.000000 0.000000 0.000000 0.000000 2.000000 0.000000 0.000000 2.000000 0.000000 2.000000e+00 0.000000 25% 0.000000 0.000000 0.000000 1.000000 2.000000 1.000000 1.000000 2.000000 0.000000 3.100000e+01 6.000000 50% 1.000000 0.000000 0.000000 1.000000 2.000000 2.000000 1.000000 3.000000 0.000000 7.900000e+01 30.000000 75% 8.000000 5.000000 1.000000 2.000000 4.000000 3.000000 2.000000 7.000000 0.000000 1.930000e+02 96.000000 max 293283.000000 173753.000000 18290.000000 71891.000000 206756.000000 190026.000000 59049.000000 439699.000000 11755.000000 5.098585e+06 25904.000000 chidf = pd . DataFrame () target = \"mau\" chidf [ target ] = df [ target ] quant_value = 0.99 tar_value = np . quantile ( chidf [ target ], quant_value ) chidf [ target ] = chidf [ target ] > tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } > { tar_value : .0f } \" ) sns . histplot ( b , label = f \" { target } <= { tar_value : .0f } \" ) plt . title ( ind ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) welchsum .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature test stat p-value upper q avg lower q avg reject null 0 n_tracks 10.277868 4.444906e-20 214.33164 193.07872 True 1 n_artists 5.367785 2.238566e-07 84.92819 81.98974 True 2 n_local_tracks -2.602519 1.006900e-02 2.59716 2.84386 False 3 n_albums -0.827392 4.090126e-01 85.92611 86.46785 False Let's perform the same test again this time let's say we're only interested in playlists with at least 10 tracks and fewer than 1000 tracks chidf = pd . DataFrame () target = sub_targets [ 2 ] chidf [ target ] = df [ target ] chidf [ 'n_tracks' ] = df [ 'n_tracks' ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) chidf [ target ] = chidf [ target ] > tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } > { tar_value : .0f } \" ) sns . histplot ( b , label = f \" { target } <= { tar_value : .0f } \" ) plt . title ( ind ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) welchsum .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature test stat p-value upper q avg lower q avg reject null 0 n_tracks 115.613349 3.417496e-174 231.30575 136.10481 True 1 n_artists 97.323391 2.230656e-167 108.74091 70.18516 True 2 n_albums 94.393421 2.063549e-160 114.38747 74.44801 True 3 n_local_tracks 15.122963 4.889333e-34 3.04746 1.99517 True Categorical-Continuous Conclusions \u00b6 Our conclusions are the same. There is a clear delineation between number of tracks, albums, and artists for popular and unpopular playlists Putting it All Together \u00b6 sub_targets ['mau_previous_month', 'mau_both_months', 'monthly_stream30s', 'stream30s'] des_features ['mood_1', 'mood_2', 'mood_3', 'genre_1', 'genre_2', 'genre_3'] master = pd . DataFrame () for target in sub_targets : # target = sub_targets[0] for quant_value in [ 0.9 , 0.99 ]: # quant_value = 0.90 chidf = pd . DataFrame () chidf [ target ] = df [ target ] tar_value = np . quantile ( chidf [ target ], quant_value ) chidf [ target ] = chidf [ target ] > tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ target , quant_value , tar_value , ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'target' , 'upper q' , 'upper q value' , 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum = chisum . loc [( chisum [ 'reject null' ] == True ) & ( chisum [ 'multiplier' ] > 2 )] . sort_values ( 'multiplier' , ascending = False ) master = pd . concat (( master , chisum )) master .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null 2 mau_previous_month 0.90 9.0 genre_1 Latin 5590.525321 0.000000e+00 [[355002, 11016], [33352, 3996]] 3.861095 True 18 mau_previous_month 0.90 9.0 genre_1 Children's 434.974313 1.343518e-96 [[364768, 1250], [36950, 398]] 3.143224 True 1 mau_previous_month 0.90 9.0 mood_1 Lively 2312.708732 0.000000e+00 [[358030, 7988], [34990, 2358]] 3.020517 True 22 mau_previous_month 0.90 9.0 genre_1 Traditional 357.345743 1.065483e-79 [[364829, 1189], [36989, 359]] 2.978032 True 7 mau_previous_month 0.90 9.0 genre_2 Jazz 1046.212802 1.619916e-229 [[362333, 3685], [36262, 1086]] 2.944750 True ... ... ... ... ... ... ... ... ... ... ... 36 stream30s 0.99 113.0 genre_2 Easy Listening 26.570340 2.541152e-07 [[397291, 2078], [3952, 45]] 2.177002 True 29 stream30s 0.99 113.0 genre_2 Traditional 39.102302 4.021695e-10 [[396243, 3126], [3930, 67]] 2.161001 True 24 stream30s 0.99 113.0 genre_3 Jazz 46.586071 8.768129e-12 [[395431, 3938], [3914, 83]] 2.129376 True 22 stream30s 0.99 113.0 mood_2 Easygoing 48.122685 4.003676e-12 [[394690, 4679], [3902, 95]] 2.053711 True 18 stream30s 0.99 113.0 mood_2 Lively 53.658720 2.385226e-13 [[394007, 5362], [3889, 108]] 2.040624 True 182 rows \u00d7 10 columns master [ 'group' ] . value_counts () - 22 Romantic 19 Lively 17 Traditional 16 Children's 16 Jazz 14 Latin 12 Serious 8 Easy Listening 8 Soundtrack 8 Other 7 New Age 7 Holiday 6 Peaceful 6 Spoken & Audio 4 Fiery 3 Tender 3 Easygoing 3 Sophisticated 2 Somber 1 Name: group, dtype: int64 master . loc [ master [ 'upper q' ] == 0.90 ][ 'group' ] . value_counts () - 12 Lively 7 Traditional 7 Children's 7 Jazz 7 Latin 7 Romantic 6 Other 5 Serious 5 Holiday 5 Easy Listening 4 Soundtrack 4 Spoken & Audio 3 Fiery 3 Sophisticated 2 New Age 1 Tender 1 Name: group, dtype: int64 sort_key = { i : j for i , j in zip ( master [ 'group' ] . value_counts () . index . values , range ( master [ 'group' ] . nunique ()))} master [ 'rank' ] = master [ 'group' ] . apply ( lambda x : sort_key [ x ]) master . sort_values ( 'rank' , inplace = True ) # master.drop('rank', axis=1, inplace=True) master . loc [ master [ 'group' ] != '-' ][: 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 7 monthly_stream30s 0.99 2843.0 mood_2 Romantic 146.934024 8.112487e-34 [[389339, 9994], [3810, 223]] 2.280176 True 1 6 stream30s 0.99 113.0 mood_2 Romantic 148.026986 4.679851e-34 [[389374, 9995], [3775, 222]] 2.290974 True 1 4 monthly_stream30s 0.99 2843.0 mood_1 Romantic 175.072639 5.772239e-40 [[390131, 9202], [3812, 221]] 2.457919 True 1 2 mau 0.99 143.0 mood_1 Romantic 202.823985 5.053546e-46 [[390156, 9193], [3787, 230]] 2.577588 True 1 1 mau 0.90 9.0 mood_2 Romantic 1531.190216 0.000000e+00 [[355299, 8035], [37850, 2182]] 2.549159 True 1 8 mau_previous_month 0.90 9.0 mood_3 Romantic 1013.797108 1.800082e-222 [[357949, 8069], [35525, 1823]] 2.276429 True 1 4 mau_previous_month 0.99 130.0 mood_1 Romantic 156.500834 6.579992e-36 [[390127, 9209], [3816, 214]] 2.375740 True 1 8 mau 0.90 9.0 mood_3 Romantic 1170.355016 1.690629e-256 [[355429, 7905], [38045, 1987]] 2.348287 True 1 6 mau 0.99 143.0 mood_2 Romantic 105.450504 9.729814e-25 [[389336, 10013], [3813, 204]] 2.080289 True 1 5 mau_previous_month 0.99 130.0 mood_3 Romantic 112.605179 2.633191e-26 [[389647, 9689], [3827, 203]] 2.133192 True 1 6 monthly_stream30s 0.99 2843.0 mood_3 Romantic 149.750731 1.965370e-34 [[389660, 9673], [3814, 219]] 2.313066 True 1 3 mau_both_months 0.99 19.0 mood_1 Romantic 109.693770 1.143607e-25 [[390177, 9231], [3766, 192]] 2.154933 True 1 6 mau_previous_month 0.90 9.0 mood_1 Romantic 1142.816205 1.633755e-250 [[358408, 7610], [35535, 1813]] 2.402893 True 1 10 stream30s 0.99 113.0 mood_3 Romantic 136.025552 1.969792e-31 [[389689, 9680], [3785, 212]] 2.254825 True 1 5 mau 0.99 143.0 mood_3 Romantic 122.574129 1.728356e-28 [[389664, 9685], [3810, 207]] 2.185929 True 1 6 mau 0.90 9.0 mood_1 Romantic 1328.179994 8.498925e-291 [[355892, 7442], [38051, 1981]] 2.489700 True 1 6 mau_previous_month 0.99 130.0 mood_2 Romantic 104.434543 1.624732e-24 [[389323, 10013], [3826, 204]] 2.073152 True 1 8 stream30s 0.99 113.0 mood_1 Romantic 139.245969 3.891401e-32 [[390152, 9217], [3791, 206]] 2.300158 True 1 5 mau_previous_month 0.90 9.0 mood_2 Romantic 1379.938658 4.806442e-302 [[357822, 8196], [35327, 2021]] 2.497610 True 1 1 mau_both_months 0.90 2.0 mood_1 Lively 750.247385 3.544959e-165 [[361665, 8747], [31355, 1599]] 2.108575 True 2 sub_targets ['mau_previous_month', 'mau_both_months', 'mau', 'monthly_stream30s', 'stream30s'] master . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 12 stream30s 0.99 113.0 mood_3 - 125.854082 3.309444e-29 [[397434, 1935], [3927, 70]] 3.661181 True 0 11 monthly_stream30s 0.99 2843.0 mood_2 - 109.163417 1.494430e-25 [[397529, 1804], [3969, 64]] 3.553294 True 0 67 mau_previous_month 0.90 9.0 genre_1 - 95.863487 1.230846e-22 [[365249, 769], [37173, 175]] 2.236007 True 0 10 monthly_stream30s 0.99 2843.0 mood_1 - 112.668942 2.549855e-26 [[397605, 1728], [3970, 63]] 3.651389 True 0 7 stream30s 0.99 113.0 mood_1 - 141.501726 1.249779e-32 [[397646, 1723], [3929, 68]] 3.994277 True 0 master . loc [ master [ 'feature' ] . str . contains ( 'genre' )] . groupby ( 'group' )[[ 'multiplier' , 'rank' ]] . mean () . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } multiplier rank group Tender 3.033890 16.0 - 2.935235 0.0 Peaceful 2.564297 13.0 Other 2.494292 10.0 Lively 2.364492 2.0 Romantic 2.318001 1.0 Fiery 2.244027 15.0 Somber 2.194114 19.0 Serious 2.190306 7.0 Easygoing 2.088064 17.0 Sophisticated 2.055203 18.0 master [ 'rank' ] = master [ 'group' ] . apply ( lambda x : sort_key [ x ]) master . groupby ( 'group' )[[ 'multiplier' , 'rank' ]] . mean () . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } multiplier rank group - 3.049100 0.0 Tender 3.033890 16.0 Latin 3.001282 6.0 Children's 2.871261 4.0 Holiday 2.836528 12.0 New Age 2.754796 11.0 Spoken & Audio 2.610393 14.0 Peaceful 2.564297 13.0 Other 2.425104 10.0 Easy Listening 2.407295 8.0 Lively 2.364492 2.0 Traditional 2.361342 3.0 Jazz 2.342954 5.0 Romantic 2.318001 1.0 Fiery 2.244027 15.0 Soundtrack 2.209295 9.0 Somber 2.194114 19.0 Serious 2.190306 7.0 Easygoing 2.088064 17.0 Sophisticated 2.055203 18.0 master . to_csv ( \"chi_square_results.csv\" ) con_master = pd . DataFrame () for target in sub_targets : # target = sub_targets[2] for quant_value in [ 0.90 , 0.99 ]: chidf = pd . DataFrame () chidf [ target ] = df [ target ] chidf [ 'n_tracks' ] = df [ 'n_tracks' ] # quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) chidf [ target ] = chidf [ target ] > tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ target , quant_value , ind , testt , p , a_avg , b_avg , p < cutoff ]])]) print ( target , quant_value ) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } > { tar_value : .0f } \" ) sns . histplot ( b , label = f \" { target } <= { tar_value : .0f } \" ) plt . title ( ind ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'target' , 'quantile' , 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) con_master = pd . concat (( con_master , welchsum )) con_master mau_previous_month 0.9 mau_previous_month 0.9 mau_previous_month 0.9 mau_previous_month 0.9 mau_previous_month 0.99 mau_previous_month 0.99 mau_previous_month 0.99 mau_previous_month 0.99 mau_both_months 0.9 mau_both_months 0.9 mau_both_months 0.9 mau_both_months 0.9 mau_both_months 0.99 mau_both_months 0.99 mau_both_months 0.99 mau_both_months 0.99 mau 0.9 mau 0.9 mau 0.9 mau 0.9 mau 0.99 mau 0.99 mau 0.99 mau 0.99 monthly_stream30s 0.9 monthly_stream30s 0.9 monthly_stream30s 0.9 monthly_stream30s 0.9 monthly_stream30s 0.99 monthly_stream30s 0.99 monthly_stream30s 0.99 monthly_stream30s 0.99 stream30s 0.9 stream30s 0.9 stream30s 0.9 stream30s 0.9 stream30s 0.99 stream30s 0.99 stream30s 0.99 stream30s 0.99 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target quantile feature test stat p-value upper q avg lower q avg reject null 0 mau_previous_month 0.90 n_albums -23.264501 1.517148e-58 69.19828 78.75130 True 1 mau_previous_month 0.90 n_artists -19.090166 9.131465e-47 67.78967 74.42581 True 2 mau_previous_month 0.90 n_local_tracks -8.591563 3.210041e-15 1.68487 2.13934 True 3 mau_previous_month 0.90 n_tracks 4.900218 2.017971e-06 149.27223 145.40243 True 0 mau_previous_month 0.99 n_tracks 19.149805 1.101097e-46 157.92259 144.56996 True 1 mau_previous_month 0.99 n_artists 9.668152 4.508161e-18 77.26126 73.71656 True 2 mau_previous_month 0.99 n_local_tracks -4.443426 1.514586e-05 1.89286 2.11507 True 3 mau_previous_month 0.99 n_albums 1.862787 6.399527e-02 78.89529 78.24458 False 0 mau_both_months 0.90 n_tracks 49.521659 1.017659e-108 181.22258 141.77758 True 1 mau_both_months 0.90 n_albums 44.662168 7.684105e-105 96.16066 75.92092 True 2 mau_both_months 0.90 n_artists 44.359056 9.041628e-103 90.79743 72.15272 True 3 mau_both_months 0.90 n_local_tracks 13.737285 1.342361e-30 2.78731 1.97483 True 0 mau_both_months 0.99 n_tracks 43.038413 5.369851e-102 175.40377 145.00116 True 1 mau_both_months 0.99 n_artists 38.561073 1.471847e-93 88.24552 73.26184 True 2 mau_both_months 0.99 n_albums 34.193348 1.157948e-84 91.12947 77.20951 True 3 mau_both_months 0.99 n_local_tracks 6.722576 1.917602e-10 2.56940 2.10191 True 0 mau 0.90 n_albums -28.035344 2.209065e-70 67.80156 79.48186 True 1 mau 0.90 n_artists -23.052205 7.021697e-58 66.03151 74.84314 True 2 mau 0.90 n_local_tracks -9.891800 5.454116e-19 1.57376 2.12208 True 3 mau 0.90 n_tracks 1.804461 7.267873e-02 146.48072 145.09618 False 0 mau 0.99 n_tracks 12.627041 3.513887e-27 155.01260 145.83850 True 1 mau 0.99 n_artists 7.983360 1.264344e-13 76.43482 73.73105 True 2 mau 0.99 n_local_tracks -6.172898 4.276522e-09 1.76129 2.07410 True 3 mau 0.99 n_albums 1.442954 1.506168e-01 78.53564 77.96526 False 0 monthly_stream30s 0.90 n_tracks 116.726338 2.452095e-164 232.32350 136.98027 True 1 monthly_stream30s 0.90 n_artists 92.368904 2.578108e-157 108.07236 70.08310 True 2 monthly_stream30s 0.90 n_albums 86.396836 1.619061e-153 114.85460 74.19437 True 3 monthly_stream30s 0.90 n_local_tracks 17.521798 4.704385e-40 3.01074 1.97501 True 0 monthly_stream30s 0.99 n_tracks 72.651978 1.071572e-144 199.50667 144.19406 True 1 monthly_stream30s 0.99 n_albums 40.530369 8.810322e-98 95.06869 77.58295 True 2 monthly_stream30s 0.99 n_artists 41.165863 1.560381e-97 90.42413 74.19337 True 3 monthly_stream30s 0.99 n_local_tracks 6.120756 5.135842e-09 2.37637 2.04232 True 0 stream30s 0.90 n_tracks 90.846516 2.364112e-160 207.07344 139.38590 True 1 stream30s 0.90 n_albums 68.563722 6.972523e-137 105.31471 75.42986 True 2 stream30s 0.90 n_artists 68.402932 2.057561e-132 99.37767 70.87686 True 3 stream30s 0.90 n_local_tracks 14.588639 6.290309e-32 2.89681 1.93857 True 0 stream30s 0.99 n_tracks 77.043302 2.214047e-149 201.25989 144.76511 True 1 stream30s 0.99 n_artists 47.632996 2.794842e-107 92.60628 73.13416 True 2 stream30s 0.99 n_albums 44.900868 5.246137e-103 98.01367 78.12288 True 3 stream30s 0.99 n_local_tracks 4.520672 1.062456e-05 2.29328 2.05241 True con_master .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target quantile feature test stat p-value upper q avg lower q avg reject null 0 mau_previous_month 0.90 n_albums -23.264501 1.517148e-58 69.19828 78.75130 True 1 mau_previous_month 0.90 n_artists -19.090166 9.131465e-47 67.78967 74.42581 True 2 mau_previous_month 0.90 n_local_tracks -8.591563 3.210041e-15 1.68487 2.13934 True 3 mau_previous_month 0.90 n_tracks 4.900218 2.017971e-06 149.27223 145.40243 True 0 mau_previous_month 0.99 n_tracks 19.149805 1.101097e-46 157.92259 144.56996 True 1 mau_previous_month 0.99 n_artists 9.668152 4.508161e-18 77.26126 73.71656 True 2 mau_previous_month 0.99 n_local_tracks -4.443426 1.514586e-05 1.89286 2.11507 True 3 mau_previous_month 0.99 n_albums 1.862787 6.399527e-02 78.89529 78.24458 False 0 mau_both_months 0.90 n_tracks 49.521659 1.017659e-108 181.22258 141.77758 True 1 mau_both_months 0.90 n_albums 44.662168 7.684105e-105 96.16066 75.92092 True 2 mau_both_months 0.90 n_artists 44.359056 9.041628e-103 90.79743 72.15272 True 3 mau_both_months 0.90 n_local_tracks 13.737285 1.342361e-30 2.78731 1.97483 True 0 mau_both_months 0.99 n_tracks 43.038413 5.369851e-102 175.40377 145.00116 True 1 mau_both_months 0.99 n_artists 38.561073 1.471847e-93 88.24552 73.26184 True 2 mau_both_months 0.99 n_albums 34.193348 1.157948e-84 91.12947 77.20951 True 3 mau_both_months 0.99 n_local_tracks 6.722576 1.917602e-10 2.56940 2.10191 True 0 mau 0.90 n_albums -28.035344 2.209065e-70 67.80156 79.48186 True 1 mau 0.90 n_artists -23.052205 7.021697e-58 66.03151 74.84314 True 2 mau 0.90 n_local_tracks -9.891800 5.454116e-19 1.57376 2.12208 True 3 mau 0.90 n_tracks 1.804461 7.267873e-02 146.48072 145.09618 False 0 mau 0.99 n_tracks 12.627041 3.513887e-27 155.01260 145.83850 True 1 mau 0.99 n_artists 7.983360 1.264344e-13 76.43482 73.73105 True 2 mau 0.99 n_local_tracks -6.172898 4.276522e-09 1.76129 2.07410 True 3 mau 0.99 n_albums 1.442954 1.506168e-01 78.53564 77.96526 False 0 monthly_stream30s 0.90 n_tracks 116.726338 2.452095e-164 232.32350 136.98027 True 1 monthly_stream30s 0.90 n_artists 92.368904 2.578108e-157 108.07236 70.08310 True 2 monthly_stream30s 0.90 n_albums 86.396836 1.619061e-153 114.85460 74.19437 True 3 monthly_stream30s 0.90 n_local_tracks 17.521798 4.704385e-40 3.01074 1.97501 True 0 monthly_stream30s 0.99 n_tracks 72.651978 1.071572e-144 199.50667 144.19406 True 1 monthly_stream30s 0.99 n_albums 40.530369 8.810322e-98 95.06869 77.58295 True 2 monthly_stream30s 0.99 n_artists 41.165863 1.560381e-97 90.42413 74.19337 True 3 monthly_stream30s 0.99 n_local_tracks 6.120756 5.135842e-09 2.37637 2.04232 True 0 stream30s 0.90 n_tracks 90.846516 2.364112e-160 207.07344 139.38590 True 1 stream30s 0.90 n_albums 68.563722 6.972523e-137 105.31471 75.42986 True 2 stream30s 0.90 n_artists 68.402932 2.057561e-132 99.37767 70.87686 True 3 stream30s 0.90 n_local_tracks 14.588639 6.290309e-32 2.89681 1.93857 True 0 stream30s 0.99 n_tracks 77.043302 2.214047e-149 201.25989 144.76511 True 1 stream30s 0.99 n_artists 47.632996 2.794842e-107 92.60628 73.13416 True 2 stream30s 0.99 n_albums 44.900868 5.246137e-103 98.01367 78.12288 True 3 stream30s 0.99 n_local_tracks 4.520672 1.062456e-05 2.29328 2.05241 True con_master . to_csv ( \"t_test_results.csv\" ) Models (Multi-Feature Analysis) \u00b6 Deciles - Random Forest \u00b6 sub_targets ['mau_previous_month', 'mau_both_months', 'mau', 'monthly_stream30s', 'stream30s'] target = sub_targets [ - 2 ] y = df [ target ] . values labels = y . copy () names = [] for idx , quant in zip ( range ( 11 ), np . linspace ( 0 , 1 , num = 11 )): if idx == 0 : prev = quant continue if idx == 1 : labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels names ['less than 13 listens', '13 < listens <= 24', '24 < listens <= 38', '38 < listens <= 55', '55 < listens <= 79', '79 < listens <= 111', '111 < listens <= 159', '159 < listens <= 240', '240 < listens <= 432', '432 < listens <= 42497334'] X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 42 , train_size = 0.8 ) model = RandomForestClassifier () model . fit ( X_train , y_train ) RandomForestClassifier() y_hat_test = model . predict ( X_test ) print ( f \"Train Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) Train Acc: 0.14 Test Acc: 0.14 print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) precision recall f1-score support 1 0.19 0.26 0.22 8363 2 0.13 0.13 0.13 7866 3 0.13 0.12 0.13 8173 4 0.10 0.09 0.10 7773 5 0.11 0.10 0.10 8252 6 0.11 0.09 0.10 7976 7 0.11 0.10 0.10 8018 8 0.12 0.10 0.11 8185 9 0.14 0.14 0.14 8009 10 0.20 0.26 0.23 8059 accuracy 0.14 80674 macro avg 0.13 0.14 0.14 80674 weighted avg 0.13 0.14 0.14 80674 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) <AxesSubplot:> # grab feature importances imp = model . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in model . estimators_ ], axis = 0 ) # build feature names feature_names = con_features + list ( enc . get_feature_names_out ()) # create new dataframe feat = pd . DataFrame ([ feature_names , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 n_tracks 0.152852 0.006907 1 n_albums 0.135581 0.007403 2 n_artists 0.133666 0.007421 3 n_local_tracks 0.038311 0.005365 4 genre_2_Pop 0.011607 0.000991 5 genre_3_Pop 0.01145 0.003792 6 genre_3_Alternative 0.010917 0.002062 7 genre_3_Rock 0.009709 0.002517 8 mood_3_Excited 0.009644 0.000618 9 mood_2_Excited 0.009271 0.000782 10 genre_2_Alternative 0.009073 0.003263 11 mood_3_Yearning 0.00904 0.001758 12 genre_3_Indie Rock 0.00876 0.000795 13 mood_3_Defiant 0.008758 0.000674 14 mood_3_Urgent 0.008581 0.000502 15 mood_2_Defiant 0.008537 0.000787 16 mood_3_Empowering 0.008351 0.001044 17 mood_3_Sensual 0.008343 0.000575 18 mood_2_Yearning 0.008315 0.00197 19 genre_2_Rock 0.008229 0.000827 Quartiles - Random Forest \u00b6 ### Create Categories y = df [ target ] . values labels = y . copy () names = [] lim = 5 for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx == 0 : prev = quant continue if idx == 1 : labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels ### Create Training Data X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 42 , train_size = 0.8 ) ### Train Model model = RandomForestClassifier () model . fit ( X_train , y_train ) ### Asses Performance y_hat_test = model . predict ( X_test ) y_hat_train = model . predict ( X_train ) print ( f \"Train Acc: { accuracy_score ( y_train , y_hat_train ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax ) Train Acc: 0.99 Test Acc: 0.33 precision recall f1-score support 1 0.37 0.43 0.40 20461 2 0.27 0.23 0.25 19966 3 0.27 0.23 0.25 20082 4 0.39 0.44 0.41 20165 accuracy 0.33 80674 macro avg 0.33 0.33 0.33 80674 weighted avg 0.33 0.33 0.33 80674 <AxesSubplot:> Binary, 90 th Percentile, Random Forest \u00b6 ### Create Categories y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 5 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels ### Create Training Data X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test , weight_train , weight_test = train_test_split ( X , y , weights , random_state = 42 , train_size = 0.8 ) ### Strateification Code # strat_y0_idx = np.array(random.sample(list(np.argwhere(y_train==3).reshape(-1)), np.unique(y_train, return_counts=True)[1][1])) # strat_y1_idx = np.argwhere(y_train==4).reshape(-1) # strat_idx = np.hstack((strat_y0_idx, strat_y1_idx)) # X_train = X_train[strat_idx] # y_train = y_train[strat_idx] ### Train Model model = RandomForestClassifier () model . fit ( X_train , y_train ) ### Assess Performance y_hat_test = model . predict ( X_test ) y_hat_train = model . predict ( X_train ) print ( f \"Train Acc: { accuracy_score ( y_train , y_hat_train ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( Train Acc: 0.76 Test Acc: 0.76 precision recall f1-score support 3 0.76 0.98 0.86 60509 4 0.58 0.08 0.13 20165 accuracy 0.76 80674 macro avg 0.67 0.53 0.50 80674 weighted avg 0.72 0.76 0.68 80674 <AxesSubplot:> Forward Selection Model \u00b6 ### y print ( target ) y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = 0 names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = 1 weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels #### X X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( np . ones (( X_con . shape [ 0 ], 1 )), X_con , X_cat )) feature_names = [ 'intercept' ] + con_features + list ( enc . get_feature_names_out ()) data = pd . DataFrame ( X , columns = feature_names ) print ( names ) monthly_stream30s ['less than 432 listens', '432 < listens <= 42497334'] def add_feature ( feature_names , basemodel , data , y , r2max = 0 , model = 'linear' , disp = 0 ): feature_max = None bestsum = None newmodel = None for feature in feature_names : basemodel [ feature ] = data [ feature ] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () score = float ( str ( pd . DataFrame ( summ . tables [ 0 ]) . loc [ 3 , 3 ])) if ( score > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = score feature_max = feature bestsum = est2 . summary () newmodel = basemodel . copy () if disp == 1 : print ( f \"new r2max, { feature_max } , { r2max } \" ) basemodel . drop ( labels = feature , axis = 1 , inplace = True ) return r2max , feature_max , bestsum , newmodel candidates = feature_names . copy () basemodel = pd . DataFrame () r2max = 0 with open ( \"canidates.txt\" , \"w+\" ) as f : file_data = f . read () for i in candidates : f . write ( f \" { i } \\n \" ) basemodel . to_csv ( \"basemodel.csv\" ) with open ( \"canidates.txt\" , \"r\" ) as f : # file_data = f.read() new = [] for line in f : current_place = line [: - 1 ] new . append ( current_place ) new = pd . read_csv ( \"basemodel.csv\" , index_col = 0 ) with open ( \"fwd_selection_results.txt\" , \"r+\" ) as f : for line in f : pass lastline = line [: - 1 ] stuff = lastline . split ( \", \" ) new = float ( stuff [ - 1 ]) new 0.04052 while True : newr2max , feature_max , bestsum , newmodel = add_feature ( feature_names = candidates , basemodel = basemodel , data = data , y = y , r2max = r2max ) if newr2max > r2max : r2max = newr2max print ( f \"new r2max, { feature_max } , { r2max } \" ) with open ( \"fwd_selection_results.txt\" , \"a+\" ) as f : file_data = f . read () f . write ( f \"new r2max, { feature_max } , { r2max } \\n \" ) candidates . remove ( feature_max ) with open ( \"canidates.txt\" , \"w+\" ) as f : file_data = f . read () for i in candidates : f . write ( f \" { i } \\n \" ) basemodel = newmodel basemodel . to_csv ( \"basemodel.csv\" ) continue else : break new r2max, n_albums, 0.02614 new r2max, genre_1_Latin, 0.03093 new r2max, genre_1_Indie Rock, 0.03274 new r2max, genre_1_Rap, 0.03431 new r2max, genre_1_Dance & House, 0.03568 new r2max, genre_1_Rock, 0.03674 new r2max, mood_1_Energizing, 0.03772 new r2max, genre_1_Children's, 0.03863 new r2max, mood_1_Tender, 0.03931 new r2max, mood_1_Other, 0.03995 new r2max, n_tracks, 0.04052 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) Input In [675], in <module> 1 while True: ----> 2 newr2max, feature_max, bestsum, newmodel = add_feature( 3 feature_names=candidates, 4 basemodel=basemodel, 5 data=data, 6 y=y, 7 r2max=r2max) 8 if newr2max > r2max: 9 r2max = newr2max Input In [669], in add_feature(feature_names, basemodel, data, y, r2max, model, disp) 8 est = Logit(y, X2) 9 est2 = est.fit(disp=0) ---> 10 summ = est2.summary() 11 score = float(str(pd.DataFrame(summ.tables[0]).loc[3, 3])) 12 if (score > r2max) and not (est2.pvalues > cutoff).any(): File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:4015, in BinaryResults.summary(self, yname, xname, title, alpha, yname_list) 4012 @Appender(DiscreteResults.summary.__doc__) 4013 def summary(self, yname=None, xname=None, title=None, alpha=.05, 4014 yname_list=None): -> 4015 smry = super(BinaryResults, self).summary(yname, xname, title, alpha, 4016 yname_list) 4017 fittedvalues = self.model.cdf(self.fittedvalues) 4018 absprederror = np.abs(self.model.endog - fittedvalues) File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:3769, in DiscreteResults.summary(self, yname, xname, title, alpha, yname_list) 3731 \"\"\" 3732 Summarize the Regression Results. 3733 (...) 3755 statsmodels.iolib.summary.Summary : Class that hold summary results. 3756 \"\"\" 3758 top_left = [('Dep. Variable:', None), 3759 ('Model:', [self.model.__class__.__name__]), 3760 ('Method:', ['MLE']), (...) 3763 ('converged:', [\"%s\" % self.mle_retvals['converged']]), 3764 ] 3766 top_right = [('No. Observations:', None), 3767 ('Df Residuals:', None), 3768 ('Df Model:', None), -> 3769 ('Pseudo R-squ.:', [\"%#6.4g\" % self.prsquared]), 3770 ('Log-Likelihood:', None), 3771 ('LL-Null:', [\"%#8.5g\" % self.llnull]), 3772 ('LLR p-value:', [\"%#6.4g\" % self.llr_pvalue]) 3773 ] 3775 if hasattr(self, 'cov_type'): 3776 top_left.append(('Covariance Type:', [self.cov_type])) File ~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/_libs/properties.pyx:37, in pandas._libs.properties.CachedProperty.__get__() File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:3511, in DiscreteResults.prsquared(self) 3506 @cache_readonly 3507 def prsquared(self): 3508 \"\"\" 3509 McFadden's pseudo-R-squared. `1 - (llf / llnull)` 3510 \"\"\" -> 3511 return 1 - self.llf/self.llnull File ~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/_libs/properties.pyx:37, in pandas._libs.properties.CachedProperty.__get__() File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:3604, in DiscreteResults.llnull(self) 3601 res_null = mod_null.fit(start_params=sp_null, **opt_kwds) 3602 else: 3603 # this should be a reasonably method case across versions -> 3604 res_null = mod_null.fit(start_params=sp_null, method='nm', 3605 warn_convergence=False, 3606 maxiter=10000, disp=0) 3607 res_null = mod_null.fit(start_params=res_null.params, method='bfgs', 3608 warn_convergence=False, 3609 maxiter=10000, disp=0) 3611 if getattr(self, '_attach_nullmodel', False) is not False: File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:1983, in Logit.fit(self, start_params, method, maxiter, full_output, disp, callback, **kwargs) 1980 @Appender(DiscreteModel.fit.__doc__) 1981 def fit(self, start_params=None, method='newton', maxiter=35, 1982 full_output=1, disp=1, callback=None, **kwargs): -> 1983 bnryfit = super().fit(start_params=start_params, 1984 method=method, 1985 maxiter=maxiter, 1986 full_output=full_output, 1987 disp=disp, 1988 callback=callback, 1989 **kwargs) 1991 discretefit = LogitResults(self, bnryfit) 1992 return BinaryResultsWrapper(discretefit) File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:230, in DiscreteModel.fit(self, start_params, method, maxiter, full_output, disp, callback, **kwargs) 227 else: 228 pass # TODO: make a function factory to have multiple call-backs --> 230 mlefit = super().fit(start_params=start_params, 231 method=method, 232 maxiter=maxiter, 233 full_output=full_output, 234 disp=disp, 235 callback=callback, 236 **kwargs) 238 return mlefit File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/base/model.py:563, in LikelihoodModel.fit(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs) 560 del kwargs[\"use_t\"] 562 optimizer = Optimizer() --> 563 xopt, retvals, optim_settings = optimizer._fit(f, score, start_params, 564 fargs, kwargs, 565 hessian=hess, 566 method=method, 567 disp=disp, 568 maxiter=maxiter, 569 callback=callback, 570 retall=retall, 571 full_output=full_output) 572 # Restore cov_type, cov_kwds and use_t 573 optim_settings.update(kwds) File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/base/optimizer.py:241, in Optimizer._fit(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall) 238 fit_funcs.update(extra_fit_funcs) 240 func = fit_funcs[method] --> 241 xopt, retvals = func(objective, gradient, start_params, fargs, kwargs, 242 disp=disp, maxiter=maxiter, callback=callback, 243 retall=retall, full_output=full_output, 244 hess=hessian) 246 optim_settings = {'optimizer': method, 'start_params': start_params, 247 'maxiter': maxiter, 'full_output': full_output, 248 'disp': disp, 'fargs': fargs, 'callback': callback, 249 'retall': retall, \"extra_fit_funcs\": extra_fit_funcs} 250 optim_settings.update(kwargs) File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/base/optimizer.py:728, in _fit_nm(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess) 726 ftol = kwargs.setdefault('ftol', 0.0001) 727 maxfun = kwargs.setdefault('maxfun', None) --> 728 retvals = optimize.fmin(f, start_params, args=fargs, xtol=xtol, 729 ftol=ftol, maxiter=maxiter, maxfun=maxfun, 730 full_output=full_output, disp=disp, retall=retall, 731 callback=callback) 732 if full_output: 733 if not retall: File ~/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/optimize/optimize.py:580, in fmin(func, x0, args, xtol, ftol, maxiter, maxfun, full_output, disp, retall, callback, initial_simplex) 471 \"\"\" 472 Minimize a function using the downhill simplex algorithm. 473 (...) 570 571 \"\"\" 572 opts = {'xatol': xtol, 573 'fatol': ftol, 574 'maxiter': maxiter, (...) 577 'return_all': retall, 578 'initial_simplex': initial_simplex} --> 580 res = _minimize_neldermead(func, x0, args, callback=callback, **opts) 581 if full_output: 582 retlist = res['x'], res['fun'], res['nit'], res['nfev'], res['status'] File ~/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/optimize/optimize.py:768, in _minimize_neldermead(func, x0, args, callback, maxiter, maxfev, disp, return_all, initial_simplex, xatol, fatol, adaptive, bounds, **unknown_options) 766 if bounds is not None: 767 xr = np.clip(xr, lower_bound, upper_bound) --> 768 fxr = func(xr) 769 doshrink = 0 771 if fxr < fsim[0]: File ~/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/optimize/optimize.py:464, in _wrap_function.<locals>.function_wrapper(x, *wrapper_args) 462 def function_wrapper(x, *wrapper_args): 463 ncalls[0] += 1 --> 464 return function(np.copy(x), *(wrapper_args + args)) File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/base/model.py:531, in LikelihoodModel.fit.<locals>.f(params, *args) 530 def f(params, *args): --> 531 return -self.loglike(params, *args) / nobs File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:1872, in Logit.loglike(self, params) 1870 q = 2*self.endog - 1 1871 X = self.exog -> 1872 return np.sum(np.log(self.cdf(q*np.dot(X,params)))) File <__array_function__ internals>:5, in dot(*args, **kwargs) KeyboardInterrupt: candidates ['n_artists', 'n_local_tracks', 'mood_1_-', 'mood_1_Aggressive', 'mood_1_Brooding', 'mood_1_Cool', 'mood_1_Defiant', 'mood_1_Easygoing', 'mood_1_Empowering', 'mood_1_Excited', 'mood_1_Fiery', 'mood_1_Gritty', 'mood_1_Lively', 'mood_1_Melancholy', 'mood_1_Peaceful', 'mood_1_Romantic', 'mood_1_Rowdy', 'mood_1_Sensual', 'mood_1_Sentimental', 'mood_1_Serious', 'mood_1_Somber', 'mood_1_Sophisticated', 'mood_1_Stirring', 'mood_1_Upbeat', 'mood_1_Urgent', 'mood_1_Yearning', 'mood_2_-', 'mood_2_Aggressive', 'mood_2_Brooding', 'mood_2_Cool', 'mood_2_Defiant', 'mood_2_Easygoing', 'mood_2_Empowering', 'mood_2_Energizing', 'mood_2_Excited', 'mood_2_Fiery', 'mood_2_Gritty', 'mood_2_Lively', 'mood_2_Melancholy', 'mood_2_Other', 'mood_2_Peaceful', 'mood_2_Romantic', 'mood_2_Rowdy', 'mood_2_Sensual', 'mood_2_Sentimental', 'mood_2_Serious', 'mood_2_Somber', 'mood_2_Sophisticated', 'mood_2_Stirring', 'mood_2_Tender', 'mood_2_Upbeat', 'mood_2_Urgent', 'mood_2_Yearning', 'mood_3_-', 'mood_3_Aggressive', 'mood_3_Brooding', 'mood_3_Cool', 'mood_3_Defiant', 'mood_3_Easygoing', 'mood_3_Empowering', 'mood_3_Energizing', 'mood_3_Excited', 'mood_3_Fiery', 'mood_3_Gritty', 'mood_3_Lively', 'mood_3_Melancholy', 'mood_3_Other', 'mood_3_Peaceful', 'mood_3_Romantic', 'mood_3_Rowdy', 'mood_3_Sensual', 'mood_3_Sentimental', 'mood_3_Serious', 'mood_3_Somber', 'mood_3_Sophisticated', 'mood_3_Stirring', 'mood_3_Tender', 'mood_3_Upbeat', 'mood_3_Urgent', 'mood_3_Yearning', 'genre_1_-', 'genre_1_Alternative', 'genre_1_Blues', 'genre_1_Classical', 'genre_1_Country & Folk', 'genre_1_Easy Listening', 'genre_1_Electronica', 'genre_1_Holiday', 'genre_1_Jazz', 'genre_1_Metal', 'genre_1_New Age', 'genre_1_Other', 'genre_1_Pop', 'genre_1_Punk', 'genre_1_R&B', 'genre_1_Reggae', 'genre_1_Religious', 'genre_1_Soundtrack', 'genre_1_Spoken & Audio', 'genre_1_Traditional', 'genre_2_-', 'genre_2_Alternative', 'genre_2_Blues', \"genre_2_Children's\", 'genre_2_Classical', 'genre_2_Country & Folk', 'genre_2_Dance & House', 'genre_2_Easy Listening', 'genre_2_Electronica', 'genre_2_Holiday', 'genre_2_Indie Rock', 'genre_2_Jazz', 'genre_2_Latin', 'genre_2_Metal', 'genre_2_New Age', 'genre_2_Other', 'genre_2_Pop', 'genre_2_Punk', 'genre_2_R&B', 'genre_2_Rap', 'genre_2_Reggae', 'genre_2_Religious', 'genre_2_Rock', 'genre_2_Soundtrack', 'genre_2_Spoken & Audio', 'genre_2_Traditional', 'genre_3_-', 'genre_3_Alternative', 'genre_3_Blues', \"genre_3_Children's\", 'genre_3_Classical', 'genre_3_Country & Folk', 'genre_3_Dance & House', 'genre_3_Easy Listening', 'genre_3_Electronica', 'genre_3_Holiday', 'genre_3_Indie Rock', 'genre_3_Jazz', 'genre_3_Latin', 'genre_3_Metal', 'genre_3_New Age', 'genre_3_Other', 'genre_3_Pop', 'genre_3_Punk', 'genre_3_R&B', 'genre_3_Rap', 'genre_3_Reggae', 'genre_3_Religious', 'genre_3_Rock', 'genre_3_Soundtrack', 'genre_3_Spoken & Audio', 'genre_3_Traditional'] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns res_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] intercept -2.0979 0.008 -273.128 0.000 -2.113 -2.083 n_albums 0.3430 0.006 61.214 0.000 0.332 0.354 genre_1_Latin 0.6929 0.023 30.536 0.000 0.648 0.737 genre_1_Indie Rock -0.4654 0.016 -28.755 0.000 -0.497 -0.434 genre_1_Rap -0.3804 0.016 -23.163 0.000 -0.413 -0.348 genre_1_Dance & House -0.3978 0.022 -18.022 0.000 -0.441 -0.355 genre_1_Rock -0.3562 0.023 -15.423 0.000 -0.402 -0.311 mood_1_Energizing -0.3623 0.025 -14.323 0.000 -0.412 -0.313 genre_1_Children's 0.9479 0.061 15.652 0.000 0.829 1.067 mood_1_Tender 0.6629 0.047 14.006 0.000 0.570 0.756 mood_1_Other 0.8465 0.062 13.611 0.000 0.725 0.968 n_tracks 0.0688 0.006 11.549 0.000 0.057 0.080 mood_2_Serious 0.0046 0.080 0.058 0.954 -0.151 0.161 Binary, 99 th Percentile \u00b6 ### Create Categories y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels ### Create Training Data X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test , weight_train , weight_test = train_test_split ( X , y , weights , random_state = 42 , train_size = 0.8 ) ### Train Model model = RandomForestClassifier () model . fit ( X_train , y_train , weight_train ) ### Asses Performance y_hat_test = model . predict ( X_test ) y_hat_train = model . predict ( X_train ) print ( f \"Train Acc: { accuracy_score ( y_train , y_hat_train ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax ) Train Acc: 1.00 Test Acc: 0.90 precision recall f1-score support 9 0.90 0.99 0.94 72615 10 0.27 0.03 0.05 8059 accuracy 0.90 80674 macro avg 0.59 0.51 0.50 80674 weighted avg 0.84 0.90 0.86 80674 <AxesSubplot:> Other Metrics \u00b6 30s listens/tot listens (listen conversions) also like a bounce rate Users both months/users prev month (user conversions) combine with mau > mau_previous_month Best small time performers (less than X total monthly listens + high conversion) Best new user playlist (owner has only 1 popular playlist) Listen and User Conversions, MAU Growing \u00b6 df [ 'listen_conversions' ] = df [ 'stream30s' ] / df [ 'streams' ] df [ 'listen_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'user_retention' ] = df [ 'mau_both_months' ] / df [ 'mau_previous_month' ] df [ 'user_retention' ] . fillna ( value = 0 , inplace = True ) df [ 'user_conversions' ] = df [ 'mau' ] / df [ 'users' ] df [ 'user_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growing' ] = df [ 'mau' ] > df [ 'mau_previous_month' ] df [ 'mau_growth' ] = df [ 'mau' ] / df [ 'mau_previous_month' ] df [ 'mau_growth' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growth' ] . replace ([ np . inf , - np . inf ], 1 , inplace = True ) new_metrics = [ 'listen_conversions' , 'user_conversions' , 'user_retention' , 'mau_growth' ] df [ new_metrics ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } listen_conversions user_conversions user_retention mau_growth count 403366.000000 403366.000000 403366.000000 403366.000000 mean 0.334701 0.724072 0.571070 1.513218 std 0.399968 0.261708 0.392073 17.459669 min 0.000000 0.020348 0.000000 0.031250 25% 0.000000 0.500000 0.200000 1.000000 50% 0.000000 0.666667 0.500000 1.066667 75% 0.730769 1.000000 1.000000 2.000000 max 1.000000 1.000000 1.000000 7859.000000 df [ 'listen_conversions' ] . plot ( kind = 'hist' , bins = 10 ) <AxesSubplot:ylabel='Frequency'> df [ 'user_conversions' ] . plot ( kind = 'hist' , bins = 10 ) <AxesSubplot:ylabel='Frequency'> df [ 'user_retention' ] . plot ( kind = 'hist' , bins = 10 ) <AxesSubplot:ylabel='Frequency'> df . loc [ df [ 'mau_growth' ] < 10 ][ 'mau_growth' ] . plot ( kind = 'hist' , bins = 20 ) <AxesSubplot:ylabel='Frequency'> df [ 'mau_growing' ] . value_counts () . plot ( kind = 'bar' ) <AxesSubplot:> df [ 'new_success' ] = df [ new_metrics ] . apply ( lambda x : ( x > 0.5 ) if ( max ( x ) == 1 ) else ( x > 1 )) . all ( axis = 1 ) df [ 'new_success' ] . value_counts () False 362869 True 40497 Name: new_success, dtype: int64 df . loc [ df [ 'new_success' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playlist_uri owner streams stream30s dau wau mau mau_previous_month mau_both_months users skippers owner_country n_tracks n_local_tracks n_artists n_albums monthly_stream30s monthly_owner_stream30s tokens genre_1 genre_2 genre_3 mood_1 mood_2 mood_3 success listen_conversions user_retention user_conversions mau_growing mau_growth new_success 14 spotify:user:9a3580868994077be27d244788d494cd:... 9a3580868994077be27d244788d494cd 28 15 1 1 2 1 1 2 0 US 321 0 170 205 83 77 [\"sunny\", \"daze\"] Alternative Indie Rock Electronica Brooding Excited Sensual False 0.535714 1.0 1.000000 True 2.000000 True 18 spotify:user:7abbdbd3119687473b8f2986e73e2ad6:... 7abbdbd3119687473b8f2986e73e2ad6 9 5 1 2 2 1 1 2 0 US 373 8 1 1 18 11 [] Pop Alternative Indie Rock Empowering Excited Urgent False 0.555556 1.0 1.000000 True 2.000000 True 20 spotify:user:838141e861005b6a955cb389c19671a5:... 838141e861005b6a955cb389c19671a5 32 25 2 3 4 3 3 5 1 US 904 0 81 125 327 253 [\"metalcore\", \"forever\"] Punk Metal Rock Defiant Urgent Aggressive False 0.781250 1.0 0.800000 True 1.333333 True 36 spotify:user:2217942070bcaa5f1e651e27744b4402:... 2217942070bcaa5f1e651e27744b4402 18 17 1 2 4 3 3 5 1 US 141 1 122 131 567 0 [\"chill\"] Rap Dance & House Alternative Excited Defiant Energizing False 0.944444 1.0 0.800000 True 1.333333 True 59 spotify:user:dfde15dd16b4ad87a75036276b4c9f66:... dfde15dd16b4ad87a75036276b4c9f66 5 5 1 1 2 1 1 3 0 US 84 0 73 78 254 239 [\"vegas\"] Rock Pop R&B Upbeat Excited Empowering False 1.000000 1.0 0.666667 True 2.000000 True ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 403329 spotify:user:358b83239c6a2557fbfb053330d49a41:... 358b83239c6a2557fbfb053330d49a41 4 4 1 1 3 1 1 3 0 US 33 0 28 31 271 32 [\"one\", \"dirt\", \"road\"] Country & Folk Rock - Yearning Empowering Gritty False 1.000000 1.0 1.000000 True 3.000000 True 403336 spotify:user:a0781a2de47beb8bd693f3022f316327:... a0781a2de47beb8bd693f3022f316327 856 855 3 10 10 5 5 10 0 US 168 0 6 9 33747 1391 [\"evning\", \"song\"] - - - - - - True 0.998832 1.0 1.000000 True 2.000000 True 403338 spotify:user:06f6dd666f1bbf9148c792b87ed4d22f:... 06f6dd666f1bbf9148c792b87ed4d22f 5 4 1 1 2 1 1 2 0 US 59 0 34 46 21 9 [\"rhc\"] Religious Pop Alternative Empowering Upbeat Brooding False 0.800000 1.0 1.000000 True 2.000000 True 403348 spotify:user:c6af258245d55221cebedb1175f08d83:... c6af258245d55221cebedb1175f08d83 13 11 1 1 2 1 1 2 0 US 31 0 30 29 208 206 [\"zumba\", \"val\", \"silva\", \"playlist\"] Latin Pop Dance & House Aggressive Excited Defiant False 0.846154 1.0 1.000000 True 2.000000 True 403353 spotify:user:5461b6b460dd512d7b4fd4fb488f3520:... 5461b6b460dd512d7b4fd4fb488f3520 2 2 1 1 2 1 1 2 0 US 146 0 115 123 405 321 [\"myfavorites\"] Indie Rock Electronica Alternative Yearning Energizing Brooding False 1.000000 1.0 1.000000 True 2.000000 True 40497 rows \u00d7 32 columns chidf = pd . DataFrame () target = 'new_success' chidf [ target ] = df [ target ] # quant_value = 0.90 # tar_value = np.quantile(chidf[target], quant_value) # chidf[target] = chidf[target] > tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum . loc [ chisum [ 'reject null' ] == True ] . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 6 genre_1 Dance & House 231.225731 3.221322e-52 [[334768, 28101], [36487, 4010]] 1.309267 True 2 genre_1 Indie Rock 386.328998 5.212769e-86 [[300809, 62060], [31986, 8511]] 1.289733 True 3 mood_1 Excited 289.821405 5.438394e-65 [[306376, 56493], [32871, 7626]] 1.258184 True 4 mood_1 Defiant 285.014998 6.064223e-64 [[291222, 71647], [31065, 9432]] 1.234123 True 16 genre_2 Electronica 124.733558 5.820843e-29 [[335186, 27683], [36772, 3725]] 1.226540 True ... ... ... ... ... ... ... ... 70 mood_1 Somber 30.852148 2.784538e-08 [[361994, 875], [40456, 41]] 0.419270 True 0 genre_3 - 1404.327669 2.410008e-307 [[324633, 38236], [38610, 1887]] 0.414947 True 1 genre_2 - 861.809401 1.968786e-189 [[342541, 20328], [39619, 878]] 0.373430 True 24 mood_1 Other 81.806778 1.500630e-19 [[361232, 1637], [40439, 58]] 0.316494 True 42 genre_1 Spoken & Audio 58.779116 1.764037e-14 [[361755, 1114], [40460, 37]] 0.296965 True 101 rows \u00d7 7 columns chisum . loc [ chisum [ 'reject null' ] == True ] . sort_values ( 'multiplier' , ascending = True )[: 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 42 genre_1 Spoken & Audio 58.779116 1.764037e-14 [[361755, 1114], [40460, 37]] 0.296965 True 24 mood_1 Other 81.806778 1.500630e-19 [[361232, 1637], [40439, 58]] 0.316494 True 1 genre_2 - 861.809401 1.968786e-189 [[342541, 20328], [39619, 878]] 0.373430 True 0 genre_3 - 1404.327669 2.410008e-307 [[324633, 38236], [38610, 1887]] 0.414947 True 70 mood_1 Somber 30.852148 2.784538e-08 [[361994, 875], [40456, 41]] 0.419270 True 73 genre_1 Easy Listening 30.613123 3.149562e-08 [[361984, 885], [40455, 42]] 0.424642 True 40 mood_2 - 60.796108 6.330294e-15 [[361087, 1782], [40411, 86]] 0.431224 True 43 mood_1 - 57.600397 3.211607e-14 [[361161, 1708], [40414, 83]] 0.434269 True 37 mood_3 - 64.489845 9.703118e-16 [[360957, 1912], [40404, 93]] 0.434536 True 48 genre_1 Children's 52.188042 5.043231e-13 [[361298, 1571], [40420, 77]] 0.438111 True 32 mood_1 Easygoing 72.784800 1.445861e-17 [[360451, 2418], [40371, 126]] 0.465255 True 56 mood_3 Serious 43.083601 5.245004e-11 [[361404, 1465], [40420, 77]] 0.469948 True 59 genre_2 Other 41.614387 1.111721e-10 [[361446, 1423], [40422, 75]] 0.471283 True 82 mood_2 Other 25.423296 4.603257e-07 [[361970, 899], [40449, 48]] 0.477800 True 60 genre_1 Traditional 39.228043 3.770852e-10 [[361402, 1467], [40416, 81]] 0.493733 True 39 genre_3 Easy Listening 61.357952 4.758655e-15 [[360552, 2317], [40368, 129]] 0.497272 True 47 genre_2 Easy Listening 53.106215 3.159911e-13 [[360858, 2011], [40385, 112]] 0.497648 True 65 mood_2 Stirring 34.226638 4.905289e-09 [[361548, 1321], [40423, 74]] 0.501033 True 57 mood_1 Serious 42.044137 8.923632e-11 [[361247, 1622], [40406, 91]] 0.501590 True 10 genre_1 Soundtrack 169.038371 1.200050e-38 [[356345, 6524], [40127, 370]] 0.503642 True chidf = pd . DataFrame () target = \"success\" chidf [ target ] = df [ target ] # chidf.iloc[:int(chidf.shape[0]/2),:] = True # chidf.iloc[int(chidf.shape[0]/2):,:] = False # quant_value = 0.99 # tar_value = np.quantile(chidf[target], quant_value) # chidf[target] = chidf[target] > tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } == True\" ) sns . histplot ( b , label = f \" { target } == False\" ) plt . title ( ind ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) sns . histplot ( df , x = 'listen_conversions' , hue = 'mau_growing' , bins = 10 ) <AxesSubplot:xlabel='listen_conversions', ylabel='Count'> sns . histplot ( df , x = 'user_conversions' , hue = 'mau_growing' , bins = 10 ) <AxesSubplot:xlabel='user_conversions', ylabel='Count'> sns . histplot ( df , x = 'user_conversions' , hue = df [ 'dau' ] > 1 , bins = 10 ) <AxesSubplot:xlabel='user_conversions', ylabel='Count'> ( df [ 'mau' ] > 5 ) . describe () count 403366 unique 2 top False freq 338256 Name: mau, dtype: object np . quantile ( df [ 'mau' ], 0.9 ) 9.0 Considering outliers \u00b6 df = df . loc [ df [ targets ] . apply ( lambda x : ( x < 3 * x . std ()) if ( x . dtype == int or x . dtype == float ) else x ) . all ( axis = 1 )] df = df . loc [ df [ 'owner' ] != 'spotify' ] Multiple Criteria for Success \u00b6 df [ 'success' ] = df [ sub_targets ] . apply ( lambda x : x > np . quantile ( x , 0.75 )) . all ( axis = 1 ) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) df . loc [ df [ 'success' ] == True ] . groupby ( 'n_tracks' )[[ 'wau' ]] . mean () . plot ( ls = '' , marker = '.' , ax = ax ) ax . set_xlim ( 0 , 200 ) ax . set_ylim ( 0 , 5000 ) (0.0, 5000.0) chidf = pd . DataFrame () target = 'success' chidf [ target ] = df [ target ] # quant_value = 0.90 # tar_value = np.quantile(chidf[target], quant_value) # chidf[target] = chidf[target] > tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum . loc [ chisum [ 'reject null' ] == True ] . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 0 genre_1 Latin 3516.528142 0.000000e+00 [[371720, 12806], [16634, 2206]] 3.849561 True 4 genre_2 Jazz 708.535543 4.164954e-156 [[380364, 4162], [18231, 609]] 3.052841 True 1 mood_1 Lively 1287.773814 5.130420e-282 [[375424, 9102], [17596, 1244]] 2.916028 True 14 genre_1 Children's 208.802667 2.506648e-47 [[383079, 1447], [18639, 201]] 2.854916 True 20 genre_1 Traditional 149.152847 2.655403e-34 [[383152, 1374], [18666, 174]] 2.599455 True ... ... ... ... ... ... ... ... 22 genre_2 Indie Rock 137.000630 1.205469e-31 [[353648, 30878], [17772, 1068]] 0.688267 True 34 mood_1 Brooding 84.460032 3.920608e-20 [[366942, 17584], [18247, 593]] 0.678177 True 9 genre_2 Alternative 331.424544 4.704591e-74 [[320464, 64062], [16650, 2190]] 0.657974 True 11 mood_1 Yearning 223.850708 1.307610e-50 [[347224, 37302], [17631, 1209]] 0.638303 True 2 genre_1 Indie Rock 866.348545 2.029540e-190 [[315752, 68774], [17043, 1797]] 0.484087 True 92 rows \u00d7 7 columns ind = 'n_tracks' target = 'wau' mean_wau_vs_track = [] for track in range ( 1 , 201 ): means = [] for i in range ( 10 ): boot = random . sample ( list ( df . loc [ ( df [ 'success' ] == True ) & ( df [ ind ] == track ) ][ target ] . values ), k = min ( len ( list ( df . loc [ ( df [ 'success' ] == True ) & ( df [ ind ] == track ) ][ target ] . values )), 1000 )) means . append ( np . mean ( boot )) mean_wau_vs_track . append ( np . mean ( means )) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) plt . plot ( range ( len ( mean_wau_vs_track )), mean_wau_vs_track , ls = '' , marker = '.' ) # ax.set_ylim(0,5) [<matplotlib.lines.Line2D at 0x7f838e3d99a0>] len ( df . loc [ ( df [ 'success' ] == True ) & ( df [ ind ] == track ) ][ target ] . values ) 14 Dependency \u00b6 master = pd . DataFrame () for target in new_metrics : # target = sub_targets[0] chidf = pd . DataFrame () chidf [ target ] = df [ target ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) tar_value = 0.8 chidf [ target ] = chidf [ target ] >= tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ target , ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'target' , 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) # chisum = chisum.loc[(chisum['reject null'] == True) & (chisum['multiplier'] > 2)].sort_values('multiplier', ascending=False) master = pd . concat (( master , chisum )) master .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target feature group chi p-value cTable multiplier reject null 0 listen_conversions genre_1 Rap 399.045079 8.888113e-89 [[263266, 51481], [76577, 12042]] 0.804170 True 1 listen_conversions genre_1 Latin 340.400638 5.219381e-76 [[303952, 10795], [84402, 4217]] 1.406803 True 2 listen_conversions mood_1 Defiant 324.874553 1.256443e-72 [[249581, 65166], [72706, 15913]] 0.838248 True 3 listen_conversions genre_3 - 282.376331 2.279083e-63 [[284762, 29985], [78481, 10138]] 1.226777 True 4 listen_conversions genre_2 - 259.043360 2.773590e-58 [[299145, 15602], [83015, 5604]] 1.294324 True ... ... ... ... ... ... ... ... ... 154 user_conversions mood_1 Gritty 0.255320 6.133538e-01 [[235846, 4671], [159649, 3200]] 1.012051 False 155 user_conversions mood_1 Melancholy 0.183720 6.681957e-01 [[237216, 3301], [160587, 2262]] 1.012233 False 156 user_conversions mood_3 Gritty 0.091581 7.621766e-01 [[233926, 6591], [158413, 4436]] 0.993866 False 157 user_conversions mood_2 Urgent 0.026083 8.716985e-01 [[227220, 13297], [153866, 8983]] 0.997635 False 158 user_conversions genre_2 Spoken & Audio 0.006088 9.378078e-01 [[239335, 1182], [162045, 804]] 1.004637 False 318 rows \u00d7 8 columns master . loc [( master [ 'reject null' ] == True ) & ( master [ 'multiplier' ] > 1.5 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target feature group chi p-value cTable multiplier reject null 7 listen_conversions genre_1 New Age 136.770883 1.353331e-31 [[313269, 1478], [87913, 706]] 1.702137 True 9 listen_conversions mood_1 Tender 115.194233 7.135481e-27 [[312449, 2298], [87647, 972]] 1.507851 True 18 listen_conversions genre_2 New Age 71.594191 2.643338e-17 [[313509, 1238], [88081, 538]] 1.546783 True 23 listen_conversions genre_1 Children's 60.468486 7.476593e-15 [[313592, 1155], [88126, 493]] 1.518888 True master . loc [( master [ 'reject null' ] == True ) & ( master [ 'multiplier' ] < .5 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target feature group chi p-value cTable multiplier reject null 0 user_conversions genre_2 - 3922.996570 0.000000e+00 [[223516, 17001], [158644, 4205]] 0.348479 True 1 user_conversions genre_1 Latin 1976.239449 0.000000e+00 [[228943, 11574], [159411, 3438]] 0.426610 True 3 user_conversions genre_3 - 6170.632022 0.000000e+00 [[209266, 31251], [153977, 8872]] 0.385834 True 4 user_conversions mood_1 Lively 1155.365166 3.060886e-253 [[232673, 7844], [160347, 2502]] 0.462844 True 11 user_conversions genre_2 Jazz 534.239051 3.378526e-118 [[236893, 3624], [161702, 1147]] 0.463673 True 16 user_conversions mood_3 - 436.706320 5.640063e-97 [[238863, 1654], [162498, 351]] 0.311941 True 18 user_conversions mood_2 - 395.335894 5.705144e-88 [[238982, 1535], [162516, 333]] 0.319010 True 21 user_conversions mood_1 - 379.442742 1.645452e-84 [[239045, 1472], [162530, 319]] 0.318734 True 23 user_conversions mood_1 Other 340.234697 5.672282e-76 [[239134, 1383], [162537, 312]] 0.331911 True 26 user_conversions mood_1 Tender 313.405005 3.958040e-70 [[238072, 2445], [162024, 825]] 0.495797 True 30 user_conversions genre_1 Spoken & Audio 282.114763 2.598715e-63 [[239551, 966], [162664, 185]] 0.282034 True 32 user_conversions genre_1 Children's 267.069910 4.937672e-60 [[239209, 1308], [162509, 340]] 0.382623 True 41 user_conversions genre_1 New Age 239.970389 3.991738e-54 [[238860, 1657], [162322, 527]] 0.468009 True 45 user_conversions genre_2 Easy Listening 225.530477 5.624862e-51 [[238912, 1605], [162331, 518]] 0.474997 True 48 user_conversions genre_1 Traditional 214.931791 1.153312e-48 [[239311, 1206], [162507, 342]] 0.417609 True 52 user_conversions genre_1 - 175.754457 4.096893e-40 [[239754, 763], [162668, 181]] 0.349637 True 54 user_conversions mood_1 Serious 162.207641 3.726709e-37 [[239237, 1280], [162416, 433]] 0.498285 True 64 user_conversions genre_1 Easy Listening 146.743632 8.928423e-34 [[239783, 734], [162656, 193]] 0.387623 True 77 user_conversions mood_1 Somber 112.482411 2.801392e-26 [[239813, 704], [162637, 212]] 0.444034 True 95 user_conversions genre_1 Blues 64.312880 1.061495e-15 [[240162, 355], [162750, 99]] 0.411519 True 96 user_conversions genre_3 Holiday 62.618863 2.508395e-15 [[240238, 279], [162782, 67]] 0.354410 True 99 user_conversions genre_2 Holiday 55.198819 1.089342e-13 [[240302, 215], [162803, 46]] 0.315802 True 117 user_conversions genre_1 Other 24.171106 8.814482e-07 [[240383, 134], [162812, 37]] 0.407675 True new_master = pd . DataFrame () for target in new_metrics : # target = sub_targets[2] chidf = pd . DataFrame () chidf [ target ] = df [ target ] chidf [ 'n_tracks' ] = df [ 'n_tracks' ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) tar_value = 0.8 chidf [ target ] = chidf [ target ] >= tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ target , ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } >= { tar_value : .0f } \" ) sns . histplot ( b , label = f \" { target } < { tar_value : .0f } \" ) plt . title ( f \" { target } , { ind } \" ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'target' , 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) new_master = pd . concat (( new_master , welchsum )) new_master .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target feature test stat p-value upper q avg lower q avg reject null 0 listen_conversions n_tracks 10.791139 1.289400e-21 151.33751 143.74398 True 1 listen_conversions n_albums 2.441572 1.550563e-02 78.52885 77.60380 False 2 listen_conversions n_local_tracks -1.514198 1.315704e-01 2.04345 2.13447 False 3 listen_conversions n_artists 1.186743 2.367580e-01 73.94089 73.48707 False 0 user_conversions n_artists 29.009043 3.528897e-73 80.09057 69.51769 True 1 user_conversions n_albums 27.865311 5.520382e-70 84.69724 73.45058 True 2 user_conversions n_tracks 12.465380 1.108146e-26 150.70376 140.85719 True 3 user_conversions n_local_tracks 3.208929 1.563093e-03 2.20957 2.02793 False Conclusions \u00b6 Discrete, Independent Variables \u00b6 We note that there is class imbalance in the discrete independent variables: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 10 )) dff = pd . DataFrame ( df [ des_features [ 0 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 1 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 2 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'mood' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'mood' , x = 'count' , orient = 'h' , ax = ax [ 0 ]) dff = pd . DataFrame ( df [ des_features [ 3 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 4 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 5 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'genre' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'genre' , x = 'count' , orient = 'h' , ax = ax [ 1 ]) plt . tight_layout () This class imbalance can have a variety of effects (and might be derived from a variety of sources). For example, users will have more choice when listening to popular genres likeIndie Rock and Rap, and less choice with genres like Blues and Easy listening. As it so happens, when we look to the relationship between genre/mood and the dependent variables, many of the genre/moods with smaller class sizes will have a positive multiplier effect on the dependent variable Continuous, Independent Variables \u00b6 The four continuous variables of focus in this dataset are highly tailed. Due to this, our statistical tests will require bootstrapping. quant = 0.999 con_features = [ 'n_albums' , 'n_artists' , 'n_tracks' , 'n_local_tracks' ] for target in con_features : cutoff = np . quantile ( df [ target ], quant ) y = df . loc [ df [ target ] < cutoff ] removed = df . loc [ ~ ( df [ target ] < cutoff )] print ( f \"removed items: { removed . shape [ 0 ] } \" ) y . plot ( kind = 'hist' , y = target , bins = 100 , density = True ) plt . show () removed items: 404 removed items: 405 removed items: 404 removed items: 406 an example of bootstrapping n_albums means = [] ind = con_features [ 0 ] for i in range ( 100 ): boot = random . sample ( list ( df . loc [ ( df [ ind ] > 9 ) & ( df [ ind ] < 999 ) ][ ind ] . values ), k = 1000 ) means . append ( np . mean ( boot )) stuff = plt . hist ( means , bins = 100 , density = True ) Discrete, Dependent Variables \u00b6 For the purposes of investigating a \"successful\" playlist, there are 5 primary metrics: targets ['streams', 'stream30s', 'dau', 'wau', 'mau', 'mau_previous_month', 'mau_both_months', 'users', 'skippers', 'monthly_stream30s', 'monthly_owner_stream30s'] df [ sub_targets ] . describe () . round ( 1 ) . to_excel ( \"file.xlsx\" ) and \"top\" performers in each of these metrics were based on top 10% and top 1% quantiles: print ( 'p99 targets' ) for target in sub_targets : space = ' ' * ( 20 - len ( str ( target ))) print ( f \" { target } : { space } { np . quantile ( df [ target ], 0.99 ) } \" ) print () print ( 'p90 targets' ) for target in sub_targets : space = ' ' * ( 20 - len ( str ( target ))) print ( f \" { target } : { space } { np . quantile ( df [ target ], 0.90 ) } \" ) p99 targets mau_previous_month: 130.0 mau_both_months: 19.0 mau: 143.0 monthly_stream30s: 2843.0 stream30s: 113.0 p90 targets mau_previous_month: 9.0 mau_both_months: 2.0 mau: 9.0 monthly_stream30s: 432.0 stream30s: 17.0 You can imagine with these metrics, some concerns are: what if a playlist was made in the current month, or even current day? playlist is not properly represented by the data how do we normalize by playlists that already have a high visibility? i.e. what if a playlist is \"good\" but just isn't getting noticed? can compute conversion metrics: 30 second listens / total listens mau both months / mau previous month While noting these shortcomings, to keep the analysis focused I singled out the previously mentioned targets, with a focus on monthly_stream30s as the north star metric. monthly_stream30s is advantageous as a nort star metric since it contains data from the entire month (reducing variance) only contains relevant listens (greater than 30 seconds long). Some disadvantages of this metric are that it doesn't account for just a few listeners who may be providing the majority of listens, and playlists that were made in the current month will be undervalued. Dependency \u00b6 Chi Square \u00b6 In the chi-square test, the contigency table was used to calculate a multiplier effect. This is a ratio of ratios: the count of upper quantile over bottom quantile for the given group over the count of upper quantile over bottom quantile for non-group. In other words, it articulates how much more likely a sample in the given group is likely to be in the upper quantile vs a sample not in the given group chisq_results = pd . read_csv ( \"chi_square_results.csv\" , index_col = 0 ) chisq_results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 12 stream30s 0.99 113.0 mood_3 - 125.854082 3.309444e-29 [[397434 1935]\\n [ 3927 70]] 3.661181 True 0 11 monthly_stream30s 0.99 2843.0 mood_2 - 109.163417 1.494430e-25 [[397529 1804]\\n [ 3969 64]] 3.553294 True 0 67 mau_previous_month 0.90 9.0 genre_1 - 95.863487 1.230846e-22 [[365249 769]\\n [ 37173 175]] 2.236007 True 0 10 monthly_stream30s 0.99 2843.0 mood_1 - 112.668942 2.549855e-26 [[397605 1728]\\n [ 3970 63]] 3.651389 True 0 7 stream30s 0.99 113.0 mood_1 - 141.501726 1.249779e-32 [[397646 1723]\\n [ 3929 68]] 3.994277 True 0 chisq_results [ 'target' ] . unique () array(['stream30s', 'monthly_stream30s', 'mau_previous_month', 'mau', 'mau_both_months'], dtype=object) chisq_results [ 'upper q' ] . unique () array([0.99, 0.9 ]) Taking together the five targets, the two upper quantiles, and the six categorical independent variables, we can identify which group occured the most frequently as a variable of influence: chisq_results . loc [( chisq_results [ 'feature' ] . str . contains ( 'genre' )) & ( chisq_results [ 'group' ] != '-' )][ 'group' ] . value_counts () Traditional 16 Children's 16 Jazz 14 Latin 12 Easy Listening 8 Soundtrack 8 New Age 7 Holiday 6 Spoken & Audio 4 Other 2 Name: group, dtype: int64 Using these value counts as a \"rank\" we can then groupby this rank and see how each group is influencing the propensity to be in the upper quadrant Taking \"Romantic\" as an example, we see that it's multiplier effect is relatively consistent across the five targets and two quantiles: sort_key = { i : j for i , j in zip ( chisq_results [ 'group' ] . value_counts () . index . values , range ( chisq_results [ 'group' ] . nunique ()))} chisq_results [ 'rank' ] = chisq_results [ 'group' ] . apply ( lambda x : sort_key [ x ]) chisq_results . sort_values ( 'rank' , inplace = True ) # chisq_results.drop('rank', axis=1, inplace=True) chisq_results . loc [ chisq_results [ 'group' ] != '-' ][: 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 3 mau_both_months 0.99 19.0 mood_1 Romantic 109.693770 1.143607e-25 [[390177 9231]\\n [ 3766 192]] 2.154933 True 1 5 mau_previous_month 0.90 9.0 mood_2 Romantic 1379.938658 4.806442e-302 [[357822 8196]\\n [ 35327 2021]] 2.497610 True 1 8 stream30s 0.99 113.0 mood_1 Romantic 139.245969 3.891401e-32 [[390152 9217]\\n [ 3791 206]] 2.300158 True 1 6 mau_previous_month 0.99 130.0 mood_2 Romantic 104.434543 1.624732e-24 [[389323 10013]\\n [ 3826 204]] 2.073152 True 1 6 mau 0.90 9.0 mood_1 Romantic 1328.179994 8.498925e-291 [[355892 7442]\\n [ 38051 1981]] 2.489700 True 1 5 mau 0.99 143.0 mood_3 Romantic 122.574129 1.728356e-28 [[389664 9685]\\n [ 3810 207]] 2.185929 True 1 10 stream30s 0.99 113.0 mood_3 Romantic 136.025552 1.969792e-31 [[389689 9680]\\n [ 3785 212]] 2.254825 True 1 6 mau_previous_month 0.90 9.0 mood_1 Romantic 1142.816205 1.633755e-250 [[358408 7610]\\n [ 35535 1813]] 2.402893 True 1 6 monthly_stream30s 0.99 2843.0 mood_3 Romantic 149.750731 1.965370e-34 [[389660 9673]\\n [ 3814 219]] 2.313066 True 1 4 monthly_stream30s 0.99 2843.0 mood_1 Romantic 175.072639 5.772239e-40 [[390131 9202]\\n [ 3812 221]] 2.457919 True 1 6 mau 0.99 143.0 mood_2 Romantic 105.450504 9.729814e-25 [[389336 10013]\\n [ 3813 204]] 2.080289 True 1 5 mau_previous_month 0.99 130.0 mood_3 Romantic 112.605179 2.633191e-26 [[389647 9689]\\n [ 3827 203]] 2.133192 True 1 6 stream30s 0.99 113.0 mood_2 Romantic 148.026986 4.679851e-34 [[389374 9995]\\n [ 3775 222]] 2.290974 True 1 2 mau 0.99 143.0 mood_1 Romantic 202.823985 5.053546e-46 [[390156 9193]\\n [ 3787 230]] 2.577588 True 1 7 monthly_stream30s 0.99 2843.0 mood_2 Romantic 146.934024 8.112487e-34 [[389339 9994]\\n [ 3810 223]] 2.280176 True 1 8 mau_previous_month 0.90 9.0 mood_3 Romantic 1013.797108 1.800082e-222 [[357949 8069]\\n [ 35525 1823]] 2.276429 True 1 4 mau_previous_month 0.99 130.0 mood_1 Romantic 156.500834 6.579992e-36 [[390127 9209]\\n [ 3816 214]] 2.375740 True 1 8 mau 0.90 9.0 mood_3 Romantic 1170.355016 1.690629e-256 [[355429 7905]\\n [ 38045 1987]] 2.348287 True 1 1 mau 0.90 9.0 mood_2 Romantic 1531.190216 0.000000e+00 [[355299 8035]\\n [ 37850 2182]] 2.549159 True 1 2 mau 0.90 9.0 mood_1 Lively 2423.134070 0.000000e+00 [[355493 7841]\\n [ 37527 2505]] 3.026380 True 2 chisq_results . loc [( chisq_results [ 'group' ] == 'Traditional' ) & ( chisq_results [ 'target' ] == 'monthly_stream30s' )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 36 monthly_stream30s 0.99 2843.0 genre_3 Traditional 29.032918 7.115879e-08 [[396376 2957]\\n [ 3973 60]] 2.024364 True 3 27 monthly_stream30s 0.99 2843.0 genre_2 Traditional 47.457479 5.621008e-12 [[396211 3122]\\n [ 3962 71]] 2.274246 True 3 Let's use this idea of average multiplier effect, and average chi-square statistic to summarize by group. Sorting by the test statistic, we see the top 5 most influential groups: chisq_results . groupby ( 'group' )[[ 'chi' , 'multiplier' , 'rank' ]] . mean () . sort_values ( 'chi' , ascending = False )[: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi multiplier rank group Latin 1686.610898 3.001282 6.0 - 766.884882 3.049100 0.0 Sophisticated 581.181538 2.055203 18.0 Lively 523.373076 2.364492 2.0 Romantic 493.442950 2.318001 1.0 Soundtrack 345.506268 2.209295 9.0 Jazz 323.657066 2.342954 5.0 Fiery 261.957158 2.244027 15.0 Tender 212.399270 3.033890 16.0 Traditional 176.194741 2.361342 3.0 Sorting instead by the multiplier, we can see which group has the heaviest influence chisq_results . groupby ( 'group' )[[ 'chi' , 'multiplier' , 'rank' ]] . mean () . sort_values ( 'multiplier' , ascending = False )[: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi multiplier rank group - 766.884882 3.049100 0.0 Tender 212.399270 3.033890 16.0 Latin 1686.610898 3.001282 6.0 Children's 165.058604 2.871261 4.0 Holiday 41.741338 2.836528 12.0 New Age 75.783147 2.754796 10.0 Spoken & Audio 163.859264 2.610393 14.0 Peaceful 61.046237 2.564297 13.0 Other 166.299708 2.425104 11.0 Easy Listening 99.533804 2.407295 8.0 Sorting instead by rank, we see which groups show up most frequently chisq_results . groupby ( 'group' )[[ 'chi' , 'multiplier' , 'rank' ]] . mean () . sort_values ( 'rank' , ascending = True )[: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi multiplier rank group - 766.884882 3.049100 0.0 Romantic 493.442950 2.318001 1.0 Lively 523.373076 2.364492 2.0 Traditional 176.194741 2.361342 3.0 Children's 165.058604 2.871261 4.0 Jazz 323.657066 2.342954 5.0 Latin 1686.610898 3.001282 6.0 Serious 103.700606 2.190306 7.0 Easy Listening 99.533804 2.407295 8.0 Soundtrack 345.506268 2.209295 9.0 chisq_results . loc [ chisq_results [ 'target' ] == 'monthly_stream30s' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 14 monthly_stream30s 0.99 2843.0 mood_3 - 95.615882 1.394829e-22 [[397392 1941]\\n [ 3969 64]] 3.301357 True 0 3 monthly_stream30s 0.99 2843.0 genre_1 - 198.911522 3.608821e-45 [[398442 891]\\n [ 3980 53]] 5.954979 True 0 10 monthly_stream30s 0.99 2843.0 mood_1 - 112.668942 2.549855e-26 [[397605 1728]\\n [ 3970 63]] 3.651389 True 0 11 monthly_stream30s 0.99 2843.0 mood_2 - 109.163417 1.494430e-25 [[397529 1804]\\n [ 3969 64]] 3.553294 True 0 6 monthly_stream30s 0.99 2843.0 mood_3 Romantic 149.750731 1.965370e-34 [[389660 9673]\\n [ 3814 219]] 2.313066 True 1 4 monthly_stream30s 0.99 2843.0 mood_1 Romantic 175.072639 5.772239e-40 [[390131 9202]\\n [ 3812 221]] 2.457919 True 1 7 monthly_stream30s 0.99 2843.0 mood_2 Romantic 146.934024 8.112487e-34 [[389339 9994]\\n [ 3810 223]] 2.280176 True 1 22 monthly_stream30s 0.99 2843.0 mood_2 Lively 62.570224 2.571115e-15 [[393976 5357]\\n [ 3920 113]] 2.120023 True 2 5 monthly_stream30s 0.99 2843.0 mood_1 Lively 172.134248 2.529542e-39 [[389222 10111]\\n [ 3798 235]] 2.381860 True 2 36 monthly_stream30s 0.99 2843.0 genre_3 Traditional 29.032918 7.115879e-08 [[396376 2957]\\n [ 3973 60]] 2.024364 True 3 27 monthly_stream30s 0.99 2843.0 genre_2 Traditional 47.457479 5.621008e-12 [[396211 3122]\\n [ 3962 71]] 2.274246 True 3 37 monthly_stream30s 0.99 2843.0 genre_2 Children's 28.313598 1.031687e-07 [[397689 1644]\\n [ 3994 39]] 2.362100 True 4 2 monthly_stream30s 0.99 2843.0 genre_1 Children's 207.229586 5.524721e-47 [[397760 1573]\\n [ 3958 75]] 4.791570 True 4 6 monthly_stream30s 0.90 432.0 genre_1 Children's 262.624693 4.596280e-59 [[361785 1286]\\n [ 39933 362]] 2.550270 True 4 16 monthly_stream30s 0.99 2843.0 genre_2 Jazz 79.207991 5.590349e-19 [[394671 4662]\\n [ 3924 109]] 2.351584 True 5 30 monthly_stream30s 0.99 2843.0 genre_3 Jazz 39.188768 3.847472e-10 [[395392 3941]\\n [ 3953 80]] 2.030414 True 5 0 monthly_stream30s 0.99 2843.0 genre_1 Latin 537.892273 5.419582e-119 [[384749 14584]\\n [ 3605 428]] 3.132127 True 6 0 monthly_stream30s 0.90 432.0 genre_1 Latin 1150.625294 3.280867e-252 [[350782 12289]\\n [ 37572 2723]] 2.068731 True 6 50 monthly_stream30s 0.99 2843.0 mood_2 Serious 20.339173 6.485903e-06 [[397730 1603]\\n [ 3998 35]] 2.172101 True 7 38 monthly_stream30s 0.99 2843.0 genre_2 Easy Listening 28.186480 1.101715e-07 [[397256 2077]\\n [ 3987 46]] 2.206712 True 8 29 monthly_stream30s 0.99 2843.0 genre_1 Easy Listening 40.400033 2.069376e-10 [[398435 898]\\n [ 4004 29]] 3.213550 True 8 20 monthly_stream30s 0.99 2843.0 genre_1 Soundtrack 66.073066 4.345131e-16 [[392575 6758]\\n [ 3897 136]] 2.027276 True 9 28 monthly_stream30s 0.99 2843.0 genre_1 New Age 43.730647 3.768245e-11 [[397202 2131]\\n [ 3980 53]] 2.482109 True 10 12 monthly_stream30s 0.90 432.0 genre_1 New Age 166.484617 4.335181e-38 [[361286 1785]\\n [ 39896 399]] 2.024214 True 10 21 monthly_stream30s 0.99 2843.0 genre_3 New Age 63.004025 2.062846e-15 [[397632 1701]\\n [ 3982 51]] 2.993960 True 10 15 monthly_stream30s 0.99 2843.0 genre_2 New Age 85.761620 2.029879e-20 [[397614 1719]\\n [ 3976 57]] 3.315998 True 10 33 monthly_stream30s 0.99 2843.0 mood_1 Other 30.443472 3.437382e-08 [[397678 1655]\\n [ 3993 40]] 2.407101 True 11 11 monthly_stream30s 0.90 432.0 mood_1 Other 197.598843 6.979647e-45 [[361719 1352]\\n [ 39952 343]] 2.296943 True 11 26 monthly_stream30s 0.99 2843.0 mood_1 Peaceful 47.834009 4.638752e-12 [[397055 2278]\\n [ 3976 57]] 2.498765 True 13 17 monthly_stream30s 0.99 2843.0 mood_3 Peaceful 69.964512 6.038104e-17 [[396736 2597]\\n [ 3963 70]] 2.698383 True 13 12 monthly_stream30s 0.99 2843.0 mood_2 Peaceful 99.188851 2.295356e-23 [[396395 2938]\\n [ 3948 85]] 2.904813 True 13 52 monthly_stream30s 0.99 2843.0 genre_1 Spoken & Audio 19.783961 8.670724e-06 [[398209 1124]\\n [ 4006 27]] 2.387798 True 14 19 monthly_stream30s 0.90 432.0 genre_1 Spoken & Audio 120.508309 4.896128e-28 [[362147 924]\\n [ 40068 227]] 2.220451 True 14 1 monthly_stream30s 0.99 2843.0 mood_1 Tender 218.759022 1.686848e-49 [[396180 3153]\\n [ 3916 117]] 3.754151 True 16 23 monthly_stream30s 0.99 2843.0 mood_2 Easygoing 61.911050 3.593275e-15 [[394661 4672]\\n [ 3931 102]] 2.191889 True 17 25 monthly_stream30s 0.99 2843.0 mood_3 Easygoing 49.108110 2.422366e-12 [[394265 5068]\\n [ 3931 102]] 2.018593 True 17 34 monthly_stream30s 0.99 2843.0 mood_3 Somber 29.620347 5.255040e-08 [[397107 2226]\\n [ 3984 49]] 2.194114 True 19 It creates some fog to jumble together mood/genres this way. We can instead separate them and ask questions like: What is the most influential primary genre on monthly streams over 30 seconds? \u00b6 Answer: Children's followed by Latin Reason: both genre's appear as influential in other guardrail metrics (high rank), have high test statistics, and are influential in both p99 and p90 with multiplier effects of [4.8, 2.6] and [3.1, 2.1], respectively. chisq_results . loc [( chisq_results [ 'feature' ] == 'genre_1' ) & ( chisq_results [ 'target' ] == 'monthly_stream30s' )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 3 monthly_stream30s 0.99 2843.0 genre_1 - 198.911522 3.608821e-45 [[398442 891]\\n [ 3980 53]] 5.954979 True 0 2 monthly_stream30s 0.99 2843.0 genre_1 Children's 207.229586 5.524721e-47 [[397760 1573]\\n [ 3958 75]] 4.791570 True 4 6 monthly_stream30s 0.90 432.0 genre_1 Children's 262.624693 4.596280e-59 [[361785 1286]\\n [ 39933 362]] 2.550270 True 4 0 monthly_stream30s 0.99 2843.0 genre_1 Latin 537.892273 5.419582e-119 [[384749 14584]\\n [ 3605 428]] 3.132127 True 6 0 monthly_stream30s 0.90 432.0 genre_1 Latin 1150.625294 3.280867e-252 [[350782 12289]\\n [ 37572 2723]] 2.068731 True 6 29 monthly_stream30s 0.99 2843.0 genre_1 Easy Listening 40.400033 2.069376e-10 [[398435 898]\\n [ 4004 29]] 3.213550 True 8 20 monthly_stream30s 0.99 2843.0 genre_1 Soundtrack 66.073066 4.345131e-16 [[392575 6758]\\n [ 3897 136]] 2.027276 True 9 28 monthly_stream30s 0.99 2843.0 genre_1 New Age 43.730647 3.768245e-11 [[397202 2131]\\n [ 3980 53]] 2.482109 True 10 12 monthly_stream30s 0.90 432.0 genre_1 New Age 166.484617 4.335181e-38 [[361286 1785]\\n [ 39896 399]] 2.024214 True 10 52 monthly_stream30s 0.99 2843.0 genre_1 Spoken & Audio 19.783961 8.670724e-06 [[398209 1124]\\n [ 4006 27]] 2.387798 True 14 19 monthly_stream30s 0.90 432.0 genre_1 Spoken & Audio 120.508309 4.896128e-28 [[362147 924]\\n [ 40068 227]] 2.220451 True 14 What is the most influential primary mood on monthly streams over 30 seconds? \u00b6 Answer: Romantic and Lively Reason: Romantic and Lively moods appear multiple times as highly influential (high rank) they have high multipliers. A contendent may be Tender, as it has a high multiplier effect as well at 3.75 chisq_results . loc [( chisq_results [ 'feature' ] == 'mood_1' ) & ( chisq_results [ 'target' ] == 'monthly_stream30s' )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 10 monthly_stream30s 0.99 2843.0 mood_1 - 112.668942 2.549855e-26 [[397605 1728]\\n [ 3970 63]] 3.651389 True 0 4 monthly_stream30s 0.99 2843.0 mood_1 Romantic 175.072639 5.772239e-40 [[390131 9202]\\n [ 3812 221]] 2.457919 True 1 5 monthly_stream30s 0.99 2843.0 mood_1 Lively 172.134248 2.529542e-39 [[389222 10111]\\n [ 3798 235]] 2.381860 True 2 33 monthly_stream30s 0.99 2843.0 mood_1 Other 30.443472 3.437382e-08 [[397678 1655]\\n [ 3993 40]] 2.407101 True 11 11 monthly_stream30s 0.90 432.0 mood_1 Other 197.598843 6.979647e-45 [[361719 1352]\\n [ 39952 343]] 2.296943 True 11 26 monthly_stream30s 0.99 2843.0 mood_1 Peaceful 47.834009 4.638752e-12 [[397055 2278]\\n [ 3976 57]] 2.498765 True 13 1 monthly_stream30s 0.99 2843.0 mood_1 Tender 218.759022 1.686848e-49 [[396180 3153]\\n [ 3916 117]] 3.754151 True 16 Which Categorical Feature is most influential overall? \u00b6 Answer: genre_1, followed by genre_2 and mood_1 Reason: we see that these features appear multiple times across the 5 different targets and 2 different quantiles chisq_results [ 'feature' ] . value_counts () genre_1 48 genre_2 39 mood_1 34 mood_2 28 mood_3 17 genre_3 16 Name: feature, dtype: int64 What are the shortcomings of this analysis? \u00b6 We haven't taken into account confounding variables. For example, perhaps Latin genre is typically associated with Lively mood. Then which variable is it that actually contributes to a highly performing playlist? We have strategies for dealing with this. We can stratify the confounding variables by over or under sampling. We can also consider them together in a forward selection logistic model. We will take the latter approach later on in the analysis. We haven't considered the categorical variables alongside the continuous variables, so we don't know how they fit overall in terms of relative improtance. We will approach this the same way as the confounding variables issue, and incorporate all variables in a logistic regression. t-Test \u00b6 ttest_results = pd . read_csv ( \"t_test_results.csv\" , index_col = 0 ) ttest_results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target feature test stat p-value upper q avg lower q avg reject null 0 mau_previous_month n_albums -25.318099 4.772854e-63 69.77939 78.70629 True 1 mau_previous_month n_artists -15.418330 9.408966e-36 68.08641 73.90254 True 2 mau_previous_month n_local_tracks -9.550137 7.728853e-18 1.60489 2.07692 True 3 mau_previous_month n_tracks 6.086774 5.913654e-09 149.50371 145.10534 True 0 mau_both_months n_artists 52.402365 2.845239e-114 91.41907 71.97618 True Models \u00b6 log_results = pd . read_csv ( \"../../scripts/fwd_selection_results.txt\" , header = None , index_col = 0 ) log_results . columns = [ 'feature' , 'pseudo r2' ] log_results . reset_index ( inplace = True , drop = True ) log_results . drop ( 0 , axis = 0 , inplace = True ) log_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature pseudo r2 1 intercept 2.197000e-12 2 n_albums 2.614000e-02 3 genre_1_Latin 3.093000e-02 4 genre_1_Indie Rock 3.274000e-02 5 genre_1_Rap 3.431000e-02 6 genre_1_Dance & House 3.568000e-02 7 genre_1_Rock 3.674000e-02 8 mood_1_Energizing 3.772000e-02 9 genre_1_Children's 3.863000e-02 10 mood_1_Tender 3.931000e-02 11 mood_1_Other 3.995000e-02 12 n_tracks 4.052000e-02 13 mood_1_Peaceful 4.106000e-02 14 mood_1_Romantic 4.161000e-02 15 genre_1_Electronica 4.208000e-02 16 genre_2_Indie Rock 4.248000e-02 17 mood_2_Energizing 4.287000e-02 18 genre_1_R&B 4.319000e-02 19 genre_3_Indie Rock 4.353000e-02 20 genre_1_Classical 4.379000e-02 21 genre_2_Alternative 4.403000e-02 22 genre_2_Metal 4.427000e-02 23 mood_2_Peaceful 4.449000e-02 24 mood_2_Romantic 4.472000e-02 25 mood_3_Romantic 4.498000e-02 26 genre_3_Alternative 4.522000e-02 target = \"monthly_stream30s\" y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = 0 names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = 1 weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels basemodel = pd . read_csv ( \"../../scripts/basemodel.csv\" , index_col = 0 ) X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns res_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] intercept -2.0919 0.012 -180.211 0.000 -2.115 -2.069 n_albums 0.2659 0.012 21.370 0.000 0.241 0.290 genre_1_Latin 0.5389 0.025 21.354 0.000 0.489 0.588 genre_1_Indie Rock -0.5919 0.020 -30.130 0.000 -0.630 -0.553 genre_1_Rap -0.3552 0.018 -19.651 0.000 -0.391 -0.320 genre_1_Dance & House -0.3364 0.023 -14.444 0.000 -0.382 -0.291 genre_1_Rock -0.4325 0.024 -18.072 0.000 -0.479 -0.386 mood_1_Energizing -0.3012 0.026 -11.592 0.000 -0.352 -0.250 genre_1_Children's 0.7694 0.061 12.545 0.000 0.649 0.890 mood_1_Tender 0.6086 0.053 11.496 0.000 0.505 0.712 mood_1_Other 0.8435 0.062 13.497 0.000 0.721 0.966 n_tracks 0.0465 0.006 7.665 0.000 0.035 0.058 mood_1_Peaceful 0.7355 0.057 12.849 0.000 0.623 0.848 mood_1_Romantic 0.3608 0.032 11.187 0.000 0.298 0.424 genre_1_Electronica -0.2585 0.033 -7.726 0.000 -0.324 -0.193 genre_2_Indie Rock -0.2338 0.022 -10.617 0.000 -0.277 -0.191 mood_2_Energizing -0.1235 0.018 -6.837 0.000 -0.159 -0.088 genre_1_R&B -0.2373 0.030 -7.999 0.000 -0.295 -0.179 genre_3_Indie Rock -0.1994 0.022 -8.880 0.000 -0.243 -0.155 genre_1_Classical -0.5369 0.059 -9.114 0.000 -0.652 -0.421 genre_2_Alternative 0.1578 0.018 8.915 0.000 0.123 0.192 genre_2_Metal 0.3654 0.039 9.356 0.000 0.289 0.442 mood_2_Peaceful 0.4354 0.053 8.150 0.000 0.331 0.540 mood_2_Romantic 0.2643 0.031 8.628 0.000 0.204 0.324 mood_3_Romantic 0.2600 0.031 8.363 0.000 0.199 0.321 genre_3_Alternative 0.1152 0.018 6.548 0.000 0.081 0.150 n_artists 0.0968 0.013 7.587 0.000 0.072 0.122 genre_1_Metal 0.3371 0.041 8.282 0.000 0.257 0.417 mood_1_Aggressive -0.2743 0.041 -6.671 0.000 -0.355 -0.194 mood_3_Peaceful 0.3313 0.057 5.778 0.000 0.219 0.444 mood_1_Empowering 0.1344 0.020 6.801 0.000 0.096 0.173 genre_1_Religious -0.1832 0.032 -5.799 0.000 -0.245 -0.121 genre_3_Metal 0.2308 0.043 5.361 0.000 0.146 0.315 genre_3_R&B -0.1163 0.022 -5.238 0.000 -0.160 -0.073 Final Figures and Tables \u00b6 df = pd . read_csv ( \"../../data/playlist_summary_external-4.txt\" , delimiter = ' \\t ' ) Dependency \u00b6 sub_targets = [ 'mau_previous_month' , 'mau_both_months' , 'monthly_stream30s' , 'stream30s' ] # sub_targets = ['mau', 'dau', 'monthly_stream30s', 'stream30s'] des_features = [ 'mood_1' , 'mood_2' , 'mood_3' , 'genre_1' , 'genre_2' , 'genre_3' ] con_features = [ 'n_albums' , 'n_artists' , 'n_tracks' , 'n_local_tracks' ] df [ 'success' ] = df [ sub_targets ] . apply ( lambda x : x > np . quantile ( x , 0.75 )) . all ( axis = 1 ) df [ 'listen_conversions' ] = df [ 'stream30s' ] / df [ 'streams' ] df [ 'listen_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'user_retention' ] = df [ 'mau_both_months' ] / df [ 'mau_previous_month' ] df [ 'user_retention' ] . fillna ( value = 0 , inplace = True ) df [ 'user_conversions' ] = df [ 'mau' ] / df [ 'users' ] df [ 'user_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growing' ] = df [ 'mau' ] > df [ 'mau_previous_month' ] df [ 'mau_growth' ] = df [ 'mau' ] / df [ 'mau_previous_month' ] df [ 'mau_growth' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growth' ] . replace ([ np . inf , - np . inf ], 1 , inplace = True ) new_metrics = [ 'listen_conversions' , 'user_conversions' , 'user_retention' , 'mau_growth' ] df [ 'new_success' ] = df [ new_metrics ] . apply ( lambda x : ( x > 0.5 ) if ( max ( x ) == 1 ) else ( x > 1 )) . all ( axis = 1 ) Discrete \u00b6 fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 10 )) dff = pd . DataFrame ( df [ des_features [ 0 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 1 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 2 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'mood' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'mood' , x = 'count' , orient = 'h' , ax = ax [ 0 ]) dff = pd . DataFrame ( df [ des_features [ 3 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 4 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 5 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'genre' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'genre' , x = 'count' , orient = 'h' , ax = ax [ 1 ]) plt . tight_layout () fig . savefig ( \"discrete_rank_bar_plot.svg\" ) def make_chisum ( target = 'success' ): chidf = pd . DataFrame () chidf [ target ] = df [ target ] chisum = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind in des_features : chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) return chisum def make_cat_plots ( target = 'success' , ind_feature = 'genre_1' ): fig , ax = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 ), sharex = 'col' , sharey = 'row' ) genre_list = chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False )[ 'group' ] . values dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'percent' ] dff = dff . reset_index () dff . loc [ dff [ target ] == True , 'percent' ] = dff . loc [ dff [ target ] == True , 'percent' ] / dff . loc [ dff [ target ] == True , 'percent' ] . sum () dff . loc [ dff [ target ] == False , 'percent' ] = dff . loc [ dff [ target ] == False , 'percent' ] / dff . loc [ dff [ target ] == False , 'percent' ] . sum () dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 0 , 0 ]) ax [ 0 , 0 ] . set_title ( 'Best and Worst Genres, Percent' ) ax [ 0 , 0 ] . set_ylabel ( '' ) ax [ 0 , 0 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 1 , 0 ]) ax [ 1 , 0 ] . set_ylabel ( '' ) dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'count' ] dff = dff . reset_index () dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 0 , 1 ]) ax [ 0 , 1 ] . set_title ( 'Best and Worst Genres, Count' ) ax [ 0 , 1 ] . set_ylabel ( '' ) ax [ 0 , 1 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 1 , 1 ]) ax [ 1 , 1 ] . set_ylabel ( '' ) plt . tight_layout () ax [ 0 , 0 ] . get_legend () . remove () ax [ 1 , 1 ] . get_legend () . remove () ax [ 1 , 0 ] . get_legend () . remove () ax [ 0 , 1 ] . legend ( framealpha = 1 , facecolor = 'white' , title = \"Success\" ) return fig ind_feature = 'genre_1' target = 'success' chisum = make_chisum ( target ) fig = make_cat_plots ( target , ind_feature ) chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False ) . to_excel ( f ' { target } _ { ind_feature } _categorical.xlsx' ) fig . savefig ( f \" { target } _ { ind_feature } _categorical.svg\" ) ind_feature = 'mood_1' target = 'success' chisum = make_chisum ( target ) fig = make_cat_plots ( target , ind_feature ) chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False ) . to_excel ( f ' { target } _ { ind_feature } _categorical.xlsx' ) fig . savefig ( f \" { target } _ { ind_feature } _categorical.svg\" ) Continuous \u00b6 def make_con_plots ( target , con_features ): fig , (( ax1 , ax2 ),( ax3 , ax4 )) = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) chidf = pd . DataFrame () chidf [ target ] = df [ target ] welchsum = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind , ax in zip ( con_features , [ ax1 , ax2 , ax3 , ax4 ]): chidf [ ind ] = df [ ind ] a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } == True\" , ax = ax ) sns . histplot ( b , label = f \" { target } == False\" , ax = ax ) ax . set_title ( ind ) welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) ax . legend () return fig , welchsum target = 'new_success' fig , welchsum = make_con_plots ( target , con_features ) welchsum . to_excel ( f \" { target } _continuous.xlsx\" ) fig . savefig ( f \" { target } _ttest.svg\" ) Models \u00b6 Logistic Regression \u00b6 ### y target = \"success\" print ( target ) y = df [ target ] . values #### X X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( np . ones (( X_con . shape [ 0 ], 1 )), X_con , X_cat )) feature_names = [ 'intercept' ] + con_features + list ( enc . get_feature_names_out ()) data = pd . DataFrame ( X , columns = feature_names ) success def add_feature ( feature_names , basemodel , data , y , r2max = 0 , model = 'linear' , disp = 0 ): feature_max = None bestsum = None newmodel = None for feature in feature_names : basemodel [ feature ] = data [ feature ] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () score = float ( str ( pd . DataFrame ( summ . tables [ 0 ]) . loc [ 3 , 3 ])) if ( score > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = score feature_max = feature bestsum = est2 . summary () newmodel = basemodel . copy () if disp == 1 : print ( f \"new r2max, { feature_max } , { r2max } \" ) basemodel . drop ( labels = feature , axis = 1 , inplace = True ) return r2max , feature_max , bestsum , newmodel candidates = feature_names . copy () basemodel = pd . DataFrame () r2max = 0 while True : newr2max , feature_max , bestsum , newmodel = add_feature ( feature_names = candidates , basemodel = basemodel , data = data , y = y , r2max = r2max ) if newr2max > r2max : r2max = newr2max print ( f \"new r2max, { feature_max } , { r2max } \" ) with open ( \"success_fwd_selection_results.txt\" , \"a+\" ) as f : file_data = f . read () f . write ( f \"new r2max, { feature_max } , { r2max } \\n \" ) candidates . remove ( feature_max ) with open ( \"success_canidates.txt\" , \"w+\" ) as f : file_data = f . read () for i in candidates : f . write ( f \" { i } \\n \" ) basemodel = newmodel basemodel . to_csv ( \"success_basemodel.csv\" ) continue else : break basemodel = pd . read_csv ( \"success_basemodel.csv\" , index_col = 0 ) X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns display ( res_df ) res_df . to_excel ( \"success_log.xlsx\" ) ### y target = \"monthly_stream30s\" print ( target ) y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = 0 names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = 1 weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels basemodel = pd . read_csv ( \"../../scripts/basemodel.csv\" , index_col = 0 ) X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns display ( res_df ) res_df . to_excel ( \"log_model_monthly_stream30s.xlsx\" ) monthly_stream30s .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] intercept -2.0737 0.016 -133.150 0.000 -2.104 -2.043 n_albums 0.2656 0.012 21.287 0.000 0.241 0.290 genre_1_Latin 0.5408 0.027 19.906 0.000 0.488 0.594 genre_1_Indie Rock -0.5832 0.020 -28.964 0.000 -0.623 -0.544 genre_1_Rap -0.3259 0.020 -16.697 0.000 -0.364 -0.288 genre_1_Dance & House -0.3034 0.027 -11.069 0.000 -0.357 -0.250 genre_1_Rock -0.4226 0.025 -16.996 0.000 -0.471 -0.374 mood_1_Energizing -0.2844 0.027 -10.670 0.000 -0.337 -0.232 genre_1_Children's 0.7845 0.062 12.608 0.000 0.663 0.906 mood_1_Tender 0.4943 0.055 9.032 0.000 0.387 0.602 mood_1_Other 0.6206 0.074 8.413 0.000 0.476 0.765 n_tracks 0.0462 0.006 7.613 0.000 0.034 0.058 mood_1_Peaceful 0.6294 0.060 10.426 0.000 0.511 0.748 mood_1_Romantic 0.2981 0.033 9.038 0.000 0.233 0.363 genre_1_Electronica -0.2326 0.034 -6.792 0.000 -0.300 -0.165 genre_2_Indie Rock -0.2050 0.023 -8.998 0.000 -0.250 -0.160 mood_2_Energizing -0.1384 0.019 -7.421 0.000 -0.175 -0.102 genre_1_R&B -0.2335 0.030 -7.696 0.000 -0.293 -0.174 genre_3_Indie Rock -0.2540 0.024 -10.792 0.000 -0.300 -0.208 genre_1_Classical -0.5126 0.060 -8.609 0.000 -0.629 -0.396 genre_2_Alternative 0.1769 0.019 9.542 0.000 0.141 0.213 genre_2_Metal 0.4257 0.040 10.738 0.000 0.348 0.503 mood_2_Peaceful 0.3761 0.055 6.856 0.000 0.269 0.484 mood_2_Romantic 0.2300 0.031 7.414 0.000 0.169 0.291 mood_3_Romantic 0.2597 0.031 8.252 0.000 0.198 0.321 genre_3_Alternative 0.0482 0.019 2.529 0.011 0.011 0.086 n_artists 0.0954 0.013 7.464 0.000 0.070 0.120 genre_1_Metal 0.4049 0.042 9.680 0.000 0.323 0.487 mood_1_Aggressive -0.2660 0.042 -6.275 0.000 -0.349 -0.183 mood_3_Peaceful 0.2912 0.058 4.983 0.000 0.177 0.406 mood_1_Empowering 0.1197 0.021 5.789 0.000 0.079 0.160 genre_1_Religious -0.2328 0.033 -7.154 0.000 -0.297 -0.169 genre_3_Metal 0.1978 0.044 4.527 0.000 0.112 0.283 genre_3_R&B -0.1897 0.024 -8.057 0.000 -0.236 -0.144 mood_3_Yearning 0.1176 0.019 6.096 0.000 0.080 0.155 mood_2_- 0.4272 0.074 5.772 0.000 0.282 0.572 genre_3_Electronica -0.1893 0.026 -7.408 0.000 -0.239 -0.139 genre_2_Latin 0.3700 0.062 5.959 0.000 0.248 0.492 mood_3_Empowering 0.0909 0.021 4.386 0.000 0.050 0.132 genre_3_- -0.1084 0.021 -5.104 0.000 -0.150 -0.067 genre_1_Spoken & Audio 0.4897 0.089 5.489 0.000 0.315 0.665 genre_2_New Age 0.3718 0.067 5.546 0.000 0.240 0.503 genre_3_New Age 0.3384 0.067 5.053 0.000 0.207 0.470 genre_3_Rap -0.1484 0.026 -5.791 0.000 -0.199 -0.098 mood_1_Rowdy -0.2223 0.051 -4.373 0.000 -0.322 -0.123 mood_2_Rowdy -0.1655 0.039 -4.267 0.000 -0.242 -0.089 mood_2_Aggressive -0.1323 0.030 -4.345 0.000 -0.192 -0.073 genre_2_Spoken & Audio 0.3211 0.068 4.717 0.000 0.188 0.455 genre_1_New Age 0.2391 0.062 3.863 0.000 0.118 0.360 genre_2_Jazz 0.1958 0.043 4.533 0.000 0.111 0.280 genre_2_Pop 0.0819 0.016 4.999 0.000 0.050 0.114 genre_3_Rock -0.0849 0.020 -4.290 0.000 -0.124 -0.046 mood_1_Cool -0.1212 0.035 -3.464 0.001 -0.190 -0.053 mood_1_Gritty -0.1494 0.044 -3.386 0.001 -0.236 -0.063 mood_1_Easygoing -0.2261 0.074 -3.056 0.002 -0.371 -0.081 genre_3_Dance & House -0.0910 0.025 -3.595 0.000 -0.141 -0.041 mood_1_Excited 0.0583 0.018 3.248 0.001 0.023 0.093 summ . tables [ 0 ] Logit Regression Results Dep. Variable: y No. Observations: 403366 Model: Logit Df Residuals: 403309 Method: MLE Df Model: 56 Date: Sun, 24 Apr 2022 Pseudo R-squ.: 0.04795 Time: 18:07:32 Log-Likelihood: -1.2475e+05 converged: True LL-Null: -1.3104e+05 Covariance Type: nonrobust LLR p-value: 0.000 basemodel = pd . read_csv ( \"../../scripts/new_basemodel.csv\" , index_col = 0 ) y = df [ 'new_success' ] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns display ( res_df ) res_df . to_excel ( \"new_success_log_model.xlsx\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] intercept -2.4336 0.012 -201.725 0.000 -2.457 -2.410 genre_3_- -0.6766 0.025 -27.158 0.000 -0.725 -0.628 n_albums 0.1399 0.015 9.597 0.000 0.111 0.169 genre_1_Indie Rock 0.2702 0.016 17.240 0.000 0.240 0.301 mood_1_Defiant 0.2505 0.018 14.035 0.000 0.215 0.285 genre_1_Dance & House 0.3042 0.021 14.388 0.000 0.263 0.346 mood_1_Excited 0.1917 0.017 11.607 0.000 0.159 0.224 mood_1_Upbeat 0.2698 0.028 9.713 0.000 0.215 0.324 genre_2_Indie Rock 0.1527 0.019 7.854 0.000 0.115 0.191 genre_1_Rap 0.1876 0.019 9.843 0.000 0.150 0.225 genre_1_Religious 0.2676 0.030 8.877 0.000 0.209 0.327 mood_2_Romantic -0.2858 0.044 -6.533 0.000 -0.372 -0.200 mood_1_Yearning 0.1965 0.020 9.809 0.000 0.157 0.236 mood_1_Romantic -0.2540 0.045 -5.620 0.000 -0.343 -0.165 mood_3_Romantic -0.2249 0.042 -5.304 0.000 -0.308 -0.142 mood_1_Other -0.6658 0.134 -4.954 0.000 -0.929 -0.402 mood_2_Yearning 0.1714 0.019 9.044 0.000 0.134 0.209 mood_3_Yearning 0.1290 0.019 6.682 0.000 0.091 0.167 mood_2_Defiant 0.1263 0.019 6.645 0.000 0.089 0.164 mood_2_Excited 0.1043 0.018 5.871 0.000 0.069 0.139 genre_1_Electronica 0.1490 0.030 5.018 0.000 0.091 0.207 n_artists -0.0723 0.015 -4.776 0.000 -0.102 -0.043 mood_3_Urgent -0.1036 0.022 -4.766 0.000 -0.146 -0.061","title":"Spotify"},{"location":"extras/X4_Spotify_Appendix/#data-science-foundations-x4-spotify","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Prompt: What makes a playlist successful? Deck: PDF","title":"Data Science Foundations  X4: Spotify"},{"location":"extras/X4_Spotify_Appendix/#what-makes-a-playlist-successful","text":"Analysis Simple metric (dependent variable) mau_previous_month mau_both_months monthly_stream30s stream30s Design metric (dependent variable) 30s listens/tot listens (listen conversions) Users both months/users prev month (user conversions) Best small time performers (less than X total monthly listens + high conversion) Best new user playlist (owner has only 1 popular playlist) Define \"top\" Top 10% mau_previous_month: 9.0 mau_both_months: 2.0 mau: 9.0 monthly_stream30s: 432.0 stream30s: 17.0 Top 1% mau_previous_month: 130.0 mau_both_months: 19.0 mau: 143.0 monthly_stream30s: 2843.0 stream30s: 113.0 Independent variables moods and genres (categorical) number of tracks, albums, artists, and local tracks (continuous) The analysis will consist of: understand the distribution characteristics of the dependent and independent variables quantify the dependency of the dependent/independent variables for each of the simple and design metrics chi-square test bootstrap/t-test Key Conclusions for the simple metrics, what I define as \"popularlity\" key genres and moods were Romantic, Latin, Children's, Lively, Traditional, and Jazz . Playlists that included these genres/moods had a positive multiplier effect (usually in the vicinicty of 2x more likely) on the key simple metric (i.e. playlists with latin as a primary genre were 2.5x more likely to be in the top 10% of streams longer than 30 seconds) for the design metrics, what I define as \"trendiness\" some of the key genres and moods become flipped in comparison to the relationship with popular playlists. In particular, Dance & House, Indie Rock, and Defiant rise to the top as labels that push a playlist into the trendy category Column Name Description playlist_uri The key, Spotify uri of the playlist owner Playlist owner, Spotify username streams Number of streams from the playlist today stream30s Number of streams over 30 seconds from playlist today dau Number of Daily Active Users, i.e. users with a stream over 30 seconds from playlist today wau Number of Weekly Active Users, i.e. users with a stream over 30 seconds from playlist in past week mau Number of Monthly Active Users, i.e. users with a stream over 30 seconds from playlist in the past month mau_previous_months Number of Monthly Active users in the month prior to this one mau_both_months Number of users that were active on the playlist both this and the previous month users Number of users streaming (all streams) from this playlist this month skippers Number of users who skipped more than 90 percent of their streams today owner_country Country of the playlist owner n_tracks Number of tracks in playlist n_local_tracks Change in number of tracks on playlist since yesterday n_artists Number of unique artists in playlist n_albums Number of unique albums in playlist monthly_stream30s Number of streams over 30 seconds this month monthly_owner_stream30s Number of streams over 30 seconds by playlist owner this month tokens List of playlist title tokens, stopwords and punctuation removed genre_1 No. 1 Genre by weight of playlist tracks, from Gracenote metadata genre_2 No. 2 Genre by weight of playlist tracks, from Gracenote metadata genre_3 No. 3 Genre by weight of playlist tracks, from Gracenote metadata mood_1 No. 1 Mood by weight of playlist tracks, from Gracenote metadata mood_2 No. 2 Mood by weight of playlist tracks, from Gracenote metadata mood_3 No. 3 Mood by weight of playlist tracks, from Gracenote metadata","title":"What makes a playlist successful?"},{"location":"extras/X4_Spotify_Appendix/#imports","text":"# basic packages import pandas as pd pd . set_option ( 'display.max_columns' , 500 ) import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.discrete.discrete_model import Logit from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.utils.class_weight import compute_class_weight # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score df = pd . read_csv ( \"../../data/playlist_summary_external-4.txt\" , delimiter = ' \\t ' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playlist_uri owner streams stream30s dau wau mau mau_previous_month mau_both_months users skippers owner_country n_tracks n_local_tracks n_artists n_albums monthly_stream30s monthly_owner_stream30s tokens genre_1 genre_2 genre_3 mood_1 mood_2 mood_3 0 spotify:user:36069af6af076ccd9e597184a67b68c9:... 36069af6af076ccd9e597184a67b68c9 27 27 1 1 3 3 0 8 0 US 52 0 4 7 30 27 [\"ambient\", \"music\", \"therapy\", \"binaural\", \"b... Dance & House New Age Country & Folk Peaceful Romantic Somber 1 spotify:user:d1144a65b1c31c5f9f56b94f831124d5:... d1144a65b1c31c5f9f56b94f831124d5 0 0 0 1 2 1 1 3 0 US 131 0 112 113 112 94 [\"good\", \"living\"] Pop Indie Rock Alternative Excited Yearning Defiant 2 spotify:user:6b7fbed9edd6418ddd3b555bba441536:... 6b7fbed9edd6418ddd3b555bba441536 4 2 1 1 7 5 0 15 0 US 43 0 35 36 63 0 [\"norte\\u00f1a\"] Latin - - Lively Upbeat Romantic 3 spotify:user:580b98725077a94c3c8d01d07390426b:... 580b98725077a94c3c8d01d07390426b 12 12 1 1 4 6 1 10 0 US 27 1 27 26 154 108 [] Dance & House Electronica Pop Excited Aggressive Defiant 4 spotify:user:1305d39070c95d161cc502e15014897d:... 1305d39070c95d161cc502e15014897d 20 4 1 1 2 1 1 2 1 US 52 0 47 51 230 0 [\"cheesy\", \"pants\"] Indie Rock Alternative Electronica Excited Defiant Yearning df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playlist_uri owner streams stream30s dau wau mau mau_previous_month mau_both_months users skippers owner_country n_tracks n_local_tracks n_artists n_albums monthly_stream30s monthly_owner_stream30s tokens genre_1 genre_2 genre_3 mood_1 mood_2 mood_3 403361 spotify:user:4672952d42bdd93b9215ce9a40394ea6:... 4672952d42bdd93b9215ce9a40394ea6 18 6 2 6 13 12 8 20 1 US 48 0 44 48 464 43 [\"discover\", \"mix\"] Indie Rock Alternative Dance & House Excited Yearning Energizing 403362 spotify:user:28c4378e099b4843f5dd42bb848c78ea:... 28c4378e099b4843f5dd42bb848c78ea 0 0 0 0 2 1 1 3 0 US 182 27 114 129 44 14 [\"ambient\", \"study\", \"music\"] Electronica Dance & House Rap Sensual Excited Brooding 403363 spotify:user:1c54302dc7e610a10c51eed81e26a168:... 1c54302dc7e610a10c51eed81e26a168 0 0 0 2 2 0 0 2 0 US 36 0 16 15 82 80 [\"october\"] Rap Indie Rock Alternative Brooding Defiant Sophisticated 403364 spotify:user:adc973443cdf1abecdfb4244e530d451:... adc973443cdf1abecdfb4244e530d451 0 0 0 0 2 0 0 2 0 US 50 0 25 25 2 0 [] Rap R&B Latin Defiant Energizing Aggressive 403365 spotify:user:b3752c94e387192b7950b687453bcf45:... b3752c94e387192b7950b687453bcf45 74 16 1 1 2 1 1 3 1 US 348 10 281 290 216 178 [\"eclecticism\"] Rap Rock Alternative Defiant Energizing Cool df . sort_values ( 'users' , ascending = False ) . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playlist_uri owner streams stream30s dau wau mau mau_previous_month mau_both_months users skippers owner_country n_tracks n_local_tracks n_artists n_albums monthly_stream30s monthly_owner_stream30s tokens genre_1 genre_2 genre_3 mood_1 mood_2 mood_3 152032 spotify:user:spotify:playlist:5FJXhjdILmRA2z5b... spotify 2527075 1461324 152929 669966 1944150 1478684 578391 3455406 86162 US 51 0 51 51 42497334 22 [\"top\", \"hits\"] Pop R&B Dance & House Excited Cool Brooding 163726 spotify:user:spotify:playlist:4hOKQuZbraPDIfaG... spotify 2629715 1513237 122005 514627 1453097 970905 364140 2448881 56707 US 100 0 93 86 40722305 0 [\"top\", \"tracks\", \"currently\", \"spotify\"] Pop Dance & House Indie Rock Excited Defiant Energizing 216752 spotify:user:spotify:playlist:3ZgmfR6lsnCwdffZ... spotify 735281 348391 43498 219817 688999 365968 109688 1233952 34145 US 100 0 100 99 9879201 0 [\"top\", \"pop\", \"tracks\", \"spotify\"] Pop R&B Rap Excited Defiant Empowering 401060 spotify:user:spotify:playlist:3MlpudZs4HT3i0yG... spotify 505876 245377 33152 121173 430129 339921 79443 973788 23846 US 43 0 41 42 5567649 44 [\"teen\", \"party\"] Pop R&B Rap Excited Yearning Urgent 307283 spotify:user:spotify:playlist:04MJzJlzOoy5bTyt... spotify 252309 124903 16480 68518 278966 448102 75371 917174 11888 US 296 0 1 1 4178965 8 [\"dance\", \"mega\", \"mix\"] Dance & House Electronica Pop Excited Aggressive Energizing df . iloc [ 403361 , 0 ] 'spotify:user:4672952d42bdd93b9215ce9a40394ea6:playlist:6W45lqDBZ1TKma71Uu2F5x' df . columns Index(['playlist_uri', 'owner', 'streams', 'stream30s', 'dau', 'wau', 'mau', 'mau_previous_month', 'mau_both_months', 'users', 'skippers', 'owner_country', 'n_tracks', 'n_local_tracks', 'n_artists', 'n_albums', 'monthly_stream30s', 'monthly_owner_stream30s', 'tokens', 'genre_1', 'genre_2', 'genre_3', 'mood_1', 'mood_2', 'mood_3'], dtype='object') id = [ df . columns [ 0 ], df . columns [ 1 ]] targets = list ( df . columns [ 2 : 11 ]) + [ \"monthly_stream30s\" , \"monthly_owner_stream30s\" ] features = set ( df . columns ) - set ( targets ) - set ( id ) features = list ( features ) print ( f \"id columns: { id } \" ) print ( f \"target columns: { targets } \" ) print ( f \"feature columns: { features } \" ) id columns: ['playlist_uri', 'owner'] target columns: ['streams', 'stream30s', 'dau', 'wau', 'mau', 'mau_previous_month', 'mau_both_months', 'users', 'skippers', 'monthly_stream30s', 'monthly_owner_stream30s'] feature columns: ['n_albums', 'n_artists', 'mood_1', 'n_tracks', 'mood_3', 'genre_1', 'genre_2', 'genre_3', 'tokens', 'owner_country', 'n_local_tracks', 'mood_2'] stream30s , dau , wau , mau , monthly_stream30s , monthly_owner_stream30s , mau_previous_months and mau_both_months are all specifically for users who have streamed the playlist for over 30 seconds Let's make the north star metric mau_previous_month - tells us how many users have streamed over 30 seconds from the playlist this past month downside : since we don't know when the playlist was created, we may falsely label some playlists as having low rate of success Let's make a guardrail metric mau_both_months - tells us if the playlist has replay value downside : since we don't know when the playlist was created, we don't know at what stage the playlist is in its lifetime, i.e. do users fall off from months 1-2 or months 10-11 stream30s - number of streams over 30 seconds today; tells us demand of playlist unormalized by number of users accessing the stream (i.e. some users will stream multiple times) downside - a few users can dominate the overall number of listens monthly_stream30s - number of streams over 30 seconds for the month; will give us a longer term comparison between streams downside - playlists created at some point in the month will be compared unequally Secondary metric monthly_owner_stream30s - tells us if the owner or the playlist is significant in making a successful playlist; semi-feature column sub_targets = [ 'mau_previous_month' , 'mau_both_months' , 'mau' , 'monthly_stream30s' , 'stream30s' ]","title":"Imports"},{"location":"extras/X4_Spotify_Appendix/#depenent-variable","text":"it looks like mau may be from an incomplete month (comparing the frequency to mau_previous_months ) df [ targets ] . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } streams stream30s dau wau mau mau_previous_month mau_both_months users skippers monthly_stream30s monthly_owner_stream30s streams 1.000000 0.994380 0.988381 0.967860 0.958000 0.905523 0.957998 0.911023 0.948062 0.984383 -0.001338 stream30s 0.994380 1.000000 0.985062 0.968307 0.957810 0.908967 0.956223 0.912391 0.937712 0.992060 -0.000767 dau 0.988381 0.985062 1.000000 0.986290 0.981306 0.938572 0.975665 0.946317 0.980372 0.980044 -0.003330 wau 0.967860 0.968307 0.986290 1.000000 0.995568 0.957752 0.974101 0.970788 0.976330 0.978300 -0.004150 mau 0.958000 0.957810 0.981306 0.995568 1.000000 0.969613 0.969983 0.983961 0.980052 0.970658 -0.004432 mau_previous_month 0.905523 0.908967 0.938572 0.957752 0.969613 1.000000 0.954992 0.990228 0.943692 0.931162 -0.004802 mau_both_months 0.957998 0.956223 0.975665 0.974101 0.969983 0.954992 1.000000 0.942426 0.951045 0.971727 -0.003219 users 0.911023 0.912391 0.946317 0.970788 0.983961 0.990228 0.942426 1.000000 0.963877 0.931219 -0.005115 skippers 0.948062 0.937712 0.980372 0.976330 0.980052 0.943692 0.951045 0.963877 1.000000 0.935228 -0.004150 monthly_stream30s 0.984383 0.992060 0.980044 0.978300 0.970658 0.931162 0.971727 0.931219 0.935228 1.000000 -0.000519 monthly_owner_stream30s -0.001338 -0.000767 -0.003330 -0.004150 -0.004432 -0.004802 -0.003219 -0.005115 -0.004150 -0.000519 1.000000 df . plot ( x = 'mau' , y = 'mau_previous_month' , ls = '' , marker = '.' ) <AxesSubplot:xlabel='mau'> df . plot ( x = 'mau' , y = 'dau' , ls = '' , marker = '.' ) <AxesSubplot:xlabel='mau'> df . plot ( x = 'mau' , y = 'wau' , ls = '' , marker = '.' ) <AxesSubplot:xlabel='mau'> df . plot ( x = 'mau' , y = 'stream30s' , ls = '' , marker = '.' ) <AxesSubplot:xlabel='mau'> df . plot ( x = 'stream30s' , y = 'monthly_owner_stream30s' , ls = '' , marker = '.' ) <AxesSubplot:xlabel='stream30s'> df . plot ( x = 'stream30s' , y = 'skippers' , ls = '' , marker = '.' ) <AxesSubplot:xlabel='stream30s'> quant = 0.99 for target in targets : cutoff = np . quantile ( df [ target ], quant ) y = df . loc [ df [ target ] < cutoff ] y . plot ( kind = 'hist' , y = target , bins = 100 ) quant = 0.997 for target in sub_targets : cutoff = np . quantile ( df [ target ], quant ) y = df . loc [ df [ target ] < cutoff ] removed = df . loc [ ~ ( df [ target ] < cutoff )] print ( f \"removed items: { removed . shape [ 0 ] } \" ) y . plot ( kind = 'hist' , y = target , bins = 100 ) plt . show () removed items: 1212 removed items: 1216 removed items: 1211 removed items: 1211 df [ sub_targets ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mau_previous_month mau_both_months monthly_stream30s stream30s count 4.033660e+05 403366.000000 4.033660e+05 4.033660e+05 mean 5.819009e+01 12.937065 1.260489e+03 4.283333e+01 std 3.827248e+03 1240.912979 1.062463e+05 3.772412e+03 min 0.000000e+00 0.000000 2.000000e+00 0.000000e+00 25% 1.000000e+00 1.000000 3.100000e+01 0.000000e+00 50% 2.000000e+00 1.000000 7.900000e+01 0.000000e+00 75% 3.000000e+00 2.000000 1.930000e+02 5.000000e+00 max 1.478684e+06 578391.000000 4.249733e+07 1.513237e+06","title":"Depenent Variable"},{"location":"extras/X4_Spotify_Appendix/#independent-variable","text":"features ['n_albums', 'n_artists', 'mood_1', 'n_tracks', 'mood_3', 'genre_1', 'genre_2', 'genre_3', 'tokens', 'owner_country', 'n_local_tracks', 'mood_2'] df [ features ] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_albums n_artists mood_1 n_tracks mood_3 genre_1 genre_2 genre_3 tokens owner_country n_local_tracks mood_2 0 7 4 Peaceful 52 Somber Dance & House New Age Country & Folk [\"ambient\", \"music\", \"therapy\", \"binaural\", \"b... US 0 Romantic 1 113 112 Excited 131 Defiant Pop Indie Rock Alternative [\"good\", \"living\"] US 0 Yearning 2 36 35 Lively 43 Romantic Latin - - [\"norte\\u00f1a\"] US 0 Upbeat 3 26 27 Excited 27 Defiant Dance & House Electronica Pop [] US 1 Aggressive 4 51 47 Excited 52 Yearning Indie Rock Alternative Electronica [\"cheesy\", \"pants\"] US 0 Defiant con_features = list ( df [ features ] . select_dtypes ( 'number' ) . columns ) print ( con_features ) des_features = list ( df [ features ] . select_dtypes ( 'object' ) . columns ) print ( des_features ) ['n_albums', 'n_artists', 'n_tracks', 'n_local_tracks'] ['mood_1', 'mood_3', 'genre_1', 'genre_2', 'genre_3', 'tokens', 'owner_country', 'mood_2'] df [ des_features ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mood_1 mood_3 genre_1 genre_2 genre_3 tokens owner_country mood_2 count 403366 403366 403366 403366 403366 403366 403366 403366 unique 27 27 26 26 26 192107 1 27 top Defiant Energizing Indie Rock Alternative Pop [] US Energizing freq 81079 56450 70571 66252 78758 32568 403366 51643 we will go ahead and remove owner_country (1 unique), owner , and tokens (cardinal) from our feature analysis id = [ df . columns [ 0 ]] targets = list ( df . columns [ 2 : 11 ]) + [ \"monthly_stream30s\" , \"monthly_owner_stream30s\" ] features = set ( df . columns ) - set ( targets ) - set ( id ) - set ([ \"owner_country\" , \"owner\" , \"tokens\" ]) features = list ( features ) print ( f \"id columns: { id } \" ) print ( f \"target columns: { targets } \" ) print ( f \"feature columns: { features } \" ) con_features = list ( df [ features ] . select_dtypes ( 'number' ) . columns ) print ( con_features ) des_features = [ 'mood_1' , 'mood_2' , 'mood_3' , 'genre_1' , 'genre_2' , 'genre_3' ] print ( des_features ) id columns: ['playlist_uri'] target columns: ['streams', 'stream30s', 'dau', 'wau', 'mau', 'mau_previous_month', 'mau_both_months', 'users', 'skippers', 'monthly_stream30s', 'monthly_owner_stream30s'] feature columns: ['n_albums', 'mood_1', 'n_artists', 'n_tracks', 'mood_3', 'genre_1', 'genre_2', 'genre_3', 'n_local_tracks', 'mood_2'] ['n_albums', 'n_artists', 'n_tracks', 'n_local_tracks'] ['mood_1', 'mood_2', 'mood_3', 'genre_1', 'genre_2', 'genre_3']","title":"Independent Variable"},{"location":"extras/X4_Spotify_Appendix/#discrete-features","text":"df [ des_features ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mood_1 mood_2 mood_3 genre_1 genre_2 genre_3 count 403366 403366 403366 403366 403366 403366 unique 27 27 27 26 26 26 top Defiant Energizing Energizing Indie Rock Alternative Pop freq 81079 51643 56450 70571 66252 78758 df . value_counts ( des_features ) mood_1 mood_2 mood_3 genre_1 genre_2 genre_3 Excited Aggressive Energizing Dance & House Electronica Pop 4824 Defiant Cool Energizing Rap R&B Pop 4458 Energizing Cool Rap R&B Pop 4003 Pop R&B 1803 Excited Rap Pop R&B 1225 ... Excited Aggressive Urgent Alternative Electronica Metal 1 Dance & House Pop 1 Upbeat Pop Soundtrack - 1 Indie Rock Alternative - 1 Yearning Urgent Upbeat Soundtrack Pop Rap 1 Length: 138379, dtype: int64 df [ des_features [: 3 ]] . value_counts () mood_1 mood_2 mood_3 Defiant Cool Energizing 15125 Energizing Cool 12278 Excited Aggressive Energizing 7564 Defiant Energizing Excited 6672 Excited Energizing 6179 ... Peaceful Urgent Yearning 1 Yearning Cool 1 Excited 1 Fiery 1 Other Urgent Aggressive 1 Length: 9326, dtype: int64 df [ des_features [ 3 :]] . value_counts () genre_1 genre_2 genre_3 Rap R&B Pop 15477 Indie Rock Alternative Rock 13102 Dance & House Electronica Pop 10800 Indie Rock Alternative Pop 9981 Electronica 7233 ... New Age Country & Folk Rock 1 Dance & House R&B 1 Rock 1 Soundtrack 1 Traditional Spoken & Audio Religious 1 Length: 6664, dtype: int64 fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 10 )) dff = pd . DataFrame ( df [ des_features [ 0 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 1 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 2 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'mood' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'mood' , x = 'count' , orient = 'h' , ax = ax [ 0 ]) dff = pd . DataFrame ( df [ des_features [ 3 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 4 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 5 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'genre' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'genre' , x = 'count' , orient = 'h' , ax = ax [ 1 ]) plt . tight_layout ()","title":"Discrete Features"},{"location":"extras/X4_Spotify_Appendix/#continuous-features","text":"df [ con_features ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } n_albums n_artists n_tracks n_local_tracks count 403366.000000 403366.000000 403366.000000 403366.000000 mean 88.224250 83.852050 201.483432 3.084035 std 133.193118 128.152488 584.077765 40.330266 min 1.000000 1.000000 1.000000 0.000000 25% 19.000000 18.000000 38.000000 0.000000 50% 48.000000 46.000000 84.000000 0.000000 75% 106.000000 100.000000 192.000000 0.000000 max 6397.000000 5226.000000 79984.000000 9117.000000 quant = 0.999 for target in con_features : cutoff = np . quantile ( df [ target ], quant ) y = df . loc [ df [ target ] < cutoff ] removed = df . loc [ ~ ( df [ target ] < cutoff )] print ( f \"removed items: { removed . shape [ 0 ] } \" ) y . plot ( kind = 'hist' , y = target , bins = 100 , density = True ) plt . show () removed items: 404 removed items: 405 removed items: 404 removed items: 406","title":"Continuous Features"},{"location":"extras/X4_Spotify_Appendix/#bootstrapping","text":"An example of how we will bootstrap to perform hypothesis tests later on means = [] ind = con_features [ 0 ] for i in range ( 100 ): boot = random . sample ( list ( df . loc [ ( df [ ind ] > 9 ) & ( df [ ind ] < 999 ) ][ ind ] . values ), k = 1000 ) means . append ( np . mean ( boot )) stuff = plt . hist ( means , bins = 100 , density = True )","title":"Bootstrapping"},{"location":"extras/X4_Spotify_Appendix/#dependency","text":"","title":"Dependency"},{"location":"extras/X4_Spotify_Appendix/#categorical-target","text":"sub_targets ['mau_previous_month', 'mau_both_months', 'mau', 'monthly_stream30s', 'stream30s'] for target in sub_targets : print ( f \"p99 { target } : { np . quantile ( df [ target ], 0.99 ) } \" ) p99 mau_previous_month: 130.0 p99 mau_both_months: 19.0 p99 mau: 143.0 p99 monthly_stream30s: 2843.0 p99 stream30s: 113.0 des_features ['mood_1', 'mood_2', 'mood_3', 'genre_1', 'genre_2', 'genre_3']","title":"Categorical Target"},{"location":"extras/X4_Spotify_Appendix/#categorical-feature","text":"","title":"Categorical Feature"},{"location":"extras/X4_Spotify_Appendix/#moods","text":"chidf = pd . DataFrame () target = sub_targets [ 2 ] chidf [ target ] = df [ target ] print ( chidf [ target ] . median ()) moods = pd . DataFrame () cutoff = 0.001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] grp = chidf . loc [ chidf [ ind ] == grp_label ][ target ] . values chi2 , p , m , cTable = stats . median_test ( grp , pop , correction = True ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 moods = pd . concat ([ moods , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) moods . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] moods = moods . sort_values ( 'p-value' ) . reset_index ( drop = True ) 79.0 moods . loc [ moods [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable + reject null 0 genre_3 - 1725.882036 0.000000e+00 [[16033, 205049], [24090, 198317]] False True 1 genre_2 - 1104.759466 3.051013e-242 [[8216, 203517], [12990, 199849]] False True 2 genre_1 Latin 651.374931 1.122254e-143 [[9000, 199027], [6012, 204339]] True True 3 mood_1 Energizing 611.189037 6.167816e-135 [[10316, 203517], [14071, 199849]] False True 4 genre_1 Rock 315.827189 1.174487e-70 [[12514, 201911], [15563, 201455]] False True ... ... ... ... ... ... ... ... 93 mood_1 Stirring 12.333846 4.448190e-04 [[877, 200454], [1044, 202912]] False True 94 mood_1 Serious 12.316512 4.489689e-04 [[778, 200454], [935, 202912]] False True 95 mood_2 Lively 12.161071 4.879735e-04 [[2588, 200454], [2882, 202912]] False True 96 mood_2 Somber 11.618507 6.529880e-04 [[792, 200454], [946, 202912]] False True 97 genre_2 Dance & House 10.834697 9.961560e-04 [[12678, 201911], [13196, 201455]] False True 98 rows \u00d7 7 columns","title":"Moods"},{"location":"extras/X4_Spotify_Appendix/#chi-square","text":"chidf = pd . DataFrame () target = sub_targets [ 2 ] chidf [ target ] = df [ target ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) chidf [ target ] = chidf [ target ] > tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True )","title":"Chi-Square"},{"location":"extras/X4_Spotify_Appendix/#categorical-categorical-conclusions","text":"increasing quant_value will render additional features; as the population performance worsens, new feature/group pairs have an opportunity to increase the multiplier Best Groups chisum . loc [( chisum [ 'reject null' ] == True ) & ( chisum [ 'multiplier' ] > 2 )] . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 6 genre_1 Children's 262.624693 4.596280e-59 [[361785, 1286], [39933, 362]] 2.550270 True 11 mood_1 Other 197.598843 6.979647e-45 [[361719, 1352], [39952, 343]] 2.296943 True 19 genre_1 Spoken & Audio 120.508309 4.896128e-28 [[362147, 924], [40068, 227]] 2.220451 True 0 genre_1 Latin 1150.625294 3.280867e-252 [[350782, 12289], [37572, 2723]] 2.068731 True 12 genre_1 New Age 166.484617 4.335181e-38 [[361286, 1785], [39896, 399]] 2.024214 True Worst Groups chisum . loc [( chisum [ 'reject null' ] == True ) & ( chisum [ 'multiplier' ] < 0.8 )] . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 28 mood_2 Sensual 85.309680 2.551113e-20 [[343873, 19198], [38598, 1697]] 0.787516 True 40 genre_1 Electronica 65.249731 6.598320e-16 [[350162, 12909], [39176, 1119]] 0.774794 True 2 genre_1 Indie Rock 366.567076 1.046303e-81 [[298164, 64907], [34631, 5664]] 0.751315 True 13 genre_3 Electronica 163.908151 1.584260e-37 [[337501, 25570], [38143, 2152]] 0.744684 True 21 mood_1 Brooding 109.456909 1.288759e-25 [[346296, 16775], [38893, 1402]] 0.744152 True 48 mood_1 Gritty 49.741710 1.753777e-12 [[355800, 7271], [39695, 600]] 0.739652 True 14 mood_1 Energizing 162.542129 3.149562e-37 [[340541, 22530], [38438, 1857]] 0.730229 True 68 mood_3 Other 27.407286 1.648091e-07 [[361541, 1530], [40196, 99]] 0.581994 True We would recommend would-be superstar playlist maker construct a playlist with the following attributes: Genre 1: Children's 2.6x more likely to be in the 90 th percentile 4.8x more likely to be in the 99 th percentile Mood 1: Other 2.3x more likely to be in the 90 th percentile 2.4x more likely to be in the 99 th percentile","title":"Categorical-Categorical Conclusions"},{"location":"extras/X4_Spotify_Appendix/#continuous-feature","text":"targets ['streams', 'stream30s', 'dau', 'wau', 'mau', 'mau_previous_month', 'mau_both_months', 'users', 'skippers', 'monthly_stream30s', 'monthly_owner_stream30s'] con_features ['n_albums', 'n_artists', 'n_tracks', 'n_local_tracks'] target = \"monthly_stream30s\" print ( target ) chidf [ target ] = df [ target ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) df . loc [ df [ target ] > tar_value ] . groupby ( 'n_albums' )[[ 'wau' ]] . mean () . plot ( ls = '' , marker = '.' , ax = ax ) ax . set_xlim ( 0 , 200 ) # ax.set_ylim(0, 100) monthly_stream30s (0.0, 200.0)","title":"Continuous Feature"},{"location":"extras/X4_Spotify_Appendix/#t-test","text":"For t tests we need to deal with the long tails in the distributions along the independent variable df [ targets ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } streams stream30s dau wau mau mau_previous_month mau_both_months users skippers monthly_stream30s monthly_owner_stream30s count 4.033660e+05 4.033660e+05 403366.000000 403366.000000 4.033660e+05 4.033660e+05 403366.000000 4.033660e+05 403366.000000 4.033660e+05 403366.000000 mean 7.101375e+01 4.283333e+01 4.418265 21.784446 6.614290e+01 5.819009e+01 12.937065 1.493085e+02 2.827749 1.260489e+03 93.556621 std 6.492014e+03 3.772412e+03 358.855685 1614.650805 4.732580e+03 3.827248e+03 1240.912979 9.247484e+03 205.059728 1.062463e+05 226.250189 min 0.000000e+00 0.000000e+00 0.000000 0.000000 2.000000e+00 0.000000e+00 0.000000 2.000000e+00 0.000000 2.000000e+00 0.000000 25% 0.000000e+00 0.000000e+00 0.000000 1.000000 2.000000e+00 1.000000e+00 1.000000 2.000000e+00 0.000000 3.100000e+01 6.000000 50% 1.000000e+00 0.000000e+00 0.000000 1.000000 2.000000e+00 2.000000e+00 1.000000 3.000000e+00 0.000000 7.900000e+01 30.000000 75% 8.000000e+00 5.000000e+00 1.000000 2.000000 4.000000e+00 3.000000e+00 2.000000 7.000000e+00 0.000000 1.930000e+02 96.000000 max 2.629715e+06 1.513237e+06 152929.000000 669966.000000 1.944150e+06 1.478684e+06 578391.000000 3.455406e+06 86162.000000 4.249733e+07 25904.000000 df . loc [ df [ 'owner' ] != 'spotify' ][ targets ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } streams stream30s dau wau mau mau_previous_month mau_both_months users skippers monthly_stream30s monthly_owner_stream30s count 402967.000000 402967.000000 402967.000000 402967.000000 402967.000000 402967.000000 402967.000000 402967.000000 402967.000000 4.029670e+05 402967.000000 mean 20.968960 11.990945 1.232421 5.275308 14.860487 13.483665 3.029327 32.824100 0.728640 3.543268e+02 93.647783 std 766.262668 404.190477 41.227771 185.706612 504.704081 548.731437 129.629183 1157.601711 27.054367 1.093559e+04 226.343585 min 0.000000 0.000000 0.000000 0.000000 2.000000 0.000000 0.000000 2.000000 0.000000 2.000000e+00 0.000000 25% 0.000000 0.000000 0.000000 1.000000 2.000000 1.000000 1.000000 2.000000 0.000000 3.100000e+01 6.000000 50% 1.000000 0.000000 0.000000 1.000000 2.000000 2.000000 1.000000 3.000000 0.000000 7.900000e+01 30.000000 75% 8.000000 5.000000 1.000000 2.000000 4.000000 3.000000 2.000000 7.000000 0.000000 1.930000e+02 96.000000 max 293283.000000 173753.000000 18290.000000 71891.000000 206756.000000 190026.000000 59049.000000 439699.000000 11755.000000 5.098585e+06 25904.000000 chidf = pd . DataFrame () target = \"mau\" chidf [ target ] = df [ target ] quant_value = 0.99 tar_value = np . quantile ( chidf [ target ], quant_value ) chidf [ target ] = chidf [ target ] > tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } > { tar_value : .0f } \" ) sns . histplot ( b , label = f \" { target } <= { tar_value : .0f } \" ) plt . title ( ind ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) welchsum .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature test stat p-value upper q avg lower q avg reject null 0 n_tracks 10.277868 4.444906e-20 214.33164 193.07872 True 1 n_artists 5.367785 2.238566e-07 84.92819 81.98974 True 2 n_local_tracks -2.602519 1.006900e-02 2.59716 2.84386 False 3 n_albums -0.827392 4.090126e-01 85.92611 86.46785 False Let's perform the same test again this time let's say we're only interested in playlists with at least 10 tracks and fewer than 1000 tracks chidf = pd . DataFrame () target = sub_targets [ 2 ] chidf [ target ] = df [ target ] chidf [ 'n_tracks' ] = df [ 'n_tracks' ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) chidf [ target ] = chidf [ target ] > tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } > { tar_value : .0f } \" ) sns . histplot ( b , label = f \" { target } <= { tar_value : .0f } \" ) plt . title ( ind ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) welchsum .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature test stat p-value upper q avg lower q avg reject null 0 n_tracks 115.613349 3.417496e-174 231.30575 136.10481 True 1 n_artists 97.323391 2.230656e-167 108.74091 70.18516 True 2 n_albums 94.393421 2.063549e-160 114.38747 74.44801 True 3 n_local_tracks 15.122963 4.889333e-34 3.04746 1.99517 True","title":"t-Test"},{"location":"extras/X4_Spotify_Appendix/#categorical-continuous-conclusions","text":"Our conclusions are the same. There is a clear delineation between number of tracks, albums, and artists for popular and unpopular playlists","title":"Categorical-Continuous Conclusions"},{"location":"extras/X4_Spotify_Appendix/#putting-it-all-together","text":"sub_targets ['mau_previous_month', 'mau_both_months', 'monthly_stream30s', 'stream30s'] des_features ['mood_1', 'mood_2', 'mood_3', 'genre_1', 'genre_2', 'genre_3'] master = pd . DataFrame () for target in sub_targets : # target = sub_targets[0] for quant_value in [ 0.9 , 0.99 ]: # quant_value = 0.90 chidf = pd . DataFrame () chidf [ target ] = df [ target ] tar_value = np . quantile ( chidf [ target ], quant_value ) chidf [ target ] = chidf [ target ] > tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ target , quant_value , tar_value , ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'target' , 'upper q' , 'upper q value' , 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum = chisum . loc [( chisum [ 'reject null' ] == True ) & ( chisum [ 'multiplier' ] > 2 )] . sort_values ( 'multiplier' , ascending = False ) master = pd . concat (( master , chisum )) master .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null 2 mau_previous_month 0.90 9.0 genre_1 Latin 5590.525321 0.000000e+00 [[355002, 11016], [33352, 3996]] 3.861095 True 18 mau_previous_month 0.90 9.0 genre_1 Children's 434.974313 1.343518e-96 [[364768, 1250], [36950, 398]] 3.143224 True 1 mau_previous_month 0.90 9.0 mood_1 Lively 2312.708732 0.000000e+00 [[358030, 7988], [34990, 2358]] 3.020517 True 22 mau_previous_month 0.90 9.0 genre_1 Traditional 357.345743 1.065483e-79 [[364829, 1189], [36989, 359]] 2.978032 True 7 mau_previous_month 0.90 9.0 genre_2 Jazz 1046.212802 1.619916e-229 [[362333, 3685], [36262, 1086]] 2.944750 True ... ... ... ... ... ... ... ... ... ... ... 36 stream30s 0.99 113.0 genre_2 Easy Listening 26.570340 2.541152e-07 [[397291, 2078], [3952, 45]] 2.177002 True 29 stream30s 0.99 113.0 genre_2 Traditional 39.102302 4.021695e-10 [[396243, 3126], [3930, 67]] 2.161001 True 24 stream30s 0.99 113.0 genre_3 Jazz 46.586071 8.768129e-12 [[395431, 3938], [3914, 83]] 2.129376 True 22 stream30s 0.99 113.0 mood_2 Easygoing 48.122685 4.003676e-12 [[394690, 4679], [3902, 95]] 2.053711 True 18 stream30s 0.99 113.0 mood_2 Lively 53.658720 2.385226e-13 [[394007, 5362], [3889, 108]] 2.040624 True 182 rows \u00d7 10 columns master [ 'group' ] . value_counts () - 22 Romantic 19 Lively 17 Traditional 16 Children's 16 Jazz 14 Latin 12 Serious 8 Easy Listening 8 Soundtrack 8 Other 7 New Age 7 Holiday 6 Peaceful 6 Spoken & Audio 4 Fiery 3 Tender 3 Easygoing 3 Sophisticated 2 Somber 1 Name: group, dtype: int64 master . loc [ master [ 'upper q' ] == 0.90 ][ 'group' ] . value_counts () - 12 Lively 7 Traditional 7 Children's 7 Jazz 7 Latin 7 Romantic 6 Other 5 Serious 5 Holiday 5 Easy Listening 4 Soundtrack 4 Spoken & Audio 3 Fiery 3 Sophisticated 2 New Age 1 Tender 1 Name: group, dtype: int64 sort_key = { i : j for i , j in zip ( master [ 'group' ] . value_counts () . index . values , range ( master [ 'group' ] . nunique ()))} master [ 'rank' ] = master [ 'group' ] . apply ( lambda x : sort_key [ x ]) master . sort_values ( 'rank' , inplace = True ) # master.drop('rank', axis=1, inplace=True) master . loc [ master [ 'group' ] != '-' ][: 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 7 monthly_stream30s 0.99 2843.0 mood_2 Romantic 146.934024 8.112487e-34 [[389339, 9994], [3810, 223]] 2.280176 True 1 6 stream30s 0.99 113.0 mood_2 Romantic 148.026986 4.679851e-34 [[389374, 9995], [3775, 222]] 2.290974 True 1 4 monthly_stream30s 0.99 2843.0 mood_1 Romantic 175.072639 5.772239e-40 [[390131, 9202], [3812, 221]] 2.457919 True 1 2 mau 0.99 143.0 mood_1 Romantic 202.823985 5.053546e-46 [[390156, 9193], [3787, 230]] 2.577588 True 1 1 mau 0.90 9.0 mood_2 Romantic 1531.190216 0.000000e+00 [[355299, 8035], [37850, 2182]] 2.549159 True 1 8 mau_previous_month 0.90 9.0 mood_3 Romantic 1013.797108 1.800082e-222 [[357949, 8069], [35525, 1823]] 2.276429 True 1 4 mau_previous_month 0.99 130.0 mood_1 Romantic 156.500834 6.579992e-36 [[390127, 9209], [3816, 214]] 2.375740 True 1 8 mau 0.90 9.0 mood_3 Romantic 1170.355016 1.690629e-256 [[355429, 7905], [38045, 1987]] 2.348287 True 1 6 mau 0.99 143.0 mood_2 Romantic 105.450504 9.729814e-25 [[389336, 10013], [3813, 204]] 2.080289 True 1 5 mau_previous_month 0.99 130.0 mood_3 Romantic 112.605179 2.633191e-26 [[389647, 9689], [3827, 203]] 2.133192 True 1 6 monthly_stream30s 0.99 2843.0 mood_3 Romantic 149.750731 1.965370e-34 [[389660, 9673], [3814, 219]] 2.313066 True 1 3 mau_both_months 0.99 19.0 mood_1 Romantic 109.693770 1.143607e-25 [[390177, 9231], [3766, 192]] 2.154933 True 1 6 mau_previous_month 0.90 9.0 mood_1 Romantic 1142.816205 1.633755e-250 [[358408, 7610], [35535, 1813]] 2.402893 True 1 10 stream30s 0.99 113.0 mood_3 Romantic 136.025552 1.969792e-31 [[389689, 9680], [3785, 212]] 2.254825 True 1 5 mau 0.99 143.0 mood_3 Romantic 122.574129 1.728356e-28 [[389664, 9685], [3810, 207]] 2.185929 True 1 6 mau 0.90 9.0 mood_1 Romantic 1328.179994 8.498925e-291 [[355892, 7442], [38051, 1981]] 2.489700 True 1 6 mau_previous_month 0.99 130.0 mood_2 Romantic 104.434543 1.624732e-24 [[389323, 10013], [3826, 204]] 2.073152 True 1 8 stream30s 0.99 113.0 mood_1 Romantic 139.245969 3.891401e-32 [[390152, 9217], [3791, 206]] 2.300158 True 1 5 mau_previous_month 0.90 9.0 mood_2 Romantic 1379.938658 4.806442e-302 [[357822, 8196], [35327, 2021]] 2.497610 True 1 1 mau_both_months 0.90 2.0 mood_1 Lively 750.247385 3.544959e-165 [[361665, 8747], [31355, 1599]] 2.108575 True 2 sub_targets ['mau_previous_month', 'mau_both_months', 'mau', 'monthly_stream30s', 'stream30s'] master . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 12 stream30s 0.99 113.0 mood_3 - 125.854082 3.309444e-29 [[397434, 1935], [3927, 70]] 3.661181 True 0 11 monthly_stream30s 0.99 2843.0 mood_2 - 109.163417 1.494430e-25 [[397529, 1804], [3969, 64]] 3.553294 True 0 67 mau_previous_month 0.90 9.0 genre_1 - 95.863487 1.230846e-22 [[365249, 769], [37173, 175]] 2.236007 True 0 10 monthly_stream30s 0.99 2843.0 mood_1 - 112.668942 2.549855e-26 [[397605, 1728], [3970, 63]] 3.651389 True 0 7 stream30s 0.99 113.0 mood_1 - 141.501726 1.249779e-32 [[397646, 1723], [3929, 68]] 3.994277 True 0 master . loc [ master [ 'feature' ] . str . contains ( 'genre' )] . groupby ( 'group' )[[ 'multiplier' , 'rank' ]] . mean () . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } multiplier rank group Tender 3.033890 16.0 - 2.935235 0.0 Peaceful 2.564297 13.0 Other 2.494292 10.0 Lively 2.364492 2.0 Romantic 2.318001 1.0 Fiery 2.244027 15.0 Somber 2.194114 19.0 Serious 2.190306 7.0 Easygoing 2.088064 17.0 Sophisticated 2.055203 18.0 master [ 'rank' ] = master [ 'group' ] . apply ( lambda x : sort_key [ x ]) master . groupby ( 'group' )[[ 'multiplier' , 'rank' ]] . mean () . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } multiplier rank group - 3.049100 0.0 Tender 3.033890 16.0 Latin 3.001282 6.0 Children's 2.871261 4.0 Holiday 2.836528 12.0 New Age 2.754796 11.0 Spoken & Audio 2.610393 14.0 Peaceful 2.564297 13.0 Other 2.425104 10.0 Easy Listening 2.407295 8.0 Lively 2.364492 2.0 Traditional 2.361342 3.0 Jazz 2.342954 5.0 Romantic 2.318001 1.0 Fiery 2.244027 15.0 Soundtrack 2.209295 9.0 Somber 2.194114 19.0 Serious 2.190306 7.0 Easygoing 2.088064 17.0 Sophisticated 2.055203 18.0 master . to_csv ( \"chi_square_results.csv\" ) con_master = pd . DataFrame () for target in sub_targets : # target = sub_targets[2] for quant_value in [ 0.90 , 0.99 ]: chidf = pd . DataFrame () chidf [ target ] = df [ target ] chidf [ 'n_tracks' ] = df [ 'n_tracks' ] # quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) chidf [ target ] = chidf [ target ] > tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ target , quant_value , ind , testt , p , a_avg , b_avg , p < cutoff ]])]) print ( target , quant_value ) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } > { tar_value : .0f } \" ) sns . histplot ( b , label = f \" { target } <= { tar_value : .0f } \" ) plt . title ( ind ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'target' , 'quantile' , 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) con_master = pd . concat (( con_master , welchsum )) con_master mau_previous_month 0.9 mau_previous_month 0.9 mau_previous_month 0.9 mau_previous_month 0.9 mau_previous_month 0.99 mau_previous_month 0.99 mau_previous_month 0.99 mau_previous_month 0.99 mau_both_months 0.9 mau_both_months 0.9 mau_both_months 0.9 mau_both_months 0.9 mau_both_months 0.99 mau_both_months 0.99 mau_both_months 0.99 mau_both_months 0.99 mau 0.9 mau 0.9 mau 0.9 mau 0.9 mau 0.99 mau 0.99 mau 0.99 mau 0.99 monthly_stream30s 0.9 monthly_stream30s 0.9 monthly_stream30s 0.9 monthly_stream30s 0.9 monthly_stream30s 0.99 monthly_stream30s 0.99 monthly_stream30s 0.99 monthly_stream30s 0.99 stream30s 0.9 stream30s 0.9 stream30s 0.9 stream30s 0.9 stream30s 0.99 stream30s 0.99 stream30s 0.99 stream30s 0.99 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target quantile feature test stat p-value upper q avg lower q avg reject null 0 mau_previous_month 0.90 n_albums -23.264501 1.517148e-58 69.19828 78.75130 True 1 mau_previous_month 0.90 n_artists -19.090166 9.131465e-47 67.78967 74.42581 True 2 mau_previous_month 0.90 n_local_tracks -8.591563 3.210041e-15 1.68487 2.13934 True 3 mau_previous_month 0.90 n_tracks 4.900218 2.017971e-06 149.27223 145.40243 True 0 mau_previous_month 0.99 n_tracks 19.149805 1.101097e-46 157.92259 144.56996 True 1 mau_previous_month 0.99 n_artists 9.668152 4.508161e-18 77.26126 73.71656 True 2 mau_previous_month 0.99 n_local_tracks -4.443426 1.514586e-05 1.89286 2.11507 True 3 mau_previous_month 0.99 n_albums 1.862787 6.399527e-02 78.89529 78.24458 False 0 mau_both_months 0.90 n_tracks 49.521659 1.017659e-108 181.22258 141.77758 True 1 mau_both_months 0.90 n_albums 44.662168 7.684105e-105 96.16066 75.92092 True 2 mau_both_months 0.90 n_artists 44.359056 9.041628e-103 90.79743 72.15272 True 3 mau_both_months 0.90 n_local_tracks 13.737285 1.342361e-30 2.78731 1.97483 True 0 mau_both_months 0.99 n_tracks 43.038413 5.369851e-102 175.40377 145.00116 True 1 mau_both_months 0.99 n_artists 38.561073 1.471847e-93 88.24552 73.26184 True 2 mau_both_months 0.99 n_albums 34.193348 1.157948e-84 91.12947 77.20951 True 3 mau_both_months 0.99 n_local_tracks 6.722576 1.917602e-10 2.56940 2.10191 True 0 mau 0.90 n_albums -28.035344 2.209065e-70 67.80156 79.48186 True 1 mau 0.90 n_artists -23.052205 7.021697e-58 66.03151 74.84314 True 2 mau 0.90 n_local_tracks -9.891800 5.454116e-19 1.57376 2.12208 True 3 mau 0.90 n_tracks 1.804461 7.267873e-02 146.48072 145.09618 False 0 mau 0.99 n_tracks 12.627041 3.513887e-27 155.01260 145.83850 True 1 mau 0.99 n_artists 7.983360 1.264344e-13 76.43482 73.73105 True 2 mau 0.99 n_local_tracks -6.172898 4.276522e-09 1.76129 2.07410 True 3 mau 0.99 n_albums 1.442954 1.506168e-01 78.53564 77.96526 False 0 monthly_stream30s 0.90 n_tracks 116.726338 2.452095e-164 232.32350 136.98027 True 1 monthly_stream30s 0.90 n_artists 92.368904 2.578108e-157 108.07236 70.08310 True 2 monthly_stream30s 0.90 n_albums 86.396836 1.619061e-153 114.85460 74.19437 True 3 monthly_stream30s 0.90 n_local_tracks 17.521798 4.704385e-40 3.01074 1.97501 True 0 monthly_stream30s 0.99 n_tracks 72.651978 1.071572e-144 199.50667 144.19406 True 1 monthly_stream30s 0.99 n_albums 40.530369 8.810322e-98 95.06869 77.58295 True 2 monthly_stream30s 0.99 n_artists 41.165863 1.560381e-97 90.42413 74.19337 True 3 monthly_stream30s 0.99 n_local_tracks 6.120756 5.135842e-09 2.37637 2.04232 True 0 stream30s 0.90 n_tracks 90.846516 2.364112e-160 207.07344 139.38590 True 1 stream30s 0.90 n_albums 68.563722 6.972523e-137 105.31471 75.42986 True 2 stream30s 0.90 n_artists 68.402932 2.057561e-132 99.37767 70.87686 True 3 stream30s 0.90 n_local_tracks 14.588639 6.290309e-32 2.89681 1.93857 True 0 stream30s 0.99 n_tracks 77.043302 2.214047e-149 201.25989 144.76511 True 1 stream30s 0.99 n_artists 47.632996 2.794842e-107 92.60628 73.13416 True 2 stream30s 0.99 n_albums 44.900868 5.246137e-103 98.01367 78.12288 True 3 stream30s 0.99 n_local_tracks 4.520672 1.062456e-05 2.29328 2.05241 True con_master .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target quantile feature test stat p-value upper q avg lower q avg reject null 0 mau_previous_month 0.90 n_albums -23.264501 1.517148e-58 69.19828 78.75130 True 1 mau_previous_month 0.90 n_artists -19.090166 9.131465e-47 67.78967 74.42581 True 2 mau_previous_month 0.90 n_local_tracks -8.591563 3.210041e-15 1.68487 2.13934 True 3 mau_previous_month 0.90 n_tracks 4.900218 2.017971e-06 149.27223 145.40243 True 0 mau_previous_month 0.99 n_tracks 19.149805 1.101097e-46 157.92259 144.56996 True 1 mau_previous_month 0.99 n_artists 9.668152 4.508161e-18 77.26126 73.71656 True 2 mau_previous_month 0.99 n_local_tracks -4.443426 1.514586e-05 1.89286 2.11507 True 3 mau_previous_month 0.99 n_albums 1.862787 6.399527e-02 78.89529 78.24458 False 0 mau_both_months 0.90 n_tracks 49.521659 1.017659e-108 181.22258 141.77758 True 1 mau_both_months 0.90 n_albums 44.662168 7.684105e-105 96.16066 75.92092 True 2 mau_both_months 0.90 n_artists 44.359056 9.041628e-103 90.79743 72.15272 True 3 mau_both_months 0.90 n_local_tracks 13.737285 1.342361e-30 2.78731 1.97483 True 0 mau_both_months 0.99 n_tracks 43.038413 5.369851e-102 175.40377 145.00116 True 1 mau_both_months 0.99 n_artists 38.561073 1.471847e-93 88.24552 73.26184 True 2 mau_both_months 0.99 n_albums 34.193348 1.157948e-84 91.12947 77.20951 True 3 mau_both_months 0.99 n_local_tracks 6.722576 1.917602e-10 2.56940 2.10191 True 0 mau 0.90 n_albums -28.035344 2.209065e-70 67.80156 79.48186 True 1 mau 0.90 n_artists -23.052205 7.021697e-58 66.03151 74.84314 True 2 mau 0.90 n_local_tracks -9.891800 5.454116e-19 1.57376 2.12208 True 3 mau 0.90 n_tracks 1.804461 7.267873e-02 146.48072 145.09618 False 0 mau 0.99 n_tracks 12.627041 3.513887e-27 155.01260 145.83850 True 1 mau 0.99 n_artists 7.983360 1.264344e-13 76.43482 73.73105 True 2 mau 0.99 n_local_tracks -6.172898 4.276522e-09 1.76129 2.07410 True 3 mau 0.99 n_albums 1.442954 1.506168e-01 78.53564 77.96526 False 0 monthly_stream30s 0.90 n_tracks 116.726338 2.452095e-164 232.32350 136.98027 True 1 monthly_stream30s 0.90 n_artists 92.368904 2.578108e-157 108.07236 70.08310 True 2 monthly_stream30s 0.90 n_albums 86.396836 1.619061e-153 114.85460 74.19437 True 3 monthly_stream30s 0.90 n_local_tracks 17.521798 4.704385e-40 3.01074 1.97501 True 0 monthly_stream30s 0.99 n_tracks 72.651978 1.071572e-144 199.50667 144.19406 True 1 monthly_stream30s 0.99 n_albums 40.530369 8.810322e-98 95.06869 77.58295 True 2 monthly_stream30s 0.99 n_artists 41.165863 1.560381e-97 90.42413 74.19337 True 3 monthly_stream30s 0.99 n_local_tracks 6.120756 5.135842e-09 2.37637 2.04232 True 0 stream30s 0.90 n_tracks 90.846516 2.364112e-160 207.07344 139.38590 True 1 stream30s 0.90 n_albums 68.563722 6.972523e-137 105.31471 75.42986 True 2 stream30s 0.90 n_artists 68.402932 2.057561e-132 99.37767 70.87686 True 3 stream30s 0.90 n_local_tracks 14.588639 6.290309e-32 2.89681 1.93857 True 0 stream30s 0.99 n_tracks 77.043302 2.214047e-149 201.25989 144.76511 True 1 stream30s 0.99 n_artists 47.632996 2.794842e-107 92.60628 73.13416 True 2 stream30s 0.99 n_albums 44.900868 5.246137e-103 98.01367 78.12288 True 3 stream30s 0.99 n_local_tracks 4.520672 1.062456e-05 2.29328 2.05241 True con_master . to_csv ( \"t_test_results.csv\" )","title":"Putting it All Together"},{"location":"extras/X4_Spotify_Appendix/#models-multi-feature-analysis","text":"","title":"Models (Multi-Feature Analysis)"},{"location":"extras/X4_Spotify_Appendix/#deciles-random-forest","text":"sub_targets ['mau_previous_month', 'mau_both_months', 'mau', 'monthly_stream30s', 'stream30s'] target = sub_targets [ - 2 ] y = df [ target ] . values labels = y . copy () names = [] for idx , quant in zip ( range ( 11 ), np . linspace ( 0 , 1 , num = 11 )): if idx == 0 : prev = quant continue if idx == 1 : labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels names ['less than 13 listens', '13 < listens <= 24', '24 < listens <= 38', '38 < listens <= 55', '55 < listens <= 79', '79 < listens <= 111', '111 < listens <= 159', '159 < listens <= 240', '240 < listens <= 432', '432 < listens <= 42497334'] X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 42 , train_size = 0.8 ) model = RandomForestClassifier () model . fit ( X_train , y_train ) RandomForestClassifier() y_hat_test = model . predict ( X_test ) print ( f \"Train Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) Train Acc: 0.14 Test Acc: 0.14 print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) precision recall f1-score support 1 0.19 0.26 0.22 8363 2 0.13 0.13 0.13 7866 3 0.13 0.12 0.13 8173 4 0.10 0.09 0.10 7773 5 0.11 0.10 0.10 8252 6 0.11 0.09 0.10 7976 7 0.11 0.10 0.10 8018 8 0.12 0.10 0.11 8185 9 0.14 0.14 0.14 8009 10 0.20 0.26 0.23 8059 accuracy 0.14 80674 macro avg 0.13 0.14 0.14 80674 weighted avg 0.13 0.14 0.14 80674 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) <AxesSubplot:> # grab feature importances imp = model . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in model . estimators_ ], axis = 0 ) # build feature names feature_names = con_features + list ( enc . get_feature_names_out ()) # create new dataframe feat = pd . DataFrame ([ feature_names , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 n_tracks 0.152852 0.006907 1 n_albums 0.135581 0.007403 2 n_artists 0.133666 0.007421 3 n_local_tracks 0.038311 0.005365 4 genre_2_Pop 0.011607 0.000991 5 genre_3_Pop 0.01145 0.003792 6 genre_3_Alternative 0.010917 0.002062 7 genre_3_Rock 0.009709 0.002517 8 mood_3_Excited 0.009644 0.000618 9 mood_2_Excited 0.009271 0.000782 10 genre_2_Alternative 0.009073 0.003263 11 mood_3_Yearning 0.00904 0.001758 12 genre_3_Indie Rock 0.00876 0.000795 13 mood_3_Defiant 0.008758 0.000674 14 mood_3_Urgent 0.008581 0.000502 15 mood_2_Defiant 0.008537 0.000787 16 mood_3_Empowering 0.008351 0.001044 17 mood_3_Sensual 0.008343 0.000575 18 mood_2_Yearning 0.008315 0.00197 19 genre_2_Rock 0.008229 0.000827","title":"Deciles - Random Forest"},{"location":"extras/X4_Spotify_Appendix/#quartiles-random-forest","text":"### Create Categories y = df [ target ] . values labels = y . copy () names = [] lim = 5 for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx == 0 : prev = quant continue if idx == 1 : labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels ### Create Training Data X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 42 , train_size = 0.8 ) ### Train Model model = RandomForestClassifier () model . fit ( X_train , y_train ) ### Asses Performance y_hat_test = model . predict ( X_test ) y_hat_train = model . predict ( X_train ) print ( f \"Train Acc: { accuracy_score ( y_train , y_hat_train ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax ) Train Acc: 0.99 Test Acc: 0.33 precision recall f1-score support 1 0.37 0.43 0.40 20461 2 0.27 0.23 0.25 19966 3 0.27 0.23 0.25 20082 4 0.39 0.44 0.41 20165 accuracy 0.33 80674 macro avg 0.33 0.33 0.33 80674 weighted avg 0.33 0.33 0.33 80674 <AxesSubplot:>","title":"Quartiles - Random Forest"},{"location":"extras/X4_Spotify_Appendix/#binary-90th-percentile-random-forest","text":"### Create Categories y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 5 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels ### Create Training Data X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test , weight_train , weight_test = train_test_split ( X , y , weights , random_state = 42 , train_size = 0.8 ) ### Strateification Code # strat_y0_idx = np.array(random.sample(list(np.argwhere(y_train==3).reshape(-1)), np.unique(y_train, return_counts=True)[1][1])) # strat_y1_idx = np.argwhere(y_train==4).reshape(-1) # strat_idx = np.hstack((strat_y0_idx, strat_y1_idx)) # X_train = X_train[strat_idx] # y_train = y_train[strat_idx] ### Train Model model = RandomForestClassifier () model . fit ( X_train , y_train ) ### Assess Performance y_hat_test = model . predict ( X_test ) y_hat_train = model . predict ( X_train ) print ( f \"Train Acc: { accuracy_score ( y_train , y_hat_train ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( Train Acc: 0.76 Test Acc: 0.76 precision recall f1-score support 3 0.76 0.98 0.86 60509 4 0.58 0.08 0.13 20165 accuracy 0.76 80674 macro avg 0.67 0.53 0.50 80674 weighted avg 0.72 0.76 0.68 80674 <AxesSubplot:>","title":"Binary, 90th Percentile, Random Forest"},{"location":"extras/X4_Spotify_Appendix/#forward-selection-model","text":"### y print ( target ) y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = 0 names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = 1 weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels #### X X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( np . ones (( X_con . shape [ 0 ], 1 )), X_con , X_cat )) feature_names = [ 'intercept' ] + con_features + list ( enc . get_feature_names_out ()) data = pd . DataFrame ( X , columns = feature_names ) print ( names ) monthly_stream30s ['less than 432 listens', '432 < listens <= 42497334'] def add_feature ( feature_names , basemodel , data , y , r2max = 0 , model = 'linear' , disp = 0 ): feature_max = None bestsum = None newmodel = None for feature in feature_names : basemodel [ feature ] = data [ feature ] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () score = float ( str ( pd . DataFrame ( summ . tables [ 0 ]) . loc [ 3 , 3 ])) if ( score > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = score feature_max = feature bestsum = est2 . summary () newmodel = basemodel . copy () if disp == 1 : print ( f \"new r2max, { feature_max } , { r2max } \" ) basemodel . drop ( labels = feature , axis = 1 , inplace = True ) return r2max , feature_max , bestsum , newmodel candidates = feature_names . copy () basemodel = pd . DataFrame () r2max = 0 with open ( \"canidates.txt\" , \"w+\" ) as f : file_data = f . read () for i in candidates : f . write ( f \" { i } \\n \" ) basemodel . to_csv ( \"basemodel.csv\" ) with open ( \"canidates.txt\" , \"r\" ) as f : # file_data = f.read() new = [] for line in f : current_place = line [: - 1 ] new . append ( current_place ) new = pd . read_csv ( \"basemodel.csv\" , index_col = 0 ) with open ( \"fwd_selection_results.txt\" , \"r+\" ) as f : for line in f : pass lastline = line [: - 1 ] stuff = lastline . split ( \", \" ) new = float ( stuff [ - 1 ]) new 0.04052 while True : newr2max , feature_max , bestsum , newmodel = add_feature ( feature_names = candidates , basemodel = basemodel , data = data , y = y , r2max = r2max ) if newr2max > r2max : r2max = newr2max print ( f \"new r2max, { feature_max } , { r2max } \" ) with open ( \"fwd_selection_results.txt\" , \"a+\" ) as f : file_data = f . read () f . write ( f \"new r2max, { feature_max } , { r2max } \\n \" ) candidates . remove ( feature_max ) with open ( \"canidates.txt\" , \"w+\" ) as f : file_data = f . read () for i in candidates : f . write ( f \" { i } \\n \" ) basemodel = newmodel basemodel . to_csv ( \"basemodel.csv\" ) continue else : break new r2max, n_albums, 0.02614 new r2max, genre_1_Latin, 0.03093 new r2max, genre_1_Indie Rock, 0.03274 new r2max, genre_1_Rap, 0.03431 new r2max, genre_1_Dance & House, 0.03568 new r2max, genre_1_Rock, 0.03674 new r2max, mood_1_Energizing, 0.03772 new r2max, genre_1_Children's, 0.03863 new r2max, mood_1_Tender, 0.03931 new r2max, mood_1_Other, 0.03995 new r2max, n_tracks, 0.04052 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) Input In [675], in <module> 1 while True: ----> 2 newr2max, feature_max, bestsum, newmodel = add_feature( 3 feature_names=candidates, 4 basemodel=basemodel, 5 data=data, 6 y=y, 7 r2max=r2max) 8 if newr2max > r2max: 9 r2max = newr2max Input In [669], in add_feature(feature_names, basemodel, data, y, r2max, model, disp) 8 est = Logit(y, X2) 9 est2 = est.fit(disp=0) ---> 10 summ = est2.summary() 11 score = float(str(pd.DataFrame(summ.tables[0]).loc[3, 3])) 12 if (score > r2max) and not (est2.pvalues > cutoff).any(): File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:4015, in BinaryResults.summary(self, yname, xname, title, alpha, yname_list) 4012 @Appender(DiscreteResults.summary.__doc__) 4013 def summary(self, yname=None, xname=None, title=None, alpha=.05, 4014 yname_list=None): -> 4015 smry = super(BinaryResults, self).summary(yname, xname, title, alpha, 4016 yname_list) 4017 fittedvalues = self.model.cdf(self.fittedvalues) 4018 absprederror = np.abs(self.model.endog - fittedvalues) File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:3769, in DiscreteResults.summary(self, yname, xname, title, alpha, yname_list) 3731 \"\"\" 3732 Summarize the Regression Results. 3733 (...) 3755 statsmodels.iolib.summary.Summary : Class that hold summary results. 3756 \"\"\" 3758 top_left = [('Dep. Variable:', None), 3759 ('Model:', [self.model.__class__.__name__]), 3760 ('Method:', ['MLE']), (...) 3763 ('converged:', [\"%s\" % self.mle_retvals['converged']]), 3764 ] 3766 top_right = [('No. Observations:', None), 3767 ('Df Residuals:', None), 3768 ('Df Model:', None), -> 3769 ('Pseudo R-squ.:', [\"%#6.4g\" % self.prsquared]), 3770 ('Log-Likelihood:', None), 3771 ('LL-Null:', [\"%#8.5g\" % self.llnull]), 3772 ('LLR p-value:', [\"%#6.4g\" % self.llr_pvalue]) 3773 ] 3775 if hasattr(self, 'cov_type'): 3776 top_left.append(('Covariance Type:', [self.cov_type])) File ~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/_libs/properties.pyx:37, in pandas._libs.properties.CachedProperty.__get__() File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:3511, in DiscreteResults.prsquared(self) 3506 @cache_readonly 3507 def prsquared(self): 3508 \"\"\" 3509 McFadden's pseudo-R-squared. `1 - (llf / llnull)` 3510 \"\"\" -> 3511 return 1 - self.llf/self.llnull File ~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/_libs/properties.pyx:37, in pandas._libs.properties.CachedProperty.__get__() File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:3604, in DiscreteResults.llnull(self) 3601 res_null = mod_null.fit(start_params=sp_null, **opt_kwds) 3602 else: 3603 # this should be a reasonably method case across versions -> 3604 res_null = mod_null.fit(start_params=sp_null, method='nm', 3605 warn_convergence=False, 3606 maxiter=10000, disp=0) 3607 res_null = mod_null.fit(start_params=res_null.params, method='bfgs', 3608 warn_convergence=False, 3609 maxiter=10000, disp=0) 3611 if getattr(self, '_attach_nullmodel', False) is not False: File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:1983, in Logit.fit(self, start_params, method, maxiter, full_output, disp, callback, **kwargs) 1980 @Appender(DiscreteModel.fit.__doc__) 1981 def fit(self, start_params=None, method='newton', maxiter=35, 1982 full_output=1, disp=1, callback=None, **kwargs): -> 1983 bnryfit = super().fit(start_params=start_params, 1984 method=method, 1985 maxiter=maxiter, 1986 full_output=full_output, 1987 disp=disp, 1988 callback=callback, 1989 **kwargs) 1991 discretefit = LogitResults(self, bnryfit) 1992 return BinaryResultsWrapper(discretefit) File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:230, in DiscreteModel.fit(self, start_params, method, maxiter, full_output, disp, callback, **kwargs) 227 else: 228 pass # TODO: make a function factory to have multiple call-backs --> 230 mlefit = super().fit(start_params=start_params, 231 method=method, 232 maxiter=maxiter, 233 full_output=full_output, 234 disp=disp, 235 callback=callback, 236 **kwargs) 238 return mlefit File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/base/model.py:563, in LikelihoodModel.fit(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs) 560 del kwargs[\"use_t\"] 562 optimizer = Optimizer() --> 563 xopt, retvals, optim_settings = optimizer._fit(f, score, start_params, 564 fargs, kwargs, 565 hessian=hess, 566 method=method, 567 disp=disp, 568 maxiter=maxiter, 569 callback=callback, 570 retall=retall, 571 full_output=full_output) 572 # Restore cov_type, cov_kwds and use_t 573 optim_settings.update(kwds) File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/base/optimizer.py:241, in Optimizer._fit(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall) 238 fit_funcs.update(extra_fit_funcs) 240 func = fit_funcs[method] --> 241 xopt, retvals = func(objective, gradient, start_params, fargs, kwargs, 242 disp=disp, maxiter=maxiter, callback=callback, 243 retall=retall, full_output=full_output, 244 hess=hessian) 246 optim_settings = {'optimizer': method, 'start_params': start_params, 247 'maxiter': maxiter, 'full_output': full_output, 248 'disp': disp, 'fargs': fargs, 'callback': callback, 249 'retall': retall, \"extra_fit_funcs\": extra_fit_funcs} 250 optim_settings.update(kwargs) File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/base/optimizer.py:728, in _fit_nm(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess) 726 ftol = kwargs.setdefault('ftol', 0.0001) 727 maxfun = kwargs.setdefault('maxfun', None) --> 728 retvals = optimize.fmin(f, start_params, args=fargs, xtol=xtol, 729 ftol=ftol, maxiter=maxiter, maxfun=maxfun, 730 full_output=full_output, disp=disp, retall=retall, 731 callback=callback) 732 if full_output: 733 if not retall: File ~/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/optimize/optimize.py:580, in fmin(func, x0, args, xtol, ftol, maxiter, maxfun, full_output, disp, retall, callback, initial_simplex) 471 \"\"\" 472 Minimize a function using the downhill simplex algorithm. 473 (...) 570 571 \"\"\" 572 opts = {'xatol': xtol, 573 'fatol': ftol, 574 'maxiter': maxiter, (...) 577 'return_all': retall, 578 'initial_simplex': initial_simplex} --> 580 res = _minimize_neldermead(func, x0, args, callback=callback, **opts) 581 if full_output: 582 retlist = res['x'], res['fun'], res['nit'], res['nfev'], res['status'] File ~/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/optimize/optimize.py:768, in _minimize_neldermead(func, x0, args, callback, maxiter, maxfev, disp, return_all, initial_simplex, xatol, fatol, adaptive, bounds, **unknown_options) 766 if bounds is not None: 767 xr = np.clip(xr, lower_bound, upper_bound) --> 768 fxr = func(xr) 769 doshrink = 0 771 if fxr < fsim[0]: File ~/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/optimize/optimize.py:464, in _wrap_function.<locals>.function_wrapper(x, *wrapper_args) 462 def function_wrapper(x, *wrapper_args): 463 ncalls[0] += 1 --> 464 return function(np.copy(x), *(wrapper_args + args)) File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/base/model.py:531, in LikelihoodModel.fit.<locals>.f(params, *args) 530 def f(params, *args): --> 531 return -self.loglike(params, *args) / nobs File ~/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:1872, in Logit.loglike(self, params) 1870 q = 2*self.endog - 1 1871 X = self.exog -> 1872 return np.sum(np.log(self.cdf(q*np.dot(X,params)))) File <__array_function__ internals>:5, in dot(*args, **kwargs) KeyboardInterrupt: candidates ['n_artists', 'n_local_tracks', 'mood_1_-', 'mood_1_Aggressive', 'mood_1_Brooding', 'mood_1_Cool', 'mood_1_Defiant', 'mood_1_Easygoing', 'mood_1_Empowering', 'mood_1_Excited', 'mood_1_Fiery', 'mood_1_Gritty', 'mood_1_Lively', 'mood_1_Melancholy', 'mood_1_Peaceful', 'mood_1_Romantic', 'mood_1_Rowdy', 'mood_1_Sensual', 'mood_1_Sentimental', 'mood_1_Serious', 'mood_1_Somber', 'mood_1_Sophisticated', 'mood_1_Stirring', 'mood_1_Upbeat', 'mood_1_Urgent', 'mood_1_Yearning', 'mood_2_-', 'mood_2_Aggressive', 'mood_2_Brooding', 'mood_2_Cool', 'mood_2_Defiant', 'mood_2_Easygoing', 'mood_2_Empowering', 'mood_2_Energizing', 'mood_2_Excited', 'mood_2_Fiery', 'mood_2_Gritty', 'mood_2_Lively', 'mood_2_Melancholy', 'mood_2_Other', 'mood_2_Peaceful', 'mood_2_Romantic', 'mood_2_Rowdy', 'mood_2_Sensual', 'mood_2_Sentimental', 'mood_2_Serious', 'mood_2_Somber', 'mood_2_Sophisticated', 'mood_2_Stirring', 'mood_2_Tender', 'mood_2_Upbeat', 'mood_2_Urgent', 'mood_2_Yearning', 'mood_3_-', 'mood_3_Aggressive', 'mood_3_Brooding', 'mood_3_Cool', 'mood_3_Defiant', 'mood_3_Easygoing', 'mood_3_Empowering', 'mood_3_Energizing', 'mood_3_Excited', 'mood_3_Fiery', 'mood_3_Gritty', 'mood_3_Lively', 'mood_3_Melancholy', 'mood_3_Other', 'mood_3_Peaceful', 'mood_3_Romantic', 'mood_3_Rowdy', 'mood_3_Sensual', 'mood_3_Sentimental', 'mood_3_Serious', 'mood_3_Somber', 'mood_3_Sophisticated', 'mood_3_Stirring', 'mood_3_Tender', 'mood_3_Upbeat', 'mood_3_Urgent', 'mood_3_Yearning', 'genre_1_-', 'genre_1_Alternative', 'genre_1_Blues', 'genre_1_Classical', 'genre_1_Country & Folk', 'genre_1_Easy Listening', 'genre_1_Electronica', 'genre_1_Holiday', 'genre_1_Jazz', 'genre_1_Metal', 'genre_1_New Age', 'genre_1_Other', 'genre_1_Pop', 'genre_1_Punk', 'genre_1_R&B', 'genre_1_Reggae', 'genre_1_Religious', 'genre_1_Soundtrack', 'genre_1_Spoken & Audio', 'genre_1_Traditional', 'genre_2_-', 'genre_2_Alternative', 'genre_2_Blues', \"genre_2_Children's\", 'genre_2_Classical', 'genre_2_Country & Folk', 'genre_2_Dance & House', 'genre_2_Easy Listening', 'genre_2_Electronica', 'genre_2_Holiday', 'genre_2_Indie Rock', 'genre_2_Jazz', 'genre_2_Latin', 'genre_2_Metal', 'genre_2_New Age', 'genre_2_Other', 'genre_2_Pop', 'genre_2_Punk', 'genre_2_R&B', 'genre_2_Rap', 'genre_2_Reggae', 'genre_2_Religious', 'genre_2_Rock', 'genre_2_Soundtrack', 'genre_2_Spoken & Audio', 'genre_2_Traditional', 'genre_3_-', 'genre_3_Alternative', 'genre_3_Blues', \"genre_3_Children's\", 'genre_3_Classical', 'genre_3_Country & Folk', 'genre_3_Dance & House', 'genre_3_Easy Listening', 'genre_3_Electronica', 'genre_3_Holiday', 'genre_3_Indie Rock', 'genre_3_Jazz', 'genre_3_Latin', 'genre_3_Metal', 'genre_3_New Age', 'genre_3_Other', 'genre_3_Pop', 'genre_3_Punk', 'genre_3_R&B', 'genre_3_Rap', 'genre_3_Reggae', 'genre_3_Religious', 'genre_3_Rock', 'genre_3_Soundtrack', 'genre_3_Spoken & Audio', 'genre_3_Traditional'] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns res_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] intercept -2.0979 0.008 -273.128 0.000 -2.113 -2.083 n_albums 0.3430 0.006 61.214 0.000 0.332 0.354 genre_1_Latin 0.6929 0.023 30.536 0.000 0.648 0.737 genre_1_Indie Rock -0.4654 0.016 -28.755 0.000 -0.497 -0.434 genre_1_Rap -0.3804 0.016 -23.163 0.000 -0.413 -0.348 genre_1_Dance & House -0.3978 0.022 -18.022 0.000 -0.441 -0.355 genre_1_Rock -0.3562 0.023 -15.423 0.000 -0.402 -0.311 mood_1_Energizing -0.3623 0.025 -14.323 0.000 -0.412 -0.313 genre_1_Children's 0.9479 0.061 15.652 0.000 0.829 1.067 mood_1_Tender 0.6629 0.047 14.006 0.000 0.570 0.756 mood_1_Other 0.8465 0.062 13.611 0.000 0.725 0.968 n_tracks 0.0688 0.006 11.549 0.000 0.057 0.080 mood_2_Serious 0.0046 0.080 0.058 0.954 -0.151 0.161","title":"Forward Selection Model"},{"location":"extras/X4_Spotify_Appendix/#binary-99th-percentile","text":"### Create Categories y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = idx names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = idx weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels ### Create Training Data X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( X_con , X_cat )) X_train , X_test , y_train , y_test , weight_train , weight_test = train_test_split ( X , y , weights , random_state = 42 , train_size = 0.8 ) ### Train Model model = RandomForestClassifier () model . fit ( X_train , y_train , weight_train ) ### Asses Performance y_hat_test = model . predict ( X_test ) y_hat_train = model . predict ( X_train ) print ( f \"Train Acc: { accuracy_score ( y_train , y_hat_train ) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , y_hat_test ) : .2f } \" ) print ( classification_report ( y_test , y_hat_test , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_hat_test ), annot = True , ax = ax ) Train Acc: 1.00 Test Acc: 0.90 precision recall f1-score support 9 0.90 0.99 0.94 72615 10 0.27 0.03 0.05 8059 accuracy 0.90 80674 macro avg 0.59 0.51 0.50 80674 weighted avg 0.84 0.90 0.86 80674 <AxesSubplot:>","title":"Binary, 99th Percentile"},{"location":"extras/X4_Spotify_Appendix/#other-metrics","text":"30s listens/tot listens (listen conversions) also like a bounce rate Users both months/users prev month (user conversions) combine with mau > mau_previous_month Best small time performers (less than X total monthly listens + high conversion) Best new user playlist (owner has only 1 popular playlist)","title":"Other Metrics"},{"location":"extras/X4_Spotify_Appendix/#listen-and-user-conversions-mau-growing","text":"df [ 'listen_conversions' ] = df [ 'stream30s' ] / df [ 'streams' ] df [ 'listen_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'user_retention' ] = df [ 'mau_both_months' ] / df [ 'mau_previous_month' ] df [ 'user_retention' ] . fillna ( value = 0 , inplace = True ) df [ 'user_conversions' ] = df [ 'mau' ] / df [ 'users' ] df [ 'user_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growing' ] = df [ 'mau' ] > df [ 'mau_previous_month' ] df [ 'mau_growth' ] = df [ 'mau' ] / df [ 'mau_previous_month' ] df [ 'mau_growth' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growth' ] . replace ([ np . inf , - np . inf ], 1 , inplace = True ) new_metrics = [ 'listen_conversions' , 'user_conversions' , 'user_retention' , 'mau_growth' ] df [ new_metrics ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } listen_conversions user_conversions user_retention mau_growth count 403366.000000 403366.000000 403366.000000 403366.000000 mean 0.334701 0.724072 0.571070 1.513218 std 0.399968 0.261708 0.392073 17.459669 min 0.000000 0.020348 0.000000 0.031250 25% 0.000000 0.500000 0.200000 1.000000 50% 0.000000 0.666667 0.500000 1.066667 75% 0.730769 1.000000 1.000000 2.000000 max 1.000000 1.000000 1.000000 7859.000000 df [ 'listen_conversions' ] . plot ( kind = 'hist' , bins = 10 ) <AxesSubplot:ylabel='Frequency'> df [ 'user_conversions' ] . plot ( kind = 'hist' , bins = 10 ) <AxesSubplot:ylabel='Frequency'> df [ 'user_retention' ] . plot ( kind = 'hist' , bins = 10 ) <AxesSubplot:ylabel='Frequency'> df . loc [ df [ 'mau_growth' ] < 10 ][ 'mau_growth' ] . plot ( kind = 'hist' , bins = 20 ) <AxesSubplot:ylabel='Frequency'> df [ 'mau_growing' ] . value_counts () . plot ( kind = 'bar' ) <AxesSubplot:> df [ 'new_success' ] = df [ new_metrics ] . apply ( lambda x : ( x > 0.5 ) if ( max ( x ) == 1 ) else ( x > 1 )) . all ( axis = 1 ) df [ 'new_success' ] . value_counts () False 362869 True 40497 Name: new_success, dtype: int64 df . loc [ df [ 'new_success' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playlist_uri owner streams stream30s dau wau mau mau_previous_month mau_both_months users skippers owner_country n_tracks n_local_tracks n_artists n_albums monthly_stream30s monthly_owner_stream30s tokens genre_1 genre_2 genre_3 mood_1 mood_2 mood_3 success listen_conversions user_retention user_conversions mau_growing mau_growth new_success 14 spotify:user:9a3580868994077be27d244788d494cd:... 9a3580868994077be27d244788d494cd 28 15 1 1 2 1 1 2 0 US 321 0 170 205 83 77 [\"sunny\", \"daze\"] Alternative Indie Rock Electronica Brooding Excited Sensual False 0.535714 1.0 1.000000 True 2.000000 True 18 spotify:user:7abbdbd3119687473b8f2986e73e2ad6:... 7abbdbd3119687473b8f2986e73e2ad6 9 5 1 2 2 1 1 2 0 US 373 8 1 1 18 11 [] Pop Alternative Indie Rock Empowering Excited Urgent False 0.555556 1.0 1.000000 True 2.000000 True 20 spotify:user:838141e861005b6a955cb389c19671a5:... 838141e861005b6a955cb389c19671a5 32 25 2 3 4 3 3 5 1 US 904 0 81 125 327 253 [\"metalcore\", \"forever\"] Punk Metal Rock Defiant Urgent Aggressive False 0.781250 1.0 0.800000 True 1.333333 True 36 spotify:user:2217942070bcaa5f1e651e27744b4402:... 2217942070bcaa5f1e651e27744b4402 18 17 1 2 4 3 3 5 1 US 141 1 122 131 567 0 [\"chill\"] Rap Dance & House Alternative Excited Defiant Energizing False 0.944444 1.0 0.800000 True 1.333333 True 59 spotify:user:dfde15dd16b4ad87a75036276b4c9f66:... dfde15dd16b4ad87a75036276b4c9f66 5 5 1 1 2 1 1 3 0 US 84 0 73 78 254 239 [\"vegas\"] Rock Pop R&B Upbeat Excited Empowering False 1.000000 1.0 0.666667 True 2.000000 True ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 403329 spotify:user:358b83239c6a2557fbfb053330d49a41:... 358b83239c6a2557fbfb053330d49a41 4 4 1 1 3 1 1 3 0 US 33 0 28 31 271 32 [\"one\", \"dirt\", \"road\"] Country & Folk Rock - Yearning Empowering Gritty False 1.000000 1.0 1.000000 True 3.000000 True 403336 spotify:user:a0781a2de47beb8bd693f3022f316327:... a0781a2de47beb8bd693f3022f316327 856 855 3 10 10 5 5 10 0 US 168 0 6 9 33747 1391 [\"evning\", \"song\"] - - - - - - True 0.998832 1.0 1.000000 True 2.000000 True 403338 spotify:user:06f6dd666f1bbf9148c792b87ed4d22f:... 06f6dd666f1bbf9148c792b87ed4d22f 5 4 1 1 2 1 1 2 0 US 59 0 34 46 21 9 [\"rhc\"] Religious Pop Alternative Empowering Upbeat Brooding False 0.800000 1.0 1.000000 True 2.000000 True 403348 spotify:user:c6af258245d55221cebedb1175f08d83:... c6af258245d55221cebedb1175f08d83 13 11 1 1 2 1 1 2 0 US 31 0 30 29 208 206 [\"zumba\", \"val\", \"silva\", \"playlist\"] Latin Pop Dance & House Aggressive Excited Defiant False 0.846154 1.0 1.000000 True 2.000000 True 403353 spotify:user:5461b6b460dd512d7b4fd4fb488f3520:... 5461b6b460dd512d7b4fd4fb488f3520 2 2 1 1 2 1 1 2 0 US 146 0 115 123 405 321 [\"myfavorites\"] Indie Rock Electronica Alternative Yearning Energizing Brooding False 1.000000 1.0 1.000000 True 2.000000 True 40497 rows \u00d7 32 columns chidf = pd . DataFrame () target = 'new_success' chidf [ target ] = df [ target ] # quant_value = 0.90 # tar_value = np.quantile(chidf[target], quant_value) # chidf[target] = chidf[target] > tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum . loc [ chisum [ 'reject null' ] == True ] . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 6 genre_1 Dance & House 231.225731 3.221322e-52 [[334768, 28101], [36487, 4010]] 1.309267 True 2 genre_1 Indie Rock 386.328998 5.212769e-86 [[300809, 62060], [31986, 8511]] 1.289733 True 3 mood_1 Excited 289.821405 5.438394e-65 [[306376, 56493], [32871, 7626]] 1.258184 True 4 mood_1 Defiant 285.014998 6.064223e-64 [[291222, 71647], [31065, 9432]] 1.234123 True 16 genre_2 Electronica 124.733558 5.820843e-29 [[335186, 27683], [36772, 3725]] 1.226540 True ... ... ... ... ... ... ... ... 70 mood_1 Somber 30.852148 2.784538e-08 [[361994, 875], [40456, 41]] 0.419270 True 0 genre_3 - 1404.327669 2.410008e-307 [[324633, 38236], [38610, 1887]] 0.414947 True 1 genre_2 - 861.809401 1.968786e-189 [[342541, 20328], [39619, 878]] 0.373430 True 24 mood_1 Other 81.806778 1.500630e-19 [[361232, 1637], [40439, 58]] 0.316494 True 42 genre_1 Spoken & Audio 58.779116 1.764037e-14 [[361755, 1114], [40460, 37]] 0.296965 True 101 rows \u00d7 7 columns chisum . loc [ chisum [ 'reject null' ] == True ] . sort_values ( 'multiplier' , ascending = True )[: 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 42 genre_1 Spoken & Audio 58.779116 1.764037e-14 [[361755, 1114], [40460, 37]] 0.296965 True 24 mood_1 Other 81.806778 1.500630e-19 [[361232, 1637], [40439, 58]] 0.316494 True 1 genre_2 - 861.809401 1.968786e-189 [[342541, 20328], [39619, 878]] 0.373430 True 0 genre_3 - 1404.327669 2.410008e-307 [[324633, 38236], [38610, 1887]] 0.414947 True 70 mood_1 Somber 30.852148 2.784538e-08 [[361994, 875], [40456, 41]] 0.419270 True 73 genre_1 Easy Listening 30.613123 3.149562e-08 [[361984, 885], [40455, 42]] 0.424642 True 40 mood_2 - 60.796108 6.330294e-15 [[361087, 1782], [40411, 86]] 0.431224 True 43 mood_1 - 57.600397 3.211607e-14 [[361161, 1708], [40414, 83]] 0.434269 True 37 mood_3 - 64.489845 9.703118e-16 [[360957, 1912], [40404, 93]] 0.434536 True 48 genre_1 Children's 52.188042 5.043231e-13 [[361298, 1571], [40420, 77]] 0.438111 True 32 mood_1 Easygoing 72.784800 1.445861e-17 [[360451, 2418], [40371, 126]] 0.465255 True 56 mood_3 Serious 43.083601 5.245004e-11 [[361404, 1465], [40420, 77]] 0.469948 True 59 genre_2 Other 41.614387 1.111721e-10 [[361446, 1423], [40422, 75]] 0.471283 True 82 mood_2 Other 25.423296 4.603257e-07 [[361970, 899], [40449, 48]] 0.477800 True 60 genre_1 Traditional 39.228043 3.770852e-10 [[361402, 1467], [40416, 81]] 0.493733 True 39 genre_3 Easy Listening 61.357952 4.758655e-15 [[360552, 2317], [40368, 129]] 0.497272 True 47 genre_2 Easy Listening 53.106215 3.159911e-13 [[360858, 2011], [40385, 112]] 0.497648 True 65 mood_2 Stirring 34.226638 4.905289e-09 [[361548, 1321], [40423, 74]] 0.501033 True 57 mood_1 Serious 42.044137 8.923632e-11 [[361247, 1622], [40406, 91]] 0.501590 True 10 genre_1 Soundtrack 169.038371 1.200050e-38 [[356345, 6524], [40127, 370]] 0.503642 True chidf = pd . DataFrame () target = \"success\" chidf [ target ] = df [ target ] # chidf.iloc[:int(chidf.shape[0]/2),:] = True # chidf.iloc[int(chidf.shape[0]/2):,:] = False # quant_value = 0.99 # tar_value = np.quantile(chidf[target], quant_value) # chidf[target] = chidf[target] > tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } == True\" ) sns . histplot ( b , label = f \" { target } == False\" ) plt . title ( ind ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) sns . histplot ( df , x = 'listen_conversions' , hue = 'mau_growing' , bins = 10 ) <AxesSubplot:xlabel='listen_conversions', ylabel='Count'> sns . histplot ( df , x = 'user_conversions' , hue = 'mau_growing' , bins = 10 ) <AxesSubplot:xlabel='user_conversions', ylabel='Count'> sns . histplot ( df , x = 'user_conversions' , hue = df [ 'dau' ] > 1 , bins = 10 ) <AxesSubplot:xlabel='user_conversions', ylabel='Count'> ( df [ 'mau' ] > 5 ) . describe () count 403366 unique 2 top False freq 338256 Name: mau, dtype: object np . quantile ( df [ 'mau' ], 0.9 ) 9.0","title":"Listen and User Conversions, MAU Growing"},{"location":"extras/X4_Spotify_Appendix/#considering-outliers","text":"df = df . loc [ df [ targets ] . apply ( lambda x : ( x < 3 * x . std ()) if ( x . dtype == int or x . dtype == float ) else x ) . all ( axis = 1 )] df = df . loc [ df [ 'owner' ] != 'spotify' ]","title":"Considering outliers"},{"location":"extras/X4_Spotify_Appendix/#multiple-criteria-for-success","text":"df [ 'success' ] = df [ sub_targets ] . apply ( lambda x : x > np . quantile ( x , 0.75 )) . all ( axis = 1 ) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) df . loc [ df [ 'success' ] == True ] . groupby ( 'n_tracks' )[[ 'wau' ]] . mean () . plot ( ls = '' , marker = '.' , ax = ax ) ax . set_xlim ( 0 , 200 ) ax . set_ylim ( 0 , 5000 ) (0.0, 5000.0) chidf = pd . DataFrame () target = 'success' chidf [ target ] = df [ target ] # quant_value = 0.90 # tar_value = np.quantile(chidf[target], quant_value) # chidf[target] = chidf[target] > tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum . loc [ chisum [ 'reject null' ] == True ] . sort_values ( 'multiplier' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature group chi p-value cTable multiplier reject null 0 genre_1 Latin 3516.528142 0.000000e+00 [[371720, 12806], [16634, 2206]] 3.849561 True 4 genre_2 Jazz 708.535543 4.164954e-156 [[380364, 4162], [18231, 609]] 3.052841 True 1 mood_1 Lively 1287.773814 5.130420e-282 [[375424, 9102], [17596, 1244]] 2.916028 True 14 genre_1 Children's 208.802667 2.506648e-47 [[383079, 1447], [18639, 201]] 2.854916 True 20 genre_1 Traditional 149.152847 2.655403e-34 [[383152, 1374], [18666, 174]] 2.599455 True ... ... ... ... ... ... ... ... 22 genre_2 Indie Rock 137.000630 1.205469e-31 [[353648, 30878], [17772, 1068]] 0.688267 True 34 mood_1 Brooding 84.460032 3.920608e-20 [[366942, 17584], [18247, 593]] 0.678177 True 9 genre_2 Alternative 331.424544 4.704591e-74 [[320464, 64062], [16650, 2190]] 0.657974 True 11 mood_1 Yearning 223.850708 1.307610e-50 [[347224, 37302], [17631, 1209]] 0.638303 True 2 genre_1 Indie Rock 866.348545 2.029540e-190 [[315752, 68774], [17043, 1797]] 0.484087 True 92 rows \u00d7 7 columns ind = 'n_tracks' target = 'wau' mean_wau_vs_track = [] for track in range ( 1 , 201 ): means = [] for i in range ( 10 ): boot = random . sample ( list ( df . loc [ ( df [ 'success' ] == True ) & ( df [ ind ] == track ) ][ target ] . values ), k = min ( len ( list ( df . loc [ ( df [ 'success' ] == True ) & ( df [ ind ] == track ) ][ target ] . values )), 1000 )) means . append ( np . mean ( boot )) mean_wau_vs_track . append ( np . mean ( means )) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) plt . plot ( range ( len ( mean_wau_vs_track )), mean_wau_vs_track , ls = '' , marker = '.' ) # ax.set_ylim(0,5) [<matplotlib.lines.Line2D at 0x7f838e3d99a0>] len ( df . loc [ ( df [ 'success' ] == True ) & ( df [ ind ] == track ) ][ target ] . values ) 14","title":"Multiple Criteria for Success"},{"location":"extras/X4_Spotify_Appendix/#dependency_1","text":"master = pd . DataFrame () for target in new_metrics : # target = sub_targets[0] chidf = pd . DataFrame () chidf [ target ] = df [ target ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) tar_value = 0.8 chidf [ target ] = chidf [ target ] >= tar_value chisum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in des_features : # ind = des_features[0] chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): # grp_label = df[ind].unique()[0] try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ target , ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'target' , 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) # chisum = chisum.loc[(chisum['reject null'] == True) & (chisum['multiplier'] > 2)].sort_values('multiplier', ascending=False) master = pd . concat (( master , chisum )) master .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target feature group chi p-value cTable multiplier reject null 0 listen_conversions genre_1 Rap 399.045079 8.888113e-89 [[263266, 51481], [76577, 12042]] 0.804170 True 1 listen_conversions genre_1 Latin 340.400638 5.219381e-76 [[303952, 10795], [84402, 4217]] 1.406803 True 2 listen_conversions mood_1 Defiant 324.874553 1.256443e-72 [[249581, 65166], [72706, 15913]] 0.838248 True 3 listen_conversions genre_3 - 282.376331 2.279083e-63 [[284762, 29985], [78481, 10138]] 1.226777 True 4 listen_conversions genre_2 - 259.043360 2.773590e-58 [[299145, 15602], [83015, 5604]] 1.294324 True ... ... ... ... ... ... ... ... ... 154 user_conversions mood_1 Gritty 0.255320 6.133538e-01 [[235846, 4671], [159649, 3200]] 1.012051 False 155 user_conversions mood_1 Melancholy 0.183720 6.681957e-01 [[237216, 3301], [160587, 2262]] 1.012233 False 156 user_conversions mood_3 Gritty 0.091581 7.621766e-01 [[233926, 6591], [158413, 4436]] 0.993866 False 157 user_conversions mood_2 Urgent 0.026083 8.716985e-01 [[227220, 13297], [153866, 8983]] 0.997635 False 158 user_conversions genre_2 Spoken & Audio 0.006088 9.378078e-01 [[239335, 1182], [162045, 804]] 1.004637 False 318 rows \u00d7 8 columns master . loc [( master [ 'reject null' ] == True ) & ( master [ 'multiplier' ] > 1.5 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target feature group chi p-value cTable multiplier reject null 7 listen_conversions genre_1 New Age 136.770883 1.353331e-31 [[313269, 1478], [87913, 706]] 1.702137 True 9 listen_conversions mood_1 Tender 115.194233 7.135481e-27 [[312449, 2298], [87647, 972]] 1.507851 True 18 listen_conversions genre_2 New Age 71.594191 2.643338e-17 [[313509, 1238], [88081, 538]] 1.546783 True 23 listen_conversions genre_1 Children's 60.468486 7.476593e-15 [[313592, 1155], [88126, 493]] 1.518888 True master . loc [( master [ 'reject null' ] == True ) & ( master [ 'multiplier' ] < .5 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target feature group chi p-value cTable multiplier reject null 0 user_conversions genre_2 - 3922.996570 0.000000e+00 [[223516, 17001], [158644, 4205]] 0.348479 True 1 user_conversions genre_1 Latin 1976.239449 0.000000e+00 [[228943, 11574], [159411, 3438]] 0.426610 True 3 user_conversions genre_3 - 6170.632022 0.000000e+00 [[209266, 31251], [153977, 8872]] 0.385834 True 4 user_conversions mood_1 Lively 1155.365166 3.060886e-253 [[232673, 7844], [160347, 2502]] 0.462844 True 11 user_conversions genre_2 Jazz 534.239051 3.378526e-118 [[236893, 3624], [161702, 1147]] 0.463673 True 16 user_conversions mood_3 - 436.706320 5.640063e-97 [[238863, 1654], [162498, 351]] 0.311941 True 18 user_conversions mood_2 - 395.335894 5.705144e-88 [[238982, 1535], [162516, 333]] 0.319010 True 21 user_conversions mood_1 - 379.442742 1.645452e-84 [[239045, 1472], [162530, 319]] 0.318734 True 23 user_conversions mood_1 Other 340.234697 5.672282e-76 [[239134, 1383], [162537, 312]] 0.331911 True 26 user_conversions mood_1 Tender 313.405005 3.958040e-70 [[238072, 2445], [162024, 825]] 0.495797 True 30 user_conversions genre_1 Spoken & Audio 282.114763 2.598715e-63 [[239551, 966], [162664, 185]] 0.282034 True 32 user_conversions genre_1 Children's 267.069910 4.937672e-60 [[239209, 1308], [162509, 340]] 0.382623 True 41 user_conversions genre_1 New Age 239.970389 3.991738e-54 [[238860, 1657], [162322, 527]] 0.468009 True 45 user_conversions genre_2 Easy Listening 225.530477 5.624862e-51 [[238912, 1605], [162331, 518]] 0.474997 True 48 user_conversions genre_1 Traditional 214.931791 1.153312e-48 [[239311, 1206], [162507, 342]] 0.417609 True 52 user_conversions genre_1 - 175.754457 4.096893e-40 [[239754, 763], [162668, 181]] 0.349637 True 54 user_conversions mood_1 Serious 162.207641 3.726709e-37 [[239237, 1280], [162416, 433]] 0.498285 True 64 user_conversions genre_1 Easy Listening 146.743632 8.928423e-34 [[239783, 734], [162656, 193]] 0.387623 True 77 user_conversions mood_1 Somber 112.482411 2.801392e-26 [[239813, 704], [162637, 212]] 0.444034 True 95 user_conversions genre_1 Blues 64.312880 1.061495e-15 [[240162, 355], [162750, 99]] 0.411519 True 96 user_conversions genre_3 Holiday 62.618863 2.508395e-15 [[240238, 279], [162782, 67]] 0.354410 True 99 user_conversions genre_2 Holiday 55.198819 1.089342e-13 [[240302, 215], [162803, 46]] 0.315802 True 117 user_conversions genre_1 Other 24.171106 8.814482e-07 [[240383, 134], [162812, 37]] 0.407675 True new_master = pd . DataFrame () for target in new_metrics : # target = sub_targets[2] chidf = pd . DataFrame () chidf [ target ] = df [ target ] chidf [ 'n_tracks' ] = df [ 'n_tracks' ] quant_value = 0.90 tar_value = np . quantile ( chidf [ target ], quant_value ) tar_value = 0.8 chidf [ target ] = chidf [ target ] >= tar_value welchsum = pd . DataFrame () cutoff = 0.0001 pop = chidf [ target ] . values for ind in con_features : # ind = con_features[0] chidf [ ind ] = df [ ind ] # for grp_label in df[ind].unique(): # try: a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) & ( chidf [ 'n_tracks' ] > 9 ) & ( chidf [ 'n_tracks' ] < 999 ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ target , ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } >= { tar_value : .0f } \" ) sns . histplot ( b , label = f \" { target } < { tar_value : .0f } \" ) plt . title ( f \" { target } , { ind } \" ) plt . legend () plt . show () # except: # pass welchsum . columns = [ 'target' , 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) new_master = pd . concat (( new_master , welchsum )) new_master .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target feature test stat p-value upper q avg lower q avg reject null 0 listen_conversions n_tracks 10.791139 1.289400e-21 151.33751 143.74398 True 1 listen_conversions n_albums 2.441572 1.550563e-02 78.52885 77.60380 False 2 listen_conversions n_local_tracks -1.514198 1.315704e-01 2.04345 2.13447 False 3 listen_conversions n_artists 1.186743 2.367580e-01 73.94089 73.48707 False 0 user_conversions n_artists 29.009043 3.528897e-73 80.09057 69.51769 True 1 user_conversions n_albums 27.865311 5.520382e-70 84.69724 73.45058 True 2 user_conversions n_tracks 12.465380 1.108146e-26 150.70376 140.85719 True 3 user_conversions n_local_tracks 3.208929 1.563093e-03 2.20957 2.02793 False","title":"Dependency"},{"location":"extras/X4_Spotify_Appendix/#conclusions","text":"","title":"Conclusions"},{"location":"extras/X4_Spotify_Appendix/#discrete-independent-variables","text":"We note that there is class imbalance in the discrete independent variables: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 10 )) dff = pd . DataFrame ( df [ des_features [ 0 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 1 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 2 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'mood' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'mood' , x = 'count' , orient = 'h' , ax = ax [ 0 ]) dff = pd . DataFrame ( df [ des_features [ 3 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 4 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 5 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'genre' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'genre' , x = 'count' , orient = 'h' , ax = ax [ 1 ]) plt . tight_layout () This class imbalance can have a variety of effects (and might be derived from a variety of sources). For example, users will have more choice when listening to popular genres likeIndie Rock and Rap, and less choice with genres like Blues and Easy listening. As it so happens, when we look to the relationship between genre/mood and the dependent variables, many of the genre/moods with smaller class sizes will have a positive multiplier effect on the dependent variable","title":"Discrete, Independent Variables"},{"location":"extras/X4_Spotify_Appendix/#continuous-independent-variables","text":"The four continuous variables of focus in this dataset are highly tailed. Due to this, our statistical tests will require bootstrapping. quant = 0.999 con_features = [ 'n_albums' , 'n_artists' , 'n_tracks' , 'n_local_tracks' ] for target in con_features : cutoff = np . quantile ( df [ target ], quant ) y = df . loc [ df [ target ] < cutoff ] removed = df . loc [ ~ ( df [ target ] < cutoff )] print ( f \"removed items: { removed . shape [ 0 ] } \" ) y . plot ( kind = 'hist' , y = target , bins = 100 , density = True ) plt . show () removed items: 404 removed items: 405 removed items: 404 removed items: 406 an example of bootstrapping n_albums means = [] ind = con_features [ 0 ] for i in range ( 100 ): boot = random . sample ( list ( df . loc [ ( df [ ind ] > 9 ) & ( df [ ind ] < 999 ) ][ ind ] . values ), k = 1000 ) means . append ( np . mean ( boot )) stuff = plt . hist ( means , bins = 100 , density = True )","title":"Continuous, Independent Variables"},{"location":"extras/X4_Spotify_Appendix/#discrete-dependent-variables","text":"For the purposes of investigating a \"successful\" playlist, there are 5 primary metrics: targets ['streams', 'stream30s', 'dau', 'wau', 'mau', 'mau_previous_month', 'mau_both_months', 'users', 'skippers', 'monthly_stream30s', 'monthly_owner_stream30s'] df [ sub_targets ] . describe () . round ( 1 ) . to_excel ( \"file.xlsx\" ) and \"top\" performers in each of these metrics were based on top 10% and top 1% quantiles: print ( 'p99 targets' ) for target in sub_targets : space = ' ' * ( 20 - len ( str ( target ))) print ( f \" { target } : { space } { np . quantile ( df [ target ], 0.99 ) } \" ) print () print ( 'p90 targets' ) for target in sub_targets : space = ' ' * ( 20 - len ( str ( target ))) print ( f \" { target } : { space } { np . quantile ( df [ target ], 0.90 ) } \" ) p99 targets mau_previous_month: 130.0 mau_both_months: 19.0 mau: 143.0 monthly_stream30s: 2843.0 stream30s: 113.0 p90 targets mau_previous_month: 9.0 mau_both_months: 2.0 mau: 9.0 monthly_stream30s: 432.0 stream30s: 17.0 You can imagine with these metrics, some concerns are: what if a playlist was made in the current month, or even current day? playlist is not properly represented by the data how do we normalize by playlists that already have a high visibility? i.e. what if a playlist is \"good\" but just isn't getting noticed? can compute conversion metrics: 30 second listens / total listens mau both months / mau previous month While noting these shortcomings, to keep the analysis focused I singled out the previously mentioned targets, with a focus on monthly_stream30s as the north star metric. monthly_stream30s is advantageous as a nort star metric since it contains data from the entire month (reducing variance) only contains relevant listens (greater than 30 seconds long). Some disadvantages of this metric are that it doesn't account for just a few listeners who may be providing the majority of listens, and playlists that were made in the current month will be undervalued.","title":"Discrete, Dependent Variables"},{"location":"extras/X4_Spotify_Appendix/#dependency_2","text":"","title":"Dependency"},{"location":"extras/X4_Spotify_Appendix/#chi-square_1","text":"In the chi-square test, the contigency table was used to calculate a multiplier effect. This is a ratio of ratios: the count of upper quantile over bottom quantile for the given group over the count of upper quantile over bottom quantile for non-group. In other words, it articulates how much more likely a sample in the given group is likely to be in the upper quantile vs a sample not in the given group chisq_results = pd . read_csv ( \"chi_square_results.csv\" , index_col = 0 ) chisq_results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 12 stream30s 0.99 113.0 mood_3 - 125.854082 3.309444e-29 [[397434 1935]\\n [ 3927 70]] 3.661181 True 0 11 monthly_stream30s 0.99 2843.0 mood_2 - 109.163417 1.494430e-25 [[397529 1804]\\n [ 3969 64]] 3.553294 True 0 67 mau_previous_month 0.90 9.0 genre_1 - 95.863487 1.230846e-22 [[365249 769]\\n [ 37173 175]] 2.236007 True 0 10 monthly_stream30s 0.99 2843.0 mood_1 - 112.668942 2.549855e-26 [[397605 1728]\\n [ 3970 63]] 3.651389 True 0 7 stream30s 0.99 113.0 mood_1 - 141.501726 1.249779e-32 [[397646 1723]\\n [ 3929 68]] 3.994277 True 0 chisq_results [ 'target' ] . unique () array(['stream30s', 'monthly_stream30s', 'mau_previous_month', 'mau', 'mau_both_months'], dtype=object) chisq_results [ 'upper q' ] . unique () array([0.99, 0.9 ]) Taking together the five targets, the two upper quantiles, and the six categorical independent variables, we can identify which group occured the most frequently as a variable of influence: chisq_results . loc [( chisq_results [ 'feature' ] . str . contains ( 'genre' )) & ( chisq_results [ 'group' ] != '-' )][ 'group' ] . value_counts () Traditional 16 Children's 16 Jazz 14 Latin 12 Easy Listening 8 Soundtrack 8 New Age 7 Holiday 6 Spoken & Audio 4 Other 2 Name: group, dtype: int64 Using these value counts as a \"rank\" we can then groupby this rank and see how each group is influencing the propensity to be in the upper quadrant Taking \"Romantic\" as an example, we see that it's multiplier effect is relatively consistent across the five targets and two quantiles: sort_key = { i : j for i , j in zip ( chisq_results [ 'group' ] . value_counts () . index . values , range ( chisq_results [ 'group' ] . nunique ()))} chisq_results [ 'rank' ] = chisq_results [ 'group' ] . apply ( lambda x : sort_key [ x ]) chisq_results . sort_values ( 'rank' , inplace = True ) # chisq_results.drop('rank', axis=1, inplace=True) chisq_results . loc [ chisq_results [ 'group' ] != '-' ][: 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 3 mau_both_months 0.99 19.0 mood_1 Romantic 109.693770 1.143607e-25 [[390177 9231]\\n [ 3766 192]] 2.154933 True 1 5 mau_previous_month 0.90 9.0 mood_2 Romantic 1379.938658 4.806442e-302 [[357822 8196]\\n [ 35327 2021]] 2.497610 True 1 8 stream30s 0.99 113.0 mood_1 Romantic 139.245969 3.891401e-32 [[390152 9217]\\n [ 3791 206]] 2.300158 True 1 6 mau_previous_month 0.99 130.0 mood_2 Romantic 104.434543 1.624732e-24 [[389323 10013]\\n [ 3826 204]] 2.073152 True 1 6 mau 0.90 9.0 mood_1 Romantic 1328.179994 8.498925e-291 [[355892 7442]\\n [ 38051 1981]] 2.489700 True 1 5 mau 0.99 143.0 mood_3 Romantic 122.574129 1.728356e-28 [[389664 9685]\\n [ 3810 207]] 2.185929 True 1 10 stream30s 0.99 113.0 mood_3 Romantic 136.025552 1.969792e-31 [[389689 9680]\\n [ 3785 212]] 2.254825 True 1 6 mau_previous_month 0.90 9.0 mood_1 Romantic 1142.816205 1.633755e-250 [[358408 7610]\\n [ 35535 1813]] 2.402893 True 1 6 monthly_stream30s 0.99 2843.0 mood_3 Romantic 149.750731 1.965370e-34 [[389660 9673]\\n [ 3814 219]] 2.313066 True 1 4 monthly_stream30s 0.99 2843.0 mood_1 Romantic 175.072639 5.772239e-40 [[390131 9202]\\n [ 3812 221]] 2.457919 True 1 6 mau 0.99 143.0 mood_2 Romantic 105.450504 9.729814e-25 [[389336 10013]\\n [ 3813 204]] 2.080289 True 1 5 mau_previous_month 0.99 130.0 mood_3 Romantic 112.605179 2.633191e-26 [[389647 9689]\\n [ 3827 203]] 2.133192 True 1 6 stream30s 0.99 113.0 mood_2 Romantic 148.026986 4.679851e-34 [[389374 9995]\\n [ 3775 222]] 2.290974 True 1 2 mau 0.99 143.0 mood_1 Romantic 202.823985 5.053546e-46 [[390156 9193]\\n [ 3787 230]] 2.577588 True 1 7 monthly_stream30s 0.99 2843.0 mood_2 Romantic 146.934024 8.112487e-34 [[389339 9994]\\n [ 3810 223]] 2.280176 True 1 8 mau_previous_month 0.90 9.0 mood_3 Romantic 1013.797108 1.800082e-222 [[357949 8069]\\n [ 35525 1823]] 2.276429 True 1 4 mau_previous_month 0.99 130.0 mood_1 Romantic 156.500834 6.579992e-36 [[390127 9209]\\n [ 3816 214]] 2.375740 True 1 8 mau 0.90 9.0 mood_3 Romantic 1170.355016 1.690629e-256 [[355429 7905]\\n [ 38045 1987]] 2.348287 True 1 1 mau 0.90 9.0 mood_2 Romantic 1531.190216 0.000000e+00 [[355299 8035]\\n [ 37850 2182]] 2.549159 True 1 2 mau 0.90 9.0 mood_1 Lively 2423.134070 0.000000e+00 [[355493 7841]\\n [ 37527 2505]] 3.026380 True 2 chisq_results . loc [( chisq_results [ 'group' ] == 'Traditional' ) & ( chisq_results [ 'target' ] == 'monthly_stream30s' )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 36 monthly_stream30s 0.99 2843.0 genre_3 Traditional 29.032918 7.115879e-08 [[396376 2957]\\n [ 3973 60]] 2.024364 True 3 27 monthly_stream30s 0.99 2843.0 genre_2 Traditional 47.457479 5.621008e-12 [[396211 3122]\\n [ 3962 71]] 2.274246 True 3 Let's use this idea of average multiplier effect, and average chi-square statistic to summarize by group. Sorting by the test statistic, we see the top 5 most influential groups: chisq_results . groupby ( 'group' )[[ 'chi' , 'multiplier' , 'rank' ]] . mean () . sort_values ( 'chi' , ascending = False )[: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi multiplier rank group Latin 1686.610898 3.001282 6.0 - 766.884882 3.049100 0.0 Sophisticated 581.181538 2.055203 18.0 Lively 523.373076 2.364492 2.0 Romantic 493.442950 2.318001 1.0 Soundtrack 345.506268 2.209295 9.0 Jazz 323.657066 2.342954 5.0 Fiery 261.957158 2.244027 15.0 Tender 212.399270 3.033890 16.0 Traditional 176.194741 2.361342 3.0 Sorting instead by the multiplier, we can see which group has the heaviest influence chisq_results . groupby ( 'group' )[[ 'chi' , 'multiplier' , 'rank' ]] . mean () . sort_values ( 'multiplier' , ascending = False )[: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi multiplier rank group - 766.884882 3.049100 0.0 Tender 212.399270 3.033890 16.0 Latin 1686.610898 3.001282 6.0 Children's 165.058604 2.871261 4.0 Holiday 41.741338 2.836528 12.0 New Age 75.783147 2.754796 10.0 Spoken & Audio 163.859264 2.610393 14.0 Peaceful 61.046237 2.564297 13.0 Other 166.299708 2.425104 11.0 Easy Listening 99.533804 2.407295 8.0 Sorting instead by rank, we see which groups show up most frequently chisq_results . groupby ( 'group' )[[ 'chi' , 'multiplier' , 'rank' ]] . mean () . sort_values ( 'rank' , ascending = True )[: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi multiplier rank group - 766.884882 3.049100 0.0 Romantic 493.442950 2.318001 1.0 Lively 523.373076 2.364492 2.0 Traditional 176.194741 2.361342 3.0 Children's 165.058604 2.871261 4.0 Jazz 323.657066 2.342954 5.0 Latin 1686.610898 3.001282 6.0 Serious 103.700606 2.190306 7.0 Easy Listening 99.533804 2.407295 8.0 Soundtrack 345.506268 2.209295 9.0 chisq_results . loc [ chisq_results [ 'target' ] == 'monthly_stream30s' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 14 monthly_stream30s 0.99 2843.0 mood_3 - 95.615882 1.394829e-22 [[397392 1941]\\n [ 3969 64]] 3.301357 True 0 3 monthly_stream30s 0.99 2843.0 genre_1 - 198.911522 3.608821e-45 [[398442 891]\\n [ 3980 53]] 5.954979 True 0 10 monthly_stream30s 0.99 2843.0 mood_1 - 112.668942 2.549855e-26 [[397605 1728]\\n [ 3970 63]] 3.651389 True 0 11 monthly_stream30s 0.99 2843.0 mood_2 - 109.163417 1.494430e-25 [[397529 1804]\\n [ 3969 64]] 3.553294 True 0 6 monthly_stream30s 0.99 2843.0 mood_3 Romantic 149.750731 1.965370e-34 [[389660 9673]\\n [ 3814 219]] 2.313066 True 1 4 monthly_stream30s 0.99 2843.0 mood_1 Romantic 175.072639 5.772239e-40 [[390131 9202]\\n [ 3812 221]] 2.457919 True 1 7 monthly_stream30s 0.99 2843.0 mood_2 Romantic 146.934024 8.112487e-34 [[389339 9994]\\n [ 3810 223]] 2.280176 True 1 22 monthly_stream30s 0.99 2843.0 mood_2 Lively 62.570224 2.571115e-15 [[393976 5357]\\n [ 3920 113]] 2.120023 True 2 5 monthly_stream30s 0.99 2843.0 mood_1 Lively 172.134248 2.529542e-39 [[389222 10111]\\n [ 3798 235]] 2.381860 True 2 36 monthly_stream30s 0.99 2843.0 genre_3 Traditional 29.032918 7.115879e-08 [[396376 2957]\\n [ 3973 60]] 2.024364 True 3 27 monthly_stream30s 0.99 2843.0 genre_2 Traditional 47.457479 5.621008e-12 [[396211 3122]\\n [ 3962 71]] 2.274246 True 3 37 monthly_stream30s 0.99 2843.0 genre_2 Children's 28.313598 1.031687e-07 [[397689 1644]\\n [ 3994 39]] 2.362100 True 4 2 monthly_stream30s 0.99 2843.0 genre_1 Children's 207.229586 5.524721e-47 [[397760 1573]\\n [ 3958 75]] 4.791570 True 4 6 monthly_stream30s 0.90 432.0 genre_1 Children's 262.624693 4.596280e-59 [[361785 1286]\\n [ 39933 362]] 2.550270 True 4 16 monthly_stream30s 0.99 2843.0 genre_2 Jazz 79.207991 5.590349e-19 [[394671 4662]\\n [ 3924 109]] 2.351584 True 5 30 monthly_stream30s 0.99 2843.0 genre_3 Jazz 39.188768 3.847472e-10 [[395392 3941]\\n [ 3953 80]] 2.030414 True 5 0 monthly_stream30s 0.99 2843.0 genre_1 Latin 537.892273 5.419582e-119 [[384749 14584]\\n [ 3605 428]] 3.132127 True 6 0 monthly_stream30s 0.90 432.0 genre_1 Latin 1150.625294 3.280867e-252 [[350782 12289]\\n [ 37572 2723]] 2.068731 True 6 50 monthly_stream30s 0.99 2843.0 mood_2 Serious 20.339173 6.485903e-06 [[397730 1603]\\n [ 3998 35]] 2.172101 True 7 38 monthly_stream30s 0.99 2843.0 genre_2 Easy Listening 28.186480 1.101715e-07 [[397256 2077]\\n [ 3987 46]] 2.206712 True 8 29 monthly_stream30s 0.99 2843.0 genre_1 Easy Listening 40.400033 2.069376e-10 [[398435 898]\\n [ 4004 29]] 3.213550 True 8 20 monthly_stream30s 0.99 2843.0 genre_1 Soundtrack 66.073066 4.345131e-16 [[392575 6758]\\n [ 3897 136]] 2.027276 True 9 28 monthly_stream30s 0.99 2843.0 genre_1 New Age 43.730647 3.768245e-11 [[397202 2131]\\n [ 3980 53]] 2.482109 True 10 12 monthly_stream30s 0.90 432.0 genre_1 New Age 166.484617 4.335181e-38 [[361286 1785]\\n [ 39896 399]] 2.024214 True 10 21 monthly_stream30s 0.99 2843.0 genre_3 New Age 63.004025 2.062846e-15 [[397632 1701]\\n [ 3982 51]] 2.993960 True 10 15 monthly_stream30s 0.99 2843.0 genre_2 New Age 85.761620 2.029879e-20 [[397614 1719]\\n [ 3976 57]] 3.315998 True 10 33 monthly_stream30s 0.99 2843.0 mood_1 Other 30.443472 3.437382e-08 [[397678 1655]\\n [ 3993 40]] 2.407101 True 11 11 monthly_stream30s 0.90 432.0 mood_1 Other 197.598843 6.979647e-45 [[361719 1352]\\n [ 39952 343]] 2.296943 True 11 26 monthly_stream30s 0.99 2843.0 mood_1 Peaceful 47.834009 4.638752e-12 [[397055 2278]\\n [ 3976 57]] 2.498765 True 13 17 monthly_stream30s 0.99 2843.0 mood_3 Peaceful 69.964512 6.038104e-17 [[396736 2597]\\n [ 3963 70]] 2.698383 True 13 12 monthly_stream30s 0.99 2843.0 mood_2 Peaceful 99.188851 2.295356e-23 [[396395 2938]\\n [ 3948 85]] 2.904813 True 13 52 monthly_stream30s 0.99 2843.0 genre_1 Spoken & Audio 19.783961 8.670724e-06 [[398209 1124]\\n [ 4006 27]] 2.387798 True 14 19 monthly_stream30s 0.90 432.0 genre_1 Spoken & Audio 120.508309 4.896128e-28 [[362147 924]\\n [ 40068 227]] 2.220451 True 14 1 monthly_stream30s 0.99 2843.0 mood_1 Tender 218.759022 1.686848e-49 [[396180 3153]\\n [ 3916 117]] 3.754151 True 16 23 monthly_stream30s 0.99 2843.0 mood_2 Easygoing 61.911050 3.593275e-15 [[394661 4672]\\n [ 3931 102]] 2.191889 True 17 25 monthly_stream30s 0.99 2843.0 mood_3 Easygoing 49.108110 2.422366e-12 [[394265 5068]\\n [ 3931 102]] 2.018593 True 17 34 monthly_stream30s 0.99 2843.0 mood_3 Somber 29.620347 5.255040e-08 [[397107 2226]\\n [ 3984 49]] 2.194114 True 19 It creates some fog to jumble together mood/genres this way. We can instead separate them and ask questions like:","title":"Chi Square"},{"location":"extras/X4_Spotify_Appendix/#what-is-the-most-influential-primary-genre-on-monthly-streams-over-30-seconds","text":"Answer: Children's followed by Latin Reason: both genre's appear as influential in other guardrail metrics (high rank), have high test statistics, and are influential in both p99 and p90 with multiplier effects of [4.8, 2.6] and [3.1, 2.1], respectively. chisq_results . loc [( chisq_results [ 'feature' ] == 'genre_1' ) & ( chisq_results [ 'target' ] == 'monthly_stream30s' )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 3 monthly_stream30s 0.99 2843.0 genre_1 - 198.911522 3.608821e-45 [[398442 891]\\n [ 3980 53]] 5.954979 True 0 2 monthly_stream30s 0.99 2843.0 genre_1 Children's 207.229586 5.524721e-47 [[397760 1573]\\n [ 3958 75]] 4.791570 True 4 6 monthly_stream30s 0.90 432.0 genre_1 Children's 262.624693 4.596280e-59 [[361785 1286]\\n [ 39933 362]] 2.550270 True 4 0 monthly_stream30s 0.99 2843.0 genre_1 Latin 537.892273 5.419582e-119 [[384749 14584]\\n [ 3605 428]] 3.132127 True 6 0 monthly_stream30s 0.90 432.0 genre_1 Latin 1150.625294 3.280867e-252 [[350782 12289]\\n [ 37572 2723]] 2.068731 True 6 29 monthly_stream30s 0.99 2843.0 genre_1 Easy Listening 40.400033 2.069376e-10 [[398435 898]\\n [ 4004 29]] 3.213550 True 8 20 monthly_stream30s 0.99 2843.0 genre_1 Soundtrack 66.073066 4.345131e-16 [[392575 6758]\\n [ 3897 136]] 2.027276 True 9 28 monthly_stream30s 0.99 2843.0 genre_1 New Age 43.730647 3.768245e-11 [[397202 2131]\\n [ 3980 53]] 2.482109 True 10 12 monthly_stream30s 0.90 432.0 genre_1 New Age 166.484617 4.335181e-38 [[361286 1785]\\n [ 39896 399]] 2.024214 True 10 52 monthly_stream30s 0.99 2843.0 genre_1 Spoken & Audio 19.783961 8.670724e-06 [[398209 1124]\\n [ 4006 27]] 2.387798 True 14 19 monthly_stream30s 0.90 432.0 genre_1 Spoken & Audio 120.508309 4.896128e-28 [[362147 924]\\n [ 40068 227]] 2.220451 True 14","title":"What is the most influential primary genre on monthly streams over 30 seconds?"},{"location":"extras/X4_Spotify_Appendix/#what-is-the-most-influential-primary-mood-on-monthly-streams-over-30-seconds","text":"Answer: Romantic and Lively Reason: Romantic and Lively moods appear multiple times as highly influential (high rank) they have high multipliers. A contendent may be Tender, as it has a high multiplier effect as well at 3.75 chisq_results . loc [( chisq_results [ 'feature' ] == 'mood_1' ) & ( chisq_results [ 'target' ] == 'monthly_stream30s' )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target upper q upper q value feature group chi p-value cTable multiplier reject null rank 10 monthly_stream30s 0.99 2843.0 mood_1 - 112.668942 2.549855e-26 [[397605 1728]\\n [ 3970 63]] 3.651389 True 0 4 monthly_stream30s 0.99 2843.0 mood_1 Romantic 175.072639 5.772239e-40 [[390131 9202]\\n [ 3812 221]] 2.457919 True 1 5 monthly_stream30s 0.99 2843.0 mood_1 Lively 172.134248 2.529542e-39 [[389222 10111]\\n [ 3798 235]] 2.381860 True 2 33 monthly_stream30s 0.99 2843.0 mood_1 Other 30.443472 3.437382e-08 [[397678 1655]\\n [ 3993 40]] 2.407101 True 11 11 monthly_stream30s 0.90 432.0 mood_1 Other 197.598843 6.979647e-45 [[361719 1352]\\n [ 39952 343]] 2.296943 True 11 26 monthly_stream30s 0.99 2843.0 mood_1 Peaceful 47.834009 4.638752e-12 [[397055 2278]\\n [ 3976 57]] 2.498765 True 13 1 monthly_stream30s 0.99 2843.0 mood_1 Tender 218.759022 1.686848e-49 [[396180 3153]\\n [ 3916 117]] 3.754151 True 16","title":"What is the most influential primary mood on monthly streams over 30 seconds?"},{"location":"extras/X4_Spotify_Appendix/#which-categorical-feature-is-most-influential-overall","text":"Answer: genre_1, followed by genre_2 and mood_1 Reason: we see that these features appear multiple times across the 5 different targets and 2 different quantiles chisq_results [ 'feature' ] . value_counts () genre_1 48 genre_2 39 mood_1 34 mood_2 28 mood_3 17 genre_3 16 Name: feature, dtype: int64","title":"Which Categorical Feature is most influential overall?"},{"location":"extras/X4_Spotify_Appendix/#what-are-the-shortcomings-of-this-analysis","text":"We haven't taken into account confounding variables. For example, perhaps Latin genre is typically associated with Lively mood. Then which variable is it that actually contributes to a highly performing playlist? We have strategies for dealing with this. We can stratify the confounding variables by over or under sampling. We can also consider them together in a forward selection logistic model. We will take the latter approach later on in the analysis. We haven't considered the categorical variables alongside the continuous variables, so we don't know how they fit overall in terms of relative improtance. We will approach this the same way as the confounding variables issue, and incorporate all variables in a logistic regression.","title":"What are the shortcomings of this analysis?"},{"location":"extras/X4_Spotify_Appendix/#t-test_1","text":"ttest_results = pd . read_csv ( \"t_test_results.csv\" , index_col = 0 ) ttest_results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target feature test stat p-value upper q avg lower q avg reject null 0 mau_previous_month n_albums -25.318099 4.772854e-63 69.77939 78.70629 True 1 mau_previous_month n_artists -15.418330 9.408966e-36 68.08641 73.90254 True 2 mau_previous_month n_local_tracks -9.550137 7.728853e-18 1.60489 2.07692 True 3 mau_previous_month n_tracks 6.086774 5.913654e-09 149.50371 145.10534 True 0 mau_both_months n_artists 52.402365 2.845239e-114 91.41907 71.97618 True","title":"t-Test"},{"location":"extras/X4_Spotify_Appendix/#models","text":"log_results = pd . read_csv ( \"../../scripts/fwd_selection_results.txt\" , header = None , index_col = 0 ) log_results . columns = [ 'feature' , 'pseudo r2' ] log_results . reset_index ( inplace = True , drop = True ) log_results . drop ( 0 , axis = 0 , inplace = True ) log_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature pseudo r2 1 intercept 2.197000e-12 2 n_albums 2.614000e-02 3 genre_1_Latin 3.093000e-02 4 genre_1_Indie Rock 3.274000e-02 5 genre_1_Rap 3.431000e-02 6 genre_1_Dance & House 3.568000e-02 7 genre_1_Rock 3.674000e-02 8 mood_1_Energizing 3.772000e-02 9 genre_1_Children's 3.863000e-02 10 mood_1_Tender 3.931000e-02 11 mood_1_Other 3.995000e-02 12 n_tracks 4.052000e-02 13 mood_1_Peaceful 4.106000e-02 14 mood_1_Romantic 4.161000e-02 15 genre_1_Electronica 4.208000e-02 16 genre_2_Indie Rock 4.248000e-02 17 mood_2_Energizing 4.287000e-02 18 genre_1_R&B 4.319000e-02 19 genre_3_Indie Rock 4.353000e-02 20 genre_1_Classical 4.379000e-02 21 genre_2_Alternative 4.403000e-02 22 genre_2_Metal 4.427000e-02 23 mood_2_Peaceful 4.449000e-02 24 mood_2_Romantic 4.472000e-02 25 mood_3_Romantic 4.498000e-02 26 genre_3_Alternative 4.522000e-02 target = \"monthly_stream30s\" y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = 0 names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = 1 weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels basemodel = pd . read_csv ( \"../../scripts/basemodel.csv\" , index_col = 0 ) X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns res_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] intercept -2.0919 0.012 -180.211 0.000 -2.115 -2.069 n_albums 0.2659 0.012 21.370 0.000 0.241 0.290 genre_1_Latin 0.5389 0.025 21.354 0.000 0.489 0.588 genre_1_Indie Rock -0.5919 0.020 -30.130 0.000 -0.630 -0.553 genre_1_Rap -0.3552 0.018 -19.651 0.000 -0.391 -0.320 genre_1_Dance & House -0.3364 0.023 -14.444 0.000 -0.382 -0.291 genre_1_Rock -0.4325 0.024 -18.072 0.000 -0.479 -0.386 mood_1_Energizing -0.3012 0.026 -11.592 0.000 -0.352 -0.250 genre_1_Children's 0.7694 0.061 12.545 0.000 0.649 0.890 mood_1_Tender 0.6086 0.053 11.496 0.000 0.505 0.712 mood_1_Other 0.8435 0.062 13.497 0.000 0.721 0.966 n_tracks 0.0465 0.006 7.665 0.000 0.035 0.058 mood_1_Peaceful 0.7355 0.057 12.849 0.000 0.623 0.848 mood_1_Romantic 0.3608 0.032 11.187 0.000 0.298 0.424 genre_1_Electronica -0.2585 0.033 -7.726 0.000 -0.324 -0.193 genre_2_Indie Rock -0.2338 0.022 -10.617 0.000 -0.277 -0.191 mood_2_Energizing -0.1235 0.018 -6.837 0.000 -0.159 -0.088 genre_1_R&B -0.2373 0.030 -7.999 0.000 -0.295 -0.179 genre_3_Indie Rock -0.1994 0.022 -8.880 0.000 -0.243 -0.155 genre_1_Classical -0.5369 0.059 -9.114 0.000 -0.652 -0.421 genre_2_Alternative 0.1578 0.018 8.915 0.000 0.123 0.192 genre_2_Metal 0.3654 0.039 9.356 0.000 0.289 0.442 mood_2_Peaceful 0.4354 0.053 8.150 0.000 0.331 0.540 mood_2_Romantic 0.2643 0.031 8.628 0.000 0.204 0.324 mood_3_Romantic 0.2600 0.031 8.363 0.000 0.199 0.321 genre_3_Alternative 0.1152 0.018 6.548 0.000 0.081 0.150 n_artists 0.0968 0.013 7.587 0.000 0.072 0.122 genre_1_Metal 0.3371 0.041 8.282 0.000 0.257 0.417 mood_1_Aggressive -0.2743 0.041 -6.671 0.000 -0.355 -0.194 mood_3_Peaceful 0.3313 0.057 5.778 0.000 0.219 0.444 mood_1_Empowering 0.1344 0.020 6.801 0.000 0.096 0.173 genre_1_Religious -0.1832 0.032 -5.799 0.000 -0.245 -0.121 genre_3_Metal 0.2308 0.043 5.361 0.000 0.146 0.315 genre_3_R&B -0.1163 0.022 -5.238 0.000 -0.160 -0.073","title":"Models"},{"location":"extras/X4_Spotify_Appendix/#final-figures-and-tables","text":"df = pd . read_csv ( \"../../data/playlist_summary_external-4.txt\" , delimiter = ' \\t ' )","title":"Final Figures and Tables"},{"location":"extras/X4_Spotify_Appendix/#dependency_3","text":"sub_targets = [ 'mau_previous_month' , 'mau_both_months' , 'monthly_stream30s' , 'stream30s' ] # sub_targets = ['mau', 'dau', 'monthly_stream30s', 'stream30s'] des_features = [ 'mood_1' , 'mood_2' , 'mood_3' , 'genre_1' , 'genre_2' , 'genre_3' ] con_features = [ 'n_albums' , 'n_artists' , 'n_tracks' , 'n_local_tracks' ] df [ 'success' ] = df [ sub_targets ] . apply ( lambda x : x > np . quantile ( x , 0.75 )) . all ( axis = 1 ) df [ 'listen_conversions' ] = df [ 'stream30s' ] / df [ 'streams' ] df [ 'listen_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'user_retention' ] = df [ 'mau_both_months' ] / df [ 'mau_previous_month' ] df [ 'user_retention' ] . fillna ( value = 0 , inplace = True ) df [ 'user_conversions' ] = df [ 'mau' ] / df [ 'users' ] df [ 'user_conversions' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growing' ] = df [ 'mau' ] > df [ 'mau_previous_month' ] df [ 'mau_growth' ] = df [ 'mau' ] / df [ 'mau_previous_month' ] df [ 'mau_growth' ] . fillna ( value = 0 , inplace = True ) df [ 'mau_growth' ] . replace ([ np . inf , - np . inf ], 1 , inplace = True ) new_metrics = [ 'listen_conversions' , 'user_conversions' , 'user_retention' , 'mau_growth' ] df [ 'new_success' ] = df [ new_metrics ] . apply ( lambda x : ( x > 0.5 ) if ( max ( x ) == 1 ) else ( x > 1 )) . all ( axis = 1 )","title":"Dependency"},{"location":"extras/X4_Spotify_Appendix/#discrete","text":"fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 10 )) dff = pd . DataFrame ( df [ des_features [ 0 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 1 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 2 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'mood' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'mood' , x = 'count' , orient = 'h' , ax = ax [ 0 ]) dff = pd . DataFrame ( df [ des_features [ 3 ]] . value_counts ()) . join ( pd . DataFrame ( df [ des_features [ 4 ]] . value_counts ())) . join ( pd . DataFrame ( df [ des_features [ 5 ]] . value_counts ())) dff = dff . reset_index () . melt ( id_vars = 'index' ) dff . columns = [ 'genre' , 'order' , 'count' ] sns . barplot ( data = dff , hue = 'order' , y = 'genre' , x = 'count' , orient = 'h' , ax = ax [ 1 ]) plt . tight_layout () fig . savefig ( \"discrete_rank_bar_plot.svg\" ) def make_chisum ( target = 'success' ): chidf = pd . DataFrame () chidf [ target ] = df [ target ] chisum = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind in des_features : chidf [ ind ] = df [ ind ] for grp_label in df [ ind ] . unique (): try : cTable = chidf . groupby ( chidf [ ind ] == grp_label )[ target ] . value_counts () . values . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 1 ] / cTable [ 0 ] pos = ratio [ 1 ] / ratio [ 0 ] chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , grp_label , chi2 , p , cTable , pos , p < cutoff ]])]) except : pass chisum . columns = [ 'feature' , 'group' , 'chi' , 'p-value' , 'cTable' , 'multiplier' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) return chisum def make_cat_plots ( target = 'success' , ind_feature = 'genre_1' ): fig , ax = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 ), sharex = 'col' , sharey = 'row' ) genre_list = chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False )[ 'group' ] . values dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'percent' ] dff = dff . reset_index () dff . loc [ dff [ target ] == True , 'percent' ] = dff . loc [ dff [ target ] == True , 'percent' ] / dff . loc [ dff [ target ] == True , 'percent' ] . sum () dff . loc [ dff [ target ] == False , 'percent' ] = dff . loc [ dff [ target ] == False , 'percent' ] / dff . loc [ dff [ target ] == False , 'percent' ] . sum () dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 0 , 0 ]) ax [ 0 , 0 ] . set_title ( 'Best and Worst Genres, Percent' ) ax [ 0 , 0 ] . set_ylabel ( '' ) ax [ 0 , 0 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'percent' , ax = ax [ 1 , 0 ]) ax [ 1 , 0 ] . set_ylabel ( '' ) dff = pd . DataFrame ( df . groupby ([ ind_feature ])[ target ] . value_counts ( sort = False )) dff . columns = [ 'count' ] dff = dff . reset_index () dff = dff . set_index ( ind_feature ) . loc [ genre_list ,:] dff = dff . reset_index () sns . barplot ( data = dff . iloc [: 10 ,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 0 , 1 ]) ax [ 0 , 1 ] . set_title ( 'Best and Worst Genres, Count' ) ax [ 0 , 1 ] . set_ylabel ( '' ) ax [ 0 , 1 ] . set_xlabel ( '' ) sns . barplot ( data = dff . iloc [ - 10 :,:], hue = target , y = ind_feature , x = 'count' , ax = ax [ 1 , 1 ]) ax [ 1 , 1 ] . set_ylabel ( '' ) plt . tight_layout () ax [ 0 , 0 ] . get_legend () . remove () ax [ 1 , 1 ] . get_legend () . remove () ax [ 1 , 0 ] . get_legend () . remove () ax [ 0 , 1 ] . legend ( framealpha = 1 , facecolor = 'white' , title = \"Success\" ) return fig ind_feature = 'genre_1' target = 'success' chisum = make_chisum ( target ) fig = make_cat_plots ( target , ind_feature ) chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False ) . to_excel ( f ' { target } _ { ind_feature } _categorical.xlsx' ) fig . savefig ( f \" { target } _ { ind_feature } _categorical.svg\" ) ind_feature = 'mood_1' target = 'success' chisum = make_chisum ( target ) fig = make_cat_plots ( target , ind_feature ) chisum . loc [( chisum [ 'feature' ] == ind_feature ) & ( chisum [ 'reject null' ] == True )] . sort_values ( 'multiplier' , ascending = False ) . to_excel ( f ' { target } _ { ind_feature } _categorical.xlsx' ) fig . savefig ( f \" { target } _ { ind_feature } _categorical.svg\" )","title":"Discrete"},{"location":"extras/X4_Spotify_Appendix/#continuous","text":"def make_con_plots ( target , con_features ): fig , (( ax1 , ax2 ),( ax3 , ax4 )) = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) chidf = pd . DataFrame () chidf [ target ] = df [ target ] welchsum = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind , ax in zip ( con_features , [ ax1 , ax2 , ax3 , ax4 ]): chidf [ ind ] = df [ ind ] a = [] b = [] for i in range ( 100 ): boot1 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == True ) ][ ind ] . values ), k = 1000 ) boot2 = random . sample ( list ( chidf . loc [ ( chidf [ target ] == False ) ][ ind ] . values ), k = 1000 ) a . append ( np . mean ( boot1 )) b . append ( np . mean ( boot2 )) testt , p = stats . ttest_ind ( a , b , equal_var = False ) a_avg = np . mean ( a ) b_avg = np . mean ( b ) welchsum = pd . concat ([ welchsum , pd . DataFrame ([[ ind , testt , p , a_avg , b_avg , p < cutoff ]])]) sns . histplot ( a , color = 'tab:orange' , label = f \" { target } == True\" , ax = ax ) sns . histplot ( b , label = f \" { target } == False\" , ax = ax ) ax . set_title ( ind ) welchsum . columns = [ 'feature' , 'test stat' , 'p-value' , 'upper q avg' , 'lower q avg' , 'reject null' ] welchsum = welchsum . sort_values ( 'p-value' ) . reset_index ( drop = True ) ax . legend () return fig , welchsum target = 'new_success' fig , welchsum = make_con_plots ( target , con_features ) welchsum . to_excel ( f \" { target } _continuous.xlsx\" ) fig . savefig ( f \" { target } _ttest.svg\" )","title":"Continuous"},{"location":"extras/X4_Spotify_Appendix/#models_1","text":"","title":"Models"},{"location":"extras/X4_Spotify_Appendix/#logistic-regression","text":"### y target = \"success\" print ( target ) y = df [ target ] . values #### X X = df [ des_features + con_features ] enc = OneHotEncoder () std = StandardScaler () X_cat = enc . fit_transform ( X [ des_features ]) . toarray () X_con = std . fit_transform ( X [ con_features ]) X = np . hstack (( np . ones (( X_con . shape [ 0 ], 1 )), X_con , X_cat )) feature_names = [ 'intercept' ] + con_features + list ( enc . get_feature_names_out ()) data = pd . DataFrame ( X , columns = feature_names ) success def add_feature ( feature_names , basemodel , data , y , r2max = 0 , model = 'linear' , disp = 0 ): feature_max = None bestsum = None newmodel = None for feature in feature_names : basemodel [ feature ] = data [ feature ] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () score = float ( str ( pd . DataFrame ( summ . tables [ 0 ]) . loc [ 3 , 3 ])) if ( score > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = score feature_max = feature bestsum = est2 . summary () newmodel = basemodel . copy () if disp == 1 : print ( f \"new r2max, { feature_max } , { r2max } \" ) basemodel . drop ( labels = feature , axis = 1 , inplace = True ) return r2max , feature_max , bestsum , newmodel candidates = feature_names . copy () basemodel = pd . DataFrame () r2max = 0 while True : newr2max , feature_max , bestsum , newmodel = add_feature ( feature_names = candidates , basemodel = basemodel , data = data , y = y , r2max = r2max ) if newr2max > r2max : r2max = newr2max print ( f \"new r2max, { feature_max } , { r2max } \" ) with open ( \"success_fwd_selection_results.txt\" , \"a+\" ) as f : file_data = f . read () f . write ( f \"new r2max, { feature_max } , { r2max } \\n \" ) candidates . remove ( feature_max ) with open ( \"success_canidates.txt\" , \"w+\" ) as f : file_data = f . read () for i in candidates : f . write ( f \" { i } \\n \" ) basemodel = newmodel basemodel . to_csv ( \"success_basemodel.csv\" ) continue else : break basemodel = pd . read_csv ( \"success_basemodel.csv\" , index_col = 0 ) X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns display ( res_df ) res_df . to_excel ( \"success_log.xlsx\" ) ### y target = \"monthly_stream30s\" print ( target ) y = df [ target ] . values labels = y . copy () names = [] weights = y . copy () weights . dtype = 'float' lim = 11 dom_class_weight = 1 / ( lim - 1 - 1 ) for idx , quant in zip ( range ( lim ), np . linspace ( 0 , 1 , num = lim )): if idx < lim - 2 : prev = quant continue elif idx == lim - 2 : weights [ y <= np . quantile ( y , quant )] = dom_class_weight labels [ labels <= np . quantile ( y , quant )] = 0 names += [ f \"less than { np . quantile ( y , quant ) : .0f } listens\" ] else : labels [( labels > np . quantile ( y , prev )) & ( labels <= np . quantile ( y , quant ))] = 1 weights [( y > np . quantile ( y , prev )) & ( y <= np . quantile ( y , quant ))] = 1.0 names += [ f \" { np . quantile ( y , prev ) : .0f } < listens <= { np . quantile ( y , quant ) : .0f } \" ] prev = quant y = labels basemodel = pd . read_csv ( \"../../scripts/basemodel.csv\" , index_col = 0 ) X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns display ( res_df ) res_df . to_excel ( \"log_model_monthly_stream30s.xlsx\" ) monthly_stream30s .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] intercept -2.0737 0.016 -133.150 0.000 -2.104 -2.043 n_albums 0.2656 0.012 21.287 0.000 0.241 0.290 genre_1_Latin 0.5408 0.027 19.906 0.000 0.488 0.594 genre_1_Indie Rock -0.5832 0.020 -28.964 0.000 -0.623 -0.544 genre_1_Rap -0.3259 0.020 -16.697 0.000 -0.364 -0.288 genre_1_Dance & House -0.3034 0.027 -11.069 0.000 -0.357 -0.250 genre_1_Rock -0.4226 0.025 -16.996 0.000 -0.471 -0.374 mood_1_Energizing -0.2844 0.027 -10.670 0.000 -0.337 -0.232 genre_1_Children's 0.7845 0.062 12.608 0.000 0.663 0.906 mood_1_Tender 0.4943 0.055 9.032 0.000 0.387 0.602 mood_1_Other 0.6206 0.074 8.413 0.000 0.476 0.765 n_tracks 0.0462 0.006 7.613 0.000 0.034 0.058 mood_1_Peaceful 0.6294 0.060 10.426 0.000 0.511 0.748 mood_1_Romantic 0.2981 0.033 9.038 0.000 0.233 0.363 genre_1_Electronica -0.2326 0.034 -6.792 0.000 -0.300 -0.165 genre_2_Indie Rock -0.2050 0.023 -8.998 0.000 -0.250 -0.160 mood_2_Energizing -0.1384 0.019 -7.421 0.000 -0.175 -0.102 genre_1_R&B -0.2335 0.030 -7.696 0.000 -0.293 -0.174 genre_3_Indie Rock -0.2540 0.024 -10.792 0.000 -0.300 -0.208 genre_1_Classical -0.5126 0.060 -8.609 0.000 -0.629 -0.396 genre_2_Alternative 0.1769 0.019 9.542 0.000 0.141 0.213 genre_2_Metal 0.4257 0.040 10.738 0.000 0.348 0.503 mood_2_Peaceful 0.3761 0.055 6.856 0.000 0.269 0.484 mood_2_Romantic 0.2300 0.031 7.414 0.000 0.169 0.291 mood_3_Romantic 0.2597 0.031 8.252 0.000 0.198 0.321 genre_3_Alternative 0.0482 0.019 2.529 0.011 0.011 0.086 n_artists 0.0954 0.013 7.464 0.000 0.070 0.120 genre_1_Metal 0.4049 0.042 9.680 0.000 0.323 0.487 mood_1_Aggressive -0.2660 0.042 -6.275 0.000 -0.349 -0.183 mood_3_Peaceful 0.2912 0.058 4.983 0.000 0.177 0.406 mood_1_Empowering 0.1197 0.021 5.789 0.000 0.079 0.160 genre_1_Religious -0.2328 0.033 -7.154 0.000 -0.297 -0.169 genre_3_Metal 0.1978 0.044 4.527 0.000 0.112 0.283 genre_3_R&B -0.1897 0.024 -8.057 0.000 -0.236 -0.144 mood_3_Yearning 0.1176 0.019 6.096 0.000 0.080 0.155 mood_2_- 0.4272 0.074 5.772 0.000 0.282 0.572 genre_3_Electronica -0.1893 0.026 -7.408 0.000 -0.239 -0.139 genre_2_Latin 0.3700 0.062 5.959 0.000 0.248 0.492 mood_3_Empowering 0.0909 0.021 4.386 0.000 0.050 0.132 genre_3_- -0.1084 0.021 -5.104 0.000 -0.150 -0.067 genre_1_Spoken & Audio 0.4897 0.089 5.489 0.000 0.315 0.665 genre_2_New Age 0.3718 0.067 5.546 0.000 0.240 0.503 genre_3_New Age 0.3384 0.067 5.053 0.000 0.207 0.470 genre_3_Rap -0.1484 0.026 -5.791 0.000 -0.199 -0.098 mood_1_Rowdy -0.2223 0.051 -4.373 0.000 -0.322 -0.123 mood_2_Rowdy -0.1655 0.039 -4.267 0.000 -0.242 -0.089 mood_2_Aggressive -0.1323 0.030 -4.345 0.000 -0.192 -0.073 genre_2_Spoken & Audio 0.3211 0.068 4.717 0.000 0.188 0.455 genre_1_New Age 0.2391 0.062 3.863 0.000 0.118 0.360 genre_2_Jazz 0.1958 0.043 4.533 0.000 0.111 0.280 genre_2_Pop 0.0819 0.016 4.999 0.000 0.050 0.114 genre_3_Rock -0.0849 0.020 -4.290 0.000 -0.124 -0.046 mood_1_Cool -0.1212 0.035 -3.464 0.001 -0.190 -0.053 mood_1_Gritty -0.1494 0.044 -3.386 0.001 -0.236 -0.063 mood_1_Easygoing -0.2261 0.074 -3.056 0.002 -0.371 -0.081 genre_3_Dance & House -0.0910 0.025 -3.595 0.000 -0.141 -0.041 mood_1_Excited 0.0583 0.018 3.248 0.001 0.023 0.093 summ . tables [ 0 ] Logit Regression Results Dep. Variable: y No. Observations: 403366 Model: Logit Df Residuals: 403309 Method: MLE Df Model: 56 Date: Sun, 24 Apr 2022 Pseudo R-squ.: 0.04795 Time: 18:07:32 Log-Likelihood: -1.2475e+05 converged: True LL-Null: -1.3104e+05 Covariance Type: nonrobust LLR p-value: 0.000 basemodel = pd . read_csv ( \"../../scripts/new_basemodel.csv\" , index_col = 0 ) y = df [ 'new_success' ] X2 = basemodel . values est = Logit ( y , X2 ) est2 = est . fit ( disp = 0 ) summ = est2 . summary () res_table = summ . tables [ 1 ] res_df = pd . DataFrame ( res_table . data ) cols = res_df . iloc [ 0 ] cols = [ str ( i ) for i in cols ] res_df . drop ( 0 , axis = 0 , inplace = True ) res_df . set_index ( 0 , inplace = True ) res_df . columns = cols [ 1 :] res_df . index = basemodel . columns display ( res_df ) res_df . to_excel ( \"new_success_log_model.xlsx\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef std err z P>|z| [0.025 0.975] intercept -2.4336 0.012 -201.725 0.000 -2.457 -2.410 genre_3_- -0.6766 0.025 -27.158 0.000 -0.725 -0.628 n_albums 0.1399 0.015 9.597 0.000 0.111 0.169 genre_1_Indie Rock 0.2702 0.016 17.240 0.000 0.240 0.301 mood_1_Defiant 0.2505 0.018 14.035 0.000 0.215 0.285 genre_1_Dance & House 0.3042 0.021 14.388 0.000 0.263 0.346 mood_1_Excited 0.1917 0.017 11.607 0.000 0.159 0.224 mood_1_Upbeat 0.2698 0.028 9.713 0.000 0.215 0.324 genre_2_Indie Rock 0.1527 0.019 7.854 0.000 0.115 0.191 genre_1_Rap 0.1876 0.019 9.843 0.000 0.150 0.225 genre_1_Religious 0.2676 0.030 8.877 0.000 0.209 0.327 mood_2_Romantic -0.2858 0.044 -6.533 0.000 -0.372 -0.200 mood_1_Yearning 0.1965 0.020 9.809 0.000 0.157 0.236 mood_1_Romantic -0.2540 0.045 -5.620 0.000 -0.343 -0.165 mood_3_Romantic -0.2249 0.042 -5.304 0.000 -0.308 -0.142 mood_1_Other -0.6658 0.134 -4.954 0.000 -0.929 -0.402 mood_2_Yearning 0.1714 0.019 9.044 0.000 0.134 0.209 mood_3_Yearning 0.1290 0.019 6.682 0.000 0.091 0.167 mood_2_Defiant 0.1263 0.019 6.645 0.000 0.089 0.164 mood_2_Excited 0.1043 0.018 5.871 0.000 0.069 0.139 genre_1_Electronica 0.1490 0.030 5.018 0.000 0.091 0.207 n_artists -0.0723 0.015 -4.776 0.000 -0.102 -0.043 mood_3_Urgent -0.1036 0.022 -4.766 0.000 -0.146 -0.061","title":"Logistic Regression"},{"location":"extras/X5_Candy_Ribbons/","text":"Causality Analysis \u00b6 TODO \u00b6 ols assumptions for univariate analyses residual plots forward selection lines up more or less with chi square tests include some colinearity measure? logistic regression (binned seconds, binned total outs) random forest does little better than random guessing business impact reorganize hypothesis tests section univariate vs multivariate cat v vat, cat v quant, quant v quant, quant v cat 1.0 Imports \u00b6 Import Libraries \u00b6 # basic packages import pandas as pd import numpy as np import random import copy # visualization packages from ipywidgets import interact import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures , MinMaxScaler from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , silhouette_score , calinski_harabasz_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score def make_patch_spines_invisible ( ax ): ax . set_frame_on ( True ) ax . patch . set_visible ( False ) for sp in ax . spines . values (): sp . set_visible ( False ) from matplotlib import colors from itertools import cycle def my_colors (): tab = cycle ( colors . TABLEAU_COLORS ) return tab Load Data \u00b6 consider only feeds consider feeds and operating conditions feeds = [ 'ADD ARGUS ARGUSTAT AT 31 HK' , 'ADD ARGUS ARGUTHERM AO 77 PP' , 'ADD POLYPLAST PPM AO 01 UV PP' , 'ADD SCHULMAN DTY 20 AOUV' , 'BEIGE SCHULMAN PZ 91738 1 5 ZA' , 'BLACK SCHULMAN P7381' , 'DEVAN 2SPIN PP 106' , 'GREEN POLYPLAST COLOUR MB PP 6821' , 'GREEN SCHULMAN PZ 302446 1 5 T' , 'High MFR Black' , 'GREEN SCHULMAN PZ 34198 1 5 T' , 'MFR 4 Black' , 'MFR 4 Black Brown' , 'High MFR Grey' , 'MFR 4 Green' , 'PP BOREALIS HE465FB' , 'MFR 4 Black Colour Blending' , 'PP INEOS 100 GA02' , 'PP INEOS 100 GA04' , 'PP INEOS 100 GA09' , 'MFR 4 Grey Colour Blending' , 'PP INEOS GA012' , 'PP POLYCHIM HB12XF' , 'MFR 4 White' , 'PP TOTAL 7089' , 'PP TOTAL PPH 4065' , 'MFR 4 White Colour Blending' , 'PP BOREALIS HE370FB' , 'PP UNIPETROL GB005' , 'SILIKAT PBH ABPP 05' , 'SILIKAT POLYPLUS AB 1001 PP' , 'UV ARGUS ARGUVIN UV 361 PP' , 'UV ARGUS ARX 904 11 PP' , 'UV ARGUS ARX 904 11 PP.1' , 'UV ARGUS ARX 904 11 PP.2' , 'UV POLYPLAST 6005 PP' , 'UV SCHULMAN FPPUV 38' , 'MFR 4 Grey' , 'PP POLYCHIM HL10XF' , 'MFR 4 Grey Grey' , 'PP POLYMER TEST MFI4' , 'PP TOTAL 4069' , 'RR MASTERBATCH TEST' , 'UV ARGUS ARX V 16 706 UV PP' ] df = pd . read_csv ( '../../../../../Dropbox/work/mfganalytic/data/dupont/TyparMasterReportWithSampleDataImprovement2017-today.csv' , header = 1 ) df = df . loc [ df [ 'Product' ] . notnull ()] df [ feeds ] = df [ feeds ] . div ( df [ feeds ] . sum ( axis = 1 ), axis = 0 ) #normalize feed rates df [ 'Time Delta' ] = pd . to_datetime ( df [ 'Doff Time' ]) . diff () df [ 'Total Seconds Out' ] = df [[ i for i in df . columns if 'Sec' in i ]] . sum ( 1 ) # df.dropna(inplace=True) hangs = [ df . columns [ i ] for i , j in enumerate ( 'Hang' in col for col in df . columns ) if j ] speeds = [ df . columns [ i ] for i , j in enumerate ( 'speed' in col for col in df . columns ) if j ] outs = [ df . columns [ i ] for i , j in enumerate ( 'Out' in col for col in df . columns ) if j ] prods = df [ 'Product' ] . unique () counts = [ i for i in df . columns if 'Count' in i ] #from LOEWEN virgin_polymer = [ 'PP INEOS 100 GA04' , 'PP POLYCHIM HB12XF' , 'PP TOTAL 7089' , #INEOS 100 GA09 until 2018 (2k rolls) 'PP TOTAL PPH 4065' , 'PP UNIPETROL GB005' , 'PP POLYCHIM HL10XF' ] #HL10XF not used after 2016, #BOREALIS virgin_polymer = [] for col in df . columns : if ( 'PP INEOS' in col ) or ( 'PP TOTAL' in col ) or ( 'PP UNI' in col ) or ( 'PP BORE' in col ) or ( 'PP POLY' in col ): virgin_polymer . append ( col ) pressures = [] for col in df . columns : if 'Press' in col or 'PRESS' in col : pressures . append ( col ) silicates = [] for col in df . columns : if 'SIL' in col : silicates . append ( col ) uvs = [] for col in df . columns : if 'UV ' in col : uvs . append ( col ) recycle = [] for col in df . columns : if 'MFR' in col : recycle . append ( col ) seconds = [] for col in df . columns : if 'Sec' in col : seconds . append ( col ) master_batch = list ( set ( feeds ) - set ( recycle ) - set ( virgin_polymer )) operation = list ( set ( df . columns ) - set ( master_batch ) - set ( virgin_polymer ) - set ( recycle ) - set ( hangs ) - set ( outs )) operation . sort () A summary of the columns print ( f 'pressures: { len ( pressures ) } ' ) print ( f 'silicates: { len ( silicates ) } ' ) print ( f 'uvs: { len ( uvs ) } ' ) print ( f 'feeds: { len ( feeds ) } ' ) print ( f 'master_batch: { len ( master_batch ) } ' ) print ( f 'virgin_polymer: { len ( virgin_polymer ) } ' ) print ( f 'recycle: { len ( recycle ) } ' ) print ( f 'operation: { len ( operation ) } ' ) print () print ( f 'hangs: { len ( hangs ) } ' ) print ( f 'speeds: { len ( speeds ) } ' ) print ( f 'outs: { len ( outs ) } ' ) print ( f 'seconds: { len ( seconds ) } ' ) print () print ( 'summary target columns: TotalHangs, Total Position Out' ) print () print ( df . shape ) pressures: 8 silicates: 2 uvs: 8 feeds: 44 master_batch: 20 virgin_polymer: 13 recycle: 11 operation: 180 hangs: 9 speeds: 9 outs: 10 seconds: 9 summary target columns: TotalHangs, Total Position Out (17897, 243) all_feats = df . select_dtypes ( float ) . columns [ df . select_dtypes ( float ) . columns . isin ( feeds + operation )] . values df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Millroll ID Product Roll Width Sample Number Doff Time Block1Pos Out Seconds Block2Pos Out Seconds Block3Pos Out Seconds Block4Pos Out Seconds Block1Pos Out Count ... TKW5 AVG - Tack - Tear West 5 Meter TPOP - NUMBER OF TPO PEEKS (TPOP) TTMD AVG - TRAP TEAR MD (TTMD) TTXD AVG - TRAP TEAR XD (TTXD) UPEX - UWU PLOT EXTREM (UPEX) UWG - UWU GRAVIMETRIC WITH SCALE (UWG) UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP) WATP - WATER PERMEABILITY (WATP) Time Delta Total Seconds Out 0 PM10022907 136215.0 5200.0 NaN 1/1/2017 12:43:57 AM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaT 9.0 1 PM10022908 136215.0 5200.0 NaN 1/1/2017 1:32:49 AM NaN 164.0 240.0 NaN NaN ... NaN NaN NaN NaN NaN 138.70 NaN NaN 0 days 00:48:52 536.0 2 PM10022909 136215.0 5200.0 NaN 1/1/2017 2:21:40 AM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:48:51 20.0 3 PM10022910 136215.0 5200.0 NaN 1/1/2017 3:10:34 AM NaN 320.0 264.0 NaN NaN ... NaN NaN 36.96 33.59 NaN 135.98 NaN NaN 0 days 00:48:54 1001.0 4 PM10022911 136215.0 5200.0 NaN 1/1/2017 3:59:24 AM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:48:50 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 PM10040811 90215.0 4250.0 P113099,P113100,P113101,P113102,P113103 2/25/2019 7:56:16 PM NaN NaN NaN NaN NaN ... NaN NaN 21.30 22.10 NaN 89.60 NaN NaN 0 days 00:57:24 0.0 17893 PM10040812 90215.0 4250.0 P113104 2/25/2019 8:53:41 PM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:57:25 0.0 17894 PM10040813 90215.0 4250.0 P113105,P113106,P113107 2/25/2019 9:51:06 PM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 89.30 NaN NaN 0 days 00:57:25 0.0 17895 PM10040814 90215.0 4250.0 P113108 2/25/2019 10:48:32 PM NaN NaN 576.0 NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:57:26 799.0 17896 PM10040815 90215.0 4250.0 P113109,P113110,P113111,P113112 2/25/2019 11:45:55 PM NaN NaN 429.0 NaN NaN ... NaN NaN 23.20 22.60 NaN 91.40 NaN NaN 0 days 00:57:23 429.0 17897 rows \u00d7 243 columns 2.0 Understand The Dependent Variable \u00b6 What is the hit rate in these upset columns? \u00b6 Around 7500 cases with no problem; 10000 cases with a position out target = \"Total Position Out\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 10355 False 7542 Name: Total Position Out, dtype: int64 There are 30% higher incidence of hangs target = \"TotalHangs\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 13954 False 3943 Name: TotalHangs, dtype: int64 target1 = \"TotalHangs\" target2 = \"Total Position Out\" print ( f \"both upsets: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"only hangs: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) print ( f \"only outs: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"neither upset: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) both upsets: 10161 only hangs: 3793 only outs: 194 neither upset: 3749 What is the magnitude of the outages? \u00b6 target = \"Total Position Out\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f0ad7207b80>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total Position Out Unnamed 0 Total Position Out 1.00000 0.62591 Unnamed 0 0.62591 1.00000 target = \"TotalHangs\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f0ad3eff460>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TotalHangs Unnamed 0 TotalHangs 1.000000 0.390949 Unnamed 0 0.390949 1.000000 Distribution around the target variable, total seconds \u00b6 We will certainly encounter trouble in an OLS estimate with this shape of the target variable; extreme skewness, upper tail, perhaps outliers tot_seconds = df [ seconds ] . sum ( 1 ) tot_seconds . kurt () 923.6728632470827 tot_seconds . skew () 25.09323820232841 tot_seconds . plot . kde () <AxesSubplot:ylabel='Density'> tot_seconds . describe () count 17897.000000 mean 2093.512879 std 8060.466090 min 0.000000 25% 20.000000 50% 666.000000 75% 1970.000000 max 396570.000000 dtype: float64 Do block positions have the same behavior? \u00b6 target = \"TotalHangs\" df . groupby ( target )[ hangs ] . count () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs TotalHangs 0.0 0 0 0 0 0 0 0 0 3943 1.0 510 682 745 977 510 682 745 977 2914 2.0 709 761 901 1109 709 761 901 1109 2424 3.0 721 776 930 1103 721 776 930 1103 1929 4.0 700 773 875 993 700 773 875 993 1577 5.0 609 668 744 828 609 668 745 828 1189 6.0 523 498 599 631 523 498 599 631 883 7.0 387 414 472 505 387 414 472 505 640 8.0 321 338 376 409 321 338 376 409 503 9.0 286 275 305 339 286 275 305 339 405 10.0 219 206 244 248 219 206 244 248 296 11.0 144 154 168 171 144 154 168 171 203 12.0 148 139 159 171 148 139 159 171 191 13.0 105 100 117 106 105 100 117 106 131 14.0 93 88 95 96 93 88 95 96 109 15.0 97 89 94 97 97 89 94 97 108 16.0 70 64 72 76 70 64 72 76 81 17.0 51 50 51 47 51 50 51 47 55 18.0 29 30 32 33 29 30 32 33 35 19.0 32 32 33 32 32 32 33 32 35 20.0 32 32 31 34 32 32 31 34 35 21.0 26 27 25 24 26 27 25 24 28 22.0 21 20 24 24 21 20 24 24 24 23.0 23 23 24 23 23 23 24 23 25 24.0 16 17 18 19 16 17 18 19 20 25.0 7 8 8 7 7 8 8 7 8 26.0 11 10 10 11 11 10 10 11 11 27.0 8 9 9 8 8 9 9 8 9 28.0 10 10 10 10 10 10 10 10 10 29.0 6 5 6 6 6 5 6 6 6 30.0 4 3 4 4 4 3 4 4 4 31.0 8 8 8 8 8 8 8 8 8 32.0 4 3 4 4 4 3 4 4 4 33.0 7 5 7 6 7 5 7 6 7 34.0 9 8 9 8 9 8 9 8 9 35.0 6 6 6 6 6 6 6 6 6 36.0 6 5 6 6 6 5 6 6 6 37.0 3 3 3 3 3 3 3 3 3 38.0 1 1 1 0 1 1 1 0 1 39.0 1 1 1 1 1 1 1 1 1 40.0 2 2 2 2 2 2 2 2 2 42.0 1 1 1 1 1 1 1 1 1 43.0 3 3 3 3 3 3 3 3 3 45.0 3 3 3 3 3 3 3 3 3 46.0 2 2 2 2 2 2 2 2 2 48.0 1 1 1 1 1 1 1 1 1 52.0 1 1 1 1 1 1 1 1 1 53.0 1 1 1 1 1 1 1 1 1 54.0 1 1 1 1 1 1 1 1 1 58.0 1 1 1 1 1 1 1 1 1 62.0 1 1 1 1 1 1 1 1 1 63.0 1 1 1 1 1 1 1 1 1 70.0 1 1 1 1 1 1 1 1 1 72.0 1 1 1 1 1 1 1 1 1 140.0 1 1 1 1 1 1 1 1 1 df [[ i for i in outs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:> df [[ i for i in hangs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:> 3.0 Look for dependent-independent signal \u00b6 Are there linear relationships between the dependent and independent variables? \u00b6 corr = df . corr () ( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 ) Product False Roll Width False Block1Pos Out Seconds True Block2Pos Out Seconds True Block3Pos Out Seconds True ... UPEX - UWU PLOT EXTREM (UPEX) False UWG - UWU GRAVIMETRIC WITH SCALE (UWG) False UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP) False WATP - WATER PERMEABILITY (WATP) False Total Seconds Out True Length: 237, dtype: bool cor_filt = corr . loc [( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 )] . drop ( labels = outs + hangs , axis = 0 )[ outs + hangs ] cor_filt .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Pos Out Seconds Block2Pos Out Seconds Block3Pos Out Seconds Block4Pos Out Seconds Block1Pos Out Count Block2Pos Out Count Block3Pos Out Count Block4Pos Out Count Total Position Out Total Seconds Out Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs ADD ARGUS ARGUSTAT AT 31 HK -0.103999 -0.417688 -0.383222 0.202131 -0.056194 -0.412274 0.155126 0.145535 0.014216 0.029563 0.121682 -0.256723 -0.012676 0.099690 -0.069354 -0.041492 0.094150 0.041900 0.344896 PP INEOS 100 GA02 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 -1.000000 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 MFR 4 White 0.289648 -0.016268 0.451446 -0.155932 0.297256 -0.000098 0.376795 -0.003129 0.225425 0.182687 0.231764 -0.045076 0.283286 -0.119481 0.108396 -0.057188 0.317089 0.008018 0.189109 PP TOTAL 7089 0.160268 0.171023 0.210796 0.093720 0.192159 0.210942 0.206859 0.076058 0.260505 0.207522 0.087954 0.050670 0.052622 -0.050506 0.218148 0.251676 0.134881 0.121080 0.331897 SILIKAT POLYPLUS AB 1001 PP 0.015132 0.125801 0.379559 0.255662 -0.018715 0.012019 0.161698 0.067661 0.127830 0.137376 -0.016317 -0.018187 -0.018956 0.086781 0.011207 -0.020307 0.092291 0.109259 0.069217 RJET15219WG1 - Corona Charge I AVG BLK#2 -0.120204 -0.120509 -0.056377 -0.002054 -0.198138 -0.221189 -0.193393 -0.024541 -0.232863 -0.149862 -0.090896 -0.060404 -0.054169 -0.052683 -0.215325 -0.216545 -0.182990 -0.120387 -0.308217 BLFL - COLOUR BLACK FILAMENT L value (BLFL) -0.360086 -0.157845 -0.245565 -0.268837 -0.153321 -0.162697 -0.212481 -0.247721 -0.067632 -0.121690 0.097783 0.146515 -0.346923 -0.110987 0.402310 -0.274153 -0.378670 -0.116264 -0.117501 BRFA - COLOUR BROWN FILAMENT a value (BRFa) 0.159056 0.250229 -0.384880 0.109024 0.134248 0.112836 -0.380715 -0.054074 0.099040 0.166926 0.170184 -0.065344 -0.195062 0.094596 0.195404 -0.333632 -0.026209 0.177576 0.173995 BRFB - COLOUR BROWN FILAMENT b value (BRFb) 0.413876 0.047748 0.332752 0.342115 0.145794 -0.028170 0.246787 0.234624 0.216156 0.311363 0.148441 -0.210193 0.118275 0.285286 -0.027055 -0.275750 0.100830 0.328240 0.164345 BRFL - COLOUR BROWN FILAMENT L value (BRFL) 0.198864 -0.178050 0.483242 0.205796 0.017456 -0.220432 0.411110 0.090885 0.039031 0.128326 0.337130 -0.326368 0.075738 0.145878 0.195055 -0.467536 -0.075130 0.188013 -0.000575 E20M AVG - SST ELONGATION 20X20 MD (E20M) 0.106870 -0.096775 -0.249986 -0.039633 0.009518 -0.049131 -0.367065 -0.074018 -0.070058 -0.071425 -0.124456 -0.016993 -0.070312 -0.036314 -0.145788 -0.075968 -0.197153 -0.069477 -0.072872 EF1M AVG - SST ELONGATION FOLDED 10 CM MD (EF1M) 1.000000 NaN 1.000000 -0.081738 1.000000 NaN NaN NaN -0.101606 -0.178993 0.313364 -0.936781 -1.000000 -0.517401 0.306235 -0.950303 -1.000000 -0.324932 -0.112334 EF1X AVG - SST ELONGATION FOLDED 10 CM XD (EF1X) 1.000000 NaN 1.000000 0.126777 1.000000 NaN NaN NaN 0.010039 -0.063399 0.896084 -0.961383 -1.000000 -0.609904 0.892730 -0.971830 -1.000000 -0.340772 0.050483 NATM AVG - Nail Tear MD EN 13859-1 (NATM) -0.517421 -0.835147 0.511409 -0.258360 -0.296500 -0.906038 NaN -0.333754 -0.100244 0.013436 -0.338129 0.580493 0.826191 -0.322585 -0.348782 0.063309 0.929207 -0.430201 0.086484 NATX AVG - Nail Tear XD EN 13859-1 (NATX) 0.269389 -0.864128 0.542651 0.071499 -0.053990 -0.927938 NaN -0.056465 -0.084131 0.046366 -0.190830 0.541297 0.777131 -0.114506 -0.096640 0.051081 0.902927 -0.295087 0.093804 O90 - O90 EN ISO 12956 (O90E) NaN NaN NaN 0.807962 NaN NaN NaN NaN -0.265093 -0.268107 -0.280791 -0.378899 NaN -0.451610 -0.307148 -0.267261 NaN -0.269680 -0.193539 PU AVG - PUNCTURE \"US Rod\" (PU) 0.173625 -0.790779 -0.165265 0.451194 -0.127912 -0.952390 -0.184515 0.734933 -0.040323 -0.021124 0.096383 -0.998695 -0.288786 0.156274 0.066142 0.363427 -0.297118 0.479459 -0.239496 TKO4 AVG - Tack-Tear Ost 4Meter -0.170480 -0.383667 -0.326335 0.006590 -0.104058 -0.308260 -0.329544 0.047734 -0.171814 -0.195662 -0.214633 -0.440748 -0.182875 -0.027353 -0.077638 -0.447423 -0.187893 0.114764 -0.160340 TKO5 AVG - Tack - Tear Ost 5 Meter 1.000000 0.134107 0.560717 -0.070116 NaN 0.106485 0.152594 -0.525375 -0.373817 -0.306252 0.338272 0.155577 0.034557 -0.453643 0.286066 0.056270 -0.066270 -0.400941 -0.387297 TKW4 AVG - Tack-Tear West 4Meter -0.334291 -0.282220 -0.265607 -0.134467 -0.320379 -0.231417 -0.300679 -0.265049 -0.274024 -0.250617 -0.277325 -0.375559 -0.168965 -0.154119 -0.236424 -0.282687 -0.168865 -0.162801 -0.316193 TKW5 AVG - Tack - Tear West 5 Meter 1.000000 0.051583 0.100609 0.018368 NaN 0.014194 -0.167032 -0.366762 -0.279911 -0.190366 0.069184 0.108657 0.224892 -0.326866 -0.003422 0.043730 0.039670 -0.362928 -0.270898 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt , cmap = cmap ) <AxesSubplot:> Filter N > 100 \u00b6 cor_filt = cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]] fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]], cmap = cmap ) <AxesSubplot:> Operating Conditions \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Operations Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Operations Correlations to Hangs and Outs') Feeds \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ ~ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Feeds Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Feeds Correlations to Hangs and Outs') ind_vars = list ( cor_filt . index [ ~ cor_filt . index . isin ( operation )]) ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] fig , ax = plt . subplots ( 1 , 3 , figsize = ( 30 , 10 )) indices = np . argwhere ( ax ) for index , ind_var in enumerate ( ind_vars ): df [[ ind_var ] + seconds ] . melt ( id_vars = ind_var , value_vars = None , var_name = None ,) . plot ( x = ind_var , y = 'value' , ax = ax [ index ], ls = '' , marker = '.' ) 4.0 Understand the Independent Variable \u00b6 Descriptive Stats on Ind Var \u00b6 all the feeds sum to 1 (feeds are mass or volume fractions), so we probably won't need to worry about scaling df [ feeds ] . sum ( 1 ) 0 1.0 1 1.0 2 1.0 3 1.0 4 1.0 ... 17892 1.0 17893 1.0 17894 1.0 17895 1.0 17896 1.0 Length: 17897, dtype: float64 many of the feeds are not used at all skew = df [ feeds ] . skew () kurt = df [ feeds ] . kurtosis () null = df [ feeds ] . isnull () . sum () n = df [ feeds ] . notnull () . sum () med = df [ feeds ] . median () men = df [ feeds ] . mean () dff = pd . DataFrame ([ skew , kurt , null , n , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'null cnt' , 'n' , 'median' , 'mean' ] dff . loc [ dff [ 'n' ] > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis null cnt n median mean ADD ARGUS ARGUSTAT AT 31 HK -1.122018 -0.275303 17798.0 99.0 0.006560 0.005080 ADD ARGUS ARGUTHERM AO 77 PP 14.927951 248.300227 15627.0 2270.0 0.005249 0.010705 ADD POLYPLAST PPM AO 01 UV PP 5.788514 87.051660 16769.0 1128.0 0.006397 0.006218 ADD SCHULMAN DTY 20 AOUV 6.573319 99.052816 13484.0 4413.0 0.004997 0.005411 BEIGE SCHULMAN PZ 91738 1 5 ZA -0.455858 10.108094 16975.0 922.0 0.008922 0.008708 BLACK SCHULMAN P7381 2.301415 16.313957 4471.0 13426.0 0.000317 0.002040 DEVAN 2SPIN PP 106 0.770601 11.066370 13279.0 4618.0 0.006658 0.005554 GREEN POLYPLAST COLOUR MB PP 6821 -1.856631 3.540741 16894.0 1003.0 0.007519 0.006822 GREEN SCHULMAN PZ 302446 1 5 T 0.363239 8.864015 17150.0 747.0 0.018764 0.018006 High MFR Black 0.473484 -1.518082 14862.0 3035.0 0.047883 0.146677 MFR 4 Black 1.612264 4.261601 12595.0 5302.0 0.030099 0.098792 MFR 4 Black Brown -0.086521 -0.342944 16360.0 1537.0 0.189244 0.150273 High MFR Grey -0.364616 -1.483643 15900.0 1997.0 0.226358 0.199138 MFR 4 Green 0.149918 8.244109 15985.0 1912.0 0.279622 0.262574 PP INEOS 100 GA02 NaN NaN 17895.0 2.0 0.107927 0.107927 PP INEOS 100 GA04 1.294449 5.764334 6684.0 11213.0 0.208757 0.216941 PP INEOS 100 GA09 -0.449540 -0.968103 16802.0 1095.0 0.224353 0.219208 PP POLYCHIM HB12XF -0.222658 -0.684893 16525.0 1372.0 0.275755 0.291827 MFR 4 White 1.136619 2.053071 17559.0 338.0 0.098961 0.111129 PP TOTAL 7089 -0.216726 -1.333059 16346.0 1551.0 0.276430 0.288643 PP TOTAL PPH 4065 0.353911 0.740658 611.0 17286.0 0.441009 0.424684 PP UNIPETROL GB005 0.117291 1.045121 11444.0 6453.0 0.324773 0.295906 SILIKAT PBH ABPP 05 2.690146 26.471577 15967.0 1930.0 0.005001 0.005229 SILIKAT POLYPLUS AB 1001 PP 6.874010 77.288671 17360.0 537.0 0.002506 0.002642 UV ARGUS ARX 904 11 PP 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.1 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.2 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV POLYPLAST 6005 PP 1.426790 1.634549 17506.0 391.0 0.020009 0.026245 UV SCHULMAN FPPUV 38 2.114261 14.372541 17141.0 756.0 0.029986 0.032261 MFR 4 Grey 1.718802 10.970192 8567.0 9330.0 0.269445 0.263401 MFR 4 Grey Grey 3.607370 20.342313 15865.0 2032.0 0.040035 0.068547 UV ARGUS ARX V 16 706 UV PP 1.889895 6.474782 7972.0 9925.0 0.002110 0.007994 our ind_vars feeds have some strong correlations with other feeds In particular, PP TOTAL 7089 and SILIKAT POLYPLUS AB 1001 PP have a correlation w/ eachother of .923 ind_corr = df [ feeds ] . corr ()[ ind_vars ] . dropna ( how = 'all' ) ind_corr . loc [( abs ( ind_corr ) > 0.5 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 ind_vars feeds exhibit bimodal distributions (2+ recipes) for ind in ind_vars : df [ ind ] . plot . kde () plt . title ( ind ) plt . xlim ( 0 ) plt . show () 5.0 Hypothesis Tests \u00b6 Non-Parametric \u00b6 Univariate Categorical to Categorical (Chi-Square) \u00b6 ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 for ind in ind_vars : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 10182 False 7377 True True 173 False 165 Name: Total Position Out, dtype: int64 [[10182 173] [ 7377 165]] chi2 stat: 6.02 p-value: 1.41e-02 PP TOTAL 7089 Total Position Out False True 9171 False 7175 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[9171 1184] [7175 367]] chi2 stat: 237.00 p-value: 1.78e-53 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 9996 False 7364 True True 359 False 178 Name: Total Position Out, dtype: int64 [[9996 359] [7364 178]] chi2 stat: 17.99 p-value: 2.22e-05 chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 chisum = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum [ 'type' ] = None chisum . loc [ chisum [ 'feed' ] . isin ( master_batch ), 'type' ] = 'master_batch' chisum . loc [ chisum [ 'feed' ] . isin ( recycle ), 'type' ] = 'recyle' chisum . loc [ chisum [ 'feed' ] . isin ( virgin_polymer ), 'type' ] = 'virgin_polymer' chisum . loc [ chisum [ 'feed' ] . isin ( silicates ), 'type' ] = 'silicates' chisum . loc [ chisum [ 'feed' ] . isin ( uvs ), 'type' ] = 'uvs' chisum . loc [ chisum [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null type 0 PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1 PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2 High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3 SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 4 BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 5 PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 6 ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 7 DEVAN 2SPIN PP 106 35.237690 2.918209e-09 [[7511, 2844], [5768, 1774]] False True master_batch 8 High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 9 GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 10 MFR 4 Black Brown 20.494540 5.980160e-06 [[9550, 805], [6810, 732]] True True recyle 11 UV ARGUS ARX V 16 706 UV PP 20.048858 7.548859e-06 [[4465, 5890], [3507, 4035]] False True uvs 12 ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 13 SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 14 GREEN SCHULMAN PZ 302446 1 5 T 14.155700 1.682865e-04 [[9973, 382], [7177, 365]] True True master_batch 15 MFR 4 Grey Grey 9.004184 2.693622e-03 [[9464, 1141], [6401, 891]] True True recyle Check confounding variables \u00b6 Isolate Products (product conflation with feed) Let's isolate the comparison by only products made with the feed ( ind_var ) in question We see that MFR 4 White and SILIKAT POLYPLUS AB 1001 PP are very imbalanced compared to PP TOTAL 7089 target = 'Total Position Out' for ind in ind_vars : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 3978 False 2933 True True 173 False 165 Name: Total Position Out, dtype: int64 [[3978 173] [2933 165]] chi2 stat: 5.10 p-value: 2.40e-02 PP TOTAL 7089 Total Position Out False True 2301 False 1393 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[2301 1184] [1393 367]] chi2 stat: 96.05 p-value: 1.12e-22 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 2418 False 1602 True True 359 False 178 Name: Total Position Out, dtype: int64 [[2418 359] [1602 178]] chi2 stat: 8.66 p-value: 3.25e-03 target = 'Total Position Out' chisum2 = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum2 = pd . concat ([ chisum2 , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum2 . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum2 = chisum2 . sort_values ( 'p-value' ) . reset_index ( drop = True ) tests = chisum . set_index ( 'feed' ) . join ( chisum2 . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], rsuffix = ', confound' ) tests . loc [( tests [ 'reject null' ] == True ) & ( tests [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value cTable + reject null type p-value, confound reject null, confound feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True we can compare this with our feed correlation table - there may be other explanatory variables 'hiding' ind_corr . loc [( abs ( ind_corr ) > 0.3 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 BLACK SCHULMAN P7381 0.196431 -0.470838 0.082699 DEVAN 2SPIN PP 106 NaN -0.415896 0.097590 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA04 0.342077 -0.248904 0.402788 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP TOTAL PPH 4065 -0.419569 -0.160492 0.089303 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 UV ARGUS ARX 904 11 PP NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.1 NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.2 NaN -0.461986 NaN MFR 4 Grey -0.303323 -0.381548 0.038125 MFR 4 Grey Grey 0.395658 -0.102141 -0.086338 UV ARGUS ARX V 16 706 UV PP 0.246254 -0.439913 -0.084811 Univariate Categorical to Quantitative (Moods Median) \u00b6 A limitation of the previous analysis is that our threshold for counting an upset in the line was pretty low - any delay (position out > 0 seconds). A way we can naturally increase this threshold is to compare medians (of position out (secs)) across our groups. chidf = pd . DataFrame () target = 'Seconds' chidf [ target ] = df [ seconds ] . sum ( 1 ) moods = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind in feeds : try : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) grp = chidf . loc [ chidf [ ind ] == True ][ target ] . values chi2 , p , m , cTable = stats . median_test ( grp , pop , correction = False ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] < 1 moods = pd . concat ([ moods , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass moods . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] moods = moods . sort_values ( 'p-value' ) . reset_index ( drop = True ) moods . loc [ moods [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null 0 PP TOTAL 7089 284.071422 9.736103e-64 [[1093, 8620], [458, 9277]] False True 1 MFR 4 Grey Grey 161.505327 5.305934e-37 [[744, 9215], [1288, 8682]] True True 2 PP POLYCHIM HB12XF 140.079653 2.557382e-32 [[897, 8734], [475, 9163]] False True 3 High MFR Grey 134.851343 3.558380e-31 [[1244, 8697], [753, 9200]] False True 4 SILIKAT PBH ABPP 05 132.735135 1.033157e-30 [[1205, 8704], [725, 9193]] False True 5 SILIKAT POLYPLUS AB 1001 PP 40.522176 1.943972e-10 [[341, 8870], [196, 9027]] False True 6 DEVAN 2SPIN PP 106 39.379816 3.488847e-10 [[2499, 8758], [2119, 9139]] False True 7 BLACK SCHULMAN P7381 33.697655 6.437826e-09 [[6961, 8686], [6465, 9211]] False True 8 GREEN POLYPLAST COLOUR MB PP 6821 32.518968 1.180348e-08 [[589, 8854], [414, 9043]] False True 9 PP INEOS 100 GA09 26.828649 2.223170e-07 [[630, 8854], [465, 9043]] False True 10 ADD ARGUS ARGUTHERM AO 77 PP 26.802325 2.253660e-07 [[1250, 8823], [1020, 9074]] False True 11 UV ARGUS ARX V 16 706 UV PP 23.787198 1.075957e-06 [[5152, 8744], [4773, 9153]] False True 12 High MFR Black 18.389746 1.800242e-05 [[1626, 8835], [1409, 9062]] False True 13 ADD POLYPLAST PPM AO 01 UV PP 15.536764 8.091620e-05 [[499, 9000], [629, 8897]] True True 14 UV SCHULMAN FPPUV 38 12.417442 4.253424e-04 [[330, 8983], [426, 8914]] True True 15 MFR 4 White 11.470090 7.072526e-04 [[138, 8971], [200, 8926]] True True 16 GREEN SCHULMAN PZ 302446 1 5 T 9.924320 1.631091e-03 [[331, 8983], [416, 8914]] True True we see that overall the moods test is slightly less pessimistic chisum [ 'reject null' ] . value_counts () True 16 False 16 Name: reject null, dtype: int64 moods [ 'reject null' ] . value_counts () True 17 False 15 Name: reject null, dtype: int64 testsf = tests . join ( moods . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], lsuffix = ', chi' , rsuffix = ', moods' ) testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True 2.223170e-07 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True 2.253660e-07 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True 1.800242e-05 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True 1.180348e-08 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True 8.091620e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True 1.943972e-10 True Non-Parametric Conclusions \u00b6 problem_vars = testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] . index print ( f \"Compared with correlation \\n the dependency tests show that the top 5 problem variables are: \\n { [ f ' { i } ' for i in problem_vars ] } \" ) Compared with correlation the dependency tests show that the top 5 problem variables are: ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True Parametric \u00b6 Univariate Quantitative to Quantitative (Linear Regression) \u00b6 for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () plt . plot ( x , ( y - y_pred ), ls = '' , marker = '.' ) plt . ylabel ( 'Residual' ) plt . xlabel ( 'x' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( f \"test statistic: { t } \" ) print ( f \"p-value: { stats . t . sf ( x = t , df = n - 2 ) } \" ) R2: 3.34e-02 RSE: 9.97e-03 b: -2.51e-03 m: 3.61e-01 n: 338 x_bar: 1.11e-01 SE_b: 1.51e-03 SE_m: 1.26e-02 test statistic: 28.56535956265302 p-value: 3.2661498371310494e-92 R2: 4.31e-02 RSE: 5.27e-03 b: -3.19e-03 m: 9.08e-02 n: 1551 x_bar: 2.89e-01 SE_b: 3.09e-04 SE_m: 9.66e-04 test statistic: 94.02166426512674 p-value: 0.0 R2: 1.89e-02 RSE: 5.94e-03 b: -1.36e-02 m: 1.57e+01 n: 537 x_bar: 2.64e-03 SE_b: 1.04e-03 SE_m: 3.81e-01 test statistic: 41.11483903422569 p-value: 4.981863349397906e-168 for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.033 Model: OLS Adj. R-squared: 0.030 Method: Least Squares F-statistic: 11.60 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.000739 Time: 10:02:50 Log-Likelihood: 360.28 No. Observations: 338 AIC: -716.6 Df Residuals: 336 BIC: -708.9 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0025 0.013 -0.199 0.843 -0.027 0.022 x1 0.3612 0.106 3.406 0.001 0.153 0.570 ============================================================================== Omnibus: 457.321 Durbin-Watson: 1.128 Prob(Omnibus): 0.000 Jarque-Bera (JB): 55378.392 Skew: 6.475 Prob(JB): 0.00 Kurtosis: 64.356 Cond. No. 23.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.043 Model: OLS Adj. R-squared: 0.042 Method: Least Squares F-statistic: 69.71 Date: Sat, 02 Apr 2022 Prob (F-statistic): 1.50e-16 Time: 10:02:50 Log-Likelihood: 2180.0 No. Observations: 1551 AIC: -4356. Df Residuals: 1549 BIC: -4345. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0032 0.003 -0.917 0.359 -0.010 0.004 x1 0.0908 0.011 8.349 0.000 0.069 0.112 ============================================================================== Omnibus: 2262.527 Durbin-Watson: 1.176 Prob(Omnibus): 0.000 Jarque-Bera (JB): 660230.502 Skew: 8.562 Prob(JB): 0.00 Kurtosis: 102.615 Cond. No. 7.82 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.019 Model: OLS Adj. R-squared: 0.017 Method: Least Squares F-statistic: 10.29 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.00142 Time: 10:02:50 Log-Likelihood: 622.03 No. Observations: 537 AIC: -1240. Df Residuals: 535 BIC: -1231. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0136 0.013 -1.020 0.308 -0.040 0.013 x1 15.6795 4.888 3.208 0.001 6.078 25.281 ============================================================================== Omnibus: 739.911 Durbin-Watson: 1.268 Prob(Omnibus): 0.000 Jarque-Bera (JB): 108127.158 Skew: 7.213 Prob(JB): 0.00 Kurtosis: 71.003 Cond. No. 1.49e+03 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.49e+03. This might indicate that there are strong multicollinearity or other numerical problems. Feature Engineering \u00b6 Introducing polynomial features results in poor coefficient estimates ind = ind_vars [ 1 ] lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) features = PolynomialFeatures ( degree = 3 ) x = features . fit_transform ( x . reshape ( - 1 , 1 )) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.056 Model: OLS Adj. R-squared: 0.055 Method: Least Squares F-statistic: 30.83 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.35e-19 Time: 10:03:01 Log-Likelihood: 2190.9 No. Observations: 1551 AIC: -4374. Df Residuals: 1547 BIC: -4352. Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0030 0.009 0.334 0.739 -0.014 0.020 x1 0.1612 0.124 1.301 0.193 -0.082 0.404 x2 -0.9203 0.523 -1.761 0.078 -1.945 0.105 x3 1.6922 0.664 2.550 0.011 0.390 2.994 ============================================================================== Omnibus: 2252.329 Durbin-Watson: 1.188 Prob(Omnibus): 0.000 Jarque-Bera (JB): 651735.784 Skew: 8.487 Prob(JB): 0.00 Kurtosis: 101.979 Cond. No. 594. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. R2: 5.64e-02 RSE: 6.03e-03 b: 2.95e-03 m: 0.00e+00 n: 1551 x_bar: 3.58e-01 SE_b: 1.69e-04 SE_m: 1.96e-04 0.0 0.5 Multivariate Quantitative to Quantitative (Multivariate Linear Regression) \u00b6 lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) lindf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ADD ARGUS ARGUSTAT AT 31 HK ADD ARGUS ARGUTHERM AO 77 PP ADD POLYPLAST PPM AO 01 UV PP ADD SCHULMAN DTY 20 AOUV BEIGE SCHULMAN PZ 91738 1 5 ZA BLACK SCHULMAN P7381 DEVAN 2SPIN PP 106 GREEN POLYPLAST COLOUR MB PP 6821 GREEN SCHULMAN PZ 302446 1 5 T High MFR Black ... UV POLYPLAST 6005 PP UV SCHULMAN FPPUV 38 MFR 4 Grey PP POLYCHIM HL10XF MFR 4 Grey Grey PP POLYMER TEST MFI4 PP TOTAL 4069 RR MASTERBATCH TEST UV ARGUS ARX V 16 706 UV PP seconds 0 0.0 0.0 0.0 0.005337 0.0 0.000253 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.409353 0.0 0.0 0.0 0.0 0.0 0.000000 9.0 1 0.0 0.0 0.0 0.005867 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.452007 0.0 0.0 0.0 0.0 0.0 0.000000 536.0 2 0.0 0.0 0.0 0.005895 0.0 0.000270 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.449608 0.0 0.0 0.0 0.0 0.0 0.000000 20.0 3 0.0 0.0 0.0 0.005743 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.440883 0.0 0.0 0.0 0.0 0.0 0.000000 1001.0 4 0.0 0.0 0.0 0.004553 0.0 0.000211 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.349459 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.339726 0.0 0.0 0.0 0.0 0.0 0.002062 0.0 17893 0.0 0.0 0.0 0.000000 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.342659 0.0 0.0 0.0 0.0 0.0 0.002071 0.0 17894 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345886 0.0 0.0 0.0 0.0 0.0 0.002107 0.0 17895 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.344945 0.0 0.0 0.0 0.0 0.0 0.002095 799.0 17896 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345288 0.0 0.0 0.0 0.0 0.0 0.002105 429.0 17897 rows \u00d7 45 columns lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) x = lindf [ feeds ] . values # features = PolynomialFeatures(degree=3) # x = features.fit_transform(x.reshape(-1,1)) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.050 Model: OLS Adj. R-squared: 0.049 Method: Least Squares F-statistic: 31.47 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.76e-174 Time: 10:03:19 Log-Likelihood: 44791. No. Observations: 17897 AIC: -8.952e+04 Df Residuals: 17866 BIC: -8.928e+04 Df Model: 30 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0123 0.002 5.316 0.000 0.008 0.017 x1 -1.0553 0.382 -2.760 0.006 -1.805 -0.306 x2 -0.0119 0.010 -1.241 0.215 -0.031 0.007 x3 0.0351 0.100 0.352 0.725 -0.161 0.231 x4 -0.2377 0.069 -3.438 0.001 -0.373 -0.102 x5 0.4001 0.145 2.750 0.006 0.115 0.685 x6 -0.0224 0.128 -0.175 0.861 -0.272 0.228 x7 -0.3174 0.096 -3.305 0.001 -0.506 -0.129 x8 -0.5909 0.209 -2.825 0.005 -1.001 -0.181 x9 -0.4059 0.087 -4.655 0.000 -0.577 -0.235 x10 -0.0241 0.005 -5.206 0.000 -0.033 -0.015 x11 -2.473e-14 1.17e-14 -2.107 0.035 -4.77e-14 -1.73e-15 x12 -0.0050 0.004 -1.371 0.171 -0.012 0.002 x13 -0.0288 0.007 -4.319 0.000 -0.042 -0.016 x14 -0.0268 0.005 -5.644 0.000 -0.036 -0.018 x15 -0.0200 0.006 -3.620 0.000 -0.031 -0.009 x16 -1.023e-14 3.5e-15 -2.923 0.003 -1.71e-14 -3.37e-15 x17 -9.109e-16 2.5e-16 -3.649 0.000 -1.4e-15 -4.22e-16 x18 -0.0994 0.093 -1.073 0.283 -0.281 0.082 x19 -0.0055 0.003 -2.044 0.041 -0.011 -0.000 x20 0.0003 0.005 0.058 0.954 -0.009 0.010 x21 1.325e-15 1.2e-16 11.078 0.000 1.09e-15 1.56e-15 x22 -2.494e-16 8.68e-17 -2.873 0.004 -4.2e-16 -7.93e-17 x23 0.0145 0.004 3.827 0.000 0.007 0.022 x24 -0.0100 0.010 -1.051 0.293 -0.029 0.009 x25 0.0216 0.004 5.724 0.000 0.014 0.029 x26 -0.0101 0.002 -4.198 0.000 -0.015 -0.005 x27 -1.03e-16 8.73e-17 -1.180 0.238 -2.74e-16 6.8e-17 x28 9.329e-16 7.53e-17 12.396 0.000 7.85e-16 1.08e-15 x29 -0.0074 0.003 -2.881 0.004 -0.012 -0.002 x30 1.7091 0.132 12.994 0.000 1.451 1.967 x31 4.1538 0.376 11.053 0.000 3.417 4.890 x32 0 0 nan nan 0 0 x33 0.0031 0.012 0.263 0.792 -0.020 0.026 x34 0.0031 0.012 0.263 0.792 -0.020 0.026 x35 0.0031 0.012 0.263 0.792 -0.020 0.026 x36 0.0790 0.042 1.868 0.062 -0.004 0.162 x37 0.0525 0.030 1.774 0.076 -0.006 0.111 x38 -0.0064 0.003 -2.521 0.012 -0.011 -0.001 x39 0 0 nan nan 0 0 x40 -0.0136 0.005 -2.762 0.006 -0.023 -0.004 x41 0 0 nan nan 0 0 x42 0 0 nan nan 0 0 x43 0 0 nan nan 0 0 x44 0.1990 0.048 4.119 0.000 0.104 0.294 ============================================================================== Omnibus: 43479.020 Durbin-Watson: 1.074 Prob(Omnibus): 0.000 Jarque-Bera (JB): 693938974.691 Skew: 25.434 Prob(JB): 0.00 Kurtosis: 966.322 Cond. No. 1.11e+16 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 1.79e-28. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. R2: 5.02e-02 RSE: 1.67e-03 b: 1.23e-02 m: -1.06e+00 n: 17897 x_bar: 2.26e-02 SE_b: 1.25e-05 SE_m: 2.14e-05 -49284.75476441197 1.0 Forward Selection \u00b6 def add_feature ( features , basemodel , data , y , r2max ): for feed in features : basemodel [ feed ] = data [ feed ] basemodel . fillna ( 0 , inplace = True ) x = basemodel [[ col for col in basemodel . columns if col != 'seconds' ]] . values ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () if ( est2 . rsquared > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = est2 . rsquared feedmax = feed bestsum = est2 . summary () newmodel = basemodel . copy () else : pass basemodel . drop ( labels = feed , axis = 1 , inplace = True ) return r2max , feedmax , bestsum , newmodel lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) r2max = 0 candidates = feeds . copy () basemodel = lindf while True : newr2max , feedmax , bestsum , newmodel = add_feature ( features = candidates , basemodel = basemodel , data = df , y = y , r2max = 0 ) if newr2max > r2max : # print(newr2max, feedmax) candidates . remove ( feedmax ) r2max = newr2max basemodel = newmodel continue else : break /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) we see some of the usual suspects from the chi-square tests. Notably some are missing: High MFR Grey , BLACK SCHULMAN P7381 , PP INEOS 100 GA09 and others. basemodel . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } seconds PP TOTAL 7089 PP POLYCHIM HB12XF SILIKAT PBH ABPP 05 SILIKAT POLYPLUS AB 1001 PP MFR 4 Green ADD SCHULMAN DTY 20 AOUV GREEN SCHULMAN PZ 302446 1 5 T MFR 4 Grey UV ARGUS ARX V 16 706 UV PP GREEN SCHULMAN PZ 34198 1 5 T 0 9.0 0.0 0.0 0.0 0.0 0.0 0.005337 0.0 0.409353 0.0 0.0 1 536.0 0.0 0.0 0.0 0.0 0.0 0.005867 0.0 0.452007 0.0 0.0 2 20.0 0.0 0.0 0.0 0.0 0.0 0.005895 0.0 0.449608 0.0 0.0 3 1001.0 0.0 0.0 0.0 0.0 0.0 0.005743 0.0 0.440883 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.004553 0.0 0.349459 0.0 0.0 forward_selection_feeds = basemodel . columns [ 1 :] bestsum OLS Regression Results Dep. Variable: y R-squared: 0.046 Model: OLS Adj. R-squared: 0.045 Method: Least Squares F-statistic: 94.99 Date: Sat, 02 Apr 2022 Prob (F-statistic): 5.51e-174 Time: 10:04:41 Log-Likelihood: 44748. No. Observations: 17897 AIC: -8.948e+04 Df Residuals: 17887 BIC: -8.940e+04 Df Model: 9 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 0.0034 0.000 11.288 0.000 0.003 0.004 x1 0.0214 0.002 11.535 0.000 0.018 0.025 x2 0.0146 0.002 7.837 0.000 0.011 0.018 x3 1.5742 0.110 14.375 0.000 1.360 1.789 x4 4.1700 0.363 11.497 0.000 3.459 4.881 x5 -0.0181 0.003 -6.276 0.000 -0.024 -0.012 x6 -0.2879 0.065 -4.450 0.000 -0.415 -0.161 x7 -0.1932 0.052 -3.698 0.000 -0.296 -0.091 x8 0.0037 0.001 3.322 0.001 0.002 0.006 x9 0.0736 0.028 2.641 0.008 0.019 0.128 x10 0 0 nan nan 0 0 x11 0 0 nan nan 0 0 Omnibus: 43415.763 Durbin-Watson: 1.068 Prob(Omnibus): 0.000 Jarque-Bera (JB): 683838684.139 Skew: 25.346 Prob(JB): 0.00 Kurtosis: 959.275 Cond. No. inf Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 0. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular. Multivariate Conclusions \u00b6 y poses a problem with out of the box regression. Extreme tails will upset the residual plots. We some overlap of the parametric and non-parametric approaches: [ i for i in basemodel . columns if i in ( problem_vars )] ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05'] Multivariate Quantitative to Categorical (Binned Output Variable) \u00b6 Logistic regression does little better than random guessing lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) x = lindf [ ind ] . values #.reshape(-1,1) model = LogisticRegression () model . fit ( x , y ) y_pred = model . predict ( x ) print ( f \"Train Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) Train Acc: 0.37 Test Acc: 0.37 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( print ( classification_report ( y , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.35 0.80 0.49 4527 2.0 0.36 0.32 0.34 4434 3.0 0.30 0.03 0.06 4463 4.0 0.43 0.32 0.37 4473 accuracy 0.37 17897 macro avg 0.36 0.37 0.31 17897 weighted avg 0.36 0.37 0.31 17897 <AxesSubplot:> If we are careful about regularizing the random forest model, it does a little bit better lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) X = lindf [ ind ] . values #.reshape(-1,1) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 0.99 Test Acc: 0.39 y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.41 0.48 0.44 898 2.0 0.33 0.30 0.31 881 3.0 0.29 0.27 0.28 893 4.0 0.50 0.51 0.51 908 accuracy 0.39 3580 macro avg 0.38 0.39 0.38 3580 weighted avg 0.38 0.39 0.38 3580 <AxesSubplot:> # grab feature importances imp = model . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in model . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ ind , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 PP TOTAL PPH 4065 0.150387 0.008053 1 BLACK SCHULMAN P7381 0.118158 0.008725 2 PP INEOS 100 GA04 0.100635 0.005217 3 UV ARGUS ARX V 16 706 UV PP 0.090776 0.006273 4 MFR 4 Grey 0.08828 0.005656 5 PP UNIPETROL GB005 0.060558 0.003569 6 MFR 4 Black 0.052589 0.003695 7 ADD SCHULMAN DTY 20 AOUV 0.041478 0.003554 8 DEVAN 2SPIN PP 106 0.038438 0.004061 9 High MFR Black 0.028838 0.002515 10 MFR 4 Grey Grey 0.025758 0.002319 11 ADD ARGUS ARGUTHERM AO 77 PP 0.024526 0.002253 12 High MFR Grey 0.020652 0.004188 13 MFR 4 Green 0.016508 0.002725 14 SILIKAT PBH ABPP 05 0.016283 0.002291 15 PP TOTAL 7089 0.01624 0.004897 16 ADD POLYPLAST PPM AO 01 UV PP 0.013499 0.001685 17 MFR 4 Black Brown 0.013401 0.002131 18 PP POLYCHIM HB12XF 0.01245 0.002185 19 PP INEOS 100 GA09 0.010354 0.00156 6.0 Business Impact \u00b6 What is the so what? From Hyp tests - target is either 'Total Position Out' or 'Total Seconds Out' We had culprits from pearsons: ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] culprits from non parametric tests: list ( problem_vars ) ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] and culprits from linear regression forward_selection_feeds Index(['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'MFR 4 Green', 'ADD SCHULMAN DTY 20 AOUV', 'GREEN SCHULMAN PZ 302446 1 5 T', 'MFR 4 Grey', 'UV ARGUS ARX V 16 706 UV PP', 'GREEN SCHULMAN PZ 34198 1 5 T'], dtype='object') and we can check the overlap between non-parametric and linear regression (parametric): predict_vars = [ i for i in basemodel . columns if i in ( problem_vars )] predict_vars ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05'] all_vars = set ( list ( problem_vars ) + list ( forward_selection_feeds )) all_vars {'ADD SCHULMAN DTY 20 AOUV', 'BLACK SCHULMAN P7381', 'GREEN SCHULMAN PZ 302446 1 5 T', 'GREEN SCHULMAN PZ 34198 1 5 T', 'High MFR Grey', 'MFR 4 Green', 'MFR 4 Grey', 'PP POLYCHIM HB12XF', 'PP TOTAL 7089', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'UV ARGUS ARX V 16 706 UV PP'} # add to DF business = pd . DataFrame () value_line_by_day = 99000 for ind in problem_vars : try : # products, prods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique () nprods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . nunique () dff = df . loc [ df [ 'Product' ] . isin ( prods )] # total runtime, tot_runtime = dff [ 'Time Delta' ] . sum () # total runtime with feed, tot_runtime_wfeed = dff [ dff [ ind ] > 0 ][ 'Time Delta' ] . sum () #downtime w/o feed, downtime w/ feed, avg_downtime_min = dff . groupby ( dff [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 avg_downtime_min_wfeed = avg_downtime_min [ True ] avg_downtime_min_w_ofeed = avg_downtime_min [ False ] ratio = avg_downtime_min_wfeed / avg_downtime_min_w_ofeed # average runtime, avg_runtime_min = dff [ 'Time Delta' ] . mean () . total_seconds () / 60 # days saved, dollars saved days_saved = ( tot_runtime_wfeed . total_seconds () / 60 / avg_runtime_min * avg_downtime_min . diff () . values [ - 1 ]) / 60 / 24 dollars_saved_per_batch = value_line_by_day * avg_downtime_min . diff () . values [ - 1 ] / ( 60 * 24 ) business = pd . concat (( business , pd . DataFrame ([[ ind , nprods , tot_runtime , tot_runtime_wfeed , avg_downtime_min_wfeed , avg_downtime_min_w_ofeed , ratio , avg_runtime_min , days_saved , days_saved * value_line_by_day , dollars_saved_per_batch ]], columns = [ 'Feed' , 'Products' , 'Total Runtime, Products' , 'Total Runtime With Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Average Runtime' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]))) except : print ( ind ) print ( dff . groupby ( df [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 ) business = business . sort_values ( 'Dollars Saved (Per Batch)' , ascending = False ) . reset_index ( drop = True ) business # workshop downtime ratio # feed products, feed products # Dollars saved per batch .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Products Total Runtime, Products Total Runtime With Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Average Runtime Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26 409 days 20:43:21 94 days 04:36:05 26.250639 13.671063 1.920161 70.835736 16.727319 1.656005e+06 864.845823 1 High MFR Grey 36 564 days 04:13:41 94 days 12:26:59 23.623539 12.809036 1.844287 65.459164 15.615433 1.545928e+06 743.497080 2 PP POLYCHIM HB12XF 17 383 days 00:03:51 67 days 09:13:53 21.043525 15.600015 1.348943 70.908183 5.173013 5.121283e+05 374.241303 3 BLACK SCHULMAN P7381 64 751 days 20:01:57 590 days 12:21:50 14.902871 12.363943 1.205349 63.216276 23.716605 2.347944e+06 174.551301 4 PP TOTAL 7089 13 291 days 22:06:22 79 days 15:56:47 20.651050 18.269305 1.130369 80.146114 2.367430 2.343756e+05 163.744933 pd . set_option ( 'display.precision' , 2 ) display ( business [[ 'Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]], ) pd . set_option ( 'display.precision' , 6 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26.25 13.67 1.92 16.73 1.66e+06 864.85 1 High MFR Grey 23.62 12.81 1.84 15.62 1.55e+06 743.50 2 PP POLYCHIM HB12XF 21.04 15.60 1.35 5.17 5.12e+05 374.24 3 BLACK SCHULMAN P7381 14.90 12.36 1.21 23.72 2.35e+06 174.55 4 PP TOTAL 7089 20.65 18.27 1.13 2.37 2.34e+05 163.74 We see that 'SILKAT PBH ABPP 05' as well as 'High MFR Grey' have a large amount of days saved over the two years, a strong downtime multiplier effect and the largest dollars saved per batch. Since High MFR Grey is a recycle feed, we may not have the ability to remove it from the feed list. Leaving the SILKAT (Silicate) feed the prime feedstock to remove 7.0 Visualizations \u00b6 PP TOTAL 7089 \u00b6 predict_var = predict_vars [ 1 ] outcome_var = [ i for i in df . columns if 'Seconds' in i ] def myplot ( col = list ( df . loc [ df [ predict_var ] > 0 ][ 'Product' ] . unique ())): fig , ax = plt . subplots ( figsize = ( 20 , 10 )) axt = ax . twinx () # other feeds df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True ) . dropna ( axis = 1 , how = 'all' ) . plot ( ls = '' , marker = '*' , ax = ax , alpha = 0.2 ) # predict_var feed df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True )[ predict_var ] . plot ( ls = '--' , marker = '+' , ax = ax , lw = 2 , c = 'red' ) # outcome_var df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ) . plot ( ls = '-' , marker = '+' , lw = 1 , ax = axt , alpha = 1 , c = 'blue' , label = 'Total Seconds Out' ) # outliers ax . set_ylim ( 0 ) axt . set_ylim ( 0 , np . quantile ( df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ), 0.997 )) h1 , l1 = ax . get_legend_handles_labels () h2 , l2 = axt . get_legend_handles_labels () ax . legend ( h1 + h2 , l1 + l2 , loc = 0 ) interact ( myplot ) interactive(children=(Dropdown(description='col', options=(0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.\u2026 <function __main__.myplot(col=[0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.0, 136215.0, 125215.0, 110215.0, 100215.0, 80215.0, 100617.0, 90617.0, 90215.0, 90216.0, 110520.0, 80617.0])>","title":"X5 Candy Ribbons"},{"location":"extras/X5_Candy_Ribbons/#causality-analysis","text":"","title":"Causality Analysis"},{"location":"extras/X5_Candy_Ribbons/#todo","text":"ols assumptions for univariate analyses residual plots forward selection lines up more or less with chi square tests include some colinearity measure? logistic regression (binned seconds, binned total outs) random forest does little better than random guessing business impact reorganize hypothesis tests section univariate vs multivariate cat v vat, cat v quant, quant v quant, quant v cat","title":"TODO"},{"location":"extras/X5_Candy_Ribbons/#10-imports","text":"","title":"1.0 Imports"},{"location":"extras/X5_Candy_Ribbons/#import-libraries","text":"# basic packages import pandas as pd import numpy as np import random import copy # visualization packages from ipywidgets import interact import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures , MinMaxScaler from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , silhouette_score , calinski_harabasz_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score def make_patch_spines_invisible ( ax ): ax . set_frame_on ( True ) ax . patch . set_visible ( False ) for sp in ax . spines . values (): sp . set_visible ( False ) from matplotlib import colors from itertools import cycle def my_colors (): tab = cycle ( colors . TABLEAU_COLORS ) return tab","title":"Import Libraries"},{"location":"extras/X5_Candy_Ribbons/#load-data","text":"consider only feeds consider feeds and operating conditions feeds = [ 'ADD ARGUS ARGUSTAT AT 31 HK' , 'ADD ARGUS ARGUTHERM AO 77 PP' , 'ADD POLYPLAST PPM AO 01 UV PP' , 'ADD SCHULMAN DTY 20 AOUV' , 'BEIGE SCHULMAN PZ 91738 1 5 ZA' , 'BLACK SCHULMAN P7381' , 'DEVAN 2SPIN PP 106' , 'GREEN POLYPLAST COLOUR MB PP 6821' , 'GREEN SCHULMAN PZ 302446 1 5 T' , 'High MFR Black' , 'GREEN SCHULMAN PZ 34198 1 5 T' , 'MFR 4 Black' , 'MFR 4 Black Brown' , 'High MFR Grey' , 'MFR 4 Green' , 'PP BOREALIS HE465FB' , 'MFR 4 Black Colour Blending' , 'PP INEOS 100 GA02' , 'PP INEOS 100 GA04' , 'PP INEOS 100 GA09' , 'MFR 4 Grey Colour Blending' , 'PP INEOS GA012' , 'PP POLYCHIM HB12XF' , 'MFR 4 White' , 'PP TOTAL 7089' , 'PP TOTAL PPH 4065' , 'MFR 4 White Colour Blending' , 'PP BOREALIS HE370FB' , 'PP UNIPETROL GB005' , 'SILIKAT PBH ABPP 05' , 'SILIKAT POLYPLUS AB 1001 PP' , 'UV ARGUS ARGUVIN UV 361 PP' , 'UV ARGUS ARX 904 11 PP' , 'UV ARGUS ARX 904 11 PP.1' , 'UV ARGUS ARX 904 11 PP.2' , 'UV POLYPLAST 6005 PP' , 'UV SCHULMAN FPPUV 38' , 'MFR 4 Grey' , 'PP POLYCHIM HL10XF' , 'MFR 4 Grey Grey' , 'PP POLYMER TEST MFI4' , 'PP TOTAL 4069' , 'RR MASTERBATCH TEST' , 'UV ARGUS ARX V 16 706 UV PP' ] df = pd . read_csv ( '../../../../../Dropbox/work/mfganalytic/data/dupont/TyparMasterReportWithSampleDataImprovement2017-today.csv' , header = 1 ) df = df . loc [ df [ 'Product' ] . notnull ()] df [ feeds ] = df [ feeds ] . div ( df [ feeds ] . sum ( axis = 1 ), axis = 0 ) #normalize feed rates df [ 'Time Delta' ] = pd . to_datetime ( df [ 'Doff Time' ]) . diff () df [ 'Total Seconds Out' ] = df [[ i for i in df . columns if 'Sec' in i ]] . sum ( 1 ) # df.dropna(inplace=True) hangs = [ df . columns [ i ] for i , j in enumerate ( 'Hang' in col for col in df . columns ) if j ] speeds = [ df . columns [ i ] for i , j in enumerate ( 'speed' in col for col in df . columns ) if j ] outs = [ df . columns [ i ] for i , j in enumerate ( 'Out' in col for col in df . columns ) if j ] prods = df [ 'Product' ] . unique () counts = [ i for i in df . columns if 'Count' in i ] #from LOEWEN virgin_polymer = [ 'PP INEOS 100 GA04' , 'PP POLYCHIM HB12XF' , 'PP TOTAL 7089' , #INEOS 100 GA09 until 2018 (2k rolls) 'PP TOTAL PPH 4065' , 'PP UNIPETROL GB005' , 'PP POLYCHIM HL10XF' ] #HL10XF not used after 2016, #BOREALIS virgin_polymer = [] for col in df . columns : if ( 'PP INEOS' in col ) or ( 'PP TOTAL' in col ) or ( 'PP UNI' in col ) or ( 'PP BORE' in col ) or ( 'PP POLY' in col ): virgin_polymer . append ( col ) pressures = [] for col in df . columns : if 'Press' in col or 'PRESS' in col : pressures . append ( col ) silicates = [] for col in df . columns : if 'SIL' in col : silicates . append ( col ) uvs = [] for col in df . columns : if 'UV ' in col : uvs . append ( col ) recycle = [] for col in df . columns : if 'MFR' in col : recycle . append ( col ) seconds = [] for col in df . columns : if 'Sec' in col : seconds . append ( col ) master_batch = list ( set ( feeds ) - set ( recycle ) - set ( virgin_polymer )) operation = list ( set ( df . columns ) - set ( master_batch ) - set ( virgin_polymer ) - set ( recycle ) - set ( hangs ) - set ( outs )) operation . sort () A summary of the columns print ( f 'pressures: { len ( pressures ) } ' ) print ( f 'silicates: { len ( silicates ) } ' ) print ( f 'uvs: { len ( uvs ) } ' ) print ( f 'feeds: { len ( feeds ) } ' ) print ( f 'master_batch: { len ( master_batch ) } ' ) print ( f 'virgin_polymer: { len ( virgin_polymer ) } ' ) print ( f 'recycle: { len ( recycle ) } ' ) print ( f 'operation: { len ( operation ) } ' ) print () print ( f 'hangs: { len ( hangs ) } ' ) print ( f 'speeds: { len ( speeds ) } ' ) print ( f 'outs: { len ( outs ) } ' ) print ( f 'seconds: { len ( seconds ) } ' ) print () print ( 'summary target columns: TotalHangs, Total Position Out' ) print () print ( df . shape ) pressures: 8 silicates: 2 uvs: 8 feeds: 44 master_batch: 20 virgin_polymer: 13 recycle: 11 operation: 180 hangs: 9 speeds: 9 outs: 10 seconds: 9 summary target columns: TotalHangs, Total Position Out (17897, 243) all_feats = df . select_dtypes ( float ) . columns [ df . select_dtypes ( float ) . columns . isin ( feeds + operation )] . values df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Millroll ID Product Roll Width Sample Number Doff Time Block1Pos Out Seconds Block2Pos Out Seconds Block3Pos Out Seconds Block4Pos Out Seconds Block1Pos Out Count ... TKW5 AVG - Tack - Tear West 5 Meter TPOP - NUMBER OF TPO PEEKS (TPOP) TTMD AVG - TRAP TEAR MD (TTMD) TTXD AVG - TRAP TEAR XD (TTXD) UPEX - UWU PLOT EXTREM (UPEX) UWG - UWU GRAVIMETRIC WITH SCALE (UWG) UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP) WATP - WATER PERMEABILITY (WATP) Time Delta Total Seconds Out 0 PM10022907 136215.0 5200.0 NaN 1/1/2017 12:43:57 AM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaT 9.0 1 PM10022908 136215.0 5200.0 NaN 1/1/2017 1:32:49 AM NaN 164.0 240.0 NaN NaN ... NaN NaN NaN NaN NaN 138.70 NaN NaN 0 days 00:48:52 536.0 2 PM10022909 136215.0 5200.0 NaN 1/1/2017 2:21:40 AM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:48:51 20.0 3 PM10022910 136215.0 5200.0 NaN 1/1/2017 3:10:34 AM NaN 320.0 264.0 NaN NaN ... NaN NaN 36.96 33.59 NaN 135.98 NaN NaN 0 days 00:48:54 1001.0 4 PM10022911 136215.0 5200.0 NaN 1/1/2017 3:59:24 AM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:48:50 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 PM10040811 90215.0 4250.0 P113099,P113100,P113101,P113102,P113103 2/25/2019 7:56:16 PM NaN NaN NaN NaN NaN ... NaN NaN 21.30 22.10 NaN 89.60 NaN NaN 0 days 00:57:24 0.0 17893 PM10040812 90215.0 4250.0 P113104 2/25/2019 8:53:41 PM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:57:25 0.0 17894 PM10040813 90215.0 4250.0 P113105,P113106,P113107 2/25/2019 9:51:06 PM NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 89.30 NaN NaN 0 days 00:57:25 0.0 17895 PM10040814 90215.0 4250.0 P113108 2/25/2019 10:48:32 PM NaN NaN 576.0 NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 0 days 00:57:26 799.0 17896 PM10040815 90215.0 4250.0 P113109,P113110,P113111,P113112 2/25/2019 11:45:55 PM NaN NaN 429.0 NaN NaN ... NaN NaN 23.20 22.60 NaN 91.40 NaN NaN 0 days 00:57:23 429.0 17897 rows \u00d7 243 columns","title":"Load Data"},{"location":"extras/X5_Candy_Ribbons/#20-understand-the-dependent-variable","text":"","title":"2.0 Understand The Dependent Variable"},{"location":"extras/X5_Candy_Ribbons/#what-is-the-hit-rate-in-these-upset-columns","text":"Around 7500 cases with no problem; 10000 cases with a position out target = \"Total Position Out\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 10355 False 7542 Name: Total Position Out, dtype: int64 There are 30% higher incidence of hangs target = \"TotalHangs\" df . groupby ( target ) . apply ( lambda x : x [ target ] > 0 ) . value_counts () True 13954 False 3943 Name: TotalHangs, dtype: int64 target1 = \"TotalHangs\" target2 = \"Total Position Out\" print ( f \"both upsets: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"only hangs: { df . loc [( df [ target1 ] > 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) print ( f \"only outs: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] > 0 )] . shape [ 0 ] } \" ) print ( f \"neither upset: { df . loc [( df [ target1 ] == 0 ) & ( df [ target2 ] == 0 )] . shape [ 0 ] } \" ) both upsets: 10161 only hangs: 3793 only outs: 194 neither upset: 3749","title":"What is the hit rate in these upset columns?"},{"location":"extras/X5_Candy_Ribbons/#what-is-the-magnitude-of-the-outages","text":"target = \"Total Position Out\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f0ad7207b80>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total Position Out Unnamed 0 Total Position Out 1.00000 0.62591 Unnamed 0 0.62591 1.00000 target = \"TotalHangs\" y = df . loc [ df [ target ] > 0 ][[ i for i in outs if 'Sec' in i ]] . sum ( axis = 1 ) x = df . loc [ df [ target ] > 0 ][ target ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f0ad3eff460>] pd . DataFrame ([ x , y ]) . T . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TotalHangs Unnamed 0 TotalHangs 1.000000 0.390949 Unnamed 0 0.390949 1.000000","title":"What is the magnitude of the outages?"},{"location":"extras/X5_Candy_Ribbons/#distribution-around-the-target-variable-total-seconds","text":"We will certainly encounter trouble in an OLS estimate with this shape of the target variable; extreme skewness, upper tail, perhaps outliers tot_seconds = df [ seconds ] . sum ( 1 ) tot_seconds . kurt () 923.6728632470827 tot_seconds . skew () 25.09323820232841 tot_seconds . plot . kde () <AxesSubplot:ylabel='Density'> tot_seconds . describe () count 17897.000000 mean 2093.512879 std 8060.466090 min 0.000000 25% 20.000000 50% 666.000000 75% 1970.000000 max 396570.000000 dtype: float64","title":"Distribution around the target variable, total seconds"},{"location":"extras/X5_Candy_Ribbons/#do-block-positions-have-the-same-behavior","text":"target = \"TotalHangs\" df . groupby ( target )[ hangs ] . count () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs TotalHangs 0.0 0 0 0 0 0 0 0 0 3943 1.0 510 682 745 977 510 682 745 977 2914 2.0 709 761 901 1109 709 761 901 1109 2424 3.0 721 776 930 1103 721 776 930 1103 1929 4.0 700 773 875 993 700 773 875 993 1577 5.0 609 668 744 828 609 668 745 828 1189 6.0 523 498 599 631 523 498 599 631 883 7.0 387 414 472 505 387 414 472 505 640 8.0 321 338 376 409 321 338 376 409 503 9.0 286 275 305 339 286 275 305 339 405 10.0 219 206 244 248 219 206 244 248 296 11.0 144 154 168 171 144 154 168 171 203 12.0 148 139 159 171 148 139 159 171 191 13.0 105 100 117 106 105 100 117 106 131 14.0 93 88 95 96 93 88 95 96 109 15.0 97 89 94 97 97 89 94 97 108 16.0 70 64 72 76 70 64 72 76 81 17.0 51 50 51 47 51 50 51 47 55 18.0 29 30 32 33 29 30 32 33 35 19.0 32 32 33 32 32 32 33 32 35 20.0 32 32 31 34 32 32 31 34 35 21.0 26 27 25 24 26 27 25 24 28 22.0 21 20 24 24 21 20 24 24 24 23.0 23 23 24 23 23 23 24 23 25 24.0 16 17 18 19 16 17 18 19 20 25.0 7 8 8 7 7 8 8 7 8 26.0 11 10 10 11 11 10 10 11 11 27.0 8 9 9 8 8 9 9 8 9 28.0 10 10 10 10 10 10 10 10 10 29.0 6 5 6 6 6 5 6 6 6 30.0 4 3 4 4 4 3 4 4 4 31.0 8 8 8 8 8 8 8 8 8 32.0 4 3 4 4 4 3 4 4 4 33.0 7 5 7 6 7 5 7 6 7 34.0 9 8 9 8 9 8 9 8 9 35.0 6 6 6 6 6 6 6 6 6 36.0 6 5 6 6 6 5 6 6 6 37.0 3 3 3 3 3 3 3 3 3 38.0 1 1 1 0 1 1 1 0 1 39.0 1 1 1 1 1 1 1 1 1 40.0 2 2 2 2 2 2 2 2 2 42.0 1 1 1 1 1 1 1 1 1 43.0 3 3 3 3 3 3 3 3 3 45.0 3 3 3 3 3 3 3 3 3 46.0 2 2 2 2 2 2 2 2 2 48.0 1 1 1 1 1 1 1 1 1 52.0 1 1 1 1 1 1 1 1 1 53.0 1 1 1 1 1 1 1 1 1 54.0 1 1 1 1 1 1 1 1 1 58.0 1 1 1 1 1 1 1 1 1 62.0 1 1 1 1 1 1 1 1 1 63.0 1 1 1 1 1 1 1 1 1 70.0 1 1 1 1 1 1 1 1 1 72.0 1 1 1 1 1 1 1 1 1 140.0 1 1 1 1 1 1 1 1 1 df [[ i for i in outs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:> df [[ i for i in hangs if 'Sec' in i ]] . boxplot ( vert = False ) <AxesSubplot:>","title":"Do block positions have the same behavior?"},{"location":"extras/X5_Candy_Ribbons/#30-look-for-dependent-independent-signal","text":"","title":"3.0 Look for dependent-independent signal"},{"location":"extras/X5_Candy_Ribbons/#are-there-linear-relationships-between-the-dependent-and-independent-variables","text":"corr = df . corr () ( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 ) Product False Roll Width False Block1Pos Out Seconds True Block2Pos Out Seconds True Block3Pos Out Seconds True ... UPEX - UWU PLOT EXTREM (UPEX) False UWG - UWU GRAVIMETRIC WITH SCALE (UWG) False UWGP AVG - UWU PLOT GRAVIMETRIC WITH SCALE (UWGP) False WATP - WATER PERMEABILITY (WATP) False Total Seconds Out True Length: 237, dtype: bool cor_filt = corr . loc [( abs ( corr [ outs + hangs ]) > 0.3 ) . any ( axis = 1 )] . drop ( labels = outs + hangs , axis = 0 )[ outs + hangs ] cor_filt .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Block1Pos Out Seconds Block2Pos Out Seconds Block3Pos Out Seconds Block4Pos Out Seconds Block1Pos Out Count Block2Pos Out Count Block3Pos Out Count Block4Pos Out Count Total Position Out Total Seconds Out Block1Hangs Seconds Block2Hangs Seconds Block3Hangs Seconds Block4Hangs Seconds Block1Hangs Count Block2Hangs Count Block3Hangs Count Block4Hangs Count TotalHangs ADD ARGUS ARGUSTAT AT 31 HK -0.103999 -0.417688 -0.383222 0.202131 -0.056194 -0.412274 0.155126 0.145535 0.014216 0.029563 0.121682 -0.256723 -0.012676 0.099690 -0.069354 -0.041492 0.094150 0.041900 0.344896 PP INEOS 100 GA02 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 -1.000000 NaN NaN NaN NaN NaN NaN NaN NaN -1.000000 MFR 4 White 0.289648 -0.016268 0.451446 -0.155932 0.297256 -0.000098 0.376795 -0.003129 0.225425 0.182687 0.231764 -0.045076 0.283286 -0.119481 0.108396 -0.057188 0.317089 0.008018 0.189109 PP TOTAL 7089 0.160268 0.171023 0.210796 0.093720 0.192159 0.210942 0.206859 0.076058 0.260505 0.207522 0.087954 0.050670 0.052622 -0.050506 0.218148 0.251676 0.134881 0.121080 0.331897 SILIKAT POLYPLUS AB 1001 PP 0.015132 0.125801 0.379559 0.255662 -0.018715 0.012019 0.161698 0.067661 0.127830 0.137376 -0.016317 -0.018187 -0.018956 0.086781 0.011207 -0.020307 0.092291 0.109259 0.069217 RJET15219WG1 - Corona Charge I AVG BLK#2 -0.120204 -0.120509 -0.056377 -0.002054 -0.198138 -0.221189 -0.193393 -0.024541 -0.232863 -0.149862 -0.090896 -0.060404 -0.054169 -0.052683 -0.215325 -0.216545 -0.182990 -0.120387 -0.308217 BLFL - COLOUR BLACK FILAMENT L value (BLFL) -0.360086 -0.157845 -0.245565 -0.268837 -0.153321 -0.162697 -0.212481 -0.247721 -0.067632 -0.121690 0.097783 0.146515 -0.346923 -0.110987 0.402310 -0.274153 -0.378670 -0.116264 -0.117501 BRFA - COLOUR BROWN FILAMENT a value (BRFa) 0.159056 0.250229 -0.384880 0.109024 0.134248 0.112836 -0.380715 -0.054074 0.099040 0.166926 0.170184 -0.065344 -0.195062 0.094596 0.195404 -0.333632 -0.026209 0.177576 0.173995 BRFB - COLOUR BROWN FILAMENT b value (BRFb) 0.413876 0.047748 0.332752 0.342115 0.145794 -0.028170 0.246787 0.234624 0.216156 0.311363 0.148441 -0.210193 0.118275 0.285286 -0.027055 -0.275750 0.100830 0.328240 0.164345 BRFL - COLOUR BROWN FILAMENT L value (BRFL) 0.198864 -0.178050 0.483242 0.205796 0.017456 -0.220432 0.411110 0.090885 0.039031 0.128326 0.337130 -0.326368 0.075738 0.145878 0.195055 -0.467536 -0.075130 0.188013 -0.000575 E20M AVG - SST ELONGATION 20X20 MD (E20M) 0.106870 -0.096775 -0.249986 -0.039633 0.009518 -0.049131 -0.367065 -0.074018 -0.070058 -0.071425 -0.124456 -0.016993 -0.070312 -0.036314 -0.145788 -0.075968 -0.197153 -0.069477 -0.072872 EF1M AVG - SST ELONGATION FOLDED 10 CM MD (EF1M) 1.000000 NaN 1.000000 -0.081738 1.000000 NaN NaN NaN -0.101606 -0.178993 0.313364 -0.936781 -1.000000 -0.517401 0.306235 -0.950303 -1.000000 -0.324932 -0.112334 EF1X AVG - SST ELONGATION FOLDED 10 CM XD (EF1X) 1.000000 NaN 1.000000 0.126777 1.000000 NaN NaN NaN 0.010039 -0.063399 0.896084 -0.961383 -1.000000 -0.609904 0.892730 -0.971830 -1.000000 -0.340772 0.050483 NATM AVG - Nail Tear MD EN 13859-1 (NATM) -0.517421 -0.835147 0.511409 -0.258360 -0.296500 -0.906038 NaN -0.333754 -0.100244 0.013436 -0.338129 0.580493 0.826191 -0.322585 -0.348782 0.063309 0.929207 -0.430201 0.086484 NATX AVG - Nail Tear XD EN 13859-1 (NATX) 0.269389 -0.864128 0.542651 0.071499 -0.053990 -0.927938 NaN -0.056465 -0.084131 0.046366 -0.190830 0.541297 0.777131 -0.114506 -0.096640 0.051081 0.902927 -0.295087 0.093804 O90 - O90 EN ISO 12956 (O90E) NaN NaN NaN 0.807962 NaN NaN NaN NaN -0.265093 -0.268107 -0.280791 -0.378899 NaN -0.451610 -0.307148 -0.267261 NaN -0.269680 -0.193539 PU AVG - PUNCTURE \"US Rod\" (PU) 0.173625 -0.790779 -0.165265 0.451194 -0.127912 -0.952390 -0.184515 0.734933 -0.040323 -0.021124 0.096383 -0.998695 -0.288786 0.156274 0.066142 0.363427 -0.297118 0.479459 -0.239496 TKO4 AVG - Tack-Tear Ost 4Meter -0.170480 -0.383667 -0.326335 0.006590 -0.104058 -0.308260 -0.329544 0.047734 -0.171814 -0.195662 -0.214633 -0.440748 -0.182875 -0.027353 -0.077638 -0.447423 -0.187893 0.114764 -0.160340 TKO5 AVG - Tack - Tear Ost 5 Meter 1.000000 0.134107 0.560717 -0.070116 NaN 0.106485 0.152594 -0.525375 -0.373817 -0.306252 0.338272 0.155577 0.034557 -0.453643 0.286066 0.056270 -0.066270 -0.400941 -0.387297 TKW4 AVG - Tack-Tear West 4Meter -0.334291 -0.282220 -0.265607 -0.134467 -0.320379 -0.231417 -0.300679 -0.265049 -0.274024 -0.250617 -0.277325 -0.375559 -0.168965 -0.154119 -0.236424 -0.282687 -0.168865 -0.162801 -0.316193 TKW5 AVG - Tack - Tear West 5 Meter 1.000000 0.051583 0.100609 0.018368 NaN 0.014194 -0.167032 -0.366762 -0.279911 -0.190366 0.069184 0.108657 0.224892 -0.326866 -0.003422 0.043730 0.039670 -0.362928 -0.270898 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt , cmap = cmap ) <AxesSubplot:>","title":"Are there linear relationships between the dependent and independent variables?"},{"location":"extras/X5_Candy_Ribbons/#filter-n-100","text":"cor_filt = cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]] fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [[ df [ item ] . dropna () . shape [ 0 ] > 100 for item in cor_filt . index ]], cmap = cmap ) <AxesSubplot:>","title":"Filter N &gt; 100"},{"location":"extras/X5_Candy_Ribbons/#operating-conditions","text":"fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Operations Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Operations Correlations to Hangs and Outs')","title":"Operating Conditions"},{"location":"extras/X5_Candy_Ribbons/#feeds","text":"fig , ax = plt . subplots ( figsize = ( 10 , 10 )) cmap = sns . diverging_palette ( 230 , 20 , as_cmap = True ) sns . heatmap ( cor_filt . loc [ ~ cor_filt . index . isin ( operation )], cmap = cmap ) ax . set_title ( \"Feeds Correlations to Hangs and Outs\" ) Text(0.5, 1.0, 'Feeds Correlations to Hangs and Outs') ind_vars = list ( cor_filt . index [ ~ cor_filt . index . isin ( operation )]) ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] fig , ax = plt . subplots ( 1 , 3 , figsize = ( 30 , 10 )) indices = np . argwhere ( ax ) for index , ind_var in enumerate ( ind_vars ): df [[ ind_var ] + seconds ] . melt ( id_vars = ind_var , value_vars = None , var_name = None ,) . plot ( x = ind_var , y = 'value' , ax = ax [ index ], ls = '' , marker = '.' )","title":"Feeds"},{"location":"extras/X5_Candy_Ribbons/#40-understand-the-independent-variable","text":"","title":"4.0 Understand the Independent Variable"},{"location":"extras/X5_Candy_Ribbons/#descriptive-stats-on-ind-var","text":"all the feeds sum to 1 (feeds are mass or volume fractions), so we probably won't need to worry about scaling df [ feeds ] . sum ( 1 ) 0 1.0 1 1.0 2 1.0 3 1.0 4 1.0 ... 17892 1.0 17893 1.0 17894 1.0 17895 1.0 17896 1.0 Length: 17897, dtype: float64 many of the feeds are not used at all skew = df [ feeds ] . skew () kurt = df [ feeds ] . kurtosis () null = df [ feeds ] . isnull () . sum () n = df [ feeds ] . notnull () . sum () med = df [ feeds ] . median () men = df [ feeds ] . mean () dff = pd . DataFrame ([ skew , kurt , null , n , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'null cnt' , 'n' , 'median' , 'mean' ] dff . loc [ dff [ 'n' ] > 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis null cnt n median mean ADD ARGUS ARGUSTAT AT 31 HK -1.122018 -0.275303 17798.0 99.0 0.006560 0.005080 ADD ARGUS ARGUTHERM AO 77 PP 14.927951 248.300227 15627.0 2270.0 0.005249 0.010705 ADD POLYPLAST PPM AO 01 UV PP 5.788514 87.051660 16769.0 1128.0 0.006397 0.006218 ADD SCHULMAN DTY 20 AOUV 6.573319 99.052816 13484.0 4413.0 0.004997 0.005411 BEIGE SCHULMAN PZ 91738 1 5 ZA -0.455858 10.108094 16975.0 922.0 0.008922 0.008708 BLACK SCHULMAN P7381 2.301415 16.313957 4471.0 13426.0 0.000317 0.002040 DEVAN 2SPIN PP 106 0.770601 11.066370 13279.0 4618.0 0.006658 0.005554 GREEN POLYPLAST COLOUR MB PP 6821 -1.856631 3.540741 16894.0 1003.0 0.007519 0.006822 GREEN SCHULMAN PZ 302446 1 5 T 0.363239 8.864015 17150.0 747.0 0.018764 0.018006 High MFR Black 0.473484 -1.518082 14862.0 3035.0 0.047883 0.146677 MFR 4 Black 1.612264 4.261601 12595.0 5302.0 0.030099 0.098792 MFR 4 Black Brown -0.086521 -0.342944 16360.0 1537.0 0.189244 0.150273 High MFR Grey -0.364616 -1.483643 15900.0 1997.0 0.226358 0.199138 MFR 4 Green 0.149918 8.244109 15985.0 1912.0 0.279622 0.262574 PP INEOS 100 GA02 NaN NaN 17895.0 2.0 0.107927 0.107927 PP INEOS 100 GA04 1.294449 5.764334 6684.0 11213.0 0.208757 0.216941 PP INEOS 100 GA09 -0.449540 -0.968103 16802.0 1095.0 0.224353 0.219208 PP POLYCHIM HB12XF -0.222658 -0.684893 16525.0 1372.0 0.275755 0.291827 MFR 4 White 1.136619 2.053071 17559.0 338.0 0.098961 0.111129 PP TOTAL 7089 -0.216726 -1.333059 16346.0 1551.0 0.276430 0.288643 PP TOTAL PPH 4065 0.353911 0.740658 611.0 17286.0 0.441009 0.424684 PP UNIPETROL GB005 0.117291 1.045121 11444.0 6453.0 0.324773 0.295906 SILIKAT PBH ABPP 05 2.690146 26.471577 15967.0 1930.0 0.005001 0.005229 SILIKAT POLYPLUS AB 1001 PP 6.874010 77.288671 17360.0 537.0 0.002506 0.002642 UV ARGUS ARX 904 11 PP 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.1 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV ARGUS ARX 904 11 PP.2 1.405525 3.135048 17166.0 731.0 0.019242 0.025060 UV POLYPLAST 6005 PP 1.426790 1.634549 17506.0 391.0 0.020009 0.026245 UV SCHULMAN FPPUV 38 2.114261 14.372541 17141.0 756.0 0.029986 0.032261 MFR 4 Grey 1.718802 10.970192 8567.0 9330.0 0.269445 0.263401 MFR 4 Grey Grey 3.607370 20.342313 15865.0 2032.0 0.040035 0.068547 UV ARGUS ARX V 16 706 UV PP 1.889895 6.474782 7972.0 9925.0 0.002110 0.007994 our ind_vars feeds have some strong correlations with other feeds In particular, PP TOTAL 7089 and SILIKAT POLYPLUS AB 1001 PP have a correlation w/ eachother of .923 ind_corr = df [ feeds ] . corr ()[ ind_vars ] . dropna ( how = 'all' ) ind_corr . loc [( abs ( ind_corr ) > 0.5 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 ind_vars feeds exhibit bimodal distributions (2+ recipes) for ind in ind_vars : df [ ind ] . plot . kde () plt . title ( ind ) plt . xlim ( 0 ) plt . show ()","title":"Descriptive Stats on Ind Var"},{"location":"extras/X5_Candy_Ribbons/#50-hypothesis-tests","text":"","title":"5.0 Hypothesis Tests"},{"location":"extras/X5_Candy_Ribbons/#non-parametric","text":"","title":"Non-Parametric"},{"location":"extras/X5_Candy_Ribbons/#univariate-categorical-to-categorical-chi-square","text":"ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 for ind in ind_vars : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 10182 False 7377 True True 173 False 165 Name: Total Position Out, dtype: int64 [[10182 173] [ 7377 165]] chi2 stat: 6.02 p-value: 1.41e-02 PP TOTAL 7089 Total Position Out False True 9171 False 7175 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[9171 1184] [7175 367]] chi2 stat: 237.00 p-value: 1.78e-53 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 9996 False 7364 True True 359 False 178 Name: Total Position Out, dtype: int64 [[9996 359] [7364 178]] chi2 stat: 17.99 p-value: 2.22e-05 chidf = pd . DataFrame () target = 'Total Position Out' chidf [ target ] = df [ target ] > 0 chisum = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum = pd . concat ([ chisum , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum = chisum . sort_values ( 'p-value' ) . reset_index ( drop = True ) chisum [ 'type' ] = None chisum . loc [ chisum [ 'feed' ] . isin ( master_batch ), 'type' ] = 'master_batch' chisum . loc [ chisum [ 'feed' ] . isin ( recycle ), 'type' ] = 'recyle' chisum . loc [ chisum [ 'feed' ] . isin ( virgin_polymer ), 'type' ] = 'virgin_polymer' chisum . loc [ chisum [ 'feed' ] . isin ( silicates ), 'type' ] = 'silicates' chisum . loc [ chisum [ 'feed' ] . isin ( uvs ), 'type' ] = 'uvs' chisum . loc [ chisum [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null type 0 PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1 PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2 High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3 SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 4 BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 5 PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 6 ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 7 DEVAN 2SPIN PP 106 35.237690 2.918209e-09 [[7511, 2844], [5768, 1774]] False True master_batch 8 High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 9 GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 10 MFR 4 Black Brown 20.494540 5.980160e-06 [[9550, 805], [6810, 732]] True True recyle 11 UV ARGUS ARX V 16 706 UV PP 20.048858 7.548859e-06 [[4465, 5890], [3507, 4035]] False True uvs 12 ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 13 SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 14 GREEN SCHULMAN PZ 302446 1 5 T 14.155700 1.682865e-04 [[9973, 382], [7177, 365]] True True master_batch 15 MFR 4 Grey Grey 9.004184 2.693622e-03 [[9464, 1141], [6401, 891]] True True recyle","title":"Univariate Categorical to Categorical (Chi-Square)"},{"location":"extras/X5_Candy_Ribbons/#check-confounding-variables","text":"Isolate Products (product conflation with feed) Let's isolate the comparison by only products made with the feed ( ind_var ) in question We see that MFR 4 White and SILIKAT POLYPLUS AB 1001 PP are very imbalanced compared to PP TOTAL 7089 target = 'Total Position Out' for ind in ind_vars : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 display ( chidf . groupby ( ind )[ target ] . value_counts ()) # in cTable, populations are rows (Tot Pos Out) # groups are columns (w/ the feed w/o the feed) cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T print ( cTable ) chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) print ( f \"chi2 stat: { chi2 : .2f } \\t p-value: { p : .2e } \" ) print () MFR 4 White Total Position Out False True 3978 False 2933 True True 173 False 165 Name: Total Position Out, dtype: int64 [[3978 173] [2933 165]] chi2 stat: 5.10 p-value: 2.40e-02 PP TOTAL 7089 Total Position Out False True 2301 False 1393 True True 1184 False 367 Name: Total Position Out, dtype: int64 [[2301 1184] [1393 367]] chi2 stat: 96.05 p-value: 1.12e-22 SILIKAT POLYPLUS AB 1001 PP Total Position Out False True 2418 False 1602 True True 359 False 178 Name: Total Position Out, dtype: int64 [[2418 359] [1602 178]] chi2 stat: 8.66 p-value: 3.25e-03 target = 'Total Position Out' chisum2 = pd . DataFrame () cutoff = 0.01 for ind in feeds : chidf = pd . DataFrame () # isolate products made with `ind` dff = df . loc [ df [ 'Product' ] . isin ( df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique ())] chidf [ target ] = dff [ target ] > 0 chidf [ ind ] = dff [ ind ] > 0 try : cTable = ( chidf . groupby ( ind )[ target ] . value_counts () . values ) . reshape ( 2 , 2 ) . T chi2 , p , dof , ex = stats . chi2_contingency ( cTable , correction = True , lambda_ = None ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] > 1 chisum2 = pd . concat ([ chisum2 , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass chisum2 . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] chisum2 = chisum2 . sort_values ( 'p-value' ) . reset_index ( drop = True ) tests = chisum . set_index ( 'feed' ) . join ( chisum2 . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], rsuffix = ', confound' ) tests . loc [( tests [ 'reject null' ] == True ) & ( tests [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value cTable + reject null type p-value, confound reject null, confound feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True we can compare this with our feed correlation table - there may be other explanatory variables 'hiding' ind_corr . loc [( abs ( ind_corr ) > 0.3 ) . any ( 1 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MFR 4 White PP TOTAL 7089 SILIKAT POLYPLUS AB 1001 PP ADD SCHULMAN DTY 20 AOUV -0.741967 -0.057221 -0.875009 BLACK SCHULMAN P7381 0.196431 -0.470838 0.082699 DEVAN 2SPIN PP 106 NaN -0.415896 0.097590 High MFR Black NaN -0.351428 0.538436 MFR 4 Black -0.442742 0.876072 -0.602269 PP INEOS 100 GA04 0.342077 -0.248904 0.402788 PP INEOS 100 GA09 NaN -0.654004 NaN PP POLYCHIM HB12XF NaN -0.697618 0.322222 MFR 4 White 1.000000 NaN NaN PP TOTAL 7089 NaN 1.000000 0.923427 PP TOTAL PPH 4065 -0.419569 -0.160492 0.089303 PP UNIPETROL GB005 -0.634001 -0.713590 0.148101 SILIKAT POLYPLUS AB 1001 PP NaN 0.923427 1.000000 UV ARGUS ARX 904 11 PP NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.1 NaN -0.461986 NaN UV ARGUS ARX 904 11 PP.2 NaN -0.461986 NaN MFR 4 Grey -0.303323 -0.381548 0.038125 MFR 4 Grey Grey 0.395658 -0.102141 -0.086338 UV ARGUS ARX V 16 706 UV PP 0.246254 -0.439913 -0.084811","title":"Check confounding variables"},{"location":"extras/X5_Candy_Ribbons/#univariate-categorical-to-quantitative-moods-median","text":"A limitation of the previous analysis is that our threshold for counting an upset in the line was pretty low - any delay (position out > 0 seconds). A way we can naturally increase this threshold is to compare medians (of position out (secs)) across our groups. chidf = pd . DataFrame () target = 'Seconds' chidf [ target ] = df [ seconds ] . sum ( 1 ) moods = pd . DataFrame () cutoff = 0.01 pop = chidf [ target ] . values for ind in feeds : try : chidf [ ind ] = ( df [ ind ] > 0 ) | ( df [ ind ] . notnull ()) grp = chidf . loc [ chidf [ ind ] == True ][ target ] . values chi2 , p , m , cTable = stats . median_test ( grp , pop , correction = False ) ratio = cTable [ 0 ] / cTable [ 1 ] pos = ratio [ 0 ] / ratio [ 1 ] < 1 moods = pd . concat ([ moods , pd . DataFrame ([[ ind , chi2 , p , cTable , pos , p < cutoff ]])]) except : # print(f\"\\t\\t{ind} returned error; n: {chidf[ind].sum()}\") pass moods . columns = [ 'feed' , 'chi' , 'p-value' , 'cTable' , '+' , 'reject null' ] moods = moods . sort_values ( 'p-value' ) . reset_index ( drop = True ) moods . loc [ moods [ 'reject null' ] == True ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feed chi p-value cTable + reject null 0 PP TOTAL 7089 284.071422 9.736103e-64 [[1093, 8620], [458, 9277]] False True 1 MFR 4 Grey Grey 161.505327 5.305934e-37 [[744, 9215], [1288, 8682]] True True 2 PP POLYCHIM HB12XF 140.079653 2.557382e-32 [[897, 8734], [475, 9163]] False True 3 High MFR Grey 134.851343 3.558380e-31 [[1244, 8697], [753, 9200]] False True 4 SILIKAT PBH ABPP 05 132.735135 1.033157e-30 [[1205, 8704], [725, 9193]] False True 5 SILIKAT POLYPLUS AB 1001 PP 40.522176 1.943972e-10 [[341, 8870], [196, 9027]] False True 6 DEVAN 2SPIN PP 106 39.379816 3.488847e-10 [[2499, 8758], [2119, 9139]] False True 7 BLACK SCHULMAN P7381 33.697655 6.437826e-09 [[6961, 8686], [6465, 9211]] False True 8 GREEN POLYPLAST COLOUR MB PP 6821 32.518968 1.180348e-08 [[589, 8854], [414, 9043]] False True 9 PP INEOS 100 GA09 26.828649 2.223170e-07 [[630, 8854], [465, 9043]] False True 10 ADD ARGUS ARGUTHERM AO 77 PP 26.802325 2.253660e-07 [[1250, 8823], [1020, 9074]] False True 11 UV ARGUS ARX V 16 706 UV PP 23.787198 1.075957e-06 [[5152, 8744], [4773, 9153]] False True 12 High MFR Black 18.389746 1.800242e-05 [[1626, 8835], [1409, 9062]] False True 13 ADD POLYPLAST PPM AO 01 UV PP 15.536764 8.091620e-05 [[499, 9000], [629, 8897]] True True 14 UV SCHULMAN FPPUV 38 12.417442 4.253424e-04 [[330, 8983], [426, 8914]] True True 15 MFR 4 White 11.470090 7.072526e-04 [[138, 8971], [200, 8926]] True True 16 GREEN SCHULMAN PZ 302446 1 5 T 9.924320 1.631091e-03 [[331, 8983], [416, 8914]] True True we see that overall the moods test is slightly less pessimistic chisum [ 'reject null' ] . value_counts () True 16 False 16 Name: reject null, dtype: int64 moods [ 'reject null' ] . value_counts () True 17 False 15 Name: reject null, dtype: int64 testsf = tests . join ( moods . set_index ( 'feed' )[[ 'p-value' , 'reject null' ]], lsuffix = ', chi' , rsuffix = ', moods' ) testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True PP INEOS 100 GA09 53.633573 2.415953e-13 [[9605, 750], [7197, 345]] False True virgin_polymer 1.089179e-03 True 2.223170e-07 True ADD ARGUS ARGUTHERM AO 77 PP 49.779216 1.720570e-12 [[8886, 1469], [6741, 801]] False True master_batch 3.540298e-10 True 2.253660e-07 True High MFR Black 34.917755 3.439303e-09 [[8452, 1903], [6410, 1132]] False True recyle 6.799384e-03 True 1.800242e-05 True GREEN POLYPLAST COLOUR MB PP 6821 29.252089 6.354766e-08 [[9692, 663], [7202, 340]] False True master_batch 1.482338e-11 True 1.180348e-08 True ADD POLYPLAST PPM AO 01 UV PP 19.643205 9.333462e-06 [[9774, 581], [6995, 547]] True True uvs 1.905591e-05 True 8.091620e-05 True SILIKAT POLYPLUS AB 1001 PP 17.988635 2.222278e-05 [[9996, 359], [7364, 178]] False True silicates 3.245550e-03 True 1.943972e-10 True","title":"Univariate Categorical to Quantitative (Moods Median)"},{"location":"extras/X5_Candy_Ribbons/#non-parametric-conclusions","text":"problem_vars = testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] . index print ( f \"Compared with correlation \\n the dependency tests show that the top 5 problem variables are: \\n { [ f ' { i } ' for i in problem_vars ] } \" ) Compared with correlation the dependency tests show that the top 5 problem variables are: ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] testsf . loc [( testsf [ 'reject null, chi' ] == True ) & ( testsf [ 'reject null, moods' ] == True ) & ( testsf [ 'reject null, confound' ] == True )] . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } chi p-value, chi cTable + reject null, chi type p-value, confound reject null, confound p-value, moods reject null, moods feed PP TOTAL 7089 236.997695 1.775656e-53 [[9171, 1184], [7175, 367]] False True virgin_polymer 1.118476e-22 True 9.736103e-64 True PP POLYCHIM HB12XF 160.530330 8.665205e-37 [[9338, 1017], [7187, 355]] False True virgin_polymer 2.117300e-10 True 2.557382e-32 True High MFR Grey 136.568590 1.498471e-31 [[8956, 1399], [6944, 598]] False True recyle 3.703831e-20 True 3.558380e-31 True SILIKAT PBH ABPP 05 125.809746 3.384213e-29 [[9008, 1347], [6959, 583]] False True silicates 1.053579e-05 True 1.033157e-30 True BLACK SCHULMAN P7381 105.280707 1.060033e-24 [[2353, 8237], [2118, 5189]] False True master_batch 1.224043e-26 True 6.437826e-09 True","title":"Non-Parametric Conclusions"},{"location":"extras/X5_Candy_Ribbons/#parametric","text":"","title":"Parametric"},{"location":"extras/X5_Candy_Ribbons/#univariate-quantitative-to-quantitative-linear-regression","text":"for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () plt . plot ( x , ( y - y_pred ), ls = '' , marker = '.' ) plt . ylabel ( 'Residual' ) plt . xlabel ( 'x' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( f \"test statistic: { t } \" ) print ( f \"p-value: { stats . t . sf ( x = t , df = n - 2 ) } \" ) R2: 3.34e-02 RSE: 9.97e-03 b: -2.51e-03 m: 3.61e-01 n: 338 x_bar: 1.11e-01 SE_b: 1.51e-03 SE_m: 1.26e-02 test statistic: 28.56535956265302 p-value: 3.2661498371310494e-92 R2: 4.31e-02 RSE: 5.27e-03 b: -3.19e-03 m: 9.08e-02 n: 1551 x_bar: 2.89e-01 SE_b: 3.09e-04 SE_m: 9.66e-04 test statistic: 94.02166426512674 p-value: 0.0 R2: 1.89e-02 RSE: 5.94e-03 b: -1.36e-02 m: 1.57e+01 n: 537 x_bar: 2.64e-03 SE_b: 1.04e-03 SE_m: 3.81e-01 test statistic: 41.11483903422569 p-value: 4.981863349397906e-168 for ind in ind_vars : lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.033 Model: OLS Adj. R-squared: 0.030 Method: Least Squares F-statistic: 11.60 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.000739 Time: 10:02:50 Log-Likelihood: 360.28 No. Observations: 338 AIC: -716.6 Df Residuals: 336 BIC: -708.9 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0025 0.013 -0.199 0.843 -0.027 0.022 x1 0.3612 0.106 3.406 0.001 0.153 0.570 ============================================================================== Omnibus: 457.321 Durbin-Watson: 1.128 Prob(Omnibus): 0.000 Jarque-Bera (JB): 55378.392 Skew: 6.475 Prob(JB): 0.00 Kurtosis: 64.356 Cond. No. 23.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.043 Model: OLS Adj. R-squared: 0.042 Method: Least Squares F-statistic: 69.71 Date: Sat, 02 Apr 2022 Prob (F-statistic): 1.50e-16 Time: 10:02:50 Log-Likelihood: 2180.0 No. Observations: 1551 AIC: -4356. Df Residuals: 1549 BIC: -4345. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0032 0.003 -0.917 0.359 -0.010 0.004 x1 0.0908 0.011 8.349 0.000 0.069 0.112 ============================================================================== Omnibus: 2262.527 Durbin-Watson: 1.176 Prob(Omnibus): 0.000 Jarque-Bera (JB): 660230.502 Skew: 8.562 Prob(JB): 0.00 Kurtosis: 102.615 Cond. No. 7.82 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.019 Model: OLS Adj. R-squared: 0.017 Method: Least Squares F-statistic: 10.29 Date: Sat, 02 Apr 2022 Prob (F-statistic): 0.00142 Time: 10:02:50 Log-Likelihood: 622.03 No. Observations: 537 AIC: -1240. Df Residuals: 535 BIC: -1231. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0136 0.013 -1.020 0.308 -0.040 0.013 x1 15.6795 4.888 3.208 0.001 6.078 25.281 ============================================================================== Omnibus: 739.911 Durbin-Watson: 1.268 Prob(Omnibus): 0.000 Jarque-Bera (JB): 108127.158 Skew: 7.213 Prob(JB): 0.00 Kurtosis: 71.003 Cond. No. 1.49e+03 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.49e+03. This might indicate that there are strong multicollinearity or other numerical problems.","title":"Univariate Quantitative to Quantitative (Linear Regression)"},{"location":"extras/X5_Candy_Ribbons/#feature-engineering","text":"Introducing polynomial features results in poor coefficient estimates ind = ind_vars [ 1 ] lindf = pd . DataFrame () lindf [ ind ] = df [ ind ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf = lindf . dropna () . reset_index ( drop = True ) x = lindf [ ind ] . values . reshape ( - 1 , 1 ) features = PolynomialFeatures ( degree = 3 ) x = features . fit_transform ( x . reshape ( - 1 , 1 )) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.056 Model: OLS Adj. R-squared: 0.055 Method: Least Squares F-statistic: 30.83 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.35e-19 Time: 10:03:01 Log-Likelihood: 2190.9 No. Observations: 1551 AIC: -4374. Df Residuals: 1547 BIC: -4352. Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0030 0.009 0.334 0.739 -0.014 0.020 x1 0.1612 0.124 1.301 0.193 -0.082 0.404 x2 -0.9203 0.523 -1.761 0.078 -1.945 0.105 x3 1.6922 0.664 2.550 0.011 0.390 2.994 ============================================================================== Omnibus: 2252.329 Durbin-Watson: 1.188 Prob(Omnibus): 0.000 Jarque-Bera (JB): 651735.784 Skew: 8.487 Prob(JB): 0.00 Kurtosis: 101.979 Cond. No. 594. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. R2: 5.64e-02 RSE: 6.03e-03 b: 2.95e-03 m: 0.00e+00 n: 1551 x_bar: 3.58e-01 SE_b: 1.69e-04 SE_m: 1.96e-04 0.0 0.5","title":"Feature Engineering"},{"location":"extras/X5_Candy_Ribbons/#multivariate-quantitative-to-quantitative-multivariate-linear-regression","text":"lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) lindf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ADD ARGUS ARGUSTAT AT 31 HK ADD ARGUS ARGUTHERM AO 77 PP ADD POLYPLAST PPM AO 01 UV PP ADD SCHULMAN DTY 20 AOUV BEIGE SCHULMAN PZ 91738 1 5 ZA BLACK SCHULMAN P7381 DEVAN 2SPIN PP 106 GREEN POLYPLAST COLOUR MB PP 6821 GREEN SCHULMAN PZ 302446 1 5 T High MFR Black ... UV POLYPLAST 6005 PP UV SCHULMAN FPPUV 38 MFR 4 Grey PP POLYCHIM HL10XF MFR 4 Grey Grey PP POLYMER TEST MFI4 PP TOTAL 4069 RR MASTERBATCH TEST UV ARGUS ARX V 16 706 UV PP seconds 0 0.0 0.0 0.0 0.005337 0.0 0.000253 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.409353 0.0 0.0 0.0 0.0 0.0 0.000000 9.0 1 0.0 0.0 0.0 0.005867 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.452007 0.0 0.0 0.0 0.0 0.0 0.000000 536.0 2 0.0 0.0 0.0 0.005895 0.0 0.000270 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.449608 0.0 0.0 0.0 0.0 0.0 0.000000 20.0 3 0.0 0.0 0.0 0.005743 0.0 0.000272 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.440883 0.0 0.0 0.0 0.0 0.0 0.000000 1001.0 4 0.0 0.0 0.0 0.004553 0.0 0.000211 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.349459 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17892 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.339726 0.0 0.0 0.0 0.0 0.0 0.002062 0.0 17893 0.0 0.0 0.0 0.000000 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.342659 0.0 0.0 0.0 0.0 0.0 0.002071 0.0 17894 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345886 0.0 0.0 0.0 0.0 0.0 0.002107 0.0 17895 0.0 0.0 0.0 0.000000 0.0 0.000187 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.344945 0.0 0.0 0.0 0.0 0.0 0.002095 799.0 17896 0.0 0.0 0.0 0.000000 0.0 0.000190 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.345288 0.0 0.0 0.0 0.0 0.0 0.002105 429.0 17897 rows \u00d7 45 columns lindf = pd . DataFrame () lindf [ feeds ] = df [ feeds ] lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) x = lindf [ feeds ] . values # features = PolynomialFeatures(degree=3) # x = features.fit_transform(x.reshape(-1,1)) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () print ( est2 . summary ()) model = LinearRegression () model . fit ( x , y ) y_pred = model . predict ( x ) plt . plot ( y_pred , y , ls = '' , marker = '.' ) plt . ylabel ( 'True' ) plt . xlabel ( 'Predicted' ) plt . show () n = y . shape [ 0 ] print ( f 'R2: { r2_score ( y , y_pred ) : .2e } ' ) RSE = np . sqrt ( r2_score ( y , y_pred ) / ( n - 2 )) print ( f \"RSE: { RSE : .2e } \" , end = \" \\n\\n \" ) m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # b_hat print ( f \"b: { b : .2e } \" ) print ( f \"m: { m : .2e } \" , end = \" \\n\\n \" ) print ( f \"n: { n } \" ) x_bar = np . mean ( x ) print ( f \"x_bar: { x_bar : .2e } \" ) SE_b = np . sqrt ( RSE ** 2 * (( 1 / n ) + x_bar ** 2 / np . sum (( x - x_bar ) ** 2 ))) print ( f \"SE_b: { SE_b : .2e } \" ) SE_m = np . sqrt ( RSE ** 2 / np . sum (( x - x_bar ) ** 2 )) print ( f \"SE_m: { SE_m : .2e } \" ) t = m / SE_m print ( t ) print ( stats . t . sf ( x = t , df = n - 2 )) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.050 Model: OLS Adj. R-squared: 0.049 Method: Least Squares F-statistic: 31.47 Date: Sat, 02 Apr 2022 Prob (F-statistic): 2.76e-174 Time: 10:03:19 Log-Likelihood: 44791. No. Observations: 17897 AIC: -8.952e+04 Df Residuals: 17866 BIC: -8.928e+04 Df Model: 30 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0123 0.002 5.316 0.000 0.008 0.017 x1 -1.0553 0.382 -2.760 0.006 -1.805 -0.306 x2 -0.0119 0.010 -1.241 0.215 -0.031 0.007 x3 0.0351 0.100 0.352 0.725 -0.161 0.231 x4 -0.2377 0.069 -3.438 0.001 -0.373 -0.102 x5 0.4001 0.145 2.750 0.006 0.115 0.685 x6 -0.0224 0.128 -0.175 0.861 -0.272 0.228 x7 -0.3174 0.096 -3.305 0.001 -0.506 -0.129 x8 -0.5909 0.209 -2.825 0.005 -1.001 -0.181 x9 -0.4059 0.087 -4.655 0.000 -0.577 -0.235 x10 -0.0241 0.005 -5.206 0.000 -0.033 -0.015 x11 -2.473e-14 1.17e-14 -2.107 0.035 -4.77e-14 -1.73e-15 x12 -0.0050 0.004 -1.371 0.171 -0.012 0.002 x13 -0.0288 0.007 -4.319 0.000 -0.042 -0.016 x14 -0.0268 0.005 -5.644 0.000 -0.036 -0.018 x15 -0.0200 0.006 -3.620 0.000 -0.031 -0.009 x16 -1.023e-14 3.5e-15 -2.923 0.003 -1.71e-14 -3.37e-15 x17 -9.109e-16 2.5e-16 -3.649 0.000 -1.4e-15 -4.22e-16 x18 -0.0994 0.093 -1.073 0.283 -0.281 0.082 x19 -0.0055 0.003 -2.044 0.041 -0.011 -0.000 x20 0.0003 0.005 0.058 0.954 -0.009 0.010 x21 1.325e-15 1.2e-16 11.078 0.000 1.09e-15 1.56e-15 x22 -2.494e-16 8.68e-17 -2.873 0.004 -4.2e-16 -7.93e-17 x23 0.0145 0.004 3.827 0.000 0.007 0.022 x24 -0.0100 0.010 -1.051 0.293 -0.029 0.009 x25 0.0216 0.004 5.724 0.000 0.014 0.029 x26 -0.0101 0.002 -4.198 0.000 -0.015 -0.005 x27 -1.03e-16 8.73e-17 -1.180 0.238 -2.74e-16 6.8e-17 x28 9.329e-16 7.53e-17 12.396 0.000 7.85e-16 1.08e-15 x29 -0.0074 0.003 -2.881 0.004 -0.012 -0.002 x30 1.7091 0.132 12.994 0.000 1.451 1.967 x31 4.1538 0.376 11.053 0.000 3.417 4.890 x32 0 0 nan nan 0 0 x33 0.0031 0.012 0.263 0.792 -0.020 0.026 x34 0.0031 0.012 0.263 0.792 -0.020 0.026 x35 0.0031 0.012 0.263 0.792 -0.020 0.026 x36 0.0790 0.042 1.868 0.062 -0.004 0.162 x37 0.0525 0.030 1.774 0.076 -0.006 0.111 x38 -0.0064 0.003 -2.521 0.012 -0.011 -0.001 x39 0 0 nan nan 0 0 x40 -0.0136 0.005 -2.762 0.006 -0.023 -0.004 x41 0 0 nan nan 0 0 x42 0 0 nan nan 0 0 x43 0 0 nan nan 0 0 x44 0.1990 0.048 4.119 0.000 0.104 0.294 ============================================================================== Omnibus: 43479.020 Durbin-Watson: 1.074 Prob(Omnibus): 0.000 Jarque-Bera (JB): 693938974.691 Skew: 25.434 Prob(JB): 0.00 Kurtosis: 966.322 Cond. No. 1.11e+16 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 1.79e-28. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. R2: 5.02e-02 RSE: 1.67e-03 b: 1.23e-02 m: -1.06e+00 n: 17897 x_bar: 2.26e-02 SE_b: 1.25e-05 SE_m: 2.14e-05 -49284.75476441197 1.0","title":"Multivariate Quantitative to Quantitative (Multivariate Linear Regression)"},{"location":"extras/X5_Candy_Ribbons/#forward-selection","text":"def add_feature ( features , basemodel , data , y , r2max ): for feed in features : basemodel [ feed ] = data [ feed ] basemodel . fillna ( 0 , inplace = True ) x = basemodel [[ col for col in basemodel . columns if col != 'seconds' ]] . values ### FIT AND HYP TEST X2 = sm . add_constant ( x ) est = sm . OLS ( y , X2 ) est2 = est . fit () if ( est2 . rsquared > r2max ) and not ( est2 . pvalues > cutoff ) . any (): r2max = est2 . rsquared feedmax = feed bestsum = est2 . summary () newmodel = basemodel . copy () else : pass basemodel . drop ( labels = feed , axis = 1 , inplace = True ) return r2max , feedmax , bestsum , newmodel lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values scaler = MinMaxScaler () y = scaler . fit_transform ( y . reshape ( - 1 , 1 )) r2max = 0 candidates = feeds . copy () basemodel = lindf while True : newr2max , feedmax , bestsum , newmodel = add_feature ( features = candidates , basemodel = basemodel , data = df , y = y , r2max = 0 ) if newr2max > r2max : # print(newr2max, feedmax) candidates . remove ( feedmax ) r2max = newr2max basemodel = newmodel continue else : break /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1918: RuntimeWarning: divide by zero encountered in double_scalars return np.sqrt(eigvals[0]/eigvals[-1]) we see some of the usual suspects from the chi-square tests. Notably some are missing: High MFR Grey , BLACK SCHULMAN P7381 , PP INEOS 100 GA09 and others. basemodel . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } seconds PP TOTAL 7089 PP POLYCHIM HB12XF SILIKAT PBH ABPP 05 SILIKAT POLYPLUS AB 1001 PP MFR 4 Green ADD SCHULMAN DTY 20 AOUV GREEN SCHULMAN PZ 302446 1 5 T MFR 4 Grey UV ARGUS ARX V 16 706 UV PP GREEN SCHULMAN PZ 34198 1 5 T 0 9.0 0.0 0.0 0.0 0.0 0.0 0.005337 0.0 0.409353 0.0 0.0 1 536.0 0.0 0.0 0.0 0.0 0.0 0.005867 0.0 0.452007 0.0 0.0 2 20.0 0.0 0.0 0.0 0.0 0.0 0.005895 0.0 0.449608 0.0 0.0 3 1001.0 0.0 0.0 0.0 0.0 0.0 0.005743 0.0 0.440883 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.004553 0.0 0.349459 0.0 0.0 forward_selection_feeds = basemodel . columns [ 1 :] bestsum OLS Regression Results Dep. Variable: y R-squared: 0.046 Model: OLS Adj. R-squared: 0.045 Method: Least Squares F-statistic: 94.99 Date: Sat, 02 Apr 2022 Prob (F-statistic): 5.51e-174 Time: 10:04:41 Log-Likelihood: 44748. No. Observations: 17897 AIC: -8.948e+04 Df Residuals: 17887 BIC: -8.940e+04 Df Model: 9 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 0.0034 0.000 11.288 0.000 0.003 0.004 x1 0.0214 0.002 11.535 0.000 0.018 0.025 x2 0.0146 0.002 7.837 0.000 0.011 0.018 x3 1.5742 0.110 14.375 0.000 1.360 1.789 x4 4.1700 0.363 11.497 0.000 3.459 4.881 x5 -0.0181 0.003 -6.276 0.000 -0.024 -0.012 x6 -0.2879 0.065 -4.450 0.000 -0.415 -0.161 x7 -0.1932 0.052 -3.698 0.000 -0.296 -0.091 x8 0.0037 0.001 3.322 0.001 0.002 0.006 x9 0.0736 0.028 2.641 0.008 0.019 0.128 x10 0 0 nan nan 0 0 x11 0 0 nan nan 0 0 Omnibus: 43415.763 Durbin-Watson: 1.068 Prob(Omnibus): 0.000 Jarque-Bera (JB): 683838684.139 Skew: 25.346 Prob(JB): 0.00 Kurtosis: 959.275 Cond. No. inf Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 0. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.","title":"Forward Selection"},{"location":"extras/X5_Candy_Ribbons/#multivariate-conclusions","text":"y poses a problem with out of the box regression. Extreme tails will upset the residual plots. We some overlap of the parametric and non-parametric approaches: [ i for i in basemodel . columns if i in ( problem_vars )] ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05']","title":"Multivariate Conclusions"},{"location":"extras/X5_Candy_Ribbons/#multivariate-quantitative-to-categorical-binned-output-variable","text":"Logistic regression does little better than random guessing lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) x = lindf [ ind ] . values #.reshape(-1,1) model = LogisticRegression () model . fit ( x , y ) y_pred = model . predict ( x ) print ( f \"Train Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y , model . predict ( x )) : .2f } \" ) Train Acc: 0.37 Test Acc: 0.37 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( print ( classification_report ( y , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.35 0.80 0.49 4527 2.0 0.36 0.32 0.34 4434 3.0 0.30 0.03 0.06 4463 4.0 0.43 0.32 0.37 4473 accuracy 0.37 17897 macro avg 0.36 0.37 0.31 17897 weighted avg 0.36 0.37 0.31 17897 <AxesSubplot:> If we are careful about regularizing the random forest model, it does a little bit better lindf = pd . DataFrame () lindf [ 'seconds' ] = df [ 'Total Seconds Out' ] lindf . fillna ( 0 , inplace = True ) y = lindf [ 'seconds' ] . values r2max = 0 candidates = feeds . copy () basemodel = lindf labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 names = [] names += [ f \"less than { np . quantile ( y , .25 ) : .2f } sec\" ] names += [ f \" { np . quantile ( y , .25 ) : .2f } < sec <= { np . quantile ( y , .5 ) : .2f } \" ] names += [ f \" { np . quantile ( y , .5 ) / 60 : .2f } < min <= { np . quantile ( y , .75 ) / 60 : .2f } \" ] names += [ f \"greater than { np . quantile ( y , .75 ) / 60 : .2f } min\" ] y = labels ind = feeds lindf [ ind ] = df [ ind ] lindf . fillna ( 0 , inplace = True ) X = lindf [ ind ] . values #.reshape(-1,1) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 0.99 Test Acc: 0.39 y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax , xticklabels = names , yticklabels = names ) precision recall f1-score support 1.0 0.41 0.48 0.44 898 2.0 0.33 0.30 0.31 881 3.0 0.29 0.27 0.28 893 4.0 0.50 0.51 0.51 908 accuracy 0.39 3580 macro avg 0.38 0.39 0.38 3580 weighted avg 0.38 0.39 0.38 3580 <AxesSubplot:> # grab feature importances imp = model . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in model . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ ind , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 PP TOTAL PPH 4065 0.150387 0.008053 1 BLACK SCHULMAN P7381 0.118158 0.008725 2 PP INEOS 100 GA04 0.100635 0.005217 3 UV ARGUS ARX V 16 706 UV PP 0.090776 0.006273 4 MFR 4 Grey 0.08828 0.005656 5 PP UNIPETROL GB005 0.060558 0.003569 6 MFR 4 Black 0.052589 0.003695 7 ADD SCHULMAN DTY 20 AOUV 0.041478 0.003554 8 DEVAN 2SPIN PP 106 0.038438 0.004061 9 High MFR Black 0.028838 0.002515 10 MFR 4 Grey Grey 0.025758 0.002319 11 ADD ARGUS ARGUTHERM AO 77 PP 0.024526 0.002253 12 High MFR Grey 0.020652 0.004188 13 MFR 4 Green 0.016508 0.002725 14 SILIKAT PBH ABPP 05 0.016283 0.002291 15 PP TOTAL 7089 0.01624 0.004897 16 ADD POLYPLAST PPM AO 01 UV PP 0.013499 0.001685 17 MFR 4 Black Brown 0.013401 0.002131 18 PP POLYCHIM HB12XF 0.01245 0.002185 19 PP INEOS 100 GA09 0.010354 0.00156","title":"Multivariate Quantitative to Categorical (Binned Output Variable)"},{"location":"extras/X5_Candy_Ribbons/#60-business-impact","text":"What is the so what? From Hyp tests - target is either 'Total Position Out' or 'Total Seconds Out' We had culprits from pearsons: ind_vars ['MFR 4 White', 'PP TOTAL 7089', 'SILIKAT POLYPLUS AB 1001 PP'] culprits from non parametric tests: list ( problem_vars ) ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'High MFR Grey', 'SILIKAT PBH ABPP 05', 'BLACK SCHULMAN P7381'] and culprits from linear regression forward_selection_feeds Index(['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'MFR 4 Green', 'ADD SCHULMAN DTY 20 AOUV', 'GREEN SCHULMAN PZ 302446 1 5 T', 'MFR 4 Grey', 'UV ARGUS ARX V 16 706 UV PP', 'GREEN SCHULMAN PZ 34198 1 5 T'], dtype='object') and we can check the overlap between non-parametric and linear regression (parametric): predict_vars = [ i for i in basemodel . columns if i in ( problem_vars )] predict_vars ['PP TOTAL 7089', 'PP POLYCHIM HB12XF', 'SILIKAT PBH ABPP 05'] all_vars = set ( list ( problem_vars ) + list ( forward_selection_feeds )) all_vars {'ADD SCHULMAN DTY 20 AOUV', 'BLACK SCHULMAN P7381', 'GREEN SCHULMAN PZ 302446 1 5 T', 'GREEN SCHULMAN PZ 34198 1 5 T', 'High MFR Grey', 'MFR 4 Green', 'MFR 4 Grey', 'PP POLYCHIM HB12XF', 'PP TOTAL 7089', 'SILIKAT PBH ABPP 05', 'SILIKAT POLYPLUS AB 1001 PP', 'UV ARGUS ARX V 16 706 UV PP'} # add to DF business = pd . DataFrame () value_line_by_day = 99000 for ind in problem_vars : try : # products, prods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . unique () nprods = df . loc [ df [ ind ] > 0 ][ 'Product' ] . nunique () dff = df . loc [ df [ 'Product' ] . isin ( prods )] # total runtime, tot_runtime = dff [ 'Time Delta' ] . sum () # total runtime with feed, tot_runtime_wfeed = dff [ dff [ ind ] > 0 ][ 'Time Delta' ] . sum () #downtime w/o feed, downtime w/ feed, avg_downtime_min = dff . groupby ( dff [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 avg_downtime_min_wfeed = avg_downtime_min [ True ] avg_downtime_min_w_ofeed = avg_downtime_min [ False ] ratio = avg_downtime_min_wfeed / avg_downtime_min_w_ofeed # average runtime, avg_runtime_min = dff [ 'Time Delta' ] . mean () . total_seconds () / 60 # days saved, dollars saved days_saved = ( tot_runtime_wfeed . total_seconds () / 60 / avg_runtime_min * avg_downtime_min . diff () . values [ - 1 ]) / 60 / 24 dollars_saved_per_batch = value_line_by_day * avg_downtime_min . diff () . values [ - 1 ] / ( 60 * 24 ) business = pd . concat (( business , pd . DataFrame ([[ ind , nprods , tot_runtime , tot_runtime_wfeed , avg_downtime_min_wfeed , avg_downtime_min_w_ofeed , ratio , avg_runtime_min , days_saved , days_saved * value_line_by_day , dollars_saved_per_batch ]], columns = [ 'Feed' , 'Products' , 'Total Runtime, Products' , 'Total Runtime With Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Average Runtime' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]))) except : print ( ind ) print ( dff . groupby ( df [ ind ] > 0 )[ seconds [: 4 ]] . mean () . mean ( 1 ) / 60 ) business = business . sort_values ( 'Dollars Saved (Per Batch)' , ascending = False ) . reset_index ( drop = True ) business # workshop downtime ratio # feed products, feed products # Dollars saved per batch .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Products Total Runtime, Products Total Runtime With Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Average Runtime Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26 409 days 20:43:21 94 days 04:36:05 26.250639 13.671063 1.920161 70.835736 16.727319 1.656005e+06 864.845823 1 High MFR Grey 36 564 days 04:13:41 94 days 12:26:59 23.623539 12.809036 1.844287 65.459164 15.615433 1.545928e+06 743.497080 2 PP POLYCHIM HB12XF 17 383 days 00:03:51 67 days 09:13:53 21.043525 15.600015 1.348943 70.908183 5.173013 5.121283e+05 374.241303 3 BLACK SCHULMAN P7381 64 751 days 20:01:57 590 days 12:21:50 14.902871 12.363943 1.205349 63.216276 23.716605 2.347944e+06 174.551301 4 PP TOTAL 7089 13 291 days 22:06:22 79 days 15:56:47 20.651050 18.269305 1.130369 80.146114 2.367430 2.343756e+05 163.744933 pd . set_option ( 'display.precision' , 2 ) display ( business [[ 'Feed' , 'Average Downtime With Feed' , 'Average Downtime Without Feed' , 'Downtime Mult X (with feed)' , 'Days Saved (2 YRS)' , 'Dollars Saved (2 YRS)' , 'Dollars Saved (Per Batch)' ]], ) pd . set_option ( 'display.precision' , 6 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feed Average Downtime With Feed Average Downtime Without Feed Downtime Mult X (with feed) Days Saved (2 YRS) Dollars Saved (2 YRS) Dollars Saved (Per Batch) 0 SILIKAT PBH ABPP 05 26.25 13.67 1.92 16.73 1.66e+06 864.85 1 High MFR Grey 23.62 12.81 1.84 15.62 1.55e+06 743.50 2 PP POLYCHIM HB12XF 21.04 15.60 1.35 5.17 5.12e+05 374.24 3 BLACK SCHULMAN P7381 14.90 12.36 1.21 23.72 2.35e+06 174.55 4 PP TOTAL 7089 20.65 18.27 1.13 2.37 2.34e+05 163.74 We see that 'SILKAT PBH ABPP 05' as well as 'High MFR Grey' have a large amount of days saved over the two years, a strong downtime multiplier effect and the largest dollars saved per batch. Since High MFR Grey is a recycle feed, we may not have the ability to remove it from the feed list. Leaving the SILKAT (Silicate) feed the prime feedstock to remove","title":"6.0 Business Impact"},{"location":"extras/X5_Candy_Ribbons/#70-visualizations","text":"","title":"7.0 Visualizations"},{"location":"extras/X5_Candy_Ribbons/#pp-total-7089","text":"predict_var = predict_vars [ 1 ] outcome_var = [ i for i in df . columns if 'Seconds' in i ] def myplot ( col = list ( df . loc [ df [ predict_var ] > 0 ][ 'Product' ] . unique ())): fig , ax = plt . subplots ( figsize = ( 20 , 10 )) axt = ax . twinx () # other feeds df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True ) . dropna ( axis = 1 , how = 'all' ) . plot ( ls = '' , marker = '*' , ax = ax , alpha = 0.2 ) # predict_var feed df . loc [ df [ 'Product' ] == col ][ feeds ] . reset_index ( drop = True )[ predict_var ] . plot ( ls = '--' , marker = '+' , ax = ax , lw = 2 , c = 'red' ) # outcome_var df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ) . plot ( ls = '-' , marker = '+' , lw = 1 , ax = axt , alpha = 1 , c = 'blue' , label = 'Total Seconds Out' ) # outliers ax . set_ylim ( 0 ) axt . set_ylim ( 0 , np . quantile ( df . loc [ df [ 'Product' ] == col ][ outcome_var ] . reset_index ( drop = True ) . sum ( 1 ), 0.997 )) h1 , l1 = ax . get_legend_handles_labels () h2 , l2 = axt . get_legend_handles_labels () ax . legend ( h1 + h2 , l1 + l2 , loc = 0 ) interact ( myplot ) interactive(children=(Dropdown(description='col', options=(0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.\u2026 <function __main__.myplot(col=[0.0, 125617.0, 55626.0, 110617.0, 260607.0, 165215.0, 136215.0, 125215.0, 110215.0, 100215.0, 80215.0, 100617.0, 90617.0, 90215.0, 90216.0, 110520.0, 80617.0])>","title":"PP TOTAL 7089"},{"location":"extras/X6_Data_Science_Overview/","text":"Technology Innovation 510 \u00b6 Introduction to Data Science Methods: Data Science and Visualization \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today, we'll be continuing our discussion of AI/DS/ML with our part II: Data Science (and a focus in Visualization)\ud83c\udf89 Preparing Notebook for Demos \u00b6 Importing Packages \u00b6 Once we have our packages installed, we need to import them. We can also import packages that are pre-installed in the Colab environment. import numpy as np import random import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression import seaborn as sns pd . options . display . max_rows = 999 pd . set_option ( 'display.max_colwidth' , None ) sns . set () Importing Data \u00b6 We also have the ability to import data, and use it elsewhere in the notebook \ud83d\udcdd! df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_explorers/main/assets/imdb_movies.csv\" ) # converting years to numbers for easy conditionals df [ 'year' ] = pd . to_numeric ( df [ 'year' ], errors = 'coerce' ) /usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) \ud83d\udcca What is Data Science? \u00b6 The Emergence of Data Science \u00b6 Data Science is a broad field, and depending on who you talk to, it can mean different things. In summary, many independent scientific fields began accumulating large amounts of data. At the UW in particular, these were dominated by the astronomy and oceanography departments. Folks began to realize that they needed a particular set of tools to handle large amounts of data. This culminated in the eScience studio , which began to service the data needs of many departments on campus. Today, data science not only has to do with large amounts of data, but refers generally to tools that allow us to work with a variety of data types. Because of this, machine learning is a tool within data science. But there are other tools apart from machine learning that make up the data science ecosystem. Some of them are: data visualization databases statistics You could argue for others as well (algorithms, web servers, programming, etc.), but these are the formally accepted areas. We can borrow from Drew Conway's Data Science Venn Diagram, first published on his blog in September 2010, to make further sense of this: #### \ud83d\udcad 1 Let's see if we can converge on a definition of Data Science. Talk to your neighbor, convene together, then let's share. Do this at 2 different levels: How would you explain Data Science to: 1. your grandmother 2. a young professional You may find these articles useful: * from Oracle * [A business pov on DS](https://www.oracle.com/data-science/what-is-data-science/) * [An implementation pov on DS](https://www.oracle.com/a/ocom/docs/data-science-lifecycle-ebook.pdf) * a more technical *cough* better *cough* discussion on DS from Software Carpentry: * [A research pov on DS](https://software-carpentry.org/blog/2017/12/assessing-data-science.html) * [The minues from that meeting](https://github.com/carpentries/assessment/blob/main/assessment-network/minutes/2017-11-15.md) #### \ud83d\udcac 1 I'll write these down, let's see if we can all agree on a precise definition ### Saying Stuff About Data (Statistics) When we're talking about statistics, we're really talking about data story telling. Statistics is at the C O R E of data science, really. Without a basic knowledge of statistics it'll be hard for you to construct your data narratives and have them hold water. Let's start with some simple examples of data story telling, and use these to generate our own thoughts on the matter. #### Anscombe's Quartet There's a very famous anomaly in DS caled Anscombe's quartet. Observe the following data We can construct this in python and confirm the summary statistics ourselves df = pd . read_excel ( \"https://github.com/wesleybeckner/technology_explorers/blob\" \\ \"/main/assets/data_science/anscombes.xlsx?raw=true\" , header = [ 0 , 1 ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } I II III IV X Y X Y X Y X Y 0 10 8.04 10 9.14 10 7.46 8 6.58 1 8 6.95 8 8.14 8 6.77 8 5.76 2 13 7.58 13 8.74 13 12.74 8 7.71 3 9 8.81 9 8.77 9 7.11 8 8.84 4 11 8.33 11 9.26 11 7.81 8 8.47 5 14 9.96 14 8.10 14 8.84 8 7.04 6 6 7.24 6 6.13 6 6.08 8 5.25 7 4 4.26 4 3.10 4 5.39 19 12.50 8 12 10.84 12 9.13 12 8.15 8 5.56 9 7 4.82 7 7.26 7 6.42 8 7.91 10 5 5.68 5 4.74 5 5.73 8 6.89 We can calculate the mean/variance of X and Y for samples I, II, III, and IV df . mean () I X 9.000000 Y 7.500909 II X 9.000000 Y 7.500909 III X 9.000000 Y 7.500000 IV X 9.000000 Y 7.500909 dtype: float64 # do we remember the relationship between standard deviation and variance? df . std () ** 2 I X 11.000000 Y 4.127269 II X 11.000000 Y 4.127629 III X 11.000000 Y 4.122620 IV X 11.000000 Y 4.123249 dtype: float64 For the line of best fit, recall the equation for a linear relationship between x and y: $$y(x)= m\\cdot x + b$$ model = LinearRegression () sets = [ 'I' , 'II' , 'III' , 'IV' ] for data in sets : model . fit ( df [ data ][ 'X' ] . values . reshape ( 11 , 1 ), df [ data ][ 'Y' ]) print ( \"Linear Regression Line: Y = {:.2f} X + {:.2f} \" . format ( model . coef_ [ 0 ], model . intercept_ )) Linear Regression Line: Y = 0.50X + 3.00 Linear Regression Line: Y = 0.50X + 3.00 Linear Regression Line: Y = 0.50X + 3.00 Linear Regression Line: Y = 0.50X + 3.00 $R^2$ measures the goodness of fit. $R^2$ is generally defined as the ratio of the total sum of squares $SS_{\\sf tot} $ to the residual sum of squares $SS_{\\sf res} $: The Residual Sum of Squares is defined as: $$SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2$$ We now define the total sum of squares, a measure of the total variance in the data: $$SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2$$ The $R^2$ tells us how much of the variance of the data, is captured by the model we created: $$R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}}$$ In the first equation, $\\bar{y}=\\sum_i y^{\\sf exact}_i/N$ is the average value of y for $N$ points. The best value of $R^2$ is 1 but it can also take a negative value if the error is large. for data in sets : # calc the ssr ssr = np . sum (( df [ data ][ 'Y' ] - model . predict ( df [ data ][ 'X' ] . values . reshape ( - 1 , 1 ))) ** 2 ) # calc the sst sst = np . sum (( df [ data ][ 'Y' ] - df [ data ][ 'Y' ] . mean ()) ** 2 ) # calc the r2 r2 = 1 - ( ssr / sst ) print ( \"R2 = {:.2f} \" . format ( r2 )) R2 = 0.67 R2 = 0.67 R2 = 0.67 R2 = 0.67 As we can see, everything checks out. The summary statistics are all the same! Can we answer the following: > What dataset is best described by the line of best fit? We will revisit this question when we talk about data visualization #### Taxonomy of Data Types Another important topic in data science, is simply what kind of data we are working with. This will help us decide what kind of models to build, as well as how to visualize our data, and perhaps store it as well. #### \ud83d\udcac 2 What are some examples of the different datatypes we can think of? ### Data Visualization Data visualization, like it sounds, has to do with how we display and communicate information. At the end of the day, your findings and algorithms aren't worth very much if we can't share them with others. One of the leading thinkers in the visual display of information is Edward Tufte * [Tufte's Principles](https://thedoublethink.com/tuftes-principles-for-visualizing-quantitative-information/) * [Data-ink](https://infovis-wiki.net/wiki/Data-Ink_Ratio) * [Wiki](https://en.wikipedia.org/wiki/Edward_Tufte) #### Guiding Principles of Data Visualization Another topic Tufte discusses is Data Density. Apart from ink, we should also consider the entire area dedicated to the graphic. That area should be as condensed as possible without removing ledgibility/readability. wattenberg and Viegas visualization %% HTML < video width = \"640\" height = \"580\" controls > < source src = \"https://github.com/wesleybeckner/technology_explorers/blob/main/assets/data_science/ds4.mp4?raw=true\" type = \"video/mp4\" > </ video > [Color blindness](https://www.colourblindawareness.org/colour-blindness/) is prevalent, and something we should consider when choosing colors in our visual displays #### Visualization Un-Examples **Unexample 1** **Unexample 2** **Unexample 3** **Unexample 4** #### \ud83d\udcad 3 Find an example of an interactive data visualization online. Here's one I [found](https://www.migrationpolicy.org/programs/data-hub/charts/us-immigrant-population-state-and-county) that I though was quite interesting! #### \ud83d\udcac 3 Swap visualization links with your neighbor. What do you think could be improved about each one? #### Back to Anscombe's Quartet ### Revisiting our A.I. movie data Let's revisit our imdb dataset from last week. df . loc [( df [ 'description' ] . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False ))][[ 'original_title' , 'year' , 'description' ]][: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } original_title year description 1683 Son of a Sailor 1933.0 A smooth-talking sailor looking for a quick date meets the granddaughter of an admiral and finds himself in a house full of top Navy officers, along with a couple of spies interested in plans for a new robot-controlled flight system. 3339 The Middleton Family at the New York World's Fair 1939.0 The Middleton family visits the 1939 New York World's Fair and witnesses the advent of the future, encountering robots and dishwashers for the first time. 3846 Emergency Landing 1941.0 A test pilot and his weather observer develop a \"robot\" control so airplanes can be flown without pilots, but enemy agents get wind of it and try to steal it or destroy it. 4122 Cairo 1942.0 Reporter Homer Smith accidentally draws Marcia Warren into his mission to stop Nazis from bombing Allied Convoys with robot-planes. 6274 The Perfect Woman 1949.0 In need of cash, Roger Cavendish and his valet take a job escorting the perfect woman for a night on the town. She is in fact the robotic creation of Professor Belman, but it turns out ... 7936 Devil Girl from Mars 1954.0 An uptight, leather-clad female alien, armed with a ray gun and accompanied by a menacing robot, comes to Earth to collect Earth's men as breeding stock. 7977 Gog 1954.0 A security agent investigates sabotage and murder at a secret underground laboratory, home of two experimental robots. 8155 Target Earth 1954.0 Giant robots from Venus invade Chicago. Stranded in the deserted city are strangers Frank and Nora. 8173 Tobor the Great 1954.0 A young boy-genius befriends his grandfather's robot, designed as a test pilot for space travel and coveted by foreign spies. 9047 Chiky\u00fb B\u00f4eigun 1957.0 Shortly after a giant robot destroys a village, aliens from outer space arrive offering friendship. Will the people of Earth tie these two events together? Let's create a visualization that helps answer the question: > What is the popularity of AI movies by year?\" #### \ud83d\udcac 4 What are some potential issues with the following plot? ai_movies = df . loc [( df [ 'description' ] . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False ))] fig , ax = plt . subplots () ai_movies . groupby ( 'year' ) . count ()[ 'title' ] . plot ( ls = '--' , marker = '.' , ax = ax ) ax . set_ylabel ( 'number of movies' ) Text(0, 0.5, 'number of movies') ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_43_1.png) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ai_movies . groupby ( 'year' ) . count ()[ 'imdb_title_id' ] . plot ( marker = '.' , ax = ax ) ax . set_ylabel ( \"number of movies\" ) Text(0, 0.5, 'number of movies') ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_44_1.png) It looks like the popularity is increasing, doesn't it? But is that fair to say if we do not consider the total number of movies in the database each year? Let's say we're curious about the year 1986 and what countries contributed to the spike in movies. How would we *filter* for year 1986, then *select* country and perform a count *operation*? Remember the sequence **filter \u27a1\ufe0f Select \u27a1\ufe0f operate** ai_movies . loc [ ai_movies [ 'year' ] == 1986 ][[ 'country' ]] . nunique () country 6 dtype: int64 ai_movies . loc [ ai_movies [ 'year' ] == 1986 ][[ 'country' ]] . value_counts () country USA 3 Japan 2 USA, Spain 1 USA, Japan 1 Poland, Soviet Union 1 Canada 1 dtype: int64 Looks like a good year for US-Japan movie making. How can we look into the role of different countries in AI movie making? > in the following, pandas doesn't have the ability to color according to a categorical column, out of the box. So I switch over to seaborn. For more exercises with seaborn visit [JVDP's chapter on the subject](https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html) by_country = ai_movies . groupby ([ 'country' ]) . filter ( lambda x : ( x . count () >= 5 ) . any ()) . \\ groupby ([ 'country' , 'year' ]) . apply ( lambda x : x . count ())[[ 'title' ]] . reset_index () # ai_movies.groupby(['country']).filter(lambda x: (x.count() >= 5).any()) ai_movies . groupby ([ 'country' ]) . filter ( lambda x : ( x . count () >= 5 ) . any ()) . \\ groupby ([ 'country' , 'year' ]) . apply ( lambda x : x . count ())[[ 'title' ]] . reset_index () . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year title 0 Canada 1979.0 1 1 Canada 1986.0 1 2 Canada 1994.0 1 3 Canada 2010.0 1 4 Canada 2014.0 1 by_country = ai_movies . groupby ([ 'country' ]) . filter ( lambda x : ( x . count () >= 5 ) . any ()) . \\ groupby ([ 'country' , 'year' ]) . apply ( lambda x : x . count ())[[ 'title' ]] . reset_index () fig , ax = plt . subplots ( figsize = ( 12 , 5 )) with sns . axes_style ( 'white' ): g = sns . barplot ( data = by_country , x = \"year\" , hue = 'country' , y = 'title' , ax = ax , order = range ( 1999 , 2019 )) ax . set_ylabel ( 'Number of Films' ) Text(0, 0.5, 'Number of Films') ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_54_1.png) For the last 20 years we can see that U.S./Japan have dominated the A.I. movie market # In-Class Exercises Create visualizations that help a user answer the following questions: ## \ud83d\udc0d Exercise 1: Visualizing Fractions 1. Of all the movie data, which top 10 countries have the highest fraction of AI movies? 2. Are there more movies about AI before or after 1986? df . groupby ( 'country' )[ 'description' ] . apply ( lambda x : x . loc [ x . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False )] . count () / x . shape [ 0 ]) . sort_values ()[ - 20 :] country South Africa, USA 0.050000 USA, Argentina 0.058824 USA, Germany, UK 0.090909 Switzerland, USA 0.142857 Germany, Belgium 0.142857 Singapore, USA 0.250000 Poland, Soviet Union 0.285714 UK, USA, Ireland 0.333333 UK, China, USA 0.333333 USA, UK, Italy 0.333333 India, Australia 0.333333 China, Canada, USA 0.333333 Canada, South Korea 0.500000 Hong Kong, Canada, USA 1.000000 Italy, West Germany, Spain, Monaco 1.000000 Canada, France, Spain, Germany 1.000000 Croatia, Luxembourg, Norway, Czech Republic, Slovakia, Slovenia, Bosnia and Herzegovina 1.000000 USA, Singapore 1.000000 Canada, USA, UK, Japan 1.000000 Canada, France, Morocco 1.000000 Name: description, dtype: float64 df . loc [ ~ df [ 'country' ] . str . contains ( ',' , na = True )] . groupby ( 'country' )[ 'description' ] . apply ( lambda x : x . loc [ x . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False )] . count () / x . shape [ 0 ]) . sort_values ()[ - 20 :] country Russia 0.001247 Australia 0.001274 UK 0.001459 Turkey 0.001480 Mexico 0.001527 Denmark 0.001776 South Korea 0.001813 China 0.001908 Finland 0.002232 West Germany 0.002375 Hong Kong 0.002421 Brazil 0.002717 USA 0.002876 Thailand 0.003817 Canada 0.003885 Soviet Union 0.003916 Israel 0.006250 Chile 0.009174 Belgium 0.009804 Japan 0.010075 Name: description, dtype: float64 df . loc [ ~ ( df [ 'country' ] . str . contains ( ',' , na = True ))][ 'country' ] 0 USA 1 Australia 3 USA 4 Italy 5 USA ... 85849 India 85851 Netherlands 85852 India 85853 Turkey 85854 Spain Name: country, Length: 70300, dtype: object df . loc [ ~ ( df [ 'country' ] . str . contains ( ',' , na = True ))] . groupby ( 'country' )[ 'description' ] . apply ( lambda x : x . loc [ x . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False )] . count () / x . shape [ 0 ]) . sort_values ( ascending = False )[: 10 ] country Japan 0.010075 Belgium 0.009804 Chile 0.009174 Israel 0.006250 Soviet Union 0.003916 Canada 0.003885 Thailand 0.003817 USA 0.002876 Brazil 0.002717 Hong Kong 0.002421 Name: description, dtype: float64 # Cell for 1.1 fig , ax = plt . subplots () df . loc [ ~ ( df [ 'country' ] . str . contains ( ',' , na = True ))] . groupby ( 'country' )[ 'description' ] . apply ( lambda x : x . loc [ x . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False )] . count () / x . shape [ 0 ]) . sort_values ( ascending = True )[ - 10 :] . plot ( kind = 'barh' ) ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_62_1.png) df [ 'after 1986' ] = df [ 'year' ] > 1986 df [ 'ai movie' ] = df [ 'description' ] . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False ) np . sum ([ True , True , False ]) 2 df . groupby ([ 'after 1986' , 'ai movie' ])[ 'title' ] . count () after 1986 ai movie False False 26438 True 54 True False 59214 True 149 Name: title, dtype: int64 fig , ax = plt . subplots () df . groupby ([ 'after 1986' ])[ 'ai movie' ] . apply ( lambda x : x . sum () / x . shape [ 0 ]) . plot ( kind = 'barh' ) ax . set_yticklabels ([ 'Before/During 1986' , 'After 1986' ]) ax . set_ylabel ( '' ) ax . set_xlabel ( 'Fraction of Movies with AI Theme' ) Text(0.5, 0, 'Fraction of Movies with AI Theme') ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_66_1.png) fig , ax = plt . subplots () df . groupby ([ 'after 1986' ])[ 'ai movie' ] . apply ( lambda x : x . sum () / x . shape [ 0 ]) . plot ( kind = 'barh' ) ax . set_yticklabels ([ 'Before/During 1986' , 'After 1986' ]) ax . set_xlabel ( 'Fraction of Movies with AI Theme' ) Text(0.5, 0, 'Fraction of Movies with AI Theme') ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_67_1.png) df . loc [ df [ 'year' ] > 1986 & df [ 'description' ] . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } imdb_title_id title original_title year date_published genre duration country language director writer production_company actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics after 1986 ai movie 0 tt0000009 Miss Jerry Miss Jerry 1894.0 1894-10-09 Romance 45 USA None Alexander Black Alexander Black Alexander Black Photoplays Blanche Bayliss, William Courtenay, Chauncey Depew The adventures of a female reporter in the 1890s. 5.9 154 NaN NaN NaN NaN 1.0 2.0 False False 1 tt0000574 The Story of the Kelly Gang The Story of the Kelly Gang 1906.0 1906-12-26 Biography, Crime, Drama 70 Australia None Charles Tait Charles Tait J. and N. Tait Elizabeth Tait, John Tait, Norman Campbell, Bella Cola, Will Coyne, Sam Crewes, Jack Ennis, John Forde, Vera Linden, Mr. Marshall, Mr. McKenzie, Frank Mills, Ollie Wilson True story of notorious Australian outlaw Ned Kelly (1855-80). 6.1 589 $ 2250 NaN NaN NaN 7.0 7.0 False False 2 tt0001892 Den sorte dr\u00f8m Den sorte dr\u00f8m 1911.0 1911-08-19 Drama 53 Germany, Denmark NaN Urban Gad Urban Gad, Gebhard Sch\u00e4tzler-Perasini Fotorama Asta Nielsen, Valdemar Psilander, Gunnar Helsengreen, Emil Albes, Hugo Flink, Mary Hagen Two men of high rank are both wooing the beautiful and famous equestrian acrobat Stella. While Stella ignores the jeweler Hirsch, she accepts Count von Waldberg's offer to follow her home, ... 5.8 188 NaN NaN NaN NaN 5.0 2.0 False False 3 tt0002101 Cleopatra Cleopatra 1912.0 1912-11-13 Drama, History 100 USA English Charles L. Gaskill Victorien Sardou Helen Gardner Picture Players Helen Gardner, Pearl Sindelar, Miss Fielding, Miss Robson, Helene Costello, Charles Sindelar, Mr. Howard, James R. Waite, Mr. Osborne, Harry Knowles, Mr. Paul, Mr. Brady, Mr. Corker The fabled queen of Egypt's affair with Roman general Marc Antony is ultimately disastrous for both of them. 5.2 446 $ 45000 NaN NaN NaN 25.0 3.0 False False 4 tt0002130 L'Inferno L'Inferno 1911.0 1911-03-06 Adventure, Drama, Fantasy 68 Italy Italian Francesco Bertolini, Adolfo Padovan Dante Alighieri Milano Film Salvatore Papa, Arturo Pirovano, Giuseppe de Liguoro, Pier Delle Vigne, Augusto Milla, Attilio Motta, Emilise Beretta Loosely adapted from Dante's Divine Comedy and inspired by the illustrations of Gustav Dor\u00e9 the original silent film has been restored and has a new score by Tangerine Dream. 7.0 2237 NaN NaN NaN NaN 31.0 14.0 False False ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 85850 tt9908390 Le lion Le lion 2020.0 2020-01-29 Comedy 95 France, Belgium French Ludovic Colbeau-Justin Alexandre Coquelle, Matthieu Le Naour Monkey Pack Films Dany Boon, Philippe Katerine, Anne Serra, Samuel Jouy, Sophie Verbeeck, Carole Brana, Beno\u00eet P\u00e9tr\u00e9, Aksel Ustun, Mathieu Lardot, Olivier Sa, Julien Prevost, Antoine Mathieu, David Ban, Stan, Guillaume Cl\u00e9mencin A psychiatric hospital patient pretends to be crazy. In charge of caring for this patient, a caregiver will begin to doubt the mental state of his \"prot\u00e9g\u00e9\". 5.3 398 NaN NaN $ 3507171 NaN NaN 4.0 True False 85851 tt9911196 De Beentjes van Sint-Hildegard De Beentjes van Sint-Hildegard 2020.0 2020-02-13 Comedy, Drama 103 Netherlands German, Dutch Johan Nijenhuis Radek Bajgar, Herman Finkers Johan Nijenhuis & Co Herman Finkers, Johanna ter Steege, Leonie ter Braak, Stef Assen, Annie Beumers, Jos Brummelhuis, Reinier Bulder, Daphne Bunskoek, Karlijn Koel, Karlijn Lansink, Marieke Lustenhouwer, Jan Roerink, Ferdi Stofmeel, Aniek Stokkers, Belinda van der Stoep A middle-aged veterinary surgeon believes his wife pampers him too much. In order to get away from her, he fakes the onset of dementia. 7.7 724 NaN NaN $ 7299062 NaN 6.0 4.0 True False 85852 tt9911774 Padmavyuhathile Abhimanyu Padmavyuhathile Abhimanyu 2019.0 2019-03-08 Drama 130 India Malayalam Vineesh Aaradya Vineesh Aaradya, Vineesh Aaradya RMCC Productions Anoop Chandran, Indrans, Sona Nair, Simon Britto Rodrigues NaN 7.9 265 NaN NaN NaN NaN NaN NaN True False 85853 tt9914286 Sokagin \u00c7ocuklari Sokagin \u00c7ocuklari 2019.0 2019-03-15 Drama, Family 98 Turkey Turkish Ahmet Faik Akinci Ahmet Faik Akinci, Kasim U\u00e7kan Gizem Ajans Ahmet Faik Akinci, Belma Mamati, Metin Ke\u00e7eci, Burhan Sirmabiyik, Orhan Aydin, Tevfik Yapici, Yusuf Eksi, Toygun Ates, Aziz \u00d6zuysal, Dilek \u00d6lekli, Arcan Bunial, Seval Hislisoy, Erg\u00fcl \u00c7olakoglu, G\u00fcl\u00e7in Ugur, Ibrahim Balaban NaN 6.4 194 NaN NaN $ 2833 NaN NaN NaN True False 85854 tt9914942 La vida sense la Sara Amat La vida sense la Sara Amat 2019.0 2020-02-05 Drama 74 Spain Catalan Laura Jou Coral Cruz, Pep Puig La Xarxa de Comunicaci\u00f3 Local Maria Morera Colomer, Biel Rossell Pelfort, Isaac Alcayde, Llu\u00eds Alt\u00e9s, Joan Amarg\u00f3s, Pepo Blasco, Cesc Casanovas, Oriol Cervera, Pau Escobar, Jordi Figueras, Ar\u00e9s Fuster, Judit Mart\u00edn, Mart\u00ed M\u00farcia, Mariona Pag\u00e8s, Francesca Pi\u00f1\u00f3n Pep, a 13-year-old boy, is in love with a girl from his grandparents village, Sara Amat. One summer night Sara disappears without a trace. After a few hours, Pep finds her hiding in his room. 6.7 102 NaN NaN $ 59794 NaN NaN 2.0 True False 85854 rows \u00d7 24 columns # Cell for 1.2 Text(0.5, 0, 'Fraction of Movies with AI Theme') ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_69_1.png) ## \ud83d\udc0d Exercise 2: Operating on NumPy Arrays the following arrays, x, y, and z, contain the respective locations of an object in 3-dimensional space with respect to time. 1. Return the euclidian distance between the points in x, y, and z and the origin 2. Plot the euclidian distance as blue squares if the distance is greater than 1 and as red stars otherwise in a scatterplot of distance vs time np.random.seed(42) x = np.random.randn(10) y = np.random.randn(10) z = np.random.randn(10) np . random . seed ( 42 ) x = np . random . randn ( 10 ) y = np . random . randn ( 10 ) z = np . random . randn ( 10 ) # Cell for 2.1 distance = np . sqrt ( x ** 2 + y ** 2 + z ** 2 ) # this is the distance of each point form the distance array([1.61542788, 0.53572022, 0.69469867, 2.83022413, 1.82387551, 0.61910516, 2.20102844, 0.91041554, 1.18561408, 1.54079797]) bluey = distance [ distance > 1 ] bluex = np . argwhere ( distance > 1 ) redy = distance [ distance < 1 ] redx = np . argwhere ( distance < 1 ) plt . plot ( bluex , bluey , ls = '' , marker = 's' ) plt . plot ( redx , redy , ls = '' , marker = '*' , color = 'tab:red' ) [ ] ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_75_1.png) # Cell for 2.2 [ ] ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_76_1.png)","title":"Technology Innovation 510"},{"location":"extras/X6_Data_Science_Overview/#technology-innovation-510","text":"","title":"Technology Innovation 510"},{"location":"extras/X6_Data_Science_Overview/#introduction-to-data-science-methods-data-science-and-visualization","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today, we'll be continuing our discussion of AI/DS/ML with our part II: Data Science (and a focus in Visualization)\ud83c\udf89","title":"Introduction to Data Science Methods: Data Science and Visualization"},{"location":"extras/X6_Data_Science_Overview/#preparing-notebook-for-demos","text":"","title":"Preparing Notebook for Demos"},{"location":"extras/X6_Data_Science_Overview/#importing-packages","text":"Once we have our packages installed, we need to import them. We can also import packages that are pre-installed in the Colab environment. import numpy as np import random import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression import seaborn as sns pd . options . display . max_rows = 999 pd . set_option ( 'display.max_colwidth' , None ) sns . set ()","title":"Importing Packages"},{"location":"extras/X6_Data_Science_Overview/#importing-data","text":"We also have the ability to import data, and use it elsewhere in the notebook \ud83d\udcdd! df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_explorers/main/assets/imdb_movies.csv\" ) # converting years to numbers for easy conditionals df [ 'year' ] = pd . to_numeric ( df [ 'year' ], errors = 'coerce' ) /usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result)","title":"Importing Data"},{"location":"extras/X6_Data_Science_Overview/#what-is-data-science","text":"","title":"\ud83d\udcca What is Data Science?"},{"location":"extras/X6_Data_Science_Overview/#the-emergence-of-data-science","text":"Data Science is a broad field, and depending on who you talk to, it can mean different things. In summary, many independent scientific fields began accumulating large amounts of data. At the UW in particular, these were dominated by the astronomy and oceanography departments. Folks began to realize that they needed a particular set of tools to handle large amounts of data. This culminated in the eScience studio , which began to service the data needs of many departments on campus. Today, data science not only has to do with large amounts of data, but refers generally to tools that allow us to work with a variety of data types. Because of this, machine learning is a tool within data science. But there are other tools apart from machine learning that make up the data science ecosystem. Some of them are: data visualization databases statistics You could argue for others as well (algorithms, web servers, programming, etc.), but these are the formally accepted areas. We can borrow from Drew Conway's Data Science Venn Diagram, first published on his blog in September 2010, to make further sense of this: #### \ud83d\udcad 1 Let's see if we can converge on a definition of Data Science. Talk to your neighbor, convene together, then let's share. Do this at 2 different levels: How would you explain Data Science to: 1. your grandmother 2. a young professional You may find these articles useful: * from Oracle * [A business pov on DS](https://www.oracle.com/data-science/what-is-data-science/) * [An implementation pov on DS](https://www.oracle.com/a/ocom/docs/data-science-lifecycle-ebook.pdf) * a more technical *cough* better *cough* discussion on DS from Software Carpentry: * [A research pov on DS](https://software-carpentry.org/blog/2017/12/assessing-data-science.html) * [The minues from that meeting](https://github.com/carpentries/assessment/blob/main/assessment-network/minutes/2017-11-15.md) #### \ud83d\udcac 1 I'll write these down, let's see if we can all agree on a precise definition ### Saying Stuff About Data (Statistics) When we're talking about statistics, we're really talking about data story telling. Statistics is at the C O R E of data science, really. Without a basic knowledge of statistics it'll be hard for you to construct your data narratives and have them hold water. Let's start with some simple examples of data story telling, and use these to generate our own thoughts on the matter. #### Anscombe's Quartet There's a very famous anomaly in DS caled Anscombe's quartet. Observe the following data We can construct this in python and confirm the summary statistics ourselves df = pd . read_excel ( \"https://github.com/wesleybeckner/technology_explorers/blob\" \\ \"/main/assets/data_science/anscombes.xlsx?raw=true\" , header = [ 0 , 1 ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } I II III IV X Y X Y X Y X Y 0 10 8.04 10 9.14 10 7.46 8 6.58 1 8 6.95 8 8.14 8 6.77 8 5.76 2 13 7.58 13 8.74 13 12.74 8 7.71 3 9 8.81 9 8.77 9 7.11 8 8.84 4 11 8.33 11 9.26 11 7.81 8 8.47 5 14 9.96 14 8.10 14 8.84 8 7.04 6 6 7.24 6 6.13 6 6.08 8 5.25 7 4 4.26 4 3.10 4 5.39 19 12.50 8 12 10.84 12 9.13 12 8.15 8 5.56 9 7 4.82 7 7.26 7 6.42 8 7.91 10 5 5.68 5 4.74 5 5.73 8 6.89 We can calculate the mean/variance of X and Y for samples I, II, III, and IV df . mean () I X 9.000000 Y 7.500909 II X 9.000000 Y 7.500909 III X 9.000000 Y 7.500000 IV X 9.000000 Y 7.500909 dtype: float64 # do we remember the relationship between standard deviation and variance? df . std () ** 2 I X 11.000000 Y 4.127269 II X 11.000000 Y 4.127629 III X 11.000000 Y 4.122620 IV X 11.000000 Y 4.123249 dtype: float64 For the line of best fit, recall the equation for a linear relationship between x and y: $$y(x)= m\\cdot x + b$$ model = LinearRegression () sets = [ 'I' , 'II' , 'III' , 'IV' ] for data in sets : model . fit ( df [ data ][ 'X' ] . values . reshape ( 11 , 1 ), df [ data ][ 'Y' ]) print ( \"Linear Regression Line: Y = {:.2f} X + {:.2f} \" . format ( model . coef_ [ 0 ], model . intercept_ )) Linear Regression Line: Y = 0.50X + 3.00 Linear Regression Line: Y = 0.50X + 3.00 Linear Regression Line: Y = 0.50X + 3.00 Linear Regression Line: Y = 0.50X + 3.00 $R^2$ measures the goodness of fit. $R^2$ is generally defined as the ratio of the total sum of squares $SS_{\\sf tot} $ to the residual sum of squares $SS_{\\sf res} $: The Residual Sum of Squares is defined as: $$SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2$$ We now define the total sum of squares, a measure of the total variance in the data: $$SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2$$ The $R^2$ tells us how much of the variance of the data, is captured by the model we created: $$R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}}$$ In the first equation, $\\bar{y}=\\sum_i y^{\\sf exact}_i/N$ is the average value of y for $N$ points. The best value of $R^2$ is 1 but it can also take a negative value if the error is large. for data in sets : # calc the ssr ssr = np . sum (( df [ data ][ 'Y' ] - model . predict ( df [ data ][ 'X' ] . values . reshape ( - 1 , 1 ))) ** 2 ) # calc the sst sst = np . sum (( df [ data ][ 'Y' ] - df [ data ][ 'Y' ] . mean ()) ** 2 ) # calc the r2 r2 = 1 - ( ssr / sst ) print ( \"R2 = {:.2f} \" . format ( r2 )) R2 = 0.67 R2 = 0.67 R2 = 0.67 R2 = 0.67 As we can see, everything checks out. The summary statistics are all the same! Can we answer the following: > What dataset is best described by the line of best fit? We will revisit this question when we talk about data visualization #### Taxonomy of Data Types Another important topic in data science, is simply what kind of data we are working with. This will help us decide what kind of models to build, as well as how to visualize our data, and perhaps store it as well. #### \ud83d\udcac 2 What are some examples of the different datatypes we can think of? ### Data Visualization Data visualization, like it sounds, has to do with how we display and communicate information. At the end of the day, your findings and algorithms aren't worth very much if we can't share them with others. One of the leading thinkers in the visual display of information is Edward Tufte * [Tufte's Principles](https://thedoublethink.com/tuftes-principles-for-visualizing-quantitative-information/) * [Data-ink](https://infovis-wiki.net/wiki/Data-Ink_Ratio) * [Wiki](https://en.wikipedia.org/wiki/Edward_Tufte) #### Guiding Principles of Data Visualization Another topic Tufte discusses is Data Density. Apart from ink, we should also consider the entire area dedicated to the graphic. That area should be as condensed as possible without removing ledgibility/readability. wattenberg and Viegas visualization %% HTML < video width = \"640\" height = \"580\" controls > < source src = \"https://github.com/wesleybeckner/technology_explorers/blob/main/assets/data_science/ds4.mp4?raw=true\" type = \"video/mp4\" > </ video > [Color blindness](https://www.colourblindawareness.org/colour-blindness/) is prevalent, and something we should consider when choosing colors in our visual displays #### Visualization Un-Examples **Unexample 1** **Unexample 2** **Unexample 3** **Unexample 4** #### \ud83d\udcad 3 Find an example of an interactive data visualization online. Here's one I [found](https://www.migrationpolicy.org/programs/data-hub/charts/us-immigrant-population-state-and-county) that I though was quite interesting! #### \ud83d\udcac 3 Swap visualization links with your neighbor. What do you think could be improved about each one? #### Back to Anscombe's Quartet ### Revisiting our A.I. movie data Let's revisit our imdb dataset from last week. df . loc [( df [ 'description' ] . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False ))][[ 'original_title' , 'year' , 'description' ]][: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } original_title year description 1683 Son of a Sailor 1933.0 A smooth-talking sailor looking for a quick date meets the granddaughter of an admiral and finds himself in a house full of top Navy officers, along with a couple of spies interested in plans for a new robot-controlled flight system. 3339 The Middleton Family at the New York World's Fair 1939.0 The Middleton family visits the 1939 New York World's Fair and witnesses the advent of the future, encountering robots and dishwashers for the first time. 3846 Emergency Landing 1941.0 A test pilot and his weather observer develop a \"robot\" control so airplanes can be flown without pilots, but enemy agents get wind of it and try to steal it or destroy it. 4122 Cairo 1942.0 Reporter Homer Smith accidentally draws Marcia Warren into his mission to stop Nazis from bombing Allied Convoys with robot-planes. 6274 The Perfect Woman 1949.0 In need of cash, Roger Cavendish and his valet take a job escorting the perfect woman for a night on the town. She is in fact the robotic creation of Professor Belman, but it turns out ... 7936 Devil Girl from Mars 1954.0 An uptight, leather-clad female alien, armed with a ray gun and accompanied by a menacing robot, comes to Earth to collect Earth's men as breeding stock. 7977 Gog 1954.0 A security agent investigates sabotage and murder at a secret underground laboratory, home of two experimental robots. 8155 Target Earth 1954.0 Giant robots from Venus invade Chicago. Stranded in the deserted city are strangers Frank and Nora. 8173 Tobor the Great 1954.0 A young boy-genius befriends his grandfather's robot, designed as a test pilot for space travel and coveted by foreign spies. 9047 Chiky\u00fb B\u00f4eigun 1957.0 Shortly after a giant robot destroys a village, aliens from outer space arrive offering friendship. Will the people of Earth tie these two events together? Let's create a visualization that helps answer the question: > What is the popularity of AI movies by year?\" #### \ud83d\udcac 4 What are some potential issues with the following plot? ai_movies = df . loc [( df [ 'description' ] . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False ))] fig , ax = plt . subplots () ai_movies . groupby ( 'year' ) . count ()[ 'title' ] . plot ( ls = '--' , marker = '.' , ax = ax ) ax . set_ylabel ( 'number of movies' ) Text(0, 0.5, 'number of movies') ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_43_1.png) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ai_movies . groupby ( 'year' ) . count ()[ 'imdb_title_id' ] . plot ( marker = '.' , ax = ax ) ax . set_ylabel ( \"number of movies\" ) Text(0, 0.5, 'number of movies') ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_44_1.png) It looks like the popularity is increasing, doesn't it? But is that fair to say if we do not consider the total number of movies in the database each year? Let's say we're curious about the year 1986 and what countries contributed to the spike in movies. How would we *filter* for year 1986, then *select* country and perform a count *operation*? Remember the sequence **filter \u27a1\ufe0f Select \u27a1\ufe0f operate** ai_movies . loc [ ai_movies [ 'year' ] == 1986 ][[ 'country' ]] . nunique () country 6 dtype: int64 ai_movies . loc [ ai_movies [ 'year' ] == 1986 ][[ 'country' ]] . value_counts () country USA 3 Japan 2 USA, Spain 1 USA, Japan 1 Poland, Soviet Union 1 Canada 1 dtype: int64 Looks like a good year for US-Japan movie making. How can we look into the role of different countries in AI movie making? > in the following, pandas doesn't have the ability to color according to a categorical column, out of the box. So I switch over to seaborn. For more exercises with seaborn visit [JVDP's chapter on the subject](https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html) by_country = ai_movies . groupby ([ 'country' ]) . filter ( lambda x : ( x . count () >= 5 ) . any ()) . \\ groupby ([ 'country' , 'year' ]) . apply ( lambda x : x . count ())[[ 'title' ]] . reset_index () # ai_movies.groupby(['country']).filter(lambda x: (x.count() >= 5).any()) ai_movies . groupby ([ 'country' ]) . filter ( lambda x : ( x . count () >= 5 ) . any ()) . \\ groupby ([ 'country' , 'year' ]) . apply ( lambda x : x . count ())[[ 'title' ]] . reset_index () . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year title 0 Canada 1979.0 1 1 Canada 1986.0 1 2 Canada 1994.0 1 3 Canada 2010.0 1 4 Canada 2014.0 1 by_country = ai_movies . groupby ([ 'country' ]) . filter ( lambda x : ( x . count () >= 5 ) . any ()) . \\ groupby ([ 'country' , 'year' ]) . apply ( lambda x : x . count ())[[ 'title' ]] . reset_index () fig , ax = plt . subplots ( figsize = ( 12 , 5 )) with sns . axes_style ( 'white' ): g = sns . barplot ( data = by_country , x = \"year\" , hue = 'country' , y = 'title' , ax = ax , order = range ( 1999 , 2019 )) ax . set_ylabel ( 'Number of Films' ) Text(0, 0.5, 'Number of Films') ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_54_1.png) For the last 20 years we can see that U.S./Japan have dominated the A.I. movie market # In-Class Exercises Create visualizations that help a user answer the following questions: ## \ud83d\udc0d Exercise 1: Visualizing Fractions 1. Of all the movie data, which top 10 countries have the highest fraction of AI movies? 2. Are there more movies about AI before or after 1986? df . groupby ( 'country' )[ 'description' ] . apply ( lambda x : x . loc [ x . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False )] . count () / x . shape [ 0 ]) . sort_values ()[ - 20 :] country South Africa, USA 0.050000 USA, Argentina 0.058824 USA, Germany, UK 0.090909 Switzerland, USA 0.142857 Germany, Belgium 0.142857 Singapore, USA 0.250000 Poland, Soviet Union 0.285714 UK, USA, Ireland 0.333333 UK, China, USA 0.333333 USA, UK, Italy 0.333333 India, Australia 0.333333 China, Canada, USA 0.333333 Canada, South Korea 0.500000 Hong Kong, Canada, USA 1.000000 Italy, West Germany, Spain, Monaco 1.000000 Canada, France, Spain, Germany 1.000000 Croatia, Luxembourg, Norway, Czech Republic, Slovakia, Slovenia, Bosnia and Herzegovina 1.000000 USA, Singapore 1.000000 Canada, USA, UK, Japan 1.000000 Canada, France, Morocco 1.000000 Name: description, dtype: float64 df . loc [ ~ df [ 'country' ] . str . contains ( ',' , na = True )] . groupby ( 'country' )[ 'description' ] . apply ( lambda x : x . loc [ x . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False )] . count () / x . shape [ 0 ]) . sort_values ()[ - 20 :] country Russia 0.001247 Australia 0.001274 UK 0.001459 Turkey 0.001480 Mexico 0.001527 Denmark 0.001776 South Korea 0.001813 China 0.001908 Finland 0.002232 West Germany 0.002375 Hong Kong 0.002421 Brazil 0.002717 USA 0.002876 Thailand 0.003817 Canada 0.003885 Soviet Union 0.003916 Israel 0.006250 Chile 0.009174 Belgium 0.009804 Japan 0.010075 Name: description, dtype: float64 df . loc [ ~ ( df [ 'country' ] . str . contains ( ',' , na = True ))][ 'country' ] 0 USA 1 Australia 3 USA 4 Italy 5 USA ... 85849 India 85851 Netherlands 85852 India 85853 Turkey 85854 Spain Name: country, Length: 70300, dtype: object df . loc [ ~ ( df [ 'country' ] . str . contains ( ',' , na = True ))] . groupby ( 'country' )[ 'description' ] . apply ( lambda x : x . loc [ x . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False )] . count () / x . shape [ 0 ]) . sort_values ( ascending = False )[: 10 ] country Japan 0.010075 Belgium 0.009804 Chile 0.009174 Israel 0.006250 Soviet Union 0.003916 Canada 0.003885 Thailand 0.003817 USA 0.002876 Brazil 0.002717 Hong Kong 0.002421 Name: description, dtype: float64 # Cell for 1.1 fig , ax = plt . subplots () df . loc [ ~ ( df [ 'country' ] . str . contains ( ',' , na = True ))] . groupby ( 'country' )[ 'description' ] . apply ( lambda x : x . loc [ x . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False )] . count () / x . shape [ 0 ]) . sort_values ( ascending = True )[ - 10 :] . plot ( kind = 'barh' ) ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_62_1.png) df [ 'after 1986' ] = df [ 'year' ] > 1986 df [ 'ai movie' ] = df [ 'description' ] . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False ) np . sum ([ True , True , False ]) 2 df . groupby ([ 'after 1986' , 'ai movie' ])[ 'title' ] . count () after 1986 ai movie False False 26438 True 54 True False 59214 True 149 Name: title, dtype: int64 fig , ax = plt . subplots () df . groupby ([ 'after 1986' ])[ 'ai movie' ] . apply ( lambda x : x . sum () / x . shape [ 0 ]) . plot ( kind = 'barh' ) ax . set_yticklabels ([ 'Before/During 1986' , 'After 1986' ]) ax . set_ylabel ( '' ) ax . set_xlabel ( 'Fraction of Movies with AI Theme' ) Text(0.5, 0, 'Fraction of Movies with AI Theme') ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_66_1.png) fig , ax = plt . subplots () df . groupby ([ 'after 1986' ])[ 'ai movie' ] . apply ( lambda x : x . sum () / x . shape [ 0 ]) . plot ( kind = 'barh' ) ax . set_yticklabels ([ 'Before/During 1986' , 'After 1986' ]) ax . set_xlabel ( 'Fraction of Movies with AI Theme' ) Text(0.5, 0, 'Fraction of Movies with AI Theme') ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_67_1.png) df . loc [ df [ 'year' ] > 1986 & df [ 'description' ] . str . contains ( 'artificial intelligence|a\\.i\\.|robot' , na = False )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } imdb_title_id title original_title year date_published genre duration country language director writer production_company actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics after 1986 ai movie 0 tt0000009 Miss Jerry Miss Jerry 1894.0 1894-10-09 Romance 45 USA None Alexander Black Alexander Black Alexander Black Photoplays Blanche Bayliss, William Courtenay, Chauncey Depew The adventures of a female reporter in the 1890s. 5.9 154 NaN NaN NaN NaN 1.0 2.0 False False 1 tt0000574 The Story of the Kelly Gang The Story of the Kelly Gang 1906.0 1906-12-26 Biography, Crime, Drama 70 Australia None Charles Tait Charles Tait J. and N. Tait Elizabeth Tait, John Tait, Norman Campbell, Bella Cola, Will Coyne, Sam Crewes, Jack Ennis, John Forde, Vera Linden, Mr. Marshall, Mr. McKenzie, Frank Mills, Ollie Wilson True story of notorious Australian outlaw Ned Kelly (1855-80). 6.1 589 $ 2250 NaN NaN NaN 7.0 7.0 False False 2 tt0001892 Den sorte dr\u00f8m Den sorte dr\u00f8m 1911.0 1911-08-19 Drama 53 Germany, Denmark NaN Urban Gad Urban Gad, Gebhard Sch\u00e4tzler-Perasini Fotorama Asta Nielsen, Valdemar Psilander, Gunnar Helsengreen, Emil Albes, Hugo Flink, Mary Hagen Two men of high rank are both wooing the beautiful and famous equestrian acrobat Stella. While Stella ignores the jeweler Hirsch, she accepts Count von Waldberg's offer to follow her home, ... 5.8 188 NaN NaN NaN NaN 5.0 2.0 False False 3 tt0002101 Cleopatra Cleopatra 1912.0 1912-11-13 Drama, History 100 USA English Charles L. Gaskill Victorien Sardou Helen Gardner Picture Players Helen Gardner, Pearl Sindelar, Miss Fielding, Miss Robson, Helene Costello, Charles Sindelar, Mr. Howard, James R. Waite, Mr. Osborne, Harry Knowles, Mr. Paul, Mr. Brady, Mr. Corker The fabled queen of Egypt's affair with Roman general Marc Antony is ultimately disastrous for both of them. 5.2 446 $ 45000 NaN NaN NaN 25.0 3.0 False False 4 tt0002130 L'Inferno L'Inferno 1911.0 1911-03-06 Adventure, Drama, Fantasy 68 Italy Italian Francesco Bertolini, Adolfo Padovan Dante Alighieri Milano Film Salvatore Papa, Arturo Pirovano, Giuseppe de Liguoro, Pier Delle Vigne, Augusto Milla, Attilio Motta, Emilise Beretta Loosely adapted from Dante's Divine Comedy and inspired by the illustrations of Gustav Dor\u00e9 the original silent film has been restored and has a new score by Tangerine Dream. 7.0 2237 NaN NaN NaN NaN 31.0 14.0 False False ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 85850 tt9908390 Le lion Le lion 2020.0 2020-01-29 Comedy 95 France, Belgium French Ludovic Colbeau-Justin Alexandre Coquelle, Matthieu Le Naour Monkey Pack Films Dany Boon, Philippe Katerine, Anne Serra, Samuel Jouy, Sophie Verbeeck, Carole Brana, Beno\u00eet P\u00e9tr\u00e9, Aksel Ustun, Mathieu Lardot, Olivier Sa, Julien Prevost, Antoine Mathieu, David Ban, Stan, Guillaume Cl\u00e9mencin A psychiatric hospital patient pretends to be crazy. In charge of caring for this patient, a caregiver will begin to doubt the mental state of his \"prot\u00e9g\u00e9\". 5.3 398 NaN NaN $ 3507171 NaN NaN 4.0 True False 85851 tt9911196 De Beentjes van Sint-Hildegard De Beentjes van Sint-Hildegard 2020.0 2020-02-13 Comedy, Drama 103 Netherlands German, Dutch Johan Nijenhuis Radek Bajgar, Herman Finkers Johan Nijenhuis & Co Herman Finkers, Johanna ter Steege, Leonie ter Braak, Stef Assen, Annie Beumers, Jos Brummelhuis, Reinier Bulder, Daphne Bunskoek, Karlijn Koel, Karlijn Lansink, Marieke Lustenhouwer, Jan Roerink, Ferdi Stofmeel, Aniek Stokkers, Belinda van der Stoep A middle-aged veterinary surgeon believes his wife pampers him too much. In order to get away from her, he fakes the onset of dementia. 7.7 724 NaN NaN $ 7299062 NaN 6.0 4.0 True False 85852 tt9911774 Padmavyuhathile Abhimanyu Padmavyuhathile Abhimanyu 2019.0 2019-03-08 Drama 130 India Malayalam Vineesh Aaradya Vineesh Aaradya, Vineesh Aaradya RMCC Productions Anoop Chandran, Indrans, Sona Nair, Simon Britto Rodrigues NaN 7.9 265 NaN NaN NaN NaN NaN NaN True False 85853 tt9914286 Sokagin \u00c7ocuklari Sokagin \u00c7ocuklari 2019.0 2019-03-15 Drama, Family 98 Turkey Turkish Ahmet Faik Akinci Ahmet Faik Akinci, Kasim U\u00e7kan Gizem Ajans Ahmet Faik Akinci, Belma Mamati, Metin Ke\u00e7eci, Burhan Sirmabiyik, Orhan Aydin, Tevfik Yapici, Yusuf Eksi, Toygun Ates, Aziz \u00d6zuysal, Dilek \u00d6lekli, Arcan Bunial, Seval Hislisoy, Erg\u00fcl \u00c7olakoglu, G\u00fcl\u00e7in Ugur, Ibrahim Balaban NaN 6.4 194 NaN NaN $ 2833 NaN NaN NaN True False 85854 tt9914942 La vida sense la Sara Amat La vida sense la Sara Amat 2019.0 2020-02-05 Drama 74 Spain Catalan Laura Jou Coral Cruz, Pep Puig La Xarxa de Comunicaci\u00f3 Local Maria Morera Colomer, Biel Rossell Pelfort, Isaac Alcayde, Llu\u00eds Alt\u00e9s, Joan Amarg\u00f3s, Pepo Blasco, Cesc Casanovas, Oriol Cervera, Pau Escobar, Jordi Figueras, Ar\u00e9s Fuster, Judit Mart\u00edn, Mart\u00ed M\u00farcia, Mariona Pag\u00e8s, Francesca Pi\u00f1\u00f3n Pep, a 13-year-old boy, is in love with a girl from his grandparents village, Sara Amat. One summer night Sara disappears without a trace. After a few hours, Pep finds her hiding in his room. 6.7 102 NaN NaN $ 59794 NaN NaN 2.0 True False 85854 rows \u00d7 24 columns # Cell for 1.2 Text(0.5, 0, 'Fraction of Movies with AI Theme') ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_69_1.png) ## \ud83d\udc0d Exercise 2: Operating on NumPy Arrays the following arrays, x, y, and z, contain the respective locations of an object in 3-dimensional space with respect to time. 1. Return the euclidian distance between the points in x, y, and z and the origin 2. Plot the euclidian distance as blue squares if the distance is greater than 1 and as red stars otherwise in a scatterplot of distance vs time np.random.seed(42) x = np.random.randn(10) y = np.random.randn(10) z = np.random.randn(10) np . random . seed ( 42 ) x = np . random . randn ( 10 ) y = np . random . randn ( 10 ) z = np . random . randn ( 10 ) # Cell for 2.1 distance = np . sqrt ( x ** 2 + y ** 2 + z ** 2 ) # this is the distance of each point form the distance array([1.61542788, 0.53572022, 0.69469867, 2.83022413, 1.82387551, 0.61910516, 2.20102844, 0.91041554, 1.18561408, 1.54079797]) bluey = distance [ distance > 1 ] bluex = np . argwhere ( distance > 1 ) redy = distance [ distance < 1 ] redx = np . argwhere ( distance < 1 ) plt . plot ( bluex , bluey , ls = '' , marker = 's' ) plt . plot ( redx , redy , ls = '' , marker = '*' , color = 'tab:red' ) [ ] ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_75_1.png) # Cell for 2.2 [ ] ![png](X6_Data_Science_Overview_files/X6_Data_Science_Overview_76_1.png)","title":"The Emergence of Data Science"},{"location":"extras/X7_Machine_Learning_Overview/","text":"Technology Innovation 510 \u00b6 Introduction to Data Science Methods: Machine Learning \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com \ud83c\udf89 Today, we'll be working from this digital notebook to complete exercises! If you don't have a computer, not to worry. Grab a notepad and pencil to write down your ideas and notes! \ud83c\udf89 Preparing Notebook for Demos \u00b6 Installing Packages \u00b6 In colab, we have the ability to install specific packages into our coding environment. This means we can move beyond the standard packages that are already pre installed in the Colab environment # !pip install tensorflow==1.15.0 # !apt-get update # !apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev # !pip install \"stable-baselines[mpi]==2.9.0\" # we do not need to install any packages today! Importing Packages \u00b6 Once we have our packages installed, we need to import them. We can also import packages that are pre-installed in the Colab environment. # import tensorflow as tf # from gym import spaces # import gym # from stable_baselines.common.env_checker import check_env import matplotlib.pyplot as plt import numpy as np import random from sklearn.linear_model import LinearRegression import random import pandas as pd \ud83e\udde0 What is Machine Learning? \u00b6 In the previous exercise(s) we talked about automata of the renaissance . This accentuates an important deliniation between historical AI and modern machine learning. At some point in the past, we thought, predominantly, that we would have to program a priori all the intelligence of any thinking machine. In other words, it would not necessarily learn on its own. It is like the automata of the renaissance in that all the internal mechanisms and functions would need to be setup beforehand, and initialized. Around the late 90s and into the 2000s, this way of thinking began to fade. It was replaced with the idea that computers would learn from examples. In a sentence, machine learning is just that: \"learning from examples\" and that's it. Of course, there are many ins and outs and what-have-yous. But in the context of AI, this is the most important distinction of machine learning. With this shift in ML, the traditional way of thinking about AI was recapitulated as \"symbolic AI\" - translation of human logic into computer code. Everyday Machine Learning \u00b6 \ud83d\udcad 1 \u00b6 Where do you use machine learning in your day to day activities? You will be surprised where machine learning is working behind the scenes. Let's take some time and jot down your top 5 places that you use machine learning. My top 5 ML interactions in my day-to-day: my_ml_tools = ['spotify discovery playlist', 'face recognition on my laptop', 'google maps', 'spelling correction on my iPhone', 'gmail sentence autocompletion'] \ud83d\udcac 1 \u00b6 Can we assimilate our notes? What were some of the most common places folks found that they use machine learning? Use this google sheet to fill out your answers The Algorithms of Machine Learning \u00b6 Now that we have some real world examples of where machine learning is being used and where we interact with it, let's find out exactly what kinds of algorithms are operating under the hood. #### \ud83d\udcad 2 For the application/product that influences you the most, find out what kind of algorithm(s) are being used by the application. Jot it down. It's okay if we don't understand what they mean yet. \ud83d\udcac 2 \u00b6 I'll start by sharing what I've found out about the underlying algorithms of Spotify Discover Weekly . Spotify actually uses 3 different algorithms to make its discover weekly playlist. Collaborative Filtering: similar users will like similar songs Natural Language Processing: blogs, articles, and lyrics in the songs can be used to model the songs Audio Processing: audio tracks themselves of the songs can be used to generate representations of the songs What algorithms did you all uncover? Different Kinds of Machine Learning \u00b6 So we've recognized that machine learning is in a lot of different tools we use. We've noted the names to a few of those models. We've even defined machine learning in the context of AI. Now let's add some rigor to how we categorize the different kinds of ML models. Currently we think of ML in 3 different contexts: Supervised learning Unsupervised learning Reinforcement learning #### Supervised learning Perhaps the easiest of these to understand is supervised learning. In supervised learning we have _labeled data_, that is some kind of description about the data, that we usually denote as `X_train` and coinciding with a target value, `y_train`. We use the labeled dataset to train a model that then has the ability to predict new values of `y_test` for unlabeled, unseens data, `X_test`. Remember, `y_test` is not known, we are using the model to predict this! **Music Rating** **Supervised Learning - Classification** To continue with my music example, supervised learning in this context could consist of the following: you generate a bunch of labels or \"features\" that describe a song, e.g. `[Danceability, Valence, Energy, Tempo]` and the feature vector for each song would contain these series of numbers. So Coldplay's \"Clocks\" might be `clocks = [10.4, 40.5, 80, 120]` or Lil Nas X's \"Montero\" might be `montero = [15.4, 70.7, 90, 110]`. Then, you would label every song with a 1 or 0 indicating whether you liked it or not. You could then train a model on this labeled dataset that would predict whether or not you'd like a given song. Predicting 1 (you like it) or 0 (you don't like it), makes this a **_classification_** model. This is pretty close to what Pandora was actually doing early on in the music recommendation scene (they were creating the feature vectors by hand!). If you'd like to play with this idea yourself you can access _audio feature_ data with [Spotify's Developer's API](https://developer.spotify.com/console/get-audio-features-track/) **Supervised Learning - Regression** We can imagine a slightly different situation if, instead of labeling binary 1's and 0's for if we liked a song, we give it a score. If we attempt to map the song vectors to this _continuous_ value for _score_, we have ourselves a **_regression_** model. In practice, however, Spotify only knows whether or not we've _liked_ a song. So binary it is for now. **Housing Prices** > Let's switch gears and think about housing price data from Zillow. Let's also borrow language we encountered earlier on in our Data Science discussion: Nominal, Ordinal, Interval, and Ratio data. Envision a **_mixed_** dataset of **_continuous_** and **_discrete_** variables. Some features could be continuous, floating point values like neihborhood score and housing condition. Others could be discrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a **_regression_** model. To flip back to the **_classification_** discussion, we could, instead of reporting a value, report a recommendation to buy or sell by combinging the valuation with the actual bid or sell price. What we see is that the two forms of supervised learning, classification and regression, are not too different from one another. #### Unsupervised learning A little less intuitive, unsupervised learning does not require labeled datasets. Rather, it infers something about the data. **Unsupervised Learning - Clustering** Again taking the music example, Spotify's Collaborative Filterning model is an example of unsupervised learning. The math is a bit complex, but the general idea is that we construct a giant matrix of every song and every user filled with 1's or 0's indicating whether a user has liked the song or not. In this matrix space then, every song and every user is represented by a vector. We can use some mathematical tricks to compute vector distances, and, using this, identify similar users, in other words, we **_cluster_** them. The similar users then can be recommended to like each others songs * User 1 likes: \ud83c\udf4f, \ud83c\udf50, \ud83c\udf45, and \ud83c\udf46 * User 2 likes: \ud83c\udf4a, \ud83c\udf50, \ud83c\udf45, and \ud83c\udf46! User 1 should try \ud83c\udf4a and user 2 should try \ud83c\udf4f! Or take this example: * User 3 likes: \ud83e\udd54, \ud83c\udf53, \ud83c\udf4b, and \ud83c\udf51 * User 4 likes: \ud83e\udd6d, \ud83c\udf53, \ud83c\udf4b, and \ud83c\udf48 What should user 3 try and what should user 4 try? **Unsupervised Learning - Dimensionality Reduction** There is another category of unsupervised learning called **_dimensionality reduction_** that has powerful applications in feature engineering, outlier removal, and data visualization. One of the most common forms of reduction is principal component analysis. We won't go into great detail here, but if you are curious we visited this discussion over the summer in this [notebook](https://render.githubusercontent.com/view/ipynb?color_mode=light&commit=6e91cf4bfd71b7215a75cdb09d2b25dee45943f4&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f7765736c65796265636b6e65722f746563686e6f6c6f67795f66756e64616d656e74616c732f366539316366346266643731623732313561373563646230396432623235646565343539343366342f43332532304d616368696e652532304c6561726e696e67253230492f546563685f46756e5f43335f45315f556e737570657276697365645f4c6561726e696e672e6970796e62&nwo=wesleybeckner%2Ftechnology_fundamentals&path=C3+Machine+Learning+I%2FTech_Fun_C3_E1_Unsupervised_Learning.ipynb&repository_id=384268836&repository_type=Repository#3.1-Principal-Component-Analysis). To extrapolate our spotify music data discusion, imagine that we take that huge table of users and songs. What a dimensionality reduction process will do, is attempt to consolidate that table into fewer rows and columns while minimizing data loss. For example, perhaps everyone that like's Coldplay's Clocks, also likes Little Nas X's Montero, in that case, there would be no loss of information if instead of representing both songs in the table explicitly, we represented each one implicitly with a single variable. #### Reinforcement learning Reinforcement learning is a complex and blossoming field. The basic idea of reinforcement learning is that, instead of training a model on data, it trains within an _environment_. The environment of course, produces data; but it is different from supervised learning in that the learning algorithm must make a series of steps to get the \"right answer\". Because of this, reinforcement learning introduces concepts of _steps_ (the decision the algorithm makes at a point in time), _reward_ (the immediate benefit of that decision), _value estimation_ (the perceived overall value at the end of the simulation), and _policy_ (the mechanism by which we update the behavior of the model in subsequent expsoures to the environment). > The nuts and bolts of reinforcement learning is outside the scope of what we will discuss in our few sessions together, but it is good to define it alongside the other two topics: supervised and unsupervised learning! #### \ud83d\udcac 3 Take a moment to try to categorize the models you found within supervised or unsupervised machine learning! ## \ud83e\udd89 Tenets of Machine Learning We'll take the simple linear regression as an example and discuss some of the core tenets of ML: Bias-variance trade-off, irreducible error, and regularization. ### \ud83d\udcc8 Bias-Variance Trade-Off #### (Over and Underfitting) The basic premise here is that there's some optimum number of parmeters to include in my model, if I include too few, my model will be too simple (***high bias***) and if I include too many it will be too complex and fit to noise (***high variance***) We can explore this phenomenon more easily, making up some data ourselves: # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a known underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) Text(0, 0.5, 'Y') ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_22_1.png) Now, let's pretend we've sampled from this ***population*** of data: random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] plt . plot ( X_train , y_train , ls = '' , marker = '.' ) [ ] ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_24_1.png) Now let's take two extreme scenarios, fitting a linear line and a high order polynomial, to these datapoints. Keeping in mind the larger dataset, as well as the error we introduced in our data generating function, this will really illustrate our point! # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_26_1.png) We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has *high bias* because we are forcing the functional form without much consideration to the underlying data \u2014 we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has *low variance* with respect to the underlying data. On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has *low bias* because we don't introduce many constraints on the final form of the model. it is *high variance* because depending on the underlying training data, the final outcome of the model can change quite drastically! In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further: 1. what happens when we retrain these models on different samples of the data population * and let's use this to better understand what we mean by *bias* and *variance* 2. what happens when we tie this back in with the error we introduced to the data generator? * and let's use this to better understand irreducible error random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) for samples in range ( 5 ): X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_28_0.png) As we can see, depending on what data we train our model on, the *high bias* model changes relatively slightly, while the *high variance* model changes a whole awful lot! The *high variance* model is prone to something we call *overfitting*. It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the *population* # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_30_1.png) In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance model are particuarly prone to the phenomenon of *over fitting* and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this. ### \u2755 Irreducible Error Irreducible error is ***always*** present in our data. It is a part of life, welcome to it. That being said, let's look what happens when we *pretend* there isn't any irreducible error in our population data x = np . arange ( 20 , 100 ) y = [ func ( t , err = 0 ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [ ] ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_33_1.png) random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_34_1.png) This time, our high variance model really *gets it*! And this is because the data we trained on actually *is* a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the *bias variance tradeoff* ### \ud83d\udd78\ufe0f Regularization To talk about regularization, we're going to continue with our simple high bias model example, the much revered linear regression model. Linear regression takes on the form: $$y(x)= m\\cdot x + b$$ where $y$ is some target value and, $x$ is some feature; $m$ and $b$ are the slope and intercept, respectively. To solve the problem, we need to find the values of $b$ and $m$ in equation 1 to best fit the data. In linear regression our goal is to minimize the error between computed values of positions $y^{\\sf calc}(x_i)\\equiv y^{\\sf calc}_i$ and known values $y^{\\sf exact}(x_i)\\equiv y^{\\sf exact}_i$, i.e. find $b$ and $m$ which lead to lowest value of $$\\epsilon (m,b) =SS_{\\sf res}=\\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - m\\cdot x_i - b \\right)^2$$ **Now onto Regularization** There are many other regression algorithms, the two we want to highlight here are Ridge Regression and LASSO. They differ by an added term to the loss function. Let's review. The above equation expanded to multivariate form yields: $$\\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2$$ for Ridge regression, we add a **_regularization_** term known as **_L2_** regularization: $$\\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}\\beta_{j}^2$$ for **_LASSO_** (Least Absolute Shrinkage and Selection Operator) we add **_L1_** regularization: $$\\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}|\\beta_{j}|$$ The difference between the two is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. **_Elastic Net_** is a combination of these two regularization methods. The key notion here is that ***regularization*** is a way of tempering our model, allowing it to pick for itself the most appropriate features. This crops up in many places other than simple linear regression in machine learning. **Regularization appears in...** ***Ensemble learners*** (e.g. XGBoost and Random Forests) by combining the combinations of many weak algorithms ***Neural networks*** with ***dropout*** and ***batch normalization*** Dropout is the Neural Network response to the wide success of ensemble learning. In a dropout layer, random neurons are dropped in each batch of training, i.e. their weighted updates are not sent to the next neural layer. Just as with random forests, the end result is that the neural network can be thought of as many _independent models_ that _vote_ on the final output. Put another way, when a network does not contain dropout layers, and has a capacity that exceeds that which would be suited for the true, underlying complexity level of the data, it can begin to fit to noise. This ability to fit to noise is based on very specific relationships between neurons, which fire uniquely given the particular training example. Adding dropout _breaks_ these specific neural connections, and so the neural network as a whole is forced to find weights that apply generally, as there is no guarantee they will be _turned on_ when their specific training example they would usually overfit for comes around again. Network with 50% dropout. Borrowed from Kaggle learn.","title":"Technology Innovation 510"},{"location":"extras/X7_Machine_Learning_Overview/#technology-innovation-510","text":"","title":"Technology Innovation 510"},{"location":"extras/X7_Machine_Learning_Overview/#introduction-to-data-science-methods-machine-learning","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com \ud83c\udf89 Today, we'll be working from this digital notebook to complete exercises! If you don't have a computer, not to worry. Grab a notepad and pencil to write down your ideas and notes! \ud83c\udf89","title":"Introduction to Data Science Methods: Machine Learning"},{"location":"extras/X7_Machine_Learning_Overview/#preparing-notebook-for-demos","text":"","title":"Preparing Notebook for Demos"},{"location":"extras/X7_Machine_Learning_Overview/#installing-packages","text":"In colab, we have the ability to install specific packages into our coding environment. This means we can move beyond the standard packages that are already pre installed in the Colab environment # !pip install tensorflow==1.15.0 # !apt-get update # !apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev # !pip install \"stable-baselines[mpi]==2.9.0\" # we do not need to install any packages today!","title":"Installing Packages"},{"location":"extras/X7_Machine_Learning_Overview/#importing-packages","text":"Once we have our packages installed, we need to import them. We can also import packages that are pre-installed in the Colab environment. # import tensorflow as tf # from gym import spaces # import gym # from stable_baselines.common.env_checker import check_env import matplotlib.pyplot as plt import numpy as np import random from sklearn.linear_model import LinearRegression import random import pandas as pd","title":"Importing Packages"},{"location":"extras/X7_Machine_Learning_Overview/#what-is-machine-learning","text":"In the previous exercise(s) we talked about automata of the renaissance . This accentuates an important deliniation between historical AI and modern machine learning. At some point in the past, we thought, predominantly, that we would have to program a priori all the intelligence of any thinking machine. In other words, it would not necessarily learn on its own. It is like the automata of the renaissance in that all the internal mechanisms and functions would need to be setup beforehand, and initialized. Around the late 90s and into the 2000s, this way of thinking began to fade. It was replaced with the idea that computers would learn from examples. In a sentence, machine learning is just that: \"learning from examples\" and that's it. Of course, there are many ins and outs and what-have-yous. But in the context of AI, this is the most important distinction of machine learning. With this shift in ML, the traditional way of thinking about AI was recapitulated as \"symbolic AI\" - translation of human logic into computer code.","title":"\ud83e\udde0 What is Machine Learning?"},{"location":"extras/X7_Machine_Learning_Overview/#everyday-machine-learning","text":"","title":"Everyday Machine Learning"},{"location":"extras/X7_Machine_Learning_Overview/#1","text":"Where do you use machine learning in your day to day activities? You will be surprised where machine learning is working behind the scenes. Let's take some time and jot down your top 5 places that you use machine learning. My top 5 ML interactions in my day-to-day: my_ml_tools = ['spotify discovery playlist', 'face recognition on my laptop', 'google maps', 'spelling correction on my iPhone', 'gmail sentence autocompletion']","title":"\ud83d\udcad 1"},{"location":"extras/X7_Machine_Learning_Overview/#1_1","text":"Can we assimilate our notes? What were some of the most common places folks found that they use machine learning? Use this google sheet to fill out your answers","title":"\ud83d\udcac 1"},{"location":"extras/X7_Machine_Learning_Overview/#the-algorithms-of-machine-learning","text":"Now that we have some real world examples of where machine learning is being used and where we interact with it, let's find out exactly what kinds of algorithms are operating under the hood. #### \ud83d\udcad 2 For the application/product that influences you the most, find out what kind of algorithm(s) are being used by the application. Jot it down. It's okay if we don't understand what they mean yet.","title":"The Algorithms of Machine Learning"},{"location":"extras/X7_Machine_Learning_Overview/#2","text":"I'll start by sharing what I've found out about the underlying algorithms of Spotify Discover Weekly . Spotify actually uses 3 different algorithms to make its discover weekly playlist. Collaborative Filtering: similar users will like similar songs Natural Language Processing: blogs, articles, and lyrics in the songs can be used to model the songs Audio Processing: audio tracks themselves of the songs can be used to generate representations of the songs What algorithms did you all uncover?","title":"\ud83d\udcac 2"},{"location":"extras/X7_Machine_Learning_Overview/#different-kinds-of-machine-learning","text":"So we've recognized that machine learning is in a lot of different tools we use. We've noted the names to a few of those models. We've even defined machine learning in the context of AI. Now let's add some rigor to how we categorize the different kinds of ML models. Currently we think of ML in 3 different contexts: Supervised learning Unsupervised learning Reinforcement learning #### Supervised learning Perhaps the easiest of these to understand is supervised learning. In supervised learning we have _labeled data_, that is some kind of description about the data, that we usually denote as `X_train` and coinciding with a target value, `y_train`. We use the labeled dataset to train a model that then has the ability to predict new values of `y_test` for unlabeled, unseens data, `X_test`. Remember, `y_test` is not known, we are using the model to predict this! **Music Rating** **Supervised Learning - Classification** To continue with my music example, supervised learning in this context could consist of the following: you generate a bunch of labels or \"features\" that describe a song, e.g. `[Danceability, Valence, Energy, Tempo]` and the feature vector for each song would contain these series of numbers. So Coldplay's \"Clocks\" might be `clocks = [10.4, 40.5, 80, 120]` or Lil Nas X's \"Montero\" might be `montero = [15.4, 70.7, 90, 110]`. Then, you would label every song with a 1 or 0 indicating whether you liked it or not. You could then train a model on this labeled dataset that would predict whether or not you'd like a given song. Predicting 1 (you like it) or 0 (you don't like it), makes this a **_classification_** model. This is pretty close to what Pandora was actually doing early on in the music recommendation scene (they were creating the feature vectors by hand!). If you'd like to play with this idea yourself you can access _audio feature_ data with [Spotify's Developer's API](https://developer.spotify.com/console/get-audio-features-track/) **Supervised Learning - Regression** We can imagine a slightly different situation if, instead of labeling binary 1's and 0's for if we liked a song, we give it a score. If we attempt to map the song vectors to this _continuous_ value for _score_, we have ourselves a **_regression_** model. In practice, however, Spotify only knows whether or not we've _liked_ a song. So binary it is for now. **Housing Prices** > Let's switch gears and think about housing price data from Zillow. Let's also borrow language we encountered earlier on in our Data Science discussion: Nominal, Ordinal, Interval, and Ratio data. Envision a **_mixed_** dataset of **_continuous_** and **_discrete_** variables. Some features could be continuous, floating point values like neihborhood score and housing condition. Others could be discrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a **_regression_** model. To flip back to the **_classification_** discussion, we could, instead of reporting a value, report a recommendation to buy or sell by combinging the valuation with the actual bid or sell price. What we see is that the two forms of supervised learning, classification and regression, are not too different from one another. #### Unsupervised learning A little less intuitive, unsupervised learning does not require labeled datasets. Rather, it infers something about the data. **Unsupervised Learning - Clustering** Again taking the music example, Spotify's Collaborative Filterning model is an example of unsupervised learning. The math is a bit complex, but the general idea is that we construct a giant matrix of every song and every user filled with 1's or 0's indicating whether a user has liked the song or not. In this matrix space then, every song and every user is represented by a vector. We can use some mathematical tricks to compute vector distances, and, using this, identify similar users, in other words, we **_cluster_** them. The similar users then can be recommended to like each others songs * User 1 likes: \ud83c\udf4f, \ud83c\udf50, \ud83c\udf45, and \ud83c\udf46 * User 2 likes: \ud83c\udf4a, \ud83c\udf50, \ud83c\udf45, and \ud83c\udf46! User 1 should try \ud83c\udf4a and user 2 should try \ud83c\udf4f! Or take this example: * User 3 likes: \ud83e\udd54, \ud83c\udf53, \ud83c\udf4b, and \ud83c\udf51 * User 4 likes: \ud83e\udd6d, \ud83c\udf53, \ud83c\udf4b, and \ud83c\udf48 What should user 3 try and what should user 4 try? **Unsupervised Learning - Dimensionality Reduction** There is another category of unsupervised learning called **_dimensionality reduction_** that has powerful applications in feature engineering, outlier removal, and data visualization. One of the most common forms of reduction is principal component analysis. We won't go into great detail here, but if you are curious we visited this discussion over the summer in this [notebook](https://render.githubusercontent.com/view/ipynb?color_mode=light&commit=6e91cf4bfd71b7215a75cdb09d2b25dee45943f4&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f7765736c65796265636b6e65722f746563686e6f6c6f67795f66756e64616d656e74616c732f366539316366346266643731623732313561373563646230396432623235646565343539343366342f43332532304d616368696e652532304c6561726e696e67253230492f546563685f46756e5f43335f45315f556e737570657276697365645f4c6561726e696e672e6970796e62&nwo=wesleybeckner%2Ftechnology_fundamentals&path=C3+Machine+Learning+I%2FTech_Fun_C3_E1_Unsupervised_Learning.ipynb&repository_id=384268836&repository_type=Repository#3.1-Principal-Component-Analysis). To extrapolate our spotify music data discusion, imagine that we take that huge table of users and songs. What a dimensionality reduction process will do, is attempt to consolidate that table into fewer rows and columns while minimizing data loss. For example, perhaps everyone that like's Coldplay's Clocks, also likes Little Nas X's Montero, in that case, there would be no loss of information if instead of representing both songs in the table explicitly, we represented each one implicitly with a single variable. #### Reinforcement learning Reinforcement learning is a complex and blossoming field. The basic idea of reinforcement learning is that, instead of training a model on data, it trains within an _environment_. The environment of course, produces data; but it is different from supervised learning in that the learning algorithm must make a series of steps to get the \"right answer\". Because of this, reinforcement learning introduces concepts of _steps_ (the decision the algorithm makes at a point in time), _reward_ (the immediate benefit of that decision), _value estimation_ (the perceived overall value at the end of the simulation), and _policy_ (the mechanism by which we update the behavior of the model in subsequent expsoures to the environment). > The nuts and bolts of reinforcement learning is outside the scope of what we will discuss in our few sessions together, but it is good to define it alongside the other two topics: supervised and unsupervised learning! #### \ud83d\udcac 3 Take a moment to try to categorize the models you found within supervised or unsupervised machine learning! ## \ud83e\udd89 Tenets of Machine Learning We'll take the simple linear regression as an example and discuss some of the core tenets of ML: Bias-variance trade-off, irreducible error, and regularization. ### \ud83d\udcc8 Bias-Variance Trade-Off #### (Over and Underfitting) The basic premise here is that there's some optimum number of parmeters to include in my model, if I include too few, my model will be too simple (***high bias***) and if I include too many it will be too complex and fit to noise (***high variance***) We can explore this phenomenon more easily, making up some data ourselves: # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a known underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) Text(0, 0.5, 'Y') ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_22_1.png) Now, let's pretend we've sampled from this ***population*** of data: random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] plt . plot ( X_train , y_train , ls = '' , marker = '.' ) [ ] ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_24_1.png) Now let's take two extreme scenarios, fitting a linear line and a high order polynomial, to these datapoints. Keeping in mind the larger dataset, as well as the error we introduced in our data generating function, this will really illustrate our point! # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_26_1.png) We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has *high bias* because we are forcing the functional form without much consideration to the underlying data \u2014 we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has *low variance* with respect to the underlying data. On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has *low bias* because we don't introduce many constraints on the final form of the model. it is *high variance* because depending on the underlying training data, the final outcome of the model can change quite drastically! In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further: 1. what happens when we retrain these models on different samples of the data population * and let's use this to better understand what we mean by *bias* and *variance* 2. what happens when we tie this back in with the error we introduced to the data generator? * and let's use this to better understand irreducible error random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) for samples in range ( 5 ): X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_28_0.png) As we can see, depending on what data we train our model on, the *high bias* model changes relatively slightly, while the *high variance* model changes a whole awful lot! The *high variance* model is prone to something we call *overfitting*. It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the *population* # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_30_1.png) In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance model are particuarly prone to the phenomenon of *over fitting* and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this. ### \u2755 Irreducible Error Irreducible error is ***always*** present in our data. It is a part of life, welcome to it. That being said, let's look what happens when we *pretend* there isn't any irreducible error in our population data x = np . arange ( 20 , 100 ) y = [ func ( t , err = 0 ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [ ] ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_33_1.png) random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') ![png](X7_Machine_Learning_Overview_files/X7_Machine_Learning_Overview_34_1.png) This time, our high variance model really *gets it*! And this is because the data we trained on actually *is* a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the *bias variance tradeoff* ### \ud83d\udd78\ufe0f Regularization To talk about regularization, we're going to continue with our simple high bias model example, the much revered linear regression model. Linear regression takes on the form: $$y(x)= m\\cdot x + b$$ where $y$ is some target value and, $x$ is some feature; $m$ and $b$ are the slope and intercept, respectively. To solve the problem, we need to find the values of $b$ and $m$ in equation 1 to best fit the data. In linear regression our goal is to minimize the error between computed values of positions $y^{\\sf calc}(x_i)\\equiv y^{\\sf calc}_i$ and known values $y^{\\sf exact}(x_i)\\equiv y^{\\sf exact}_i$, i.e. find $b$ and $m$ which lead to lowest value of $$\\epsilon (m,b) =SS_{\\sf res}=\\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - m\\cdot x_i - b \\right)^2$$ **Now onto Regularization** There are many other regression algorithms, the two we want to highlight here are Ridge Regression and LASSO. They differ by an added term to the loss function. Let's review. The above equation expanded to multivariate form yields: $$\\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2$$ for Ridge regression, we add a **_regularization_** term known as **_L2_** regularization: $$\\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}\\beta_{j}^2$$ for **_LASSO_** (Least Absolute Shrinkage and Selection Operator) we add **_L1_** regularization: $$\\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}|\\beta_{j}|$$ The difference between the two is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. **_Elastic Net_** is a combination of these two regularization methods. The key notion here is that ***regularization*** is a way of tempering our model, allowing it to pick for itself the most appropriate features. This crops up in many places other than simple linear regression in machine learning. **Regularization appears in...** ***Ensemble learners*** (e.g. XGBoost and Random Forests) by combining the combinations of many weak algorithms ***Neural networks*** with ***dropout*** and ***batch normalization*** Dropout is the Neural Network response to the wide success of ensemble learning. In a dropout layer, random neurons are dropped in each batch of training, i.e. their weighted updates are not sent to the next neural layer. Just as with random forests, the end result is that the neural network can be thought of as many _independent models_ that _vote_ on the final output. Put another way, when a network does not contain dropout layers, and has a capacity that exceeds that which would be suited for the true, underlying complexity level of the data, it can begin to fit to noise. This ability to fit to noise is based on very specific relationships between neurons, which fire uniquely given the particular training example. Adding dropout _breaks_ these specific neural connections, and so the neural network as a whole is forced to find weights that apply generally, as there is no guarantee they will be _turned on_ when their specific training example they would usually overfit for comes around again. Network with 50% dropout. Borrowed from Kaggle learn.","title":"Different Kinds of Machine Learning"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/","text":"Data Science Foundations Lab 1: Data Hunt I \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's a data hunt. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from ipywidgets import interact df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_explorers/main/assets/imdb_movies.csv\" ) # converting years to numbers for easy conditionals df [ 'year' ] = pd . to_numeric ( df [ 'year' ], errors = 'coerce' ) df . shape /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) (85855, 22) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics 0 tt0000009 Miss Jerry Miss Jerry 1894.0 1894-10-09 Romance 45 USA None Alexander Black ... Blanche Bayliss, William Courtenay, Chauncey D... The adventures of a female reporter in the 1890s. 5.9 154 NaN NaN NaN NaN 1.0 2.0 1 tt0000574 The Story of the Kelly Gang The Story of the Kelly Gang 1906.0 1906-12-26 Biography, Crime, Drama 70 Australia None Charles Tait ... Elizabeth Tait, John Tait, Norman Campbell, Be... True story of notorious Australian outlaw Ned ... 6.1 589 $ 2250 NaN NaN NaN 7.0 7.0 2 tt0001892 Den sorte dr\u00f8m Den sorte dr\u00f8m 1911.0 1911-08-19 Drama 53 Germany, Denmark NaN Urban Gad ... Asta Nielsen, Valdemar Psilander, Gunnar Helse... Two men of high rank are both wooing the beaut... 5.8 188 NaN NaN NaN NaN 5.0 2.0 3 rows \u00d7 22 columns \ud83c\udfa5 L1 Q1 What american director has the highest mean avg_vote? \u00b6 director Daniel Keith, Snorri Sturluson 9.3 Anthony Bawn 9.3 Derek Ahonen 9.2 Raghav Peri 9.1 James Marlowe 8.8 ... Waleed Bedour 1.2 Fred Ashman 1.1 Aeneas Middleton 1.1 Steven A. Sandt 1.1 Francis Hamada 1.1 Name: avg_vote, Length: 12463, dtype: float64 \ud83c\udfa5 L1 Q2 What american director with more than 5 movies, has the highest mean avg_vote? \u00b6 director Quentin Tarantino 7.811111 Charles Chaplin 7.764286 David Fincher 7.625000 Billy Wilder 7.580952 Martin Scorsese 7.544444 ... Barry Mahon 2.728571 Dennis Devine 2.657143 Bill Zebub 2.483333 Mark Polonia 2.462500 Christopher Forbes 2.000000 Name: avg_vote, Length: 859, dtype: float64 \ud83c\udfa5 L1 Q3 What director has the largest variance in avg_vote? \u00b6 director Deniz Denizciler 4.030509 Rudi Lagemann 3.747666 Emilio Ruiz Barrachina 3.676955 Krishna Ghattamaneni 3.676955 Milos Avramovic 3.606245 ... \u00dcmit Degirmenci NaN \u00dcmit El\u00e7i NaN \u00dcmit K\u00f6reken NaN \u00deorsteinn Gunnar Bjarnason NaN \u00de\u00f3rhildur \u00deorleifsd\u00f3ttir NaN Name: avg_vote, Length: 34733, dtype: float64 \ud83c\udfa5 L1 Q4 What director with more than 10 movies has the largest variance in avg_vote? \u00b6 director Harry Baweja 1.869954 Shaji Kailas 1.854502 Zdenek Troska 1.775984 Adam Rifkin 1.711251 Ram Gopal Varma 1.687850 ... Ford Beebe 0.224343 Ray Nazarro 0.210311 Jean Gr\u00e9millon 0.196946 Louis Feuillade 0.156428 Tsutomu Shibayama 0.126121 Name: avg_vote, Length: 1135, dtype: float64 \ud83c\udfa5 L1 Q5 What american directors with more than 5 movies have the largest variance in avg_vote? \u00b6 director Martin Brest 2.033716 David Winters 1.926049 Adam Rifkin 1.711251 Gus Trikonis 1.661271 Jerry Jameson 1.646107 ... Edward Killy 0.155265 Willis Goldbeck 0.139443 Richard T. Heffron 0.136626 Bill Plympton 0.136626 Nate Watt 0.129099 Name: avg_vote, Length: 859, dtype: float64 \ud83c\udfa5 L1 Q6 Where does M. Night Shyamalan fall on this rank scale? \u00b6 (He's number 36/859) what happens when you only include directors who, on average (based on mean), have made most their movies after 1990 and have produced 10 or more movies? (Shyamalan rises to 3/83) \ud83c\udfa5 L1 Q7 How many movies were made each year in US from 2000-2020 \u00b6 year 2000.0 363 2001.0 386 2002.0 360 2003.0 339 2004.0 362 2005.0 453 2006.0 590 2007.0 574 2008.0 592 2009.0 656 2010.0 611 2011.0 652 2012.0 738 2013.0 820 2014.0 807 2015.0 800 2016.0 869 2017.0 905 2018.0 886 2019.0 700 2020.0 276 Name: title, dtype: int64 \ud83c\udfa5 L1 Q8 Visualize The Results of Q7! \u00b6 <matplotlib.axes._subplots.AxesSubplot at 0x7fea042dc890> \ud83c\udfa5 L1 Q9 For single country movies, how many movies were made each year in each country from 2000-2020, only include countries that made more than 1000 movies in that timeframe \u00b6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year title 0 Canada 2000.0 39 1 Canada 2001.0 51 2 Canada 2002.0 49 3 Canada 2003.0 38 4 Canada 2004.0 52 \ud83c\udfa5 L1 Q10 Visualize the results from Q9! \u00b6","title":"Descriptive Statistics Data Hunt"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#data-science-foundations-lab-1-data-hunt-i","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's a data hunt. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from ipywidgets import interact df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_explorers/main/assets/imdb_movies.csv\" ) # converting years to numbers for easy conditionals df [ 'year' ] = pd . to_numeric ( df [ 'year' ], errors = 'coerce' ) df . shape /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) (85855, 22) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics 0 tt0000009 Miss Jerry Miss Jerry 1894.0 1894-10-09 Romance 45 USA None Alexander Black ... Blanche Bayliss, William Courtenay, Chauncey D... The adventures of a female reporter in the 1890s. 5.9 154 NaN NaN NaN NaN 1.0 2.0 1 tt0000574 The Story of the Kelly Gang The Story of the Kelly Gang 1906.0 1906-12-26 Biography, Crime, Drama 70 Australia None Charles Tait ... Elizabeth Tait, John Tait, Norman Campbell, Be... True story of notorious Australian outlaw Ned ... 6.1 589 $ 2250 NaN NaN NaN 7.0 7.0 2 tt0001892 Den sorte dr\u00f8m Den sorte dr\u00f8m 1911.0 1911-08-19 Drama 53 Germany, Denmark NaN Urban Gad ... Asta Nielsen, Valdemar Psilander, Gunnar Helse... Two men of high rank are both wooing the beaut... 5.8 188 NaN NaN NaN NaN 5.0 2.0 3 rows \u00d7 22 columns","title":"Data Science Foundations  Lab 1: Data Hunt I"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q1-what-american-director-has-the-highest-mean-avg_vote","text":"director Daniel Keith, Snorri Sturluson 9.3 Anthony Bawn 9.3 Derek Ahonen 9.2 Raghav Peri 9.1 James Marlowe 8.8 ... Waleed Bedour 1.2 Fred Ashman 1.1 Aeneas Middleton 1.1 Steven A. Sandt 1.1 Francis Hamada 1.1 Name: avg_vote, Length: 12463, dtype: float64","title":"\ud83c\udfa5 L1 Q1 What american director has the highest mean  avg_vote?"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q2-what-american-director-with-more-than-5-movies-has-the-highest-mean-avg_vote","text":"director Quentin Tarantino 7.811111 Charles Chaplin 7.764286 David Fincher 7.625000 Billy Wilder 7.580952 Martin Scorsese 7.544444 ... Barry Mahon 2.728571 Dennis Devine 2.657143 Bill Zebub 2.483333 Mark Polonia 2.462500 Christopher Forbes 2.000000 Name: avg_vote, Length: 859, dtype: float64","title":"\ud83c\udfa5 L1 Q2 What american director with more than 5 movies, has the highest mean avg_vote?"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q3-what-director-has-the-largest-variance-in-avg_vote","text":"director Deniz Denizciler 4.030509 Rudi Lagemann 3.747666 Emilio Ruiz Barrachina 3.676955 Krishna Ghattamaneni 3.676955 Milos Avramovic 3.606245 ... \u00dcmit Degirmenci NaN \u00dcmit El\u00e7i NaN \u00dcmit K\u00f6reken NaN \u00deorsteinn Gunnar Bjarnason NaN \u00de\u00f3rhildur \u00deorleifsd\u00f3ttir NaN Name: avg_vote, Length: 34733, dtype: float64","title":"\ud83c\udfa5 L1 Q3 What director has the largest variance in avg_vote?"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q4-what-director-with-more-than-10-movies-has-the-largest-variance-in-avg_vote","text":"director Harry Baweja 1.869954 Shaji Kailas 1.854502 Zdenek Troska 1.775984 Adam Rifkin 1.711251 Ram Gopal Varma 1.687850 ... Ford Beebe 0.224343 Ray Nazarro 0.210311 Jean Gr\u00e9millon 0.196946 Louis Feuillade 0.156428 Tsutomu Shibayama 0.126121 Name: avg_vote, Length: 1135, dtype: float64","title":"\ud83c\udfa5 L1 Q4 What director with more than 10 movies has the largest variance in avg_vote?"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q5-what-american-directors-with-more-than-5-movies-have-the-largest-variance-in-avg_vote","text":"director Martin Brest 2.033716 David Winters 1.926049 Adam Rifkin 1.711251 Gus Trikonis 1.661271 Jerry Jameson 1.646107 ... Edward Killy 0.155265 Willis Goldbeck 0.139443 Richard T. Heffron 0.136626 Bill Plympton 0.136626 Nate Watt 0.129099 Name: avg_vote, Length: 859, dtype: float64","title":"\ud83c\udfa5 L1 Q5 What american directors with more than 5 movies have the largest variance in avg_vote?"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q6-where-does-m-night-shyamalan-fall-on-this-rank-scale","text":"(He's number 36/859) what happens when you only include directors who, on average (based on mean), have made most their movies after 1990 and have produced 10 or more movies? (Shyamalan rises to 3/83)","title":"\ud83c\udfa5 L1 Q6 Where does M. Night Shyamalan fall on this rank scale?"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q7-how-many-movies-were-made-each-year-in-us-from-2000-2020","text":"year 2000.0 363 2001.0 386 2002.0 360 2003.0 339 2004.0 362 2005.0 453 2006.0 590 2007.0 574 2008.0 592 2009.0 656 2010.0 611 2011.0 652 2012.0 738 2013.0 820 2014.0 807 2015.0 800 2016.0 869 2017.0 905 2018.0 886 2019.0 700 2020.0 276 Name: title, dtype: int64","title":"\ud83c\udfa5 L1 Q7 How many movies were made each year in US from 2000-2020"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q8-visualize-the-results-of-q7","text":"<matplotlib.axes._subplots.AxesSubplot at 0x7fea042dc890>","title":"\ud83c\udfa5 L1 Q8 Visualize The Results of Q7!"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q9-for-single-country-movies-how-many-movies-were-made-each-year-in-each-country-from-2000-2020-only-include-countries-that-made-more-than-1000-movies-in-that-timeframe","text":".dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year title 0 Canada 2000.0 39 1 Canada 2001.0 51 2 Canada 2002.0 49 3 Canada 2003.0 38 4 Canada 2004.0 52","title":"\ud83c\udfa5 L1 Q9 For single country movies, how many movies were made each year in each country from 2000-2020, only include countries that made more than 1000 movies in that timeframe"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q10-visualize-the-results-from-q9","text":"","title":"\ud83c\udfa5 L1 Q10 Visualize the results from Q9!"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/","text":"Data Science Foundations Lab 2: Data Hunt II \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's the data hunt part TWO. Preparing Environment and Importing Data \u00b6 Import Packages \u00b6 ! pip install - U plotly Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1) Collecting plotly Downloading plotly-5.1.0-py2.py3-none-any.whl (20.6 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.6 MB 1.3 MB/s \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0) Collecting tenacity>=6.2.0 Downloading tenacity-8.0.1-py3-none-any.whl (24 kB) Installing collected packages: tenacity, plotly Attempting uninstall: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-5.1.0 tenacity-8.0.1 # our standard libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns from ipywidgets import interact # our stats libraries import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy # our scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score Import and Clean Data \u00b6 df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_fundamentals/main/assets/truffle_rates.csv\" ) df = df . loc [ df [ 'rate' ] > 0 ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group rate 0 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.167097 1 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.153827 2 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.100299 3 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.333008 4 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.078108 df . shape (9210, 6) Exploratory Data Analysis \u00b6 \ud83c\udf6b L2 Q1 Finding Influential Features \u00b6 Which of the five features (base_cake, truffle_type, primary_flavor, secondary_flavor, color_group) of the truffles is most influential on production rate? Back your answer with both a visualization of the distributions (boxplot, kernel denisty estimate, histogram, violin plot) and a statistical test (moods median, ANOVA, t-test) Be sure: everything is labeled (can you improve your labels with additional descriptive statistical information e.g. indicate mean, std, etc.) you meet the assumptions of your statistical test \ud83c\udf6b L2 Q1.1 Visualization \u00b6 Use any number of visualizations. Here is an example to get you started: # Example: a KDE of the truffle_type and base_cake columns fig , ax = plt . subplots ( 2 , 1 , figsize = ( 12 , 12 )) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'truffle_type' ], fill = True , ax = ax [ 0 ]) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'base_cake' ], fill = True , ax = ax [ 1 ]) <matplotlib.axes._subplots.AxesSubplot at 0x7f549eea03d0> /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 9 missing from current font. /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 9 missing from current font. \ud83c\udf6b L2 Q1.2 Statistical Analysis \u00b6 What statistical tests can you perform to evaluate your hypothesis from the visualizations (maybe you think one particular feature is significant). Here's an ANOVA on the truffle_type column to get you started: model = ols ( 'rate ~ C( {} )' . format ( 'truffle_type' ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Is this P value significant? What is the null hypothesis? How do we check the assumptions of ANOVA? base_cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(base_cake) 331.373550 5.0 2349.684756 0.0 Residual 259.606073 9204.0 NaN NaN Shapiro: 0.9281061887741089 0.0 Bartlett: 619.3727153356931 1.3175663824168166e-131 truffle_type /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Shapiro: 0.9645588994026184 1.3704698981096711e-42 /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Bartlett: 533.0206680979852 1.8031528902362296e-116 primary_flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(primary_flavor) 159.105452 47.0 71.815842 0.0 Residual 431.874171 9162.0 NaN NaN /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Shapiro: 0.9738250970840454 6.485387538059916e-38 Bartlett: 1609.0029005171464 1.848613457353585e-306 secondary_flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(secondary_flavor) 115.773877 28.0 79.884192 0.0 Residual 475.205747 9181.0 NaN NaN Shapiro: 0.9717048406600952 4.3392384038527993e-39 Bartlett: 1224.4882890761903 3.5546073028894766e-240 color_group /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(color_group) 33.878491 11.0 50.849974 1.873235e-109 Residual 557.101132 9198.0 NaN NaN Shapiro: 0.9598756432533264 1.401298464324817e-44 Bartlett: 298.6432027161358 1.6917844519244488e-57 /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. \ud83c\udf6b L2 Q2 Finding Best and Worst Groups \u00b6 \ud83c\udf6b L2 Q2.1 Compare Every Group to the Whole \u00b6 Of the primary flavors (feature), what 5 flavors (groups) would you recommend Truffletopia discontinue? Iterate through every level (i.e. pound, cheese, sponge cakes) of every category (i.e. base cake, primary flavor, secondary flavor) and use moods median testing to compare the group distribution to the grand median rate. (98, 10) After you've computed a moods median test on every group, filter any data above a significance level of 0.05 (76, 10) Return the groups with the lowest median performance (your table need not look exactly like the one I've created) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 primary_flavor Coconut 56.8675 4.66198e-14 0.310345 0.139998 0.0856284 100 2.64572e-29 [[12, 4593], [88, 4517]] 1 secondary_flavor Wild Cherry Cream 56.8675 4.66198e-14 0.310345 0.139998 0.0856284 100 2.64572e-29 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.5563 4.30253e-15 0.310345 0.129178 0.0928782 85 2.05798e-28 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.3203 7.84617e-13 0.310345 0.145727 0.0957584 91 1.11719e-28 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.5452 4.14269e-11 0.310345 0.148964 0.10588 70 2.59384e-20 [[7, 4598], [63, 4542]] 5 secondary_flavor Mixed Berry 164.099 1.43951e-37 0.310345 0.153713 0.115202 261 6.73636e-75 [[28, 4577], [233, 4372]] 6 secondary_flavor Peppermint 66.0235 4.45582e-16 0.310345 0.129107 0.12201 86 7.6449e-37 [[5, 4600], [81, 4524]] 7 base_cake Butter 696.649 1.60093e-153 0.310345 0.15951 0.136231 905 0 [[75, 4530], [830, 3775]] 8 secondary_flavor Rum 69.5192 7.56747e-17 0.310345 0.157568 0.139834 93 4.42643e-42 [[6, 4599], [87, 4518]] 9 secondary_flavor Cucumber 175.061 5.80604e-40 0.310345 0.170015 0.14097 288 4.33234e-79 [[33, 4572], [255, 4350]] 10 primary_flavor Gingersnap 131.114 2.33844e-30 0.310345 0.159268 0.143347 192 1.01371e-69 [[17, 4588], [175, 4430]] 11 primary_flavor Cherry Cream Spice 66.3302 3.81371e-16 0.310345 0.175751 0.146272 100 1.03408e-36 [[9, 4596], [91, 4514]] 12 primary_flavor Orange Brandy 97.0624 6.71776e-23 0.310345 0.185908 0.157804 186 1.86398e-49 [[26, 4579], [160, 4445]] 13 primary_flavor Irish Cream 87.5008 8.42448e-21 0.310345 0.184505 0.176935 151 6.30631e-52 [[18, 4587], [133, 4472]] 14 base_cake Chiffon 908.383 1.47733e-199 0.310345 0.208286 0.177773 1821 0 [[334, 4271], [1487, 3118]] 15 primary_flavor Ginger Lime 40.1257 2.38138e-10 0.310345 0.225157 0.181094 100 2.63308e-18 [[18, 4587], [82, 4523]] 16 primary_flavor Doughnut 98.4088 3.40338e-23 0.310345 0.234113 0.189888 300 1.03666e-40 [[65, 4540], [235, 4370]] 17 primary_flavor Butter Milk 28.3983 9.87498e-08 0.310345 0.237502 0.190708 100 1.96333e-20 [[23, 4582], [77, 4528]] 18 primary_flavor Pecan 40.8441 1.64868e-10 0.310345 0.197561 0.192372 89 2.86564e-25 [[14, 4591], [75, 4530]] 19 secondary_flavor Dill Pickle 69.8101 6.52964e-17 0.310345 0.228289 0.19916 241 6.39241e-33 [[56, 4549], [185, 4420]] We would want to cut the following primary flavors. Check to see that you get a similar answer. rip wild cherry cream. ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] \ud83c\udf6b L2 Q2.2 Beyond Statistical Testing: Using Reasoning \u00b6 Let's look at the total profile of the products associated with the five worst primary flavors. Given the number of different products made with any of these flavors, would you alter your answer at all? # 1. filter df for only bottom five flavors # 2. groupby all columns besides rate # 3. describe the rate column. # by doing this we can evaluate just how much sampling variety we have for the # worst performing flavors. bottom_five = [ 'Coconut' , 'Pink Lemonade' , 'Chocolate' , 'Wild Cherry Cream' , 'Gingersnap' ] df . loc [ df [ 'primary_flavor' ] . isin ( bottom_five )] . groupby ( list ( df . columns [: - 1 ]))[ 'rate' ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max base_cake truffle_type primary_flavor secondary_flavor color_group Butter Jelly Filled Pink Lemonade Butter Rum Rose 85.0 0.129178 0.137326 0.000061 0.032887 0.092878 0.171350 0.860045 Chiffon Candy Outer Wild Cherry Cream Rock and Rye Olive 17.0 0.094287 0.059273 0.010464 0.053976 0.077098 0.120494 0.229933 Chocolate Outer Gingersnap Dill Pickle Burgundy 59.0 0.133272 0.080414 0.021099 0.069133 0.137972 0.172066 0.401387 Jelly Filled Chocolate Tutti Frutti Burgundy 91.0 0.145727 0.135230 0.000033 0.044847 0.095758 0.185891 0.586570 Pound Candy Outer Coconut Wild Cherry Cream Taupe 100.0 0.139998 0.147723 0.000705 0.036004 0.085628 0.187318 0.775210 Chocolate Outer Gingersnap Rock and Rye Black 67.0 0.156160 0.110666 0.002846 0.074615 0.139572 0.241114 0.551898 Jelly Filled Gingersnap Kiwi Taupe 66.0 0.185662 0.132272 0.000014 0.086377 0.166340 0.247397 0.593016 Wild Cherry Cream Mango Taupe 53.0 0.166502 0.160090 0.001412 0.056970 0.108918 0.207306 0.787224 \ud83c\udf6b L2 Q2.3 The Jelly Filled Conundrum \u00b6 Your boss notices the Jelly filled truffles are being produced much faster than the candy outer truffles and suggests expanding into this product line. What is your response? Use the visualization tool below to help you think about this problem, then create any visualizations or analyses of your own. sunburst charts def sun ( path = [[ 'base_cake' , 'truffle_type' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ], [ 'truffle_type' , 'base_cake' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ]]): fig = px . sunburst ( df , path = path , color = 'rate' , color_continuous_scale = 'viridis' , ) fig . update_layout ( margin = dict ( l = 20 , r = 20 , t = 20 , b = 20 ), height = 650 ) fig . show () interact ( sun ) interactive(children=(Dropdown(description='path', options=(['base_cake', 'truffle_type', 'primary_flavor', 's\u2026 <function __main__.sun>","title":"Inferential Statistics Data Hunt"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#data-science-foundations-lab-2-data-hunt-ii","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's the data hunt part TWO.","title":"Data Science Foundations  Lab 2: Data Hunt II"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#preparing-environment-and-importing-data","text":"","title":"Preparing Environment and Importing Data"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#import-packages","text":"! pip install - U plotly Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1) Collecting plotly Downloading plotly-5.1.0-py2.py3-none-any.whl (20.6 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.6 MB 1.3 MB/s \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0) Collecting tenacity>=6.2.0 Downloading tenacity-8.0.1-py3-none-any.whl (24 kB) Installing collected packages: tenacity, plotly Attempting uninstall: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-5.1.0 tenacity-8.0.1 # our standard libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns from ipywidgets import interact # our stats libraries import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy # our scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score","title":"Import Packages"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#import-and-clean-data","text":"df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_fundamentals/main/assets/truffle_rates.csv\" ) df = df . loc [ df [ 'rate' ] > 0 ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group rate 0 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.167097 1 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.153827 2 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.100299 3 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.333008 4 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.078108 df . shape (9210, 6)","title":"Import and Clean Data"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#exploratory-data-analysis","text":"","title":"Exploratory Data Analysis"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q1-finding-influential-features","text":"Which of the five features (base_cake, truffle_type, primary_flavor, secondary_flavor, color_group) of the truffles is most influential on production rate? Back your answer with both a visualization of the distributions (boxplot, kernel denisty estimate, histogram, violin plot) and a statistical test (moods median, ANOVA, t-test) Be sure: everything is labeled (can you improve your labels with additional descriptive statistical information e.g. indicate mean, std, etc.) you meet the assumptions of your statistical test","title":"\ud83c\udf6b L2 Q1 Finding Influential Features"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q11-visualization","text":"Use any number of visualizations. Here is an example to get you started: # Example: a KDE of the truffle_type and base_cake columns fig , ax = plt . subplots ( 2 , 1 , figsize = ( 12 , 12 )) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'truffle_type' ], fill = True , ax = ax [ 0 ]) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'base_cake' ], fill = True , ax = ax [ 1 ]) <matplotlib.axes._subplots.AxesSubplot at 0x7f549eea03d0> /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 9 missing from current font. /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 9 missing from current font.","title":"\ud83c\udf6b L2 Q1.1 Visualization"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q12-statistical-analysis","text":"What statistical tests can you perform to evaluate your hypothesis from the visualizations (maybe you think one particular feature is significant). Here's an ANOVA on the truffle_type column to get you started: model = ols ( 'rate ~ C( {} )' . format ( 'truffle_type' ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Is this P value significant? What is the null hypothesis? How do we check the assumptions of ANOVA? base_cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(base_cake) 331.373550 5.0 2349.684756 0.0 Residual 259.606073 9204.0 NaN NaN Shapiro: 0.9281061887741089 0.0 Bartlett: 619.3727153356931 1.3175663824168166e-131 truffle_type /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Shapiro: 0.9645588994026184 1.3704698981096711e-42 /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Bartlett: 533.0206680979852 1.8031528902362296e-116 primary_flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(primary_flavor) 159.105452 47.0 71.815842 0.0 Residual 431.874171 9162.0 NaN NaN /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Shapiro: 0.9738250970840454 6.485387538059916e-38 Bartlett: 1609.0029005171464 1.848613457353585e-306 secondary_flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(secondary_flavor) 115.773877 28.0 79.884192 0.0 Residual 475.205747 9181.0 NaN NaN Shapiro: 0.9717048406600952 4.3392384038527993e-39 Bartlett: 1224.4882890761903 3.5546073028894766e-240 color_group /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(color_group) 33.878491 11.0 50.849974 1.873235e-109 Residual 557.101132 9198.0 NaN NaN Shapiro: 0.9598756432533264 1.401298464324817e-44 Bartlett: 298.6432027161358 1.6917844519244488e-57 /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000.","title":"\ud83c\udf6b L2 Q1.2 Statistical Analysis"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q2-finding-best-and-worst-groups","text":"","title":"\ud83c\udf6b L2 Q2 Finding Best and Worst Groups"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q21-compare-every-group-to-the-whole","text":"Of the primary flavors (feature), what 5 flavors (groups) would you recommend Truffletopia discontinue? Iterate through every level (i.e. pound, cheese, sponge cakes) of every category (i.e. base cake, primary flavor, secondary flavor) and use moods median testing to compare the group distribution to the grand median rate. (98, 10) After you've computed a moods median test on every group, filter any data above a significance level of 0.05 (76, 10) Return the groups with the lowest median performance (your table need not look exactly like the one I've created) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 primary_flavor Coconut 56.8675 4.66198e-14 0.310345 0.139998 0.0856284 100 2.64572e-29 [[12, 4593], [88, 4517]] 1 secondary_flavor Wild Cherry Cream 56.8675 4.66198e-14 0.310345 0.139998 0.0856284 100 2.64572e-29 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.5563 4.30253e-15 0.310345 0.129178 0.0928782 85 2.05798e-28 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.3203 7.84617e-13 0.310345 0.145727 0.0957584 91 1.11719e-28 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.5452 4.14269e-11 0.310345 0.148964 0.10588 70 2.59384e-20 [[7, 4598], [63, 4542]] 5 secondary_flavor Mixed Berry 164.099 1.43951e-37 0.310345 0.153713 0.115202 261 6.73636e-75 [[28, 4577], [233, 4372]] 6 secondary_flavor Peppermint 66.0235 4.45582e-16 0.310345 0.129107 0.12201 86 7.6449e-37 [[5, 4600], [81, 4524]] 7 base_cake Butter 696.649 1.60093e-153 0.310345 0.15951 0.136231 905 0 [[75, 4530], [830, 3775]] 8 secondary_flavor Rum 69.5192 7.56747e-17 0.310345 0.157568 0.139834 93 4.42643e-42 [[6, 4599], [87, 4518]] 9 secondary_flavor Cucumber 175.061 5.80604e-40 0.310345 0.170015 0.14097 288 4.33234e-79 [[33, 4572], [255, 4350]] 10 primary_flavor Gingersnap 131.114 2.33844e-30 0.310345 0.159268 0.143347 192 1.01371e-69 [[17, 4588], [175, 4430]] 11 primary_flavor Cherry Cream Spice 66.3302 3.81371e-16 0.310345 0.175751 0.146272 100 1.03408e-36 [[9, 4596], [91, 4514]] 12 primary_flavor Orange Brandy 97.0624 6.71776e-23 0.310345 0.185908 0.157804 186 1.86398e-49 [[26, 4579], [160, 4445]] 13 primary_flavor Irish Cream 87.5008 8.42448e-21 0.310345 0.184505 0.176935 151 6.30631e-52 [[18, 4587], [133, 4472]] 14 base_cake Chiffon 908.383 1.47733e-199 0.310345 0.208286 0.177773 1821 0 [[334, 4271], [1487, 3118]] 15 primary_flavor Ginger Lime 40.1257 2.38138e-10 0.310345 0.225157 0.181094 100 2.63308e-18 [[18, 4587], [82, 4523]] 16 primary_flavor Doughnut 98.4088 3.40338e-23 0.310345 0.234113 0.189888 300 1.03666e-40 [[65, 4540], [235, 4370]] 17 primary_flavor Butter Milk 28.3983 9.87498e-08 0.310345 0.237502 0.190708 100 1.96333e-20 [[23, 4582], [77, 4528]] 18 primary_flavor Pecan 40.8441 1.64868e-10 0.310345 0.197561 0.192372 89 2.86564e-25 [[14, 4591], [75, 4530]] 19 secondary_flavor Dill Pickle 69.8101 6.52964e-17 0.310345 0.228289 0.19916 241 6.39241e-33 [[56, 4549], [185, 4420]] We would want to cut the following primary flavors. Check to see that you get a similar answer. rip wild cherry cream. ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap']","title":"\ud83c\udf6b L2 Q2.1 Compare Every Group to the Whole"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q22-beyond-statistical-testing-using-reasoning","text":"Let's look at the total profile of the products associated with the five worst primary flavors. Given the number of different products made with any of these flavors, would you alter your answer at all? # 1. filter df for only bottom five flavors # 2. groupby all columns besides rate # 3. describe the rate column. # by doing this we can evaluate just how much sampling variety we have for the # worst performing flavors. bottom_five = [ 'Coconut' , 'Pink Lemonade' , 'Chocolate' , 'Wild Cherry Cream' , 'Gingersnap' ] df . loc [ df [ 'primary_flavor' ] . isin ( bottom_five )] . groupby ( list ( df . columns [: - 1 ]))[ 'rate' ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max base_cake truffle_type primary_flavor secondary_flavor color_group Butter Jelly Filled Pink Lemonade Butter Rum Rose 85.0 0.129178 0.137326 0.000061 0.032887 0.092878 0.171350 0.860045 Chiffon Candy Outer Wild Cherry Cream Rock and Rye Olive 17.0 0.094287 0.059273 0.010464 0.053976 0.077098 0.120494 0.229933 Chocolate Outer Gingersnap Dill Pickle Burgundy 59.0 0.133272 0.080414 0.021099 0.069133 0.137972 0.172066 0.401387 Jelly Filled Chocolate Tutti Frutti Burgundy 91.0 0.145727 0.135230 0.000033 0.044847 0.095758 0.185891 0.586570 Pound Candy Outer Coconut Wild Cherry Cream Taupe 100.0 0.139998 0.147723 0.000705 0.036004 0.085628 0.187318 0.775210 Chocolate Outer Gingersnap Rock and Rye Black 67.0 0.156160 0.110666 0.002846 0.074615 0.139572 0.241114 0.551898 Jelly Filled Gingersnap Kiwi Taupe 66.0 0.185662 0.132272 0.000014 0.086377 0.166340 0.247397 0.593016 Wild Cherry Cream Mango Taupe 53.0 0.166502 0.160090 0.001412 0.056970 0.108918 0.207306 0.787224","title":"\ud83c\udf6b L2 Q2.2 Beyond Statistical Testing: Using Reasoning"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q23-the-jelly-filled-conundrum","text":"Your boss notices the Jelly filled truffles are being produced much faster than the candy outer truffles and suggests expanding into this product line. What is your response? Use the visualization tool below to help you think about this problem, then create any visualizations or analyses of your own. sunburst charts def sun ( path = [[ 'base_cake' , 'truffle_type' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ], [ 'truffle_type' , 'base_cake' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ]]): fig = px . sunburst ( df , path = path , color = 'rate' , color_continuous_scale = 'viridis' , ) fig . update_layout ( margin = dict ( l = 20 , r = 20 , t = 20 , b = 20 ), height = 650 ) fig . show () interact ( sun ) interactive(children=(Dropdown(description='path', options=(['base_cake', 'truffle_type', 'primary_flavor', 's\u2026 <function __main__.sun>","title":"\ud83c\udf6b L2 Q2.3 The Jelly Filled Conundrum"},{"location":"labs/L3_Feature_Engineering/","text":"Data Science Foundations Lab 3: Practice with Feature Engineering and Pipelines \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines and feature engineering. We will use the wine dataset. import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn.model_selection import train_test_split from statsmodels.stats.outliers_influence import variance_inflation_factor wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) \ud83c\udf47 L3 Q1: Feature Derivation \u00b6 Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 One-Hot encode categorical variables in the wine dataset # Code Cell for L1 Q1 display ( wine . head ()) print ( wine . shape ) str_cols = [ 'type' ] enc = OneHotEncoder () imp = SimpleImputer () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 (6497, 13) (6497,) (6497, 13) \ud83c\udf7e L3 Q2: Feature Transformation \u00b6 Use StandardScaler on the input data and evaluate how this affects VIF, kurtosis, and skew You should ignore the one-hot encoded column(s) for this section # Non-one hot encoded columns cols = list ( wine . columns ) cols . remove ( 'density' ) cols . remove ( 'type' ) # Code Cell for L1 Q2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 41.790949 5.070143 1.724131 volatile acidity 9.482732 2.834263 1.496433 citric acid 9.344218 2.404077 0.473142 residual sugar 3.336944 4.360399 1.435221 chlorides 5.398369 50.911457 5.400680 free sulfur dioxide 8.529778 7.906238 1.220066 total sulfur dioxide 13.448130 -0.371664 -0.001177 pH 149.003349 0.374743 0.387234 sulphates 18.402953 8.667071 1.799021 alcohol 114.836088 -0.531687 0.565718 quality 63.455488 0.232322 0.189623 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 1.781336 5.070143 1.724131 volatile acidity 1.808525 2.834263 1.496433 citric acid 1.606484 2.404077 0.473142 residual sugar 1.533403 4.360399 1.435221 chlorides 1.564413 50.911457 5.400680 free sulfur dioxide 2.156598 7.906238 1.220066 total sulfur dioxide 2.872586 -0.371664 -0.001177 pH 1.413100 0.374743 0.387234 sulphates 1.364157 8.667071 1.799021 alcohol 1.696986 -0.531687 0.565718 quality 1.408210 0.232322 0.189623 \ud83c\udf77 L3 Q3: Modeling \u00b6 Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using logistic regression: dependent variable: wine quality If you are using linear regression: dependent variable: wine density # Code Cell for L1 Q3 model = LinearRegression () X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) Text(0.5, 1.0, 'Test, R2: 0.963')","title":"Practice with Feature Engineering"},{"location":"labs/L3_Feature_Engineering/#data-science-foundations-lab-3-practice-with-feature-engineering-and-pipelines","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines and feature engineering. We will use the wine dataset. import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn.model_selection import train_test_split from statsmodels.stats.outliers_influence import variance_inflation_factor wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" )","title":"Data Science Foundations  Lab 3: Practice with Feature Engineering and Pipelines"},{"location":"labs/L3_Feature_Engineering/#l3-q1-feature-derivation","text":"Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 One-Hot encode categorical variables in the wine dataset # Code Cell for L1 Q1 display ( wine . head ()) print ( wine . shape ) str_cols = [ 'type' ] enc = OneHotEncoder () imp = SimpleImputer () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 (6497, 13) (6497,) (6497, 13)","title":"\ud83c\udf47 L3 Q1: Feature Derivation"},{"location":"labs/L3_Feature_Engineering/#l3-q2-feature-transformation","text":"Use StandardScaler on the input data and evaluate how this affects VIF, kurtosis, and skew You should ignore the one-hot encoded column(s) for this section # Non-one hot encoded columns cols = list ( wine . columns ) cols . remove ( 'density' ) cols . remove ( 'type' ) # Code Cell for L1 Q2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 41.790949 5.070143 1.724131 volatile acidity 9.482732 2.834263 1.496433 citric acid 9.344218 2.404077 0.473142 residual sugar 3.336944 4.360399 1.435221 chlorides 5.398369 50.911457 5.400680 free sulfur dioxide 8.529778 7.906238 1.220066 total sulfur dioxide 13.448130 -0.371664 -0.001177 pH 149.003349 0.374743 0.387234 sulphates 18.402953 8.667071 1.799021 alcohol 114.836088 -0.531687 0.565718 quality 63.455488 0.232322 0.189623 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 1.781336 5.070143 1.724131 volatile acidity 1.808525 2.834263 1.496433 citric acid 1.606484 2.404077 0.473142 residual sugar 1.533403 4.360399 1.435221 chlorides 1.564413 50.911457 5.400680 free sulfur dioxide 2.156598 7.906238 1.220066 total sulfur dioxide 2.872586 -0.371664 -0.001177 pH 1.413100 0.374743 0.387234 sulphates 1.364157 8.667071 1.799021 alcohol 1.696986 -0.531687 0.565718 quality 1.408210 0.232322 0.189623","title":"\ud83c\udf7e L3 Q2: Feature Transformation"},{"location":"labs/L3_Feature_Engineering/#l3-q3-modeling","text":"Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using logistic regression: dependent variable: wine quality If you are using linear regression: dependent variable: wine density # Code Cell for L1 Q3 model = LinearRegression () X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) Text(0.5, 1.0, 'Test, R2: 0.963')","title":"\ud83c\udf77 L3 Q3: Modeling"},{"location":"labs/L4_Supervised_Learners/","text":"Data Science Foundations Lab 4: Practice with Supervised Learners \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines, feature engineering, and applying learning algorithms. Now that we have covered supervised learning methods, and we've covered Grid Search, we will use these tools to do a sophisticated, search of hyperparameter optimization. import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression , LinearRegression import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.model_selection import train_test_split from sklearn import metrics wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'quality' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ target ] = y to compare, here is our results performing classification on this set of data with just logistic regression: model = LogisticRegression ( max_iter = 1e4 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.60 0.07 0.12 46 5 0.58 0.61 0.59 420 6 0.52 0.68 0.59 579 7 0.44 0.19 0.26 221 8 0.00 0.00 0.00 32 accuracy 0.54 1300 macro avg 0.36 0.26 0.26 1300 weighted avg 0.51 0.54 0.50 1300 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) <AxesSubplot:> \ud83c\udfce\ufe0f L4 Q1: \u00b6 Evaluate the performance of a Random Forest on classifying wine quality # Code Cell for L1 Q2 \ud83d\udd2c L4 Q2: \u00b6 Do a grid search to optimize your Random Forest model, use whatever hyperparameters you would like RandomForestClassifier () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Code Cell for L1 Q3 from sklearn.model_selection import GridSearchCV param_grid = { 'bootstrap' : [ True , False ]} grid = GridSearchCV ( RandomForestClassifier (), param_grid , cv = 7 )","title":"Practice with Supervised Learners"},{"location":"labs/L4_Supervised_Learners/#data-science-foundations-lab-4-practice-with-supervised-learners","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines, feature engineering, and applying learning algorithms. Now that we have covered supervised learning methods, and we've covered Grid Search, we will use these tools to do a sophisticated, search of hyperparameter optimization. import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression , LinearRegression import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.model_selection import train_test_split from sklearn import metrics wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'quality' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ target ] = y to compare, here is our results performing classification on this set of data with just logistic regression: model = LogisticRegression ( max_iter = 1e4 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.60 0.07 0.12 46 5 0.58 0.61 0.59 420 6 0.52 0.68 0.59 579 7 0.44 0.19 0.26 221 8 0.00 0.00 0.00 32 accuracy 0.54 1300 macro avg 0.36 0.26 0.26 1300 weighted avg 0.51 0.54 0.50 1300 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) <AxesSubplot:>","title":"Data Science Foundations  Lab 4: Practice with Supervised Learners"},{"location":"labs/L4_Supervised_Learners/#l4-q1","text":"Evaluate the performance of a Random Forest on classifying wine quality # Code Cell for L1 Q2","title":"\ud83c\udfce\ufe0f L4 Q1:"},{"location":"labs/L4_Supervised_Learners/#l4-q2","text":"Do a grid search to optimize your Random Forest model, use whatever hyperparameters you would like RandomForestClassifier () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Code Cell for L1 Q3 from sklearn.model_selection import GridSearchCV param_grid = { 'bootstrap' : [ True , False ]} grid = GridSearchCV ( RandomForestClassifier (), param_grid , cv = 7 )","title":"\ud83d\udd2c L4 Q2:"},{"location":"labs/L5_Writing_Unit_Tests/","text":"Data Science Foundations Lab 5: Writing Unit Tests \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab, we will try our hand at writing unit tests Import Libraries \u00b6 import random import numpy as np from contextlib import contextmanager import sys , os @contextmanager def suppress_stdout (): with open ( os . devnull , \"w\" ) as devnull : old_stdout = sys . stdout sys . stdout = devnull try : yield finally : sys . stdout = old_stdout Types of Tests \u00b6 There are two main types of tests we want to distinguish: Unit test : an automatic test to test the internal workings of a class or function. It should be a stand-alone test which is not related to other resources. Integration test : an automatic test that is done on an environment, it tests the coordination of different classes and functions as well as with the running environment. This usually precedes sending code to a QA team. To this I will add: Acid test : extremely rigorous tests that push beyond the intended use cases for your classes/functions. Written when you, like me, cannot afford QA employees to actually test your code. (word origin: gold acid tests in the 1850s , acid tests in the 70's ) EDIT : you could also call this a corner, or an edge case In this lab we will focus on unit tests . Unit Tests \u00b6 Each unit test should test the smallest portion of your code possible, i.e. a single method or function. Any random number generators should be seeded so that they run the exact same way every time. Unit tests should not rely on any local files or the local environment. Why bother with Unit Tests when we have Integration tests? A major challenge with integration testing is when an integration test fails. It\u2019s very hard to diagnose a system issue without being able to isolate which part of the system is failing. Here comes the unit test to the rescue. Let's take a simple example. If I wanted to test that the sume of two numbers is correct assert sum ([ 2 , 5 ]) == 7 , \"should be 7\" Nothing is sent to the print out because the condition is satisfied. If we run, however: assert sum([2, 4]) == 7, \"should be 7\" we get an error message: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-3-d5724b127818> in <module>() ----> 1 assert sum([2, 4]) == 7, \"should be 7\" AssertionError: should be 7 To make this a Unit Test, you will want to wrap it in a function def test_sum (): assert sum ([ 1 , 2 , 3 ]) == 6 , \"Should be 6\" test_sum () print ( \"Everything passed\" ) Everything passed And if we include a test that does not pass: def test_sum(): assert sum([3, 3]) == 6, \"Should be 6\" def test_my_broken_func(): assert sum([1, 2]) == 5, \"Should be 5\" test_sum() test_my_broken_func() print(\"Everything passed\") Here our test fails, because the sum of 1 and 2 is 3 and not 5. We get a traceback that tells us the source of the error: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-13-8a552fbf52bd> in <module>() 6 7 test_sum() ----> 8 test_my_broken_func() 9 print(\"Everything passed\") <ipython-input-13-8a552fbf52bd> in test_my_broken_func() 3 4 def test_my_broken_func(): ----> 5 assert sum([1, 2]) == 5, \"Should be 5\" 6 7 test_sum() AssertionError: Should be 5 Before sending us on our merry way to practice writing unit tests, we will want to ask, what do I want to write a test about? Here, we've been testing sum(). There are many behaviors in sum() we could check, such as: Does it sum a list of whole numbers (integers)? Can it sum a tuple or set? Can it sum a list of floats? What happens if one of the numbers is negative? etc.. In the end, what you test is up to you, and depends on your intended use cases. As a general rule of thumb, your unit test should test what is relevant. The only caveat to that, is that many continuous integration services (like TravisCI ) will benchmark you based on the percentage of lines of code you have that are covered by your unit tests (ex: 85% coverage ). \u270d\ud83c\udffd L5 Q1 Write a Unit Test \u00b6 Remember our Pokeball discussion in Python Foundations ? We'll return to that here. This time writing unit tests for our classes. Sometimes when writing unit tests, it can be more complicated than checking the return value of a function. Think back on our pokemon example: class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None If I wanted to write a unit test for the release method, I couldn't directly check for the output of a function. I'll have to check for a side effect , in this case, the change of an attribute belonging to a pokeball object; that is the change to the attribute contains . class Pokeball : def __init__ ( self , contains = None , type_name = \"poke ball\" ): self . contains = contains self . type_name = type_name self . catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch ( self , pokemon ): if self . contains == None : if random . random () < self . catch_rate : self . contains = pokemon print ( f \" { pokemon } captured!\" ) else : print ( f \" { pokemon } escaped!\" ) pass else : print ( \"pokeball is not empty!\" ) def release ( self ): if self . contains == None : print ( \"Pokeball is already empty\" ) else : print ( self . contains , \"has been released\" ) self . contains = None In the following cell, finish the code to test the functionality of the release method: def test_release (): ball = Pokeball () ball . contains = 'Pikachu' ball . release () # turn the pseudo code below into an assert statement ### YOUR CODE HERE ### # assert <object.attribute> == <something>, \"some erroneous message\" test_release () Pikachu has been released \u26f9\ufe0f L5 Q2 Write a Unit Test for the Catch Rate \u00b6 First, we will check that the succcessful catch is operating correctly. Remember that we depend on random.random and condition our success on whether that random value is less than the catch_rate of the pokeball: if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon so to test whether the successful catch is working we will seed our random number generator with a value that returns less than the catch_rate of the pokeball and then write our assert statement: def test_successful_catch (): # choose a random seed such that # we know the catch call should succeed ### YOUR CODE BELOW ### # random.seed(<your number here>) ball = Pokeball () ball . catch ( 'Psyduck' ) # Someone's fave pokemon (bless 'em) ### YOUR CODE BELOW ### # <object.attribute> == <something>, \"ball did not catch as expected\" NICE. Now we will do the same thing again, this time testing for an unsuccessful catch. SO in order to do this, we need to choose a random seed that will cause our catch to fail: def test_unsuccessful_catch (): # choose a random seed such that # we know the catch call should FAIL ### YOUR CODE BELOW ### # random.seed(<your number here>) ball = Pokeball () ball . catch ( 'Psyduck' ) ### YOUR CODE BELOW ### # <object.attribute> == <something>, \"ball did not fail as expected\" When you are finished test your functions below test_unsuccessful_catch () Psyduck captured! test_successful_catch () Psyduck captured! \u2696\ufe0f L5 Q3 Write a Unit Test that Checks Whether the Overall Catch Rate is 50/50 \u00b6 For this one, we're going to take those same ideas around seeding the random number generator. However, here we'd like to run the catch function multiple times to check whether it is truly creating a 50/50 catch rate situation. Here's a pseudo code outline: seed the random number generator for 100 iterations: create a pokeball try to catch something log whether it was successful check that for the 100 attempts the success was approximately 50/50 note: you can use my suppress stdout() function to suppress the print statements from ball.catch ex: with suppress_stdout(): print(\"HELLO OUT THERE!\") quick segway : what is the actual behavior of random.seed() ? Does it produce the same number every time we call random.random() now? Check for yourself: random . seed ( 42 ) [ random . random () for i in range ( 5 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124] We see that it still produces random numbers with each call to random.random . However, those numbers are the same with every execution of the cell. What happens when we do this: [ random . random () for i in range ( 5 )] [0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] The numbers are different. BUT: random . seed ( 42 ) [ random . random () for i in range ( 10 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124, 0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] We see them here in the bottom half of the list again. So, random.seed() is seeding the random number generator such that it will produce the same sequence of random numbers every time from the given seed. This will reset whenever random.seed() is set again. This behavior is useful because it allows us to continue using random number generation in our code, (for testing, creating examples and demos, etc.) but it will be reproducable each time. End Segway # 1. seed the random number generator # 2. for 100 iterations: # * create a pokeball # * try to catch something # * log whether it was successful # 3. check that for the 100 attempts the success was approximately 50/50 def test_catch_rate (): ### YOUR CODE HERE ### ### END YOUR CODE ### # assert np.abs(np.mean(results) - 0.5) < 0.1, \"catch rate not 50/50\" pass test_catch_rate () Test Runners \u00b6 When we start to create many tests like this, it can be cumbersome to run them all at once and log which ones fail. To handle our unit tests we use what are called test runners . We won't dedicate time to any single one here but the three most common are: unittest nose2 pytest unittest is built into python. I don't like it because you have to follow a strict class/method structure when writing the tests. nose2 is popular with many useful features and is generally good for high volumes of tests. My favorite is pytest, it's flexible and has an ecosystem of plugins for extensibility.","title":"Practice with Writing Unit Tests"},{"location":"labs/L5_Writing_Unit_Tests/#data-science-foundations-lab-5-writing-unit-tests","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab, we will try our hand at writing unit tests","title":"Data Science Foundations  Lab 5: Writing Unit Tests"},{"location":"labs/L5_Writing_Unit_Tests/#import-libraries","text":"import random import numpy as np from contextlib import contextmanager import sys , os @contextmanager def suppress_stdout (): with open ( os . devnull , \"w\" ) as devnull : old_stdout = sys . stdout sys . stdout = devnull try : yield finally : sys . stdout = old_stdout","title":"Import Libraries"},{"location":"labs/L5_Writing_Unit_Tests/#types-of-tests","text":"There are two main types of tests we want to distinguish: Unit test : an automatic test to test the internal workings of a class or function. It should be a stand-alone test which is not related to other resources. Integration test : an automatic test that is done on an environment, it tests the coordination of different classes and functions as well as with the running environment. This usually precedes sending code to a QA team. To this I will add: Acid test : extremely rigorous tests that push beyond the intended use cases for your classes/functions. Written when you, like me, cannot afford QA employees to actually test your code. (word origin: gold acid tests in the 1850s , acid tests in the 70's ) EDIT : you could also call this a corner, or an edge case In this lab we will focus on unit tests .","title":"Types of Tests"},{"location":"labs/L5_Writing_Unit_Tests/#unit-tests","text":"Each unit test should test the smallest portion of your code possible, i.e. a single method or function. Any random number generators should be seeded so that they run the exact same way every time. Unit tests should not rely on any local files or the local environment. Why bother with Unit Tests when we have Integration tests? A major challenge with integration testing is when an integration test fails. It\u2019s very hard to diagnose a system issue without being able to isolate which part of the system is failing. Here comes the unit test to the rescue. Let's take a simple example. If I wanted to test that the sume of two numbers is correct assert sum ([ 2 , 5 ]) == 7 , \"should be 7\" Nothing is sent to the print out because the condition is satisfied. If we run, however: assert sum([2, 4]) == 7, \"should be 7\" we get an error message: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-3-d5724b127818> in <module>() ----> 1 assert sum([2, 4]) == 7, \"should be 7\" AssertionError: should be 7 To make this a Unit Test, you will want to wrap it in a function def test_sum (): assert sum ([ 1 , 2 , 3 ]) == 6 , \"Should be 6\" test_sum () print ( \"Everything passed\" ) Everything passed And if we include a test that does not pass: def test_sum(): assert sum([3, 3]) == 6, \"Should be 6\" def test_my_broken_func(): assert sum([1, 2]) == 5, \"Should be 5\" test_sum() test_my_broken_func() print(\"Everything passed\") Here our test fails, because the sum of 1 and 2 is 3 and not 5. We get a traceback that tells us the source of the error: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-13-8a552fbf52bd> in <module>() 6 7 test_sum() ----> 8 test_my_broken_func() 9 print(\"Everything passed\") <ipython-input-13-8a552fbf52bd> in test_my_broken_func() 3 4 def test_my_broken_func(): ----> 5 assert sum([1, 2]) == 5, \"Should be 5\" 6 7 test_sum() AssertionError: Should be 5 Before sending us on our merry way to practice writing unit tests, we will want to ask, what do I want to write a test about? Here, we've been testing sum(). There are many behaviors in sum() we could check, such as: Does it sum a list of whole numbers (integers)? Can it sum a tuple or set? Can it sum a list of floats? What happens if one of the numbers is negative? etc.. In the end, what you test is up to you, and depends on your intended use cases. As a general rule of thumb, your unit test should test what is relevant. The only caveat to that, is that many continuous integration services (like TravisCI ) will benchmark you based on the percentage of lines of code you have that are covered by your unit tests (ex: 85% coverage ).","title":"Unit Tests"},{"location":"labs/L5_Writing_Unit_Tests/#l5-q1-write-a-unit-test","text":"Remember our Pokeball discussion in Python Foundations ? We'll return to that here. This time writing unit tests for our classes. Sometimes when writing unit tests, it can be more complicated than checking the return value of a function. Think back on our pokemon example: class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None If I wanted to write a unit test for the release method, I couldn't directly check for the output of a function. I'll have to check for a side effect , in this case, the change of an attribute belonging to a pokeball object; that is the change to the attribute contains . class Pokeball : def __init__ ( self , contains = None , type_name = \"poke ball\" ): self . contains = contains self . type_name = type_name self . catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch ( self , pokemon ): if self . contains == None : if random . random () < self . catch_rate : self . contains = pokemon print ( f \" { pokemon } captured!\" ) else : print ( f \" { pokemon } escaped!\" ) pass else : print ( \"pokeball is not empty!\" ) def release ( self ): if self . contains == None : print ( \"Pokeball is already empty\" ) else : print ( self . contains , \"has been released\" ) self . contains = None In the following cell, finish the code to test the functionality of the release method: def test_release (): ball = Pokeball () ball . contains = 'Pikachu' ball . release () # turn the pseudo code below into an assert statement ### YOUR CODE HERE ### # assert <object.attribute> == <something>, \"some erroneous message\" test_release () Pikachu has been released","title":"\u270d\ud83c\udffd L5 Q1 Write a Unit Test"},{"location":"labs/L5_Writing_Unit_Tests/#l5-q2-write-a-unit-test-for-the-catch-rate","text":"First, we will check that the succcessful catch is operating correctly. Remember that we depend on random.random and condition our success on whether that random value is less than the catch_rate of the pokeball: if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon so to test whether the successful catch is working we will seed our random number generator with a value that returns less than the catch_rate of the pokeball and then write our assert statement: def test_successful_catch (): # choose a random seed such that # we know the catch call should succeed ### YOUR CODE BELOW ### # random.seed(<your number here>) ball = Pokeball () ball . catch ( 'Psyduck' ) # Someone's fave pokemon (bless 'em) ### YOUR CODE BELOW ### # <object.attribute> == <something>, \"ball did not catch as expected\" NICE. Now we will do the same thing again, this time testing for an unsuccessful catch. SO in order to do this, we need to choose a random seed that will cause our catch to fail: def test_unsuccessful_catch (): # choose a random seed such that # we know the catch call should FAIL ### YOUR CODE BELOW ### # random.seed(<your number here>) ball = Pokeball () ball . catch ( 'Psyduck' ) ### YOUR CODE BELOW ### # <object.attribute> == <something>, \"ball did not fail as expected\" When you are finished test your functions below test_unsuccessful_catch () Psyduck captured! test_successful_catch () Psyduck captured!","title":"\u26f9\ufe0f L5 Q2 Write a Unit Test for the Catch Rate"},{"location":"labs/L5_Writing_Unit_Tests/#l5-q3-write-a-unit-test-that-checks-whether-the-overall-catch-rate-is-5050","text":"For this one, we're going to take those same ideas around seeding the random number generator. However, here we'd like to run the catch function multiple times to check whether it is truly creating a 50/50 catch rate situation. Here's a pseudo code outline: seed the random number generator for 100 iterations: create a pokeball try to catch something log whether it was successful check that for the 100 attempts the success was approximately 50/50 note: you can use my suppress stdout() function to suppress the print statements from ball.catch ex: with suppress_stdout(): print(\"HELLO OUT THERE!\") quick segway : what is the actual behavior of random.seed() ? Does it produce the same number every time we call random.random() now? Check for yourself: random . seed ( 42 ) [ random . random () for i in range ( 5 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124] We see that it still produces random numbers with each call to random.random . However, those numbers are the same with every execution of the cell. What happens when we do this: [ random . random () for i in range ( 5 )] [0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] The numbers are different. BUT: random . seed ( 42 ) [ random . random () for i in range ( 10 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124, 0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] We see them here in the bottom half of the list again. So, random.seed() is seeding the random number generator such that it will produce the same sequence of random numbers every time from the given seed. This will reset whenever random.seed() is set again. This behavior is useful because it allows us to continue using random number generation in our code, (for testing, creating examples and demos, etc.) but it will be reproducable each time. End Segway # 1. seed the random number generator # 2. for 100 iterations: # * create a pokeball # * try to catch something # * log whether it was successful # 3. check that for the 100 attempts the success was approximately 50/50 def test_catch_rate (): ### YOUR CODE HERE ### ### END YOUR CODE ### # assert np.abs(np.mean(results) - 0.5) < 0.1, \"catch rate not 50/50\" pass test_catch_rate ()","title":"\u2696\ufe0f L5 Q3 Write a Unit Test that Checks Whether the Overall Catch Rate is 50/50"},{"location":"labs/L5_Writing_Unit_Tests/#test-runners","text":"When we start to create many tests like this, it can be cumbersome to run them all at once and log which ones fail. To handle our unit tests we use what are called test runners . We won't dedicate time to any single one here but the three most common are: unittest nose2 pytest unittest is built into python. I don't like it because you have to follow a strict class/method structure when writing the tests. nose2 is popular with many useful features and is generally good for high volumes of tests. My favorite is pytest, it's flexible and has an ecosystem of plugins for extensibility.","title":"Test Runners"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/","text":"Data Science Foundations Project Part 1: Statistical Analysis \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to perform statistical analysis on data generated from our tictactoe program! 1.0 Preparing Environment and Importing Data \u00b6 back to top 1.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 1.0.2 Load Dataset \u00b6 back to top data = {} for i in range ( 1000 ): game = GameEngine () game . setup_game () board = game . play_game () data [ 'game {} ' . format ( i )] = { 'board' : board . board , 'winner' : board . winner , 'starting player' : board . start_player } | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| |O| | | | | |O|X|X| |O| |O| |X| | | |O|X|X| 'O' Won! |O| |O| |X|O| | |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O| | | |X|X| 'O' Won! |X| |O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X| | | |O|X| | | |O|O| |X| |X| |O|X| | | |O|O| |X|O|X| |O|X| | | |O|O| |X|O|X| |O|X|X| | |O|O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | |X|X| | | |O| | |O|O| | |X|X| |X| |O| | |O|O| | |X|X| 'O' Won! |X| |O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | |O| | | |X|X| |O|X| | | |O| | | |X|X| |O|X| | | |O|O| | |X|X| |O|X|X| | |O|O| | |X|X| 'O' Won! |O|X|X| |O|O|O| | |X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | |O| | |O| |X| |X| | | | |O| | |O|X|X| |X| | | | |O|O| |O|X|X| |X| |X| | |O|O| |O|X|X| 'O' Won! |X| |X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | | | | |X| |X| |O| |O|O| | | | |X| |X|X|O| |O|O| | | | |X| 'O' Won! |X|X|O| |O|O| | |O| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X|X| |X| | | |O| |O| | |X|X| |X|O| | |O| |O| | |X|X| |X|O|X| |O| |O| 'O' Won! |O|X|X| |X|O|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | |O|O| | | |X| |X| |X| | |O|O| | | |X| |X|O|X| | |O|O| | | |X| |X|O|X| |X|O|O| | | |X| 'O' Won! |X|O|X| |X|O|O| | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| |X| | |O| | |O| |X| |O| |X| | |O| | 'X' Won! |O| |X| |O| |X| | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X|O| | | |O| |X|X| | 'X' Won! |O|X|O| | |X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| |X| | | | | | | | |O|X| |X|O| | | | | | | |O|X| 'X' Won! |X|O| | | |X| | | |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | |X|O| | | |X|O| | |X|O| |X|O| | | |X|O| |X|X|O| |X|O| | 'O' Won! | |X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X|O| | | |O|X| | | | | |X|O| | | |O|X| | | |X| |X|O|O| | |O|X| | | |X| |X|O|O| | |O|X| | |X|X| 'O' Won! |X|O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| |X| | | |X| | |O|O| |X| |X| |O| |X| |X|O|O| |X| |X| |O| |X| 'O' Won! |X|O|O| |X|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| |X|X| | | | |O| | |X|O| |X|X| | |O| |O| | |X|O| |X|X| | |O| |O| |X|X|O| 'O' Won! |X|X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | |X|O| | | | | |O| |X| | |X|O| |O| | | |O| |X| |X|X|O| |O| | | |O| |X| 'O' Won! |X|X|O| |O|O| | |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | |X| | | |O| |X| |O| | | |X|O| | |O| |X| |O| | | |X|O| | |O| |X| |O|X| | |X|O| | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | |X| | | | |O|O| |X|X| | |X|O| | | |O|O| 'X' Won! |X|X| | |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| | | | |X|O| |X| | | |O|X| | | |X|O| |X| | | |O|X| | |O|X|O| |X| | | |O|X|X| |O|X|O| |X|O| | |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| |X| |O| | |O|X| |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| | | |X| |O|X|O| |O|X|X| | | |X| |O|X|O| 'O' Won! |O|X|X| |O| |X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X|X| | |O|O| | |O| | | |X|X| | |O|O| | |O| | | |X|X| | |O|O|X| |O|O| | |X|X| | |O|O|X| |O|O|X| |X|X| | |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | |X| |X|O| | | |O| | | | |X| |X|O| | |X|O| | | | |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O|X| |O| |X| |X|O| | |X|O|X| |O| |X| |X|O|O| 'X' Won! |X|O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | |X| | | |O| |O| | |X| | |X|O| | |O| |O| | |X| | |X|O|X| |O| |O| 'O' Won! |O|X| | |X|O|X| |O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| 'O' Won! | |X|O| | |O| | |O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O| |X| |X|X| | |O| |O| |O| |X| |X|X| | 'X' Won! |O| |O| |O| |X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X| | | | |X| |O| |X| |O|X| | | | |X| |O|O|X| 'X' Won! |O|X|X| | | |X| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O|X| | | | | | | |X| |O|O|X| | |O| | | | |X| |O|O|X| | |O| | |X| |X| 'O' Won! |O|O|X| | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| |O| |X| | | | | | |O|X| |O| |X| |X| | | | |O|X| |O| |X| |X| | | |O|O|X| 'X' Won! |O| |X| |X| |X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | | |X|X| |O| | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| | | |X|O|O| |O|X|X| |O| |X| |X|O|O| 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O|O|X| | | | | | |X|X| |O|O|X| | | | | | |X|X| |O|O|X| | |O| | | |X|X| |O|O|X| |X|O| | | |X|X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| | |O|X| | |X|O| |O| |X| 'X' Won! |X|O|X| | |X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | | |O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| | |X|X| |X|O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | |X| |X| |O| |X| |O|O| | |X|O|X| |O|X|X| |O|O| | |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| 'O' Won! |X|O| | |X| |X| |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | |X| |O| | |X| | | |O|O| |X| |O| | |X| | | |O|O| |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X|O| | | | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| |X| | |O|O| |X|X| | |O| |X| 'X' Won! | |O|O| |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X|O| |O|X| | |X| | | | |X|O| |O|X| | |X| | | | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| 'X' Won! |X|X|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X|O| | | | |O| | | |X| |X|O|X| | | |O| | | |X| |X|O|X| |O| |O| | | |X| 'X' Won! |X|O|X| |O|X|O| | | |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | | |X| | |X|X| | |O|O| | | |X| |O|X|X| | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X|O| | |O|X| | | |X| | |X|O| | |O|X| |O| |X| | |X|O| | |O|X| 'X' Won! |O| |X| | |X|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| | |O|O|X| | |X|X| | |O| | 'X' Won! |O|O|X| | |X|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| | | |O| |O|O|X| | | |X| |X| |O| |O|O|X| | | |X| |X|O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | |X|X| | | | | |O| | | |O|X|X| | | | | |O| | | |O|X|X| |X| | | |O| | | |O|X|X| |X| | | |O| |O| |O|X|X| |X| |X| |O| |O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | |O| |X| | | |X| |X|O| | |O| |X| | | |X| |X|O|O| |O| |X| | | |X| 'X' Won! |X|O|O| |O|X|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | |X|X| |O| |O| | | | | |O|X|X| |O| |O| | |X| | |O|X|X| |O| |O| 'O' Won! | |X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| |X| | |X| | 'O' Won! |O| | | |O| |X| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | | | | | | |X|X| |O|O|X| | | |O| | |X|X| |O|O|X| | |X|O| | |X|X| |O|O|X| | |X|O| |O|X|X| |O|O|X| 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O|O| | | | |X| 'X' Won! | | |X| |O|O|X| | | |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | | |O|X| |X| | | |O| |O| | |O|X| |X| | | |O| |O| | |O|X| |X| |X| |O| |O| | |O|X| |X|O|X| |O| |O| |X|O|X| |X|O|X| 'O' Won! |O|O|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O| |O| | |X|X| | | | | |O| |O| | |X|X| | | |O| |O| |O| | |X|X| |X| |O| |O| |O| | |X|X| |X|O|O| |O| |O| 'X' Won! |X|X|X| |X|O|O| |O| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| |X| |X| | |X| | | |O|O| |X|O|X| | |X| | | |O|O| |X|O|X| |X|X| | | |O|O| |X|O|X| |X|X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | |X|X| |O| | | | | |O| | |X|X| |O| | | 'X' Won! | | |O| |X|X|X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| | |X|X| |O|O|X| | |O|O| | |X|X| 'X' Won! |O|O|X| | |O|O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | | |X|X| |O| |O| | | | | |O|X|X| |O| |O| |X| | | |O|X|X| 'O' Won! |O|O|O| |X| | | |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| | | |O| | |X| | | |X|O| |O| |O| | |X| | 'X' Won! | |X|O| |O|X|O| | |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | |X| | | |O|X| |O|X|O| | |X| | | |O|X| |O|X|O| | |X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | | | | |X| |X| |O|O| | | | | | |X|O|X| |O|O| | | | |X| |X|O|X| 'O' Won! |O|O| | | |O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | |X|O| | | |X|X| | |O| | |X|O| | | |X|X| | |O|O| |X|O|X| | |X|X| | |O|O| |X|O|X| |O|X|X| | |O|O| 'X' Won! |X|O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| |O|O| | | |X| | | | |X| |O|O| | |X|X| | | |O|X| |O|O| | |X|X| | |X|O|X| |O|O| | |X|X| | |X|O|X| |O|O| | |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| | | | | |X|X|O| |X|O|O| | | | | |X|X|O| |X|O|O| | |X| | |X|X|O| 'O' Won! |X|O|O| | |X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X|X| | |X|O| |O|O| | | |X|X| |X|X|O| |O|O| | | |X|X| 'O' Won! |X|X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | | |X| | | |O|O| |X| | | | |X| | | |O|O| |X| |X| |O|X| | | |O|O| |X| |X| |O|X| | |X|O|O| |X| |X| |O|X| | |X|O|O| |X|O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | |X| |X| |O|O| | | | | | 'O' Won! |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X|X| | |O| | |O|X|O| | |X|X| 'X' Won! | |O| | |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| |X| | |O|O| | |X|O| |X| |X| 'X' Won! | |O|O| | |X|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O|O| | |O|X| | |X| | | |O|O| |X|O|X| | |X| | | |O|O| |X|O|X| | |X|O| | |O|O| |X|O|X| |X|X|O| 'O' Won! |O|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | |X| | | |X| | |O|O| | 'O' Won! | |X| | | |X| | |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | | | | |O|O| |X| |X| |X| | | | |O|O| 'O' Won! |X| |X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| |X| | | | |O|O| |O|X|X| |X| | | | |O|O| |O|X|X| |X|X| | | |O|O| |O|X|X| |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O|O| |X|O| | 'X' Won! |X| | | |X|O|O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | 'X' Won! |X| | | |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | |O|X| |X|X| | | |O|O| | |O|X| 'O' Won! |X|X| | |O|O|O| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | | |X| | 'O' Won! |O|O|O| |X| | | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | | |O|O| | |X|X| |X| | | | |O|O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O| | |X|O|O| | |X|X| |X|O|O| |X|O|O| 'X' Won! |X|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| | | |O| |O|X|X| | | |O| | |X|O| |O|X|X| | | |O| | |X|O| |O|X|X| |O| |O| 'X' Won! | |X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X|X| | | |O|O| | | |O| |X|X| | | |O|O| | |X|O| |X|X| | 'O' Won! | |O|O| | |X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| |X| |O| |O| |X|O| | |X| |X| |O|X|O| |X|O| | |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| |X| | | | |X|O| |X|O|O| |X| | | | |X|O| 'X' Won! |X|O|O| |X| | | |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | | | |X|X|O| 'O' Won! |O| | | | |O| | |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| | | |O| | | | | |X|O|X| | | |O| |X| | | |X|O|X| | | |O| |X| |O| 'X' Won! |X|O|X| |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| 'O' Won! |O| |X| | |O| | | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| |O| 'X' Won! | | |X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O| | | |X| | |O|X|X| 'O' Won! |O|O|O| | |X| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| | |O| | |X|X| | |O| |X| | |O|O| |X|X| | 'X' Won! |O| |X| | |O|O| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| | | |O| |X|O|X| | | |X| | | |O| |X|O|X| |O| |X| | | |O| |X|O|X| |O|X|X| | | |O| |X|O|X| |O|X|X| |O| |O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | |O| | | |X| |O|X|O| | |X|O| | | |X| |O|X|O| 'O' Won! | |X|O| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | |X|O| | | | | | |O|X| |O|X|O| | | | | |X|O|X| |O|X|O| 'O' Won! | | |O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| | | |O| |X|O|X| | | |X| | | |O| |X|O|X| | | |X| |O| |O| |X|O|X| |X| |X| |O| |O| |X|O|X| |X|O|X| |O| |O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| |X| |X| |O| |X| 'O' Won! | |O|O| |X|O|X| |O| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| | | |O| |X| | | |X|X|O| | |O|O| |X| | | |X|X|O| | |O|O| |X| |X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | | | |O| |X| |X|X|O| | | |O| |O| |X| |X|X|O| |X| |O| |O| |X| |X|X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X|O|X| | | | | |O|X| | |X|O|X| | | |O| |O|X| | |X|O|X| | |X|O| |O|X| | |X|O|X| |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | | | |O| |X| | |X|O| | | | | |O|X|X| 'O' Won! | |X|O| | |O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| |X| | | | |O| | |O| |X| |X| | | |O|O| | |O| |X| |X| | | |O|O|X| |O| |X| |X| |O| |O|O|X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | | |X| | |O| | |X| |O| | | |X| | |O|O| |X| |O| |X| |X| | |O|O| |X|O|O| |X| |X| | |O|O| 'X' Won! |X|O|O| |X|X|X| | |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | |X| | | |X| |O| |O|X|O| |X| | | |X| |O| |O|X|O| |X| | | |X|X|O| 'O' Won! |O|X|O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O| |O| | |X| | | | |X| |O| |O| | |X|X| | | |X| |O| |O| |O|X|X| | | |X| |O| |O| |O|X|X| | |X|X| 'O' Won! |O|O|O| |O|X|X| | |X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | | | |X| | |X|O| |O|O| | | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| |O| |X| | |X|O| |O|O|X| |O|X|X| | |X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O| | | | |X|O| | | |X| |O| | | |X|X|O| | | |X| |O|O| | |X|X|O| |X| |X| |O|O| | |X|X|O| 'O' Won! |X| |X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| 'X' Won! |O|O|X| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | |X| | |O| | |X|O| | | | |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O|O| 'X' Won! |X|X|X| |O|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| |O| | | |X|O| | | |O|X| |O| | | |X|O|X| | |O|X| |O| | | |X|O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| |O| | |X| | | |O| | |X| |O| | |X| | |X|O| | |X| |O| |O|X| | |X|O| | 'X' Won! |X| |O| |O|X| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | |X| |X| |O|O| | | | |O| |X| |X| |O|O| | 'X' Won! | | |O| |X|X|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| |X|X| | |O| | | | |O|X| |X|X|O| |O| | | | |O|X| 'X' Won! |X|X|O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X| | | |X| |X| 'O' Won! |O|O|O| |X| | | |X| |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O| | |X|O| | | |X|X| | |O|O| |X|O| | 'X' Won! |X|X|X| | |O|O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| |O| | | | | |X| |O| 'X' Won! |X| |O| |X| | | |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| | |O| | |X|O|O| | |X|X| |X|O| | |X|O|O| |O|X|X| |X|O| | 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X| | | |O| | 'X' Won! |O| |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |X| | | |O| |O|X| | | | |X| | |X|O| |O|X| | | | |X| | |X|O| |O|X| | | |O|X| 'X' Won! |X|X|O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | | | |X| |X|O|O| | |X| | | |O|X| |X|O|O| | |X| | | |O|X| |X|O|O| | |X|X| |O|O|X| |X|O|O| | |X|X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | |X|O| | | |X| |O| | | | |X|O| | | |X| |O|O| | | |X|O| |X| |X| |O|O| | 'O' Won! | |X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| |O| | | | |X|X| | |X|O| |O| | | | |X|X| |O|X|O| |O| | | 'X' Won! |X|X|X| |O|X|O| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X|X| |O| | | | |O| | | |X|X| |O|O| | | |O| | | |X|X| |O|O|X| | |O| | |O|X|X| |O|O|X| 'X' Won! | |O|X| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | |X| | | |X| |O| |O|X|O| |X| | | |X| |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X| |X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| 'O' Won! |O|X| | | |O| | | |X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | | |O| |X|X| | |X|O| | | |O|O| |X|X| | 'X' Won! |X|O| | |X|O|O| |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | |O|X|X| | |O|X| |O|O| | |O|X|X| |X|O|X| 'O' Won! |O|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O|X| | | | | |O| |X| | |O|X| | |O| | |O|X|X| | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | 'X' Won! |O|X|X| | |O|X| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| |O| | | | | |X|O|X| |O|X|O| 'O' Won! |O| | | |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | |O|X| | | | | |X| |O| |X|O|X| | | | | |X| |O| |X|O|X| | | | | |X|O|O| |X|O|X| | | |X| |X|O|O| |X|O|X| |O| |X| |X|O|O| 'X' Won! |X|O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| |X| 'O' Won! | | | | |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| |X| |X| |O| | | | |O|X| |X|O|X| |O| | | | |O|X| 'X' Won! |X|O|X| |O|X| | | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| |X| |O| | | |O|X| | |X| |X| |O| |O| |O|X|X| |X| |X| |O| |O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | |X| | |X|O| | | |O| | | |X| | |X|O| | | |O|O| | |X| | |X|O| | |X|O|O| | |X| | |X|O|O| |X|O|O| 'X' Won! |X|X| | |X|O|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O| |O| 'X' Won! |O| | | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O|X|O| | | |X| | | |O| |O|X|O| | | |X| | |X|O| |O|X|O| | |O|X| | |X|O| |O|X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O| | 'O' Won! |X|X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X|O| | | |X| | | | | |O|X|O| | |X|X| | | | | |O|X|O| | |X|X| | | |O| |O|X|O| | |X|X| |X| |O| |O|X|O| |O|X|X| |X| |O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| |O| | |O|X| | |O|X| |X| |O| | |O|X| |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X| | | |O|O| 'X' Won! | |O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | |X| |O| | | |X| |O|O| | |X| |O| | | |X| |O|O| | |X|X|O| 'O' Won! | | |X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | |X| |O| | | | | |O|X| | 'O' Won! |X| |O| | |O| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O|X| |O| | | |X| | | | |O|X| |O| |O| |X|X| | | |O|X| |O| |O| 'O' Won! |X|X| | | |O|X| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | |X| |X| |O|X| | |O| | | |X| |X| |O|X| | |O| |O| |X| |X| |O|X| | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | |O|O| | | |X| | | |X|X| |O|O| | | |X| | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | |X|X| | | | | |O| |O| | |X|X| |O| | | |O| |O| | |X|X| |O|X| | |O| |O| | |X|X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | | | |X| |X| |O| | |O| | | |O|X| |X| |O| | |O| | |X|O|X| 'O' Won! |X|O|O| | |O| | |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O|X| | | |X| |X| |O| |O|O|X| | | |X| |X|O|O| |O|O|X| |X| |X| |X|O|O| 'O' Won! |O|O|X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X| | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | | |X|O| |O| |O| |X|X| | |X|X|O| 'O' Won! |O| |O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O|X| | | | | |O|X| | |O|O|X| | |X| | |O|X| | |O|O|X| | |X| | |O|X|O| |O|O|X| | |X|X| |O|X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X| |O| |X|O|O| | | |X| |X| |O| |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| |O| | | | | |X| 'X' Won! |X|O|O| |O|X| | | | |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | |O| | |X|X|O| |X| | | | |O| | |X|X|O| |X|O| | | |O| | 'X' Won! |X|X|O| |X|O| | |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | |X|X| | | |O| |O| |O| | |X|X| | | |O| |O|X|O| | |X|X| | |O|O| |O|X|O| | |X|X| 'X' Won! | |O|O| |O|X|O| |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | |O| | | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O|X| |X| | | |O| | | |X|O|X| |X|O| | |O| | | |X|O|X| |X|O|X| |O| | | |X|O|X| |X|O|X| |O| |O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| |O|O| | 'X' Won! |X| | | | |X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X|O| |X|O| | | |X|O| | |X|O| |X|O| | 'O' Won! | |X|O| | |X|O| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | |O| |O| | |X|X| |O| | | 'X' Won! |O| |O| |X|X|X| |O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| |X| |X| | | | | | |O|O| |X| |X| | |O| | |X|O|O| |X| |X| | |O| | |X|O|O| |X| |X| |O|O| | |X|O|O| |X| |X| |O|O|X| 'O' Won! |X|O|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| |O|O|X| |X| | | |O|X|O| |O|O|X| |X| |X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | | | |X|O|O| |X|O| | | | | | 'X' Won! |X|O|O| |X|O| | |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| |O| | | | | |X| |X| |O| |O|X| | | |O|X| |X| |O| |O|X| | | |O|X| |X|X|O| |O|X| | |O|O|X| |X|X|O| |O|X| | It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X|O| | |O| | | | |X|X| |X|O| | |O| | | |O|X|X| |X|O| | 'X' Won! |O| |X| |O|X|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | |O|O| |X| | | |X| |O| 'X' Won! |X|O|O| |X| | | |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | | |X| | | |O|X| |O| | | |O|X| | | |O|X| |O| |X| |O|X| | | |O|X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | | |O| | |X|X| |O|O| | |X| |O| | |X|X| |O|O| | 'O' Won! |X| |O| | |X|X| |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | |O|O| | |X| |X| | | | | |O|O|X| |X| |X| | | | | |O|O|X| |X|O|X| | | | | 'X' Won! |O|O|X| |X|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| |X| 'O' Won! |O| | | |O|X| | |O| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | | |O| |X|O| | |O|X|X| | | |O| |X|O| | |O|X|X| |O| |O| |X|O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | |O| | | |X| | |X|O|X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| |O|O| | |O| |X| | |X|X| |O|O| | |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | |O| | |O| |X| |X|X| | | |O| | |O| |X| |X|X| | | |O|O| |O| |X| |X|X| | |X|O|O| |O| |X| 'O' Won! |X|X|O| |X|O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X|O| |X| | | | |X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| |X| |O|X|O| | |X|O| |X|O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| | | |X| 'O' Won! |O|X|X| |O|O|O| | | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X|O| | |O| | | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O| |O|X| | | |X|X| | |O|O| |O|X| | 'O' Won! | |X|X| |O|O|O| |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | | | | | | |X|O| |O| | | |X| | | | |X|O| |O| | | |X| |O| | |X|O| |O| | | |X|X|O| 'O' Won! | |X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| | |X|X|O| | | |O| | |X| | 'O' Won! |X|X|O| | | |O| | |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| |O| | | | |X| | | |O|X| |O| | | | |X|X| | |O|X| |O| |O| | |X|X| | |O|X| |O|X|O| | |X|X| |O|O|X| |O|X|O| | |X|X| 'X' Won! |O|O|X| |O|X|O| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | | |X|O| 'O' Won! |O|X|X| | |O| | | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O|O| | |O|X| | | | |X| |O|O| | |O|X| | |X| |X| 'O' Won! |O|O|O| |O|X| | |X| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | | |X|X| | |O|O| |O| | | | |X|X| | |O|O| |O| |X| | |X|X| | |O|O| |O|O|X| | |X|X| 'X' Won! | |O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | |X|O| | | |O| |X| | | | |X|O| | |O|O| |X| | | | |X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X|X| | 'O' Won! |O|X|O| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | |X|O| |O| | | |X| | | | |X|O| |O|O| | 'X' Won! |X| | | | |X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | |O|X| |X| | | |O| |O| | |O|X| |X| | | |O|X|O| | |O|X| |X|O| | |O|X|O| |X|O|X| |X|O| | |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | 'X' Won! |X| | | |X|O| | |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | |O| | |X|O| | | |X|X| | |O| | |X|O| | | |X|X| | |O| | |X|O|O| 'X' Won! |X|X|X| | |O| | |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | |X| |O| | | |O|X| | | |O|X| |O| | | |O|X|X| | |O|X| |O| |O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | |X| | | | |O| | |X|O| |O|X| | | | |O| | |X|O| |O|X|X| | | |O| |O|X|O| |O|X|X| 'X' Won! |X| |O| |O|X|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X|O|O| |X| | | 'X' Won! |X|O| | |X|O|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X| | |O|O| | | | |X| |X|X| | |O|O| | | | |X| |X|X|O| |O|O| | | | |X| |X|X|O| |O|O| | | |X|X| 'O' Won! |X|X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | | |O| | | |X|X| |O|O| | 'X' Won! | |O| | |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | |O|X| |X| | | | |O|O| |X|O|X| |X| | | | |O|O| 'O' Won! |X|O|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | |X| |X|O|X| | | |O| 'O' Won! |O| |X| |X|O|X| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| |X| |O| | | |X| | | |O| |X| |O| |O| |X| | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| |O| |X|X| | 'X' Won! |O|X|O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |X| | |O| | |X|X|O| | |O|X| 'X' Won! |X|O| | |X|X|O| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | | 'O' Won! |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X|O| | |X| | | |O|X|O| |X|O| | |X| | | |O|X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X| |X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X|X| | |X|O|O| | | |O| |X|X| | 'X' Won! |X|O|O| | | |O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X|X| | | |O|O| |O| | | |X|X| | |X|O|O| |O| | | |X|X| | |X|O|O| |O|O| | |X|X| | 'X' Won! |X|O|O| |O|O| | |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | |X|X| | |O|O| | | |O| | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O|O| |X|X| | |O|O|X| |X|O|O| |X|X| | 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | |O|X| | | | | |O|X| | |X|O|X| | | | | |O|X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O|X| | |O|X| | |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | |O|X| |O| |X| | | | | | |O|X| |O|X|X| | | | | | |O|X| |O|X|X| | | |O| | |O|X| |O|X|X| | |X|O| |O|O|X| |O|X|X| | |X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| | |X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | |X| | |O|O| | | | |X| 'O' Won! | |X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X| | | |X|O| | | |O| |O|X| | | |X|O| | | |O| |O|X|X| 'O' Won! | |X|O| | |O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | |O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O|X| | | |X| | |O| |X| |O|X|O| | |X|X| |O| |X| |O|X|O| 'O' Won! |O|X|X| |O| |X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | |X|O| |O| | | | |X| | |O|X|O| |O| | | | |X| | 'X' Won! |O|X|O| |O|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| 'O' Won! |O| | | |X|O|X| | | |O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | | | |O|X| 'X' Won! |X| |O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X|X| | |X|O| | |O|O| | |X|X| | |X|O| |X|O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | |X|O| | | | | |X|O| | | |X|O| | | |O| |X|O| | | |X|O| | |X|O| |X|O| | | |X|O| |O|X|O| |X|O| | | |X|O| |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |X| | | |O|O|X| | | | | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| |X| |O| | | | | |X| |O| |X|O|O| | | | | |X| |O| 'X' Won! |X|O|O| |X| | | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O|O| | | | | | |X| |X| |O|O| | |X| | | |X| |X| |O|O| | |X| |O| |X| |X| |O|O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | |X| | | |X| |O| | |O| | |X|O| | |X| |O| | |O| | |X|O|X| |X| |O| |O|O| | |X|O|X| |X|X|O| |O|O| | |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O|X|O| 'O' Won! |O| | | |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | | |X| | |X| |O| |O|X| | | |X|O| |X| |O| |O|X| | |X|X|O| |X|O|O| |O|X| | |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X|X|O| 'O' Won! |O|O|O| | | |X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | | |X|X|O| | | | | |O| | | |X|X|O| | |O| | |O| | | |X|X|O| |X|O| | |O| | | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| |O| |O| | | | | | |X|X| |O|X|O| | | | | | |X|X| |O|X|O| | | |O| 'X' Won! | |X|X| |O|X|O| | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | | |O| |X| |O| |X|X| | | | |O| 'O' Won! |X| |O| |X|X|O| | | |O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | |X| |X| |X| |O| |O|O| | |X| |X| |X|X|O| |O|O| | |X| |X| |X|X|O| |O|O| | |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | | | | | |O| |X|X|O| | | | | |X| |O| |X|X|O| |O| | | |X| |O| |X|X|O| |O| |X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | | | | |O|X|O| | |X|X| | | | | |O|X|O| |O|X|X| | | |X| |O|X|O| |O|X|X| 'O' Won! |O| |X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| 'X' Won! | |O|X| | | |X| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X|O| | | |O| | |O|X| |X|X|O| | | |O| |O|O|X| |X|X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | |O| 'X' Won! | | |O| |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| |X| |O|O| | | |O|X| |X| |X| 'X' Won! |O|O| | | |O|X| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| | |O|X| |O|X| | | | |X| | |O|X| |O|X|O| | |X|X| | |O|X| |O|X|O| 'O' Won! |O|X|X| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | |X| |X|O| | | | |O| | | |X| |X|O| | |O| |O| | | |X| |X|O|X| |O| |O| 'O' Won! |O| |X| |X|O|X| |O| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | |O| |X| | | 'X' Won! | |O|X| | |X|O| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| 'X' Won! |X| | | | |X| | |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X|X| | | | | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O|X| | |O|O|X| |X|X| | |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O|O| | |X| | |X|O| | | |O|O| | |X| | |X|O|X| 'O' Won! |O|O|O| | |X| | |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | | |O|O| |X| |X| | | | | |X|O|O| |X|O|X| | | | | |X|O|O| 'X' Won! |X|O|X| |X| | | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | |X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O|X| |O|X| | |O|O|X| | |O|X| |O|X| | |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X| | | |X|O| |O| | | | |X| | |X|X|O| |O| | | | |X|O| |X|X|O| |O| | | | |X|O| |X|X|O| |O| |X| | |X|O| |X|X|O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | | | |X| |O|X|O| | | | | |X| |X| |O|X|O| | | | | |X| |X| |O|X|O| |O| | | 'X' Won! |X|X|X| |O|X|O| |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X|O| | |X| | | | |O| 'X' Won! |O|X|O| | |X| | | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X| | | | |O|O| |X| |X| |X| | | | |O|O| |X|O|X| |X| | | | |O|O| |X|O|X| |X| |X| 'O' Won! |O|O|O| |X|O|X| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | |X|X| |O|O| | |O| |X| | |X|X| 'X' Won! |O|O|X| |O| |X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| | | |O| | | |O|X|O| |X| | | |O|X| | |O|X|O| |X| | | |O|X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | |X| | | |O| | |X| |O| | |X| | |O|O| | |X| |O| | |X| | |O|O|X| |X| |O| 'O' Won! |O|X| | |O|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | |X| | |O| |X| |O|O| | | |X| | |O| |X| |O|O| | | |X| | |O|X|X| 'O' Won! |O|O| | |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| |X| |X| | | | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X|O|X| | |O|O| |O| |X| |X|O|X| | |O|O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| |X| |O| |X| | | |O|O|X| |X|O|O| |X| | | |O|O|X| 'X' Won! |X|O|O| |X|X| | |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | | |O|X| |O|X|X| |O| | | | |O|X| |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| | | |X|X| |O| |O| | |O| | | |X|X| |O|X|O| | |O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| |O|X|O| It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| |X|O| | | | | | |X| |O| |X|O|O| | | | | |X| |O| 'X' Won! |X|O|O| |X| | | |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | |O| 'X' Won! |X| | | |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | |O|O| | |X|X| | |X| | | |O|O| | |X|X| | |X| |O| |O|O| | |X|X| | |X| |O| |O|O|X| |X|X| | |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| 'O' Won! |O| |X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O| |X|X| | 'O' Won! | |X| | |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O|O| | |X| | |X|O| | |X|O|O| 'O' Won! |O|X| | |X|O| | |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | |X| | | | |O|X| |O|O| | |X| | | |X|O|X| |O|O| | |X| | | 'O' Won! |X|O|X| |O|O|O| |X| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | |O| |O|X|X| | | |X| | | |O| |O|X|X| | | |X| | |O|O| |O|X|X| | |X|X| | |O|O| 'O' Won! |O|X|X| | |X|X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| | | | | |X|X|O| | |O|X| | | | | 'O' Won! |X|X|O| | |O|X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | | | |O| |X| | |X|O| | | |X| |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |X|X| 'O' Won! |O| |X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | | |X| |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| |X|O|X| |X| | | 'O' Won! |O|X|O| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O|O| | |X| |X| | |X|O| |O|O| | 'O' Won! |X| |X| | |X|O| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | |X| | |O| |O| |O|X| | | |X| | |O| |O| |O|X| | | |X|X| 'O' Won! |O| |O| |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| |X| |X|O| | | | | | |O|O|X| |X|O| | |X| | | |O|O|X| 'O' Won! |X|O| | |X|O| | |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X| | |X|O|O| | | |X| | |X| | |X|O|O| |O| |X| | |X| | |X|O|O| |O| |X| |X|X| | |X|O|O| |O|O|X| |X|X| | 'X' Won! |X|O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | | |O| |O|X| | | | |X| | |X|O| |O|X| | | | |X| | |X|O| |O|X|O| | | |X| 'X' Won! |X|X|O| |O|X|O| | | |X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | |O| | |X| | | |O|O|X| | |O|X| |X| | | |O|O|X| |O|O|X| 'X' Won! |X| |X| |O|O|X| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| | | | | | |O|O| |O|X|X| |X| | | | |O|O| 'O' Won! |O|X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | | |X| | |X|O|O| |O|X|O| | |X| | |X|O|O| |O|X|O| | |X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O| |O| |X| |X| | |O| | |O|X|O| |X| |X| | |O|O| |O|X|O| |X| |X| 'X' Won! |X|O|O| |O|X|O| |X| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| | | | | | |X|X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| |X| | |X|X| |X|O|O| |O| |X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X|O| | | |X| | |O|X|O| |X|O| | |O|X| | |O|X|O| |X|O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | |O|O|X| | |X| | | |O| | 'X' Won! |O|O|X| | |X| | |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O|X| |X| | | | |O|X| |O|O|X| |X| | | 'X' Won! | |O|X| |O|O|X| |X| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| | |O| | 'X' Won! |O| |X| |O| |X| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | |X|O| |O|X|O| | | |X| | |X|O| |O|X|O| | | |X| |X|X|O| |O|X|O| |O| |X| |X|X|O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | |O|X| | |X|X| |O| | | | |O|X| | |X|X| |O| | | |O|O|X| 'X' Won! |X|X|X| |O| | | |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | |O| |X| | | | |X|O| | | |O| |X| |X| | |X|O| | |O|O| |X| |X| 'X' Won! | |X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| | | |O| |O|O| | | |X|X| | | |O| |O|O|X| | |X|X| |O| |O| |O|O|X| | |X|X| |O|X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | |O| | |O| | | | |X| |X| |O| | |O| | | | |X| |X|O|O| | |O| | | | |X| |X|O|O| | |O|X| | | |X| |X|O|O| |O|O|X| | | |X| |X|O|O| |O|O|X| | |X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X|O| | |X| |O| 'X' Won! |X| | | |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | |O| |O| |O|X|X| |X| | | |O| |O| |O|X|X| |X| |O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | 'O' Won! | | | | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | |O| | | |X| |O| |X|X| | |O| | | |X| |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X|X|O| |X|X|O| |O|O| | |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | | | | | |O| |X| | |X| | |O| | | |O| |X| | |X| | |O| |X| |O| |X| | |X|O| |O| |X| |O| |X| |X|X|O| |O| |X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | | | | | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| | |X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O|X| | | |X| | |O| |X| |O|X| | | |X| | |O|O|X| |O|X| | |X|X| | |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | |O|O| | |X| | |X|O|X| | |O|O| | |X| | |X|O|X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | |O|X| | |X| |O| | | |X| |O|X| | |X| |O| | | |X| |O|X|O| |X|X|O| | | |X| |O|X|O| |X|X|O| |O| |X| |O|X|O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O| | | |X| | 'O' Won! |O|X|X| |O|O|O| | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X| |O| |X|X|O| |O|O| | |X|X|O| |X|X|O| 'O' Won! |O|O|O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | | |O|O| |X| |X| |O| | | |X|O|O| |X| |X| |O|O| | |X|O|O| |X| |X| |O|O|X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O|X| | |X|X| | | | | |O|O|X| |O|X|X| |X| | | |O|O|X| |O|X|X| |X|O| | |O|O|X| |O|X|X| 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| | |O| | |O|O| | | |X|X| | |O| | |O|O|X| |O|X|X| | |O| | |O|O|X| |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | 'X' Won! |X| | | | |X|O| | |O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| |X| |X| |X| |O| 'O' Won! |O|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O|O| | | |X| |X|X| | | |O|O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| |X|O|O| | | |X| 'O' Won! |X|X|O| |X|O|O| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | | |O|X| |X|X| | |O|O| | | |O|X| 'O' Won! |X|X| | |O|O|O| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| |O| |X| |X| |X|O| | |O| |O| |X| |X| |X|O|O| |O| |O| |X| |X| 'X' Won! |X|O|O| |O|X|O| |X| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| |O|X| | | |O|X| | | |X| |O|X| | | |O|X| | | |X| |O|X|O| |X|O|X| | | |X| |O|X|O| |X|O|X| | |O|X| |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | |X|O| |O|X| | |X|O| | | |X|O| |O|X| | |X|O| | |X|X|O| |O|X| | |X|O|O| |X|X|O| |O|X| | 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| |X|O| | |X|O|O| | |X|X| |X|O|O| |X|O|O| 'X' Won! |X|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| |O| |O| | | |X| | | |X| |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |X|X| |O|X|O| |O|O|X| | |X|X| |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X| |O| | | | | |X|O| | |X|O|O| | | | | |X|O| | |X|O|O| | | | | |X|O|X| |X|O|O| |O| | | |X|O|X| |X|O|O| |O| |X| |X|O|X| 'O' Won! |X|O|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| |O|X| | | | |X| | | |O| |O|X|X| | |O|X| | | |O| |O|X|X| |X|O|X| | | |O| |O|X|X| |X|O|X| | |O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| | | | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| 'O' Won! |X| | | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| | |X|X| | | |O| | |O|O| | 'X' Won! |X|X|X| | |O| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| |X| |X|O| | | | | | |O| |X| |X|O|O| | |X| | |O| |X| |X|O|O| | |X|O| |O| |X| |X|O|O| |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | |O| |X| | | |X|O| | | | |O| |X| |X| |X|O| | |O| |O| |X| |X| |X|O|X| |O| |O| |X| |X| 'O' Won! |X|O|X| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X|O| | | |O| |O|O|X| 'X' Won! |X|X|O| | |X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | 'O' Won! |X| |O| | | |O| |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | |O| | | |X|X| |O|O|X| | |O| | | |X|X| |O|O|X| |O|O| | | |X|X| 'X' Won! |O|O|X| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| | |O|O| | |X| | | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | |X| | |O|O| | | | |X| | |X|O| |O|O| | | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| | |O|X| |X|X|O| |O|O|X| | |O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | |X| | |O| |X| | |O|O| | |X| | |O| |X| 'X' Won! |X|O|O| | |X| | |O| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | |O| | | |X| |O| |X| | | |O| |X| |X| |O| |X| | | |O|O|X| |X| |O| |X| | | |O|O|X| |X| |O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | | | |X|O|O| |X|X| | |O| | | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X| | 'O' Won! |O|X| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | |X|O| |X| | | |O|O| | | |X|O| |X| | | |O|O| | |X|X|O| |X|O| | |O|O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | |X| | |O|X| | |O| | | | |X|O| |O|X| | |O| |X| | |X|O| 'O' Won! |O|X| | |O| |X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| |O| |O| |X| | | | | |X|X|O| |O| |X| | | | | |X|X|O| |O| |X| | |O| | |X|X|O| |O| |X| |X|O| | |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | |O| |O| | | |X| | |X| | |O|X|O| | | |X| |O|X| | |O|X|O| |X| |X| |O|X| | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X|O| |O| | | |X| | | |X|X|O| |O| | | |X| | | |X|X|O| |O|O| | |X|X| | |X|X|O| |O|O| | |X|X|O| |X|X|O| |O|O| | 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| 'O' Won! | |O| | | |O|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O|O|X| 'X' Won! |X| | | | |X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| |O| | | | |X| | | | |O| |O| |X| | |X| | | | |O| |O| |X| | |X| | | |O|O| |O| |X| |X|X| | | |O|O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| | |O|O| | | | | |X|X|O| |X|O|O| | | | | 'O' Won! |X|X|O| |X|O|O| | | |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O|X| |O|X|O| |X| |X| |O|O|X| 'X' Won! |O|X|O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O| |X| |X| | | |O|X| | |O| |X| |X| |O| |O|X| | |O| |X| |X| |O| |O|X|X| |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | | | |O| |X|O|O| |X|X| | | |O|O| |X|O|O| 'X' Won! |X|X| | |X|O|O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X| | | |O| |O| |X|X| | |X| | | |O| |O| 'O' Won! |X|X| | |X| | | |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O| | | |X|X|O| | |X| | |O|O| | |X|X|O| | |X| | |O|O| | |X|X|O| | |X|X| |O|O| | |X|X|O| |O|X|X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | | |X|X| |O|X| | |O| | | | |X|X| |O|X|O| |O| | | | |X|X| |O|X|O| |O| |X| 'O' Won! |O|X|X| |O|X|O| |O| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | |X| | | | |O|O| |X|X|O| |X| | | | |O|O| 'X' Won! |X|X|O| |X| | | |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| |X| | |O| | |O|X|X| |O| |X| | |O|O| |O|X|X| |O|X|X| | |O|O| 'O' Won! |O|X|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| 'X' Won! |O| | | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | | | 'O' Won! |O| |X| |O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | |X| |X| | | 'O' Won! |O|O|O| | | |X| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | |X|X| | | | |O| | | |O| |X|X| | | | |O| | |X|O| |X|X| | | | |O| | |X|O| |X|X| | | |O|O| | |X|O| |X|X| | |X|O|O| |O|X|O| |X|X| | |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| |X| | |O|O| |X|X| | |O| |X| | |O|O| |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | |O| |X| |O|X| | | | |X| |O| |X| |O|X| | | | |X| |O|O|X| |O|X| | 'X' Won! | | |X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| |O| | | | |X|O| | | |X| |O|X| | | |X|O| | | |X| |O|X| | |O|X|O| 'X' Won! | |X|X| |O|X| | |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| | 'X' Won! |O|O| | |X|O| | |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | |O| | | |O| 'X' Won! |X|X|X| | | |O| | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| | |O|X| | |X| | | 'X' Won! |O|O|X| |O|X| | |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O|X| | | | |O| |X| |O| |O|X| | | | |O| |X|X|O| |O|X| | | | |O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X|X| | |O|O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O|X| 'O' Won! |X|O| | |X|O| | | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| | |O|O| |X| | | |X|O|X| | |O|O| |X| | | |X|O|X| | |O|O| |X| |O| |X|O|X| | |O|O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| | | |O| |X| | |O|O| |X| | | |O| |X| |X|O|O| |X|O| | |O| |X| |X|O|O| |X|O| | |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| | | | | |X| |O|X|O| |X| | | |O| |X| |O|X|O| |X|X| | |O| |X| |O|X|O| |X|X| | |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | | | |O|X|O| | | |X| |X| | | |O|X|O| |O| |X| |X| | | |O|X|O| |O| |X| |X| |X| |O|X|O| 'O' Won! |O| |X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | | |O| | |X| | |X|O| | | |X|O| | |X|O| |X|O| | | |X|O| |X|X|O| |X|O| | | |X|O| 'O' Won! |X|X|O| |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | |X| | | | |O| |O|X|X| | |X| | |O| |O| |O|X|X| 'X' Won! | |X| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| 'X' Won! |X| |X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| |X| 'O' Won! | | |O| |X|O| | |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| | |X| | | | |O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X|X| | |O|O| | |X|O|X| 'O' Won! |X|X| | |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | |X|O|X| |O| | | |O| | | |X|O|X| |O| | | |O| | | |X|O|X| |O| |X| |O| | | |X|O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O| |X| | |X| | |X|O|O| |O| |X| |O|X| | |X|O|O| |O|X|X| |O|X| | It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X|O| | | | | |X|X|O| |O|X|O| | | | | 'O' Won! |X|X|O| |O|X|O| | | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | | |O|X| | | | | |X|O|X| 'O' Won! | |O|X| | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | |X|X|O| |O| |O| | |X| | |X|X|O| 'X' Won! |O|X|O| | |X| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| |X|O| | | | |O| | |X|X| |X|O|O| | |X|O| | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O|X|O| | | | | | |X|O| |O|X|O| | | | | |X|X|O| |O|X|O| | | | | 'O' Won! |X|X|O| |O|X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | |X|O|X| | |O| | 'O' Won! |O|X| | |X|O|X| | |O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | | | |X|O|X| | |O|O| | | | | |X|O|X| | |O|O| | |X| | |X|O|X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'O' Won! |O|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| | | |O|X|O| |X|X| | 'X' Won! |O| |X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | |O|X| |X| | | |O| | | |X|O|X| |X| | | |O| | | |X|O|X| |X|O| | |O|X| | |X|O|X| |X|O| | 'O' Won! |O|X| | |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | |X|O|X| | | |O| |O| | | |X|O|X| | | |O| |O|X| | |X|O|X| |O| |O| |O|X| | |X|O|X| |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| 'X' Won! |X|O| | |O|X| | | | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O|O| | |X| | |X| | | |X|O|O| | |X| | |X| | | |X|O|O| |O|X| | |X| | | 'X' Won! |X|O|O| |O|X| | |X| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | | |X| | | |O| |O|X| | |O| |X| | | |O| |O|X|X| |O| |X| | |O|O| |O|X|X| |O| |X| | |O|O| |O|X|X| |O|X|X| 'O' Won! |O|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| |X| |O| |O| | | |X| |X| |X| |O| |O|O| | |X| |X| 'X' Won! |X| |O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | |X|O| |O| | | |O| |X| | |X|O| |O| |X| |O| |X| | |X|O| 'O' Won! |O| |X| |O| |X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | |X| | | | | |X|O|O| | | |X| | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X| |X| | |O|O| |X|O|O| 'X' Won! |X| |X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X|O|O| | | |X| | |X| | |X|O|O| | | |X| |O|X| | |X|O|O| | | |X| |O|X| | |X|O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O|X| |X| | | | |O|X| |O|O|X| 'X' Won! |X| |X| | |O|X| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | | | |X| | | |X| |O|O| | | |O|X| | | |X| |O|O| | |X|O|X| |O| |X| |O|O| | |X|O|X| 'X' Won! |O| |X| |O|O|X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X|O|O| | | |X| |O| |X| |X|O|O| | | |X| |O| |X| |X|O|O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| | |O|O| | 'X' Won! |X| | | |O|X| | |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X| | | | | |O| |O|O|X| |X| | | |X| |O| 'O' Won! |O|O|X| |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | 'O' Won! | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O| |X| |O|X| | |X|O| | |O|O|X| |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | |O|X| | |O|O| | | |X| | |O|X|X| |O|O| | | |X|O| |O|X|X| |O|O| | |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | |X| |O|X| | | |O|O| | | |X| |O|X| | | |O|O| |X| |X| |O|X|O| | |O|O| |X| |X| 'X' Won! |O|X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| |X| | |X| | | |O| | |O|O|X| |X|X| | | |O| | |O|O|X| |X|X| | |O|O| | |O|O|X| 'X' Won! |X|X|X| |O|O| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X| | | | |O|O| |X| |X| |X| | | | |O|O| |X|O|X| |X| |X| | |O|O| |X|O|X| 'O' Won! |X|O|X| | |O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| 'O' Won! |X| | | |X| | | |O|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | |O| | | |O|X| | |X|X| | |O| | | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | |O|O| |X|O|X| 'O' Won! | |X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X|O| | |X|O|O| | |X| | |X|O| | |X|O|O| | |X|O| |X|O| | 'X' Won! |X|O|O| | |X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| |X| | | |O| |O| 'O' Won! | |X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | |O| | |X| | | |X|O|O| | |O|X| |X| | | |X|O|O| | |O|X| |X| |O| |X|O|O| 'X' Won! | |O|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| | | | |O|X| |X|O|X| |O| | | | |O|X| |X|O|X| |O| | | |O|O|X| 'X' Won! |X|O|X| |O|X| | |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| |X| | | |O| |O| | |X|O| |X| | | |O| |O| | |X|O| |X| |X| |O| |O| |O|X|O| |X| |X| 'X' Won! |O| |O| |O|X|O| |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X|X| |O| | | |X| | | |O|X|X| |O| | | |X|O| | 'X' Won! |O|X|X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| |X| |X| | |O| | | |O|O| |X| |X| |X|O| | | |O|O| |X| |X| 'O' Won! |X|O| | | |O|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| |O| | | | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| |X| |X|X| | |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O| | | |X|O| |X|O|O| |X|O| | 'X' Won! |X|X|O| |X|O|O| |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O|X| | | | | | |O| | 'O' Won! |X|O|X| | |O| | | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| 'O' Won! | | |X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| |X| |O| | | | | | |X|O| |X|O|O| 'X' Won! | | |X| | |X|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | |X| |O| 'O' Won! |O|X| | | |O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X|X| | | | |O| |O|O| | |X|X| | | | |O| |O|O| | |X|X| | |X| |O| 'O' Won! |O|O|O| |X|X| | |X| |O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O| | |O| |O| |X| |X| | |O| | |O|X|O| |X| |X| | |O|O| |O|X|O| 'X' Won! |X|X|X| | |O|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| |X|X|O| | |O| | | |O|X| |X|X|O| | |O| | 'X' Won! | |O|X| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | | |O| |O|X|O| | | |X| |X| |O| |O|X|O| |O| |X| |X| |O| |O|X|O| |O|X|X| |X| |O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X|O| |O|X|X| | | | | | |X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | |X| |O|X|O| 'O' Won! |O|X|X| |O| |X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| |O| | | |O|X|X| |X| |O| |O| | | |O|X|X| |X| |O| |O|O| | |O|X|X| |X|X|O| |O|O| | 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X|X| |O| |O| | |O|X| | |X|X| 'X' Won! |O| |O| | |O|X| |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |O| 'X' Won! | |X| | |O|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X| |X| |X| |O|O| | |O| |X| |X| |X| |O|O| | |O|X|X| |X| |X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | |O| |X| |X| | |X|O| | | |O| |X|O|X| | |X|O| | | |O| |X|O|X| | |X|O| | |X|O| |X|O|X| |O|X|O| | |X|O| 'X' Won! |X|O|X| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X|X| | |O|O| |O| |X| | |X|X| | |O|O| |O|X|X| | |X|X| | |O|O| 'O' Won! |O|X|X| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X|O| | | | | |X| |O| |X|X|O| | | | | |X| |O| 'O' Won! |X|X|O| | | |O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | |X| | | |X| |O| |O|X| | |X| | | |X|O|O| |O|X| | |X|X| | |X|O|O| |O|X| | |X|X|O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| | | | |X|O| |O|O| | |X| |X| | |X|O| |O|O| | |X| |X| |O|X|O| |O|O| | 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| |X| |O|O| | 'O' Won! | | | | |X| |X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| | |X| | | |X| | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | |X|X| | |X|O|O| |O|X| | |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | | |X| |O| | | |X|O| | | |O|X| |O|X| | |X|O| | | |O|X| |O|X|O| |X|O| | | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | |O| |O| 'X' Won! | |X| | | |X| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| |O| | | | |X| | | |O|X| |O| | | | |X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O| | | |O|X|X| |X|O|X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | | |X| | | |O| |O|X|X| |O| |X| | | |O| |O|X|X| |O|X|X| | | |O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | |O| |O| | |X|X| |O| | | |O| |O| | |X|X| |O|X| | 'O' Won! |O| |O| |O|X|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| |O| |O| | | | | |X| 'X' Won! |X| |O| |O|X| | | | |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X| | | |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| | | |O| | | |O| |X|O|X| |X| |O| | | |O| |X|O|X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'O' Won! |O|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| |X| |O| | | |X| |O| |O| |X| |O| |X| |X| |O| |O| |X| |O|O|X| |X| |O| |O| |X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |O| | |O|X| | |O| | |X| |O| | |O|X| |X|O| | |X| |O| |O|O|X| |X|O| | |X|X|O| |O|O|X| |X|O| | It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O|X| | |O| | | |X|X| | |O|X| | |O|O| | |X|X| | |O|X| |X|O|O| | |X|X| |O|O|X| |X|O|O| | |X|X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| |O| | | | | |X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X| | |X|O|X| |O|X|O| |O|X| | It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | |X|X| | |O|O| | | |O| | |X|X| | |O|O| | | |O|X| |X|X| | 'O' Won! |O|O| | | |O|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X| | | | | | |O|X|X| 'O' Won! |O|X| | |O| | | |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | | | | | |X|O|O| |O|X| | | | | | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | |O|X| | 'X' Won! |X|O|O| |O|X| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| |O|X| | | | | | 'X' Won! | |O|X| |O|X| | |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| | |X| | |O| |X| |O|O|X| | |X| | |O| |X| |O|O|X| |X|X| | |O| |X| |O|O|X| |X|X| | |O|O|X| 'X' Won! |O|O|X| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X|X| | |O| | | |X| |O| |X|X|O| |O| | | |X| |O| |X|X|O| |O| |X| |X| |O| |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | |X|O| |O| |X| | | |O| | |X|O| |O| |X| 'X' Won! |X| |O| | |X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | |X| | | |X|X|O| | |O| | |X| | | |X|X|O| | |O| | |X|O| | |X|X|O| | |O|X| |X|O| | |X|X|O| |O|O|X| |X|O| | It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | | |O| | | |X| | |O|X|X| | |O| | | |X| | 'O' Won! |O|X|X| | |O| | | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| | | |X|O| | 'X' Won! |X| |O| |X| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | 'O' Won! | |X| | |O|O|O| | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| |X| | | |O| | |X| | |O|O|X| 'X' Won! |X| |O| | |X| | |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | |O|O| | 'X' Won! | | | | |X|X|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | | | | |O| | | |O|X|X| | | | | |O|X| | |O|X|X| | | | | |O|X|O| 'X' Won! |O|X|X| | |X| | |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X|O| | 'X' Won! |X|O|O| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | | |O| | | | |X| |X|O| | | |O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O| | 'O' Won! |X|O| | |O|O|X| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| | |O|O| | | | | |X| |X| 'X' Won! | |O|O| | | | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| |X| | |O| | |X| |O| |O| |X| | |O| | |X|X|O| |O| |X| | |O| | |X|X|O| |O| |X| |O|O| | |X|X|O| |O| |X| |O|O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | | | |O|X|O| |X| | | | |X| | |O|X|O| |X| | | | |X| | |O|X|O| |X| |O| |X|X| | |O|X|O| |X| |O| |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | |O| | |X| |O| | |O|X| | |O|X| |X| |O| |O|O|X| | |O|X| |X|X|O| |O|O|X| | |O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X|X| | | | |O| |O| | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O|O| | |X|X| | |O|X|O| |O|O|X| |X|X| | |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O|X|O| 'O' Won! |O|X| | |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | 'X' Won! | | |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| | | | | |X|O| | | |O|X| |O| | | |X|O| | |X|O|X| |O| | | 'O' Won! |X|O| | |X|O|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X|X| | | |O|O| 'O' Won! | | |X| |X|X| | |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | | |X| |O| | | |X|O|O| | | |X| |O|X| | |X|O|O| | | |X| |O|X| | |X|O|O| | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |O| |X|X| | | | |O| | | |O| |X|X| | | | |O| | |O|O| |X|X| | | |X|O| | |O|O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X|O|X| | | |O| |X|O|O| 'X' Won! |X|O|X| | |X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| | |O|X| |O| |O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| | |X| | |O| | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X|X| |O| | | |X|O|X| |O|X|X| |O| |O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | | |O|X| | |X|X| | |O| | | |O|X| | |X|X| |O|O| | |X|O|X| | |X|X| |O|O| | 'O' Won! |X|O|X| | |X|X| |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | | | | |X|X| |O|O| | | | | | |O|X|X| |O|O| | | | |X| |O|X|X| 'O' Won! |O|O| | |O| |X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| | | | |O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X|X| | |O|O| | |X|X|O| |X|X|O| 'X' Won! |O|O|X| |X|X|O| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | | | 'O' Won! |O|X|X| | |O| | | | |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| |X|X| | | |O|O| |X| |O| |X|X| | | |O|O| 'O' Won! |X| |O| |X|X|O| | |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | |O| | |X|X|O| | |X| | | |O| | |X|X|O| | |X|O| | |O| | 'X' Won! |X|X|O| | |X|O| | |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| |X|O| | |X|X| | | |O|O| |X|O|O| |X|X| | 'X' Won! | |O|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| |X|X| | |O| |O| 'X' Won! | | |O| |X|X|X| |O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| |X|X| | 'O' Won! | | |O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X|X| |X| | | |O| |O| |O|X|X| 'X' Won! |X| | | |O|X|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | | |O|O| | |X|X| | | | | | |O|O|X| |X|X| | |O| | | |O|O|X| 'X' Won! |X|X|X| |O| | | |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | |O| |X| |O| |X|O| | 'X' Won! |X| |O| |X| |O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | |O| | |O| | | |X| |X| |X|O| | |O| | | |X| |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| |X| |X| |X| |X|O|O| |O|O|X| 'X' Won! |X|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| | | |X|O| |X| | | |O|O|X| | |X|O| |X| |O| |O|O|X| | |X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X| |X| | |O|O| | | |X| |X| |X| | |O|O| | |O|X| 'X' Won! |X|X|X| | |O|O| | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X|O| |X|O| | |O|X| | |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X| | |O|X| |O| | | |O| |X| |X|O|X| |O| | | |O|O|X| |X|O|X| |O| | | |O|O|X| |X|O|X| |O|X| | 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | |X| | | | |X|O| |O| | | |X| | | |O|X|O| |O| | | 'X' Won! |X| | | |O|X|O| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | | |X|O| |X| | | |O| | | |O|X|O| |X| | | |O| | | |O|X|O| |X|X| | |O|O| | |O|X|O| |X|X| | 'X' Won! |O|O| | |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | |X| |O|X| | |O|O| | | | |X| |O|X|X| |O|O| | | | |X| 'O' Won! |O|X|X| |O|O| | |O| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | | | | | |O|O|X| |O|X| | | | |X| |O|O|X| |O|X| | | |O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | |X| | | | |X| | 'O' Won! |O|O|O| |X| | | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X|O| |O|X| | | | | | |X|X|O| |O|X| | | | | | |X|X|O| |O|X| | | | |O| |X|X|O| |O|X|X| | | |O| |X|X|O| |O|X|X| |O| |O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| |O| | | | | |X| |X| |O|X|O| | | | | |X| |X| |O|X|O| | | |O| 'X' Won! |X| |X| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O|X|O| |O| | | |X| |X| |O|X|O| |O|O| | 'X' Won! |X|X|X| |O|X|O| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| | |O| | 'X' Won! |X|O|O| | |X|X| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X|O| | |X|O| | | | | |O|X|O| | |X|O| | | |X| |O|X|O| | |X|O| |O| |X| 'X' Won! |O|X|O| | |X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| |X| |O|X|O| | |X|O| |X|O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | | |O| | | |X| | |X|O| | | |O| | |O|X| | |X|O| | | |O| | |O|X| | |X|O|X| | |O| | |O|X|O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|O| | |O| | |O|X|X| | |X|O| | |O|O| |O|X|X| | |X|O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X| | |X|X| | | | |O| |O|X| | |X|X| | | |O|O| |O|X| | |X|X| | |X|O|O| |O|X| | |X|X|O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X| |O| |O|O| | |X|O|X| |X| |O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X|O|X| | |O| | | | | | |X|O|X| |X|O| | | | | | 'O' Won! |X|O|X| |X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| | | |X| |O| | |O|O| 'X' Won! |X| | | |X| |O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| |O| |O| | | | | |X| |X|X|O| |O| | | | | |X| |X|X|O| |O| |O| |X| |X| |X|X|O| |O| |O| 'O' Won! |X| |X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X| | |X| |X| |O| | | |O|X|O| |X| |X| |O|X| | |O|X|O| |X| |X| |O|X| | |O|X|O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |X| |X| | | | |O| | |O| |X| |X| | | | |O|X| |O| |X| |X| | | | |O|X| |O| |X| |X| |O| |X|O|X| |O| |X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O| | | |X| | |O|X|X| |O|O| | | |X| | |O|X|X| |O|O| | | |X|X| 'O' Won! |O|X|X| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | |X|O|O| |X| |O| | | |X| |X|O|O| |X| |O| | | |X| |X|O|O| |X|O|O| | |X|X| |X|O|O| |X|O|O| 'O' Won! |O|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | | | |X|O|O| |X|X| | |O| | | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| |X| | |O| | | |O|X| |O| |X| | |O| | |X|O|X| |O| |X| |O|O| | |X|O|X| |O| |X| 'X' Won! |O|O|X| |X|O|X| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | |X| | |O|O| | | | |X| |O|X| | |O|O| | | | |X| |O|X| | |O|O|X| | | |X| |O|X| | |O|O|X| | |O|X| |O|X| | |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | | |O|X|O| |O|X| | | | |X| |O|X|O| |O|X| | 'O' Won! |O| |X| |O|X|O| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | |O|O| | |X|O| |X| |X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| | |O|O| 'O' Won! |X|X|O| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | |X|X| | | |O|X| | |O| | |X|X|O| | |O|X| | |O|X| |X|X|O| | |O|X| | |O|X| |X|X|O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| | | | | |O|X|X| | |O|O| | | | | |O|X|X| | |O|O| |X| | | |O|X|X| | |O|O| |X|O| | |O|X|X| | |O|O| |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | |X| | | |X| | |O| |O| | |X|O| | |X| | |O| |O| |X|X|O| 'O' Won! | |X| | |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X|X| | 'O' Won! |X| | | |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | | | |O| |X|O|O| |X|X| | |O| |O| |X|O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X|O| |O| | | | | |X| |X|X|O| 'O' Won! |O| | | | |O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X|X| | | | | |X|O|O| |O|X|X| | | |X| |X|O|O| |O|X|X| |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X|X| | |O|O| | | |X| | |X|X| | |O|O| | |O|X| | |X|X| |X|O|O| | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | |X| | |O|X| | | |O| 'O' Won! |O| |X| | |O|X| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |O|X| | | |X| | |O|O| | |O|X|X| 'O' Won! | |X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | |O| | |X| |O| | | |X| |X|O| | |X| |O| | | |X| |X|O| | |X|O|O| | | |X| |X|O|X| |X|O|O| | | |X| |X|O|X| |X|O|O| |O| |X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | | | |O| | |X|X| |O|O| | | |X|O| | |X|X| |O|O| | |O|X|O| | |X|X| |O|O| | |O|X|O| | |X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O| | | |O|X|X| |X|O|X| |O| | | |O|X|X| |X|O|X| |O|O| | 'X' Won! |O|X|X| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | | |O|X| |X| | | |O| | | | |O|X| |X| | | |O|O| | | |O|X| |X| |X| |O|O| | 'O' Won! | |O|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O| |X| | |X|X| |O| | | |O|O|X| | |X|X| 'X' Won! |O| |X| |O|O|X| | |X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X| | |X| | 'O' Won! |O| | | |O| |X| |O|X| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| |O|O| | |O|X|X| | |X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| |O| | | |X| | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| | |O| | 'X' Won! |X|O|O| | |X|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| | | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| |X|O| | |X| |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'X' Won! |X|O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O| |X| | |X| | |O| |X| 'O' Won! |O| |X| |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | | |O| |O|X|X| | |O| | | |X|O| |O|X|X| | |O| | |O|X|O| |O|X|X| 'X' Won! |X|O| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| | | |X|X|O| |O| |X| |O|O| | |X|X|O| |O| |X| |O|O| | |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O| |X| |O|X| | |X|O|O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| | | |X| | |O|X| | | |O|O| | |X| | |O|X| | |X|O|O| | |X| | |O|X|O| |X|O|O| | |X| | |O|X|O| |X|O|O| | |X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| |O| | | |X| |O| |X| |X| |O| | |O|X| |O| |X| |X| |O| | |O|X| |O| |X| |X|X|O| |O|O|X| |O| |X| |X|X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| 'O' Won! |X|X|O| | | |O| | | |O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | |O|X| | |X|O| |X|O| | | |O|X| | |X|O| |X|O|O| | |O|X| |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| |O| | | |X|X| | | |O|X| |O| | | |X|X| | | |O|X| |O| |O| |X|X| | |X|O|X| |O| |O| 'O' Won! |X|X| | |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | |X|O| | |X| | |X|O| | |O|X|O| | |X| | |X|O| | |O|X|O| |X|X| | |X|O| | |O|X|O| |X|X| | |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| |X|O| | |X| | | |O|O|X| |X|O| | |X| | | |O|O|X| |X|O|X| |X| | | 'O' Won! |O|O|X| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O|X| | |X| | 'O' Won! | | |O| | |O|X| |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O|X| 'O' Won! | |O| | | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | |X| | | |X| |O|O| | 'O' Won! | | |X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O|X| | |O| | | |X|X| |O|O|X| | |O| | | |X|X| 'X' Won! |O|O|X| | |O|X| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| |O|X|O| | | | | |X| |X| |O|X|O| | | | | |X|O|X| |O|X|O| | | |X| |X|O|X| |O|X|O| 'O' Won! |O| |X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X| |X| |O|X|X| 'O' Won! |O| |O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | |X| | | | |X| |O| |O| | |X|O| | | |X| |O| |O| |X|X|O| | | |X| |O| |O| |X|X|O| | |O|X| |O|X|O| |X|X|O| | |O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | |O| |X| |O| | |X|X| | | |O| |X|O|O| | |X|X| | | |O| 'X' Won! |X|O|O| |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | |X| | | |X|O|O| | |X| | |X| | | |X|O|O| |O|X| | |X| | | |X|O|O| |O|X|X| |X|O| | |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| | | |X|X| |O|O| | |X|O| | | |X|X| |O|O|X| |X|O| | | |X|X| |O|O|X| |X|O|O| | |X|X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | |X| |X| | |O|O| | |O| | |X| |X| |X|O|O| | |O| | |X| |X| 'O' Won! |X|O|O| | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | |X| | |X| | | |O| |O| | |X| | |X| |O| |O| |O| | |X| | |X| |O| |O|X|O| 'O' Won! | |X|O| |X| |O| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| | | |X| |X| | |X|O| 'O' Won! |O| | | |X|O|X| | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| |X| | |O| | |O|O|X| |X| |X| 'X' Won! | |O| | |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| |O| | | | | |X| |X|X|O| |O| | | |O| |X| |X|X|O| |O| | | |O|X|X| |X|X|O| |O| | | |O|X|X| |X|X|O| |O| |O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| | |O| | |X|O|X| | | |O| |X|O| | |X|O|X| | | |O| |X|O|O| |X|O|X| 'X' Won! |X| |O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X|O| | |O| | | |X| |X| |X|O| | |O| | | |X|O|X| |X|O| | |O| | | |X|O|X| |X|O|X| |O| | | 'O' Won! |X|O|X| |X|O|X| |O|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| | |X| | |X|O|O| | | |O| |X|X| | |X|O|O| 'O' Won! | | |O| |X|X|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| | | |O| | |X| | |O|O|X| | | |O| |X|X| | |O|O|X| |O| |O| |X|X| | 'X' Won! |O|O|X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| |X| | | | | |X| |O| |O| 'O' Won! |X| | | | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | | | |X| |O|O| | |O|X| | | |X|X| |O|O| | |O|X| | | |X|X| |O|O| | |O|X|O| 'X' Won! |X|X|X| |O|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | |O| | |X|O| | |O|X| |X| |O| | |X|O| | |O|X| |X| |O| |O|X|O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X|O| | | | | |O|O| | |X|X|O| | |X| | |O|O| | |X|X|O| |O|X| | |O|O| | 'X' Won! |X|X|O| |O|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O| |O| |X|O| | | |X| | |O|X|O| |X|O|O| | |X| | |O|X|O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| 'X' Won! | | |X| |O|X| | |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| |X| | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O|X|X| | | |O| |X|X|O| |O|X|X| |O| |O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | 'X' Won! |X| |O| |O|X| | | | |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | |O|X| | |X| |O| |O|X| | |O|X|X| |X| |O| |O|X| | |O|X|X| |X|O|O| |O|X| | It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| |O| |O| |X| |X| |X| |O| |O| 'O' Won! |O| |X| |X|O|X| |O| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | |X| | | | | |O| 'X' Won! |X|O| | |X| | | |X| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| |X| | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X| | |O| |O| |X|X| | |O|X|X| |O| |O| 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| |X| | |O| | | |X| | |O| |X| |O|O| | | |X| | |O| |X| |O|O|X| | |X| | |O|O|X| |O|O|X| |X|X| | |O|O|X| |O|O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | | | | |X| | | |O|X|O| | |O| | |X| | | |O|X|O| | |O| | |X|X| | |O|X|O| |O|O| | |X|X| | 'X' Won! |O|X|O| |O|O| | |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |O| | |X| | |O|X|O| | | |O| |X|X| | 'O' Won! |O|X|O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| |O| | |O|X| |X|X| | |O| |O| | |O|X| |X|X|O| |O| |O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X|O| |X| | | |O| | | | |X|O| |X|X| | |O| | | | |X|O| |X|X| | |O| |O| 'X' Won! | |X|O| |X|X| | |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| | |X|X| |O| |O| | | |O| | |X|X| |O|X|O| | | |O| |O|X|X| |O|X|O| 'X' Won! | |X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | |O|O|X| | |X| | | | | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O| |X| |O|O|X| |X|X| | |O|O|X| |O|O|X| |X|X| | 'X' Won! |O|O|X| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | |X| |X| | |X|O| |O|O| | |X| |X| | |X|O| |O|O| | |X| |X| |X|X|O| 'O' Won! |O|O| | |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | | |O| | |X|O|X| |O|X| | | |O|O| |X|O|X| |O|X|X| | |O|O| |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | |X| |O| |X| | |X|O| | | |X| |O| |X| |O|X|O| | | |X| |O|X|X| |O|X|O| 'O' Won! |O| |X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O| |O|X|O| | | | | | |X|O| |O|X|O| |X| | | 'O' Won! | |X|O| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | 'O' Won! |O| |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X| | | |O|X| |X| |O| | |X|O| | |O|X| |X| |O| | |X|O| | |O|X| |X|X|O| 'O' Won! |O|X|O| | |O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X|X| | | |O|O| | | |O| |X|X| | | |O|O| |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| |O| | | |X|X|O| | | |X| |O| | | 'O' Won! |X|X|O| | |O|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O| | | |X| | |X|O|O| | |O| | | |X|X| |X|O|O| | |O| | |O|X|X| |X|O|O| | |O| | |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | |O|X| |O|X| | | |X| | | |O|X| |O|X| | | |X| | |O|O|X| |O|X|X| | |X| | |O|O|X| |O|X|X| | |X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | | | |O| |X|O|O| |X| |X| | |O|O| |X|O|O| |X| |X| 'X' Won! |X|O|O| |X|O|O| |X| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O|O| | |X| |X| |O| | | |O|O|X| |X| |X| |O|O| | |O|O|X| |X| |X| 'X' Won! |O|O|X| |O|O|X| |X| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X|O|O| | |X|X| | | | | |X|O|O| |O|X|X| | | | | |X|O|O| 'X' Won! |O|X|X| | |X| | |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | |O| | | | | |X|X|O| | | |O| |X| | | |X|X|O| |O| |O| |X| | | |X|X|O| |O|X|O| |X| | | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | |X|O| |O|O| | |X| | | | |X|O| |O|O|X| |X| | | | |X|O| 'O' Won! |O|O|X| |X|O| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | |X|O| | | |X|O| | | | | |X|O|O| | |X|O| | | | | |X|O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X|O| | 'X' Won! |X|O|O| | |X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | | |X| | |X| | |O| |O| | | |X| | |X|O| |O| |O| | |X|X| | |X|O| 'O' Won! |O|O|O| | |X|X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X|O|O| 'X' Won! | |O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | |O| | |X|O| | |X|X| | | |O| | |X|O| | |X|X| | | |O| | |X|O|O| |X|X| | | |O|X| |X|O|O| |X|X| | |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | |O| | |X|X|O| | | |X| | |O| | |X|X|O| | | |X| |O|O| | |X|X|O| |X| |X| |O|O| | 'O' Won! |X|X|O| |X| |X| |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| | | |X|O| |X| |O| | |X| | 'O' Won! | |X|O| |X| |O| | |X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| |X| |O| | | | | |X| |O| |X| |O|O| | | | |X| |O|X|X| |O|O| | 'O' Won! |O| |X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | |X|X| |O| | | |O| | | 'O' Won! |O|X|X| |O| | | |O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | | |X|X| | |O| | |O| | | |O|X|X| | |O|X| |O| | | |O|X|X| | |O|X| |O| |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| | |X|X| | |O| | | |X|O| | |X|X|O| |O| | | |X|O| | |X|X|O| |O|X| | |X|O| | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | |O| | | |O|X|X| | |O| | |O|X| | |O|X|X| | |O| | |O|X|O| |O|X|X| 'X' Won! |X|O| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| |X|O| | | | |O| |X|X|O| |X|O| | | | |O| |X|X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'X' Won! |X|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| |O| |O|X| | |X|X| | |O| |O| |O|X|X| |X|X| | |O| |O| |O|X|X| |X|X|O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X| |X| | |O| | |X| |O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'X' Won! |X|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O|O| | | |X| | |X|O| | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | |X| | |O| | | |X|O| | |O|X| | |O| | | |X|O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| | |O|O| | | | | |X|X|O| |X|O|O| | | | | |X|X|O| |X|O|O| | |O| | |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | |X| | | |O|O| | | |X| | |X| |O| |O|O| | | |X| | |X|X|O| |O|O| | | |X| | |X|X|O| |O|O| | | |X|O| |X|X|O| |O|O|X| | |X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | | |O| | | |X| |X| |O|X| | |O|O| | |X| |X| |O|X| | |O|O| | |X| |X| |O|X|X| 'O' Won! |O|O|O| |X| |X| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | |O|X| |X| |O| | |X| | | |O|X| |X| |O| | |X| | | |O|X| |X|O|O| | |X| | |X|O|X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| | |O|O| | | |X| |X|O|X| | |O|O| | |X|X| |X|O|X| | |O|O| |O|X|X| |X|O|X| 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | 'O' Won! |O|X| | |O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O|X| | | |O| | |X| |X| |O|X| | | |O|O| |X| |X| 'X' Won! |O|X| | | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| 'O' Won! |X| | | | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| 'O' Won! |O| |X| |O| | | |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | | |X| |X|O| | | | |O| | | |X| |X|O| | |X| |O| | | |X| |X|O| | |X| |O| | |O|X| |X|O|X| |X| |O| | |O|X| |X|O|X| |X| |O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | |O| | |X| |X| | |O| | | |O|X| |X| |X| | |O| | | |O|X| |X| |X| |O|O| | 'X' Won! | |O|X| |X| |X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X|X| | |O| |O| | |X| | |X|X|O| |O| |O| 'X' Won! | |X| | |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X|X| | |O|O| | | |O| | |X|X| |X|O|O| | | |O| | |X|X| |X|O|O| | | |O| |O|X|X| |X|O|O| |X| |O| |O|X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O|O| |X|X|O| 'X' Won! |X|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O|X| | | |O| |X|X| | | |O|X| | |O|O| |X|X| | |X|O|X| | |O|O| |X|X| | 'O' Won! |X|O|X| |O|O|O| |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X|X| | | | | | |X|O|O| |X|X| | | | |O| 'X' Won! |X|O|O| |X|X| | |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |X|X| | |O| |O| | |X|O| |X|X| | |O| |O| |X|X|O| |X|X| | 'O' Won! |O| |O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | |X| | | |X| |O| |O|O| | |X| | | |X|X|O| |O|O| | |X| | | 'O' Won! |X|X|O| |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O| | | 'X' Won! |O|O|X| |X|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| |O| | | | | |X| |O|X|O| |O|X| | | | |X| |O|X|O| 'O' Won! |O|X| | |O| |X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |O| | | | | |X|O| | |X| |O| | | | | |X|O| | |X| |O| | | |O| 'X' Won! |X|O| | |X| |O| |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| | | | |O|X| | |X|O| |X| |O| | |O|X| | |X|O| |X| |O| | |O|X| |X|X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | | |X| | | | | |O|X|O| | |O|X| | | | | |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | |O| |X|X|O| | | | | | | |O| |X|X|O| | | |X| | | |O| |X|X|O| | |O|X| 'X' Won! |X| |O| |X|X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | |O|X| |X| | | | |O|O| | |O|X| |X| |X| | |O|O| 'O' Won! | |O|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| |O| | | | |X| | |X| |O| |O| |X| |O|X| | |X| |O| |O| |X| |O|X|X| |X| |O| |O| |X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | |O| | | |O| |X| |X|X| | |O| | | |O| |X| |X|X|O| |O| | | |O| |X| |X|X|O| |O| | | |O|X|X| 'O' Won! |X|X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X|X|O| |O| |O| | | |X| |X|X|O| |O| |O| | |O|X| 'X' Won! |X|X|O| |O|X|O| | |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | |O| |X| |O| | |O|X| | |X|O| 'O' Won! |X| |O| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | |X|X| | | | |O| | | | | |X|X| | | |O|O| | | |X| |X|X| | | |O|O| |O| |X| |X|X| | | |O|O| |O|X|X| |X|X| | | |O|O| 'O' Won! |O|X|X| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| | |O| | | |X| | |X|O|O| | |O|X| | |X|O| |X|O|O| | |O|X| | |X|O| |X|O|O| |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X|X|O| |O| | | |X|O| | |X|X|O| |O| | | 'X' Won! |X|O| | |X|X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X|O| |X| | | | |O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X| | | |O|O|X| |X|X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | | | | | |O|O|X| | |X|O| | | | | |O|O|X| |X|X|O| 'O' Won! |O| | | |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X|X| | |O|O| | | | | |O|X|X| |X|O|O| | | | | |O|X|X| 'O' Won! |X|O|O| | |O| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| 'X' Won! | |O|X| | |O|X| | | |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | |O| |O| |X| |O| |X| | | |O| |O| |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O| |O| |X| |X| | |O| | |O|X|O| |X| |X| | |O| | |O|X|O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| |X| | |O| | |O|X|X| |O| |X| | |O|O| |O|X|X| |O| |X| |X|O|O| 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X|X|O| | | | | |O|O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X|O| | |O|O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| |O|X|X| | |X| | | | |O| |O|X|X| | |X|O| | | |O| |O|X|X| | |X|O| |X| |O| |O|X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | |X|O| | | |X| | | |O| |O|X|O| | | |X| | | |O| |O|X|O| | | |X| |X| |O| 'O' Won! |O|X|O| | |O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | | | | | |X|O| |X|O| | | | | | |O|X|O| |X|O| | | | |X| |O|X|O| |X|O| | |O| |X| |O|X|O| |X|O|X| |O| |X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X| |X| |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X| |X|O|X| | | |O| |O| |X| |X|O|X| | | |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| |O| | | |X| | | | |X|O| |O| | | |X| |X| | |X|O| |O|O| | |X| |X| | |X|O| 'X' Won! |O|O| | |X|X|X| | |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| | |O| |O| | |X|X| | |O|X| |O| |O| | |X|X| |O|O|X| |O| |O| | |X|X| |O|O|X| |O|X|O| 'O' Won! |O|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X|X| | |O| | | |O|O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O| | | |O|O| |O|X|X| |X|O|X| | |O|O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| |X| |O| | | | |O|X| |O| |X| 'X' Won! |O| |X| | |O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X|O| | |O| | 'X' Won! |X|O|X| |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| 'O' Won! | | |X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| 'O' Won! |O| | | |X|O|X| | | |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | |X|O|X| |O|X| | |O|X|O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X| | | |O|X| |X| |O| | |X| | | |O|X| |X|O|O| | |X| | |X|O|X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | |X| |O| |X| |O| | |O| | 'X' Won! |X| |O| |X| |O| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| | |O| | |X|X| | 'O' Won! |O| |X| | |O| | |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X|X| | |O|O| | |O| | | |X|X| | 'X' Won! |O|O| | |O| | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | |O| | | |X|X| |X| |O| | |O| | | |X|X| |X| |O| | |O|O| | |X|X| |X|X|O| | |O|O| | |X|X| 'O' Won! |X|X|O| | |O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |X|X| | |O| | | | |X|O| |X|X|O| |O| | | | |X|O| |X|X|O| |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| | |O| | |O|X| | |X| |X| |O|O| | |O|X| | |X| |X| 'X' Won! |O|O|X| |O|X| | |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O|X|X| | |O| | |X| | | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | | | |X|O| | |X| |O| | | | | |X|O| | |X| |O| |O| | | |X|O| | |X| |O| |O| |X| |X|O|O| |X| |O| |O| |X| |X|O|O| |X| |O| |O|X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |X| | |O|X| |O|O| | |X| |X| 'X' Won! | |O|X| |O|O|X| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | |X| | | | |X|X| |O|O| | |X|O| | | |X|X| |O|O| | 'X' Won! |X|O| | | |X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |X| | |O|X| | |O|O| |X| |X| | |O|X| |X|O|O| |X| |X| | |O|X| 'O' Won! |X|O|O| |X|O|X| | |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O|X| | |O|O|X| |X|X|O| |O|X| | It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | |X|O| | | | |X| | |O| | |X|O| | | | |X| | |O|O| |X|O| | | | |X| |X|O|O| |X|O| | |O| |X| |X|O|O| |X|O|X| |O| |X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| |O| |O| | |X| | | |X|X| |O| |O| | |X| | | |X|X| |O| |O| | |X|O| 'X' Won! |X|X|X| |O| |O| | |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| |O| | | | |O| | | | |X| |O| | | |X|O| | | | |X| |O|O| | |X|O| | | | |X| |O|O| | |X|O| | |X| |X| 'O' Won! |O|O|O| |X|O| | |X| |X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | |X|O| |X| |O| | |O| | | |X|O| |X| |O| | |O|X| | |X|O| |X| |O| |O|O|X| | |X|O| |X|X|O| |O|O|X| | |X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| | |O|O| |X| | | |X|O|X| | |O|O| |X| | | 'O' Won! |X|O|X| | |O|O| |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | 'O' Won! |X| |O| |X| |O| | | |O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| |X| |X|O| | |O| | | |X|O|X| |X|O|X| |O| | | |X|O|X| |X|O|X| |O| |O| |X|O|X| 'X' Won! |X|O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X|O| | |X|O|O| | |X| | |X|O| | |X|O|O| |O|X| | |X|O| | |X|O|O| |O|X|X| |X|O| | It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X|O|X| |O| | | | | |X| |X|O|X| |O| | | |O| |X| |X|O|X| |O|X| | |O| |X| |X|O|X| |O|X|O| |O| |X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | |X| | | | |O|O| | |X| | 'O' Won! |X| | | |O|O|O| | |X| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | |X| |X|O|O| | | |O| | | |X| |X|O|O| | | |O| | |X|X| |X|O|O| | |O|O| | |X|X| |X|O|O| |X|O|O| | |X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | |X| | |O|X| | | |O| |O| |X| | |O|X| |X| |O| |O| |X| | |O|X| |X| |O| |O|O|X| |X|O|X| |X| |O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| |O| |X| |X| | |O|O| |X| |O| |X|O|X| | |O|O| |X| |O| 'X' Won! |X|O|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | | |X| |O| |O| | |O|X| | | |X| |O| |O| |X|O|X| | |O|X| |O| |O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X| |O| |X| |X|O|O| | | |X| |O| |X| |X|O|O| |O| |X| |O| |X| 'X' Won! |X|O|O| |O|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X| | | |O|O| | |O|X| | |X| | | |O|O| |X|O|X| | |X|O| | |O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| | |O| | | |X| | |O| |X| | |O| | | |X|O| |O| |X| | |O| | | |X|O| |O|X|X| | |O| | |O|X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X|X| 'O' Won! |O|O|O| | |X| | | |X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| |X| |O| |O| | | |X|O|X| |X| |O| |O| | | |X|O|X| |X|X|O| |O| | | |X|O|X| |X|X|O| |O|O| | 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| | | |O| | | |X| |O| |X| |X| |O| |O| |X| |O| |X| |X| |O| 'X' Won! |O| |X| |O|X|X| |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X| | | |X|X| | |O|O| |O|X| | | |X|X| 'X' Won! |X|O|O| |O|X| | | |X|X| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | |O| | | |O|X|X| |X| | | |O| | | |O|X|X| |X| |O| |O| | | |O|X|X| |X| |O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | 'X' Won! |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | |O| | | |X| |O| |X| |X| |O| | | |X| |O| |X| |X| |O| | |O|X| |O|X|X| |X| |O| | |O|X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| |O|X| | | |O|X| | |X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O| |X| |O|O| | |X|X| | |O| |X| |O|O|X| |X|X| | |O| |X| |O|O|X| |X|X| | |O|O|X| 'X' Won! |O|O|X| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| | | | | |X| | |O|O| |X| | | | |X|X| | |O|O| |X|O| | | |X|X| | |O|O| |X|O| | | |X|X| |X|O|O| |X|O|O| | |X|X| |X|O|O| 'X' Won! |X|O|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X|O| |X| | | |X|O| | | |X|O| |X| | | |X|O| | | |X|O| |X|O| | 'X' Won! |X|O| | | |X|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | |O|X| |O|X|O| | | |X| | |O|X| |O|X|O| | |O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X| |O| | | |O| | |O|X| |X| |O| | | |O| |X|O|X| |X|O|O| | | |O| |X|O|X| 'X' Won! |X|O|O| |X| |O| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| | |X|O| | |O|X|X| |X|O| | |X|O| | 'O' Won! |O|X|X| |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| |X| | | | |O| | |O| |X| |X| | | | |O| | |O|O|X| |X| | | |X|O| | |O|O|X| 'O' Won! |X| |O| |X|O| | |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | |O|X|O| | | | | 'X' Won! | |X| | |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X|O|O| | | |X| | | | | |X|O|O| | |X|X| | | |O| |X|O|O| | |X|X| |X| |O| |X|O|O| | |X|X| 'O' Won! |X| |O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | | | |X| | | | |X|O| | |O| | 'X' Won! |X| | | | |X|O| | |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| |O| |X| | | | | | |X|O| |O| |X| | | | | |X|X|O| 'O' Won! |O| |X| | |O| | |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | |O| | | |X|X| |X| |O| | |O| | |O|X|X| |X| |O| | |O| | |O|X|X| |X|X|O| | |O| | |O|X|X| |X|X|O| | |O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| |X| |X| | |O| | | |O|O| |X| |X| | |O| | 'X' Won! | |O|O| |X|X|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| | | |O| |O| |X|X| | |O| |X| |O| |O| |X|X| | |O|O|X| |O| |O| |X|X| | |O|O|X| |O|X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| |O| | | | | |X| |X| |O|X|O| | | | | |X| |X| |O|X|O| |O| | | 'X' Won! |X|X|X| |O|X|O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X| | | |X|O| |O| | | |O|X|X| | |X|O| |O|O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| |X| | | |X| | | |O| |O|O|X| |X| |X| | | |O| |O|O|X| |X| |X| |O| |O| |O|O|X| 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| |O| | | 'X' Won! | |O| | |X|X|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| |X| | | | | | |O|O| |X|O|X| | | |X| | |O|O| |X|O|X| 'O' Won! | |O|X| | |O|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| | | |X| |X| |O| |O| |X| | | |X|O|X| |O| |O| |X|X| | |X|O|X| |O| |O| 'O' Won! |X|X| | |X|O|X| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O| | | |X| |O|X|O| | | |O| | | |X| |O|X|O| | | |O| | |X|X| |O|X|O| | | |O| |O|X|X| |O|X|O| |X| |O| |O|X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O| |X| |X| |O| | | |X| |O|O|X| |X| |O| | | |X| |O|O|X| |X|X|O| | | |X| |O|O|X| |X|X|O| | |O|X| 'X' Won! |O|O|X| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X|O|O| | |X| | | | |X| |X|O|O| | |X|O| | | |X| |X|O|O| |X|X|O| | | |X| |X|O|O| |X|X|O| | |O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | | |X| | |X| |O| |O|X|O| | |X|X| |X| |O| |O|X|O| |O|X|X| |X| |O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | |X|X| | |O| |O| | | | | |X|X| | |O| |O| | | |O| |X|X| | |O| |O| |X| |O| |X|X| | |O| |O| |X|O|O| 'X' Won! |X|X|X| |O| |O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| | | |X|O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | |O|O| | | |O| |X| |X| |X|O|O| | |O|O| |X| |X| |X|O|O| 'X' Won! | |O|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | |X|O|O| | |X|O| | | | | |X|O|O| |X|X|O| | | | | |X|O|O| |X|X|O| | |O| | |X|O|O| |X|X|O| | |O|X| |X|O|O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| | | |O|O|X| |O| | | |X| |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| |O| 'X' Won! |X|X|X| |O|O|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X|X| 'O' Won! |O| |X| |O| | | |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|O| | | | | |O|X|X| |X|O|O| | | | | 'O' Won! |O|X|X| |X|O|O| | | |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X|O| |X| | | |O|X| | 'X' Won! |O|X|O| |X|X| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | |O|X| |O|X| | | | | | |X|O|X| |O|X| | | | | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X| | |O|X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | |O| |X|X| | | | |O| |X| |O| |X|X| | | | |O| 'O' Won! |X| |O| |X|X|O| | | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | | |X| |X|O| | |O| | | |X| |X| |X|O| | |O| | | |X| |X| |X|O| | |O| |O| |X| |X| |X|O|X| |O| |O| |X|O|X| |X|O|X| |O| |O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | | |O| | |X| | |O|X| | |O| |O| | |X| | |O|X| | |O| |O| |X|X| | |O|X| | |O| |O| |X|X|O| |O|X| | |O| |O| |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |O| |O|X|X| | | | | | | |O| 'O' Won! |O|X|X| | |O| | | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | | | | |X|X| 'O' Won! |O|O|O| | | | | | |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| | |O| | |X| | | |O| |X| | |O| | |X|O| | |O| |X| | |O|X| |X|O| | |O| |X| 'O' Won! | |O|X| |X|O| | |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | |X|O| |O| |X| |X| | | |O|X|O| |O| |X| |X|X| | |O|X|O| |O|O|X| |X|X| | |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | |O| |X| | | |X| |O|O| | |O| |X| |X| |X| 'O' Won! |O|O|O| |O| |X| |X| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| |O| |X| 'X' Won! | |O|X| |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O|O| |X| | | | |X| | | |O|O| |X|X| | | |X| | 'O' Won! |O|O|O| |X|X| | | |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | |O|X| | | |O|X| | | | | |O|X|X| | |O|X| | | | | |O|X|X| |O|O|X| 'X' Won! | | |X| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | |X| |X| | |O|O| | | | | |X| |X| | |O|O| | |O| | |X| |X| |X|O|O| | |O| | |X| |X| |X|O|O| | |O|O| 'X' Won! |X| |X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O| |O| |X| |O| | |X| | |O| |O| |X|X|O| |O|X| | |O| |O| |X|X|O| |O|X|X| |O| |O| |X|X|O| 'O' Won! |O|X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | |O| | |X|O| |X| | | | | |O| |X|X|O| |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X|X| | 'O' Won! |O|O|O| |X|X|O| |X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | | |O| |X| |X| |O|O|X| | | |O| |X|O|X| |O|O|X| | | |O| |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X|O| | |X|O| |X|O| | | |X|O| 'O' Won! | |X|O| |X|O|O| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X| |O| | | | | |X|O|O| |X|X|O| | | | | |X|O|O| |X|X|O| | |O| | 'X' Won! |X|O|O| |X|X|O| |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| | |O| | 'O' Won! | |O|X| |X|O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| |O| | |O| | | | |X| |X|X|O| | |O| | | | |X| |X|X|O| |O|O| | | | |X| |X|X|O| |O|O| | |X| |X| 'O' Won! |X|X|O| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X|X| |X| | | | | |O| |O|X|X| |X| | | | |O|O| |O|X|X| |X| |X| | |O|O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | |O| |X| | | 'X' Won! |X| |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |O|X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | | |X|X| |O|X| | |O| |O| | |X|X| |O|X| | 'X' Won! |O|X|O| | |X|X| |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | | | |O| |O| | |X|X| | |X| | |O| |O| 'O' Won! | |X|X| | |X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| |X|O|O| |O| |X| |X| |X| |X|O|O| |O|O|X| |X| |X| 'X' Won! |X|O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | |O| |O| |X| |X|X| | | | |O| |O| |X| |X|X|O| | | |O| 'X' Won! |O| |X| |X|X|O| |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | | |O|X| | | |O| |X| | | | |O|X| | |X|O| |X|O| | | |O|X| | |X|O| |X|O| | | |O|X| |X|X|O| |X|O|O| | |O|X| |X|X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O|X| | |X|O| |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | |O| | | |X|X| |X| |O| | |O| | | |X|X| |X|O|O| | |O| | | |X|X| |X|O|O| | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X|X| | | |O| |X| |O| |O|X|X| | | |O| |X|X|O| |O|X|X| | |O|O| |X|X|O| |O|X|X| 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X| |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |X|X| |O|X|O| | |O|X| |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | | |O| | |O|X| | |X| | | |X|O| | |O|X| | |X|O| | |X|O| |X|O|X| | |X|O| | |X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X|O|X| |X| | | |O|X|O| |X|O|X| |X| | | 'O' Won! |O|X|O| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| |O| | |X|X| |X|O| | |O| |O| 'O' Won! |O|X|X| |X|O| | |O| |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | |O| | |X|O| | |X|X| | | |O| | |X|O| | |X|X| | | |O| |O|X|O| 'X' Won! | |X|X| | |X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | |X| |O| |X| | | | |O|O| |X| |O| |X| |X| | |O|O| |X| |O| 'O' Won! |X| |X| |O|O|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | |O| | |O| | |X| |X| | |X|O| | |O| | |X| |X| | |X|O| | |O|O| |X| |X| | |X|O| |X|O|O| |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| |O|X| | | | |O| |X|O|X| |O|X| | | | |O| |X|O|X| |O|X| | |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | |X| |X| | | |O| | |O| | |X| |X| | | |O| | |O|O| 'X' Won! |X|X|X| | | |O| | |O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X|X| | | | |O| |O| | | |X|X| | | | |O| |O| |X| |X|X| | | |O|O| |O| |X| |X|X| | 'X' Won! | |O|O| |O| |X| |X|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O|O| | | | | |X| |X| |X|O|O| | | | | |X|O|X| |X|O|O| | | | | |X|O|X| |X|O|O| | | |X| |X|O|X| |X|O|O| |O| |X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| |X|O| | | | | | |O| |X| |X|O| | |O| | | |O| |X| |X|O| | |O| | | |O|X|X| |X|O|O| |O| | | |O|X|X| 'X' Won! |X|O|O| |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | | | |X|O|X| | | |O| | | |O| |X|O|X| | | |O| | | |O| |X|O|X| |X| |O| | |O|O| |X|O|X| |X| |O| 'X' Won! |X|O|O| |X|O|X| |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| | |X| | | |O| | |O|X|X| | |X| | |O|O| | |O|X|X| | |X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | | |O|O| |X| |X| | | | | |X|O|O| |X| |X| |O| | | |X|O|O| |X| |X| |O|X| | |X|O|O| |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | |X| | |X| | |O|O| | | | |X| | |X|O| |O|O|X| | | |X| | |X|O| 'O' Won! |O|O|X| | |O|X| | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| |O| | | | | |X| |X|O|X| |O| | | | | |X| |X|O|X| |O| |O| | | |X| |X|O|X| |O|X|O| | |O|X| |X|O|X| |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | |O| | 'X' Won! |X| | | |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| | |O|O|X| | | | | |X|X| | |O|O|X| | | | | |X|X| | |O|O|X| | |O| | 'X' Won! |X|X|X| |O|O|X| | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | | |O|O| | |X|X| |O| | | | |O|O| | |X|X| |O|X| | | |O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| |O|X| | It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X| | | |O|O| 'X' Won! | |O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O|X| | |X| |X| |O| |O| |O|X| | |X|O|X| |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O|X| | |O|O| |X| |X| | |O|X| 'X' Won! | |O|O| |X|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O| | |O| |X| |X| | | |X|O| | |O| |X| |X| | | |X|O| | |O|O|X| |X| |X| |X|O| | |O|O|X| |X| |X| |X|O|O| |O|O|X| 'X' Won! |X|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | | | 'O' Won! |O| |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| | |O|X| |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X|O| | |X| | | | |O| |O|X|O| |X|X| | |O| |O| |O|X|O| |X|X| | 'X' Won! |O|X|O| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| |O|X| | | |O| | | |X|X| |O|X| | | |O|O| | |X|X| |O|X| | | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O|O|X| | | |X| | | |O| |O|O|X| |X| |X| | | |O| |O|O|X| |X| |X| | |O|O| 'X' Won! |O|O|X| |X|X|X| | |O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O| | |X|X| |O| | | |X| |O| | |X|X| |O|O| | |X|X|O| | |X|X| |O|O| | 'O' Won! |X|X|O| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|O| |X| | | | |X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| 'X' Won! |X| |X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X|X| |O| | | |O|O| | | |X|X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O| |O| |O|O|X| | |X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | |X|O|X| | | | | | |O| | 'O' Won! |X|O|X| | |O| | | |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | |X| | | | |O| |X|O| | | |X| | |O| |O| |X|O| | | |X| | |O|X|O| |X|O| | | |X|O| |O|X|O| |X|O| | |X|X|O| |O|X|O| |X|O| | 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| | | |X| | 'O' Won! |O| | | |X|O| | | |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | |O|O| | | |X|O| | |X| | |O|O|X| | |X|O| |O|X| | |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X| | | |O|X| |O|O|X| | |X| | | |O|X| 'X' Won! |O|O|X| | |X|X| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| |X| | |O| | |X|O| | |O| |X| | |O| | |X|O| | |O|X|X| | |O| | |X|O|O| |O|X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O|O|X| |X| |X| |O| | | |O|O|X| |X| |X| |O| |O| |O|O|X| 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| |X| |O| | | |X| |O| |O| |X| |O| |X| |X| |O| |O| |X| |O|O|X| |X| |O| |O| |X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | |O| | |X|X| | | |O|X| | |O| | |X|X| | |O|O|X| | |O| | |X|X| | 'X' Won! |O|O|X| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |O| |O| | |X|X| |X|O| | |O| |O| 'O' Won! |O|X|X| |X|O| | |O| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X| | |X|X| | | | |O| |O|X| | |X|X|O| | | |O| |O|X|X| |X|X|O| |O| |O| |O|X|X| |X|X|O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | | | | | |O|X|O| |O|X|X| | | | | |O|X|O| 'O' Won! |O|X|X| | |O| | |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | |X|X| | | |O| |O| | | | |X|X| | | |O| |O| |O| | |X|X| | |X|O| |O| |O| 'O' Won! | |X|X| | |X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| | | |X|X| |X| | | |O|O| | | |X|X| |X|O| | 'X' Won! |O|O| | |X|X|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O|X| | | |O| | |X| | |X|O|X| |O| |O| | |X| | |X|O|X| |O| |O| | |X|X| 'O' Won! |X|O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O|X|O| | |X| | | | |O| |O|X|O| | |X| | | | |O| |O|X|O| | |X|X| |O| |O| |O|X|O| | |X|X| 'X' Won! |O| |O| |O|X|O| |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| | | | | |O| |O|X|O| |X|X| | | | |O| 'O' Won! |O|X|O| |X|X|O| | | |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O|X| | | |O| | |X|X| | |O|X|O| | |O| | |X|X| | 'X' Won! |O|X|O| | |O| | |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | |X| | | | |X| | |O|O| |O|X| | | | |X| | |O|O| |O|X|X| | | |X| | |O|O| |O|X|X| | |O|X| 'X' Won! |X|O|O| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O|O| | | |X| | | | | |X|O|O| | |X|X| | | | | |X|O|O| |O|X|X| | |X| | |X|O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| | |X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O|O| |O| | | |X|X| | |X|O|O| |O| | | |X|X| | |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | |O|X| | |X| | |O|O|X| |X|O|X| | |X| | 'O' Won! |O|O|X| |X|O|X| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | |X|X| | | | | 'O' Won! |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | | |X| |X|O| | | |O|O| | | |X| |X|O| | |X|O|O| | | |X| |X|O| | |X|O|O| |O| |X| |X|O| | |X|O|O| |O|X|X| |X|O| | It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| |X| |X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | | |X| | | |X| |O|X|O| |O| |X| | | |X| |O|X|O| |O|X|X| | | |X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | |O|X| | |O|O| |X| |X| | |O|X| 'O' Won! | |O|O| |X|O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| | | |O| |O| | |X|O| |X| | | |O|X|O| | |X|O| 'O' Won! |X| |O| |O|X|O| | |X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | | |O|X|O| |O|X| | 'X' Won! | |X| | |O|X|O| |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| |O|X| | | |O|X| | |O|X| |O|X| | | |O|X| | |O|X| |O|X|O| | |O|X| |X|O|X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | |X|O|X| | |O| | 'O' Won! | |O| | |X|O|X| | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X|O| | | | | |X|O| | | |X|O| |X| | | |X|O| | | |X|O| |X|O| | |X|O| | 'X' Won! |X|X|O| |X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | |O| |O| | | |X|X| | 'X' Won! | | |O| |O| | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X| |O| |O|O| | |X| | | |X|X|O| |O|O| | |X| |O| |X|X|O| |O|O| | 'X' Won! |X| |O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | | |O| | |X| |X| |O|X| | | |O| | |X| |X| |O|X| | |O|O| | 'X' Won! |X|X|X| |O|X| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O| | |O|X|O| | |X|X| | |O| | |O|X|O| | |X|X| | |O|O| |O|X|O| | |X|X| |X|O|O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | | | |X| | 'O' Won! |O|X| | |O| | | |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | |X|O|X| |O| |O| | |X| | |X|O|X| |O| |O| | |X|X| |X|O|X| |O| |O| |O|X|X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | | | | | |X|X|O| 'O' Won! |O|X| | | |O| | |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | |X|O| |X| | | | | |O| |O|X|O| |X| | | | |X|O| |O|X|O| |X| | | |O|X|O| |O|X|O| |X| | | |O|X|O| |O|X|O| |X| |X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | |X| | | | | | |X|O|O| | |X| | | | |O| |X|O|O| | |X| | | |X|O| |X|O|O| |O|X| | | |X|O| |X|O|O| 'X' Won! |O|X|X| | |X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| |O|X| | |X| | | | |O|O| |O|X|X| 'O' Won! |X| |O| | |O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| | |O|X| |X|O| | | | |X| | |O|X| |X|O| | |O| |X| | |O|X| |X|O| | |O|X|X| |O|O|X| |X|O| | |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| | | | | |X| |X|O|X| |O|O| | | | |X| 'X' Won! |X|O|X| |O|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | | |O|X| | | |X| |O|O| | |X|O|X| | | |X| |O|O| | 'O' Won! |X|O|X| | |O|X| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O|O| 'X' Won! | | |X| | |X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| |O| |X|X| | | |O| | |O|X|O| |X|X| | | |O| | |O|X|O| |X|X|O| | |O| | |O|X|O| |X|X|O| |X|O| | 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O|X| |O| | | | | |X| |O|O|X| |O| | | | | |X| |O|O|X| |O|X| | | | |X| 'O' Won! |O|O|X| |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | |X|X| | | |O| |O| |O| | |X|X| | | |O| |O|X|O| | |X|X| | |O|O| |O|X|O| | |X|X| 'X' Won! |X|O|O| |O|X|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X|O| | |X| | | |O| |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O|X| 'O' Won! |X|O| | |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X|O| | |X| |O| | |O| | |X|O| | |X|X|O| | |O| | |X|O|O| |X|X|O| | |O| | 'X' Won! |X|O|O| |X|X|O| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| |O| | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O|O| | | | |X| | |X|O| |O|O|X| |O| |X| | |X|O| |O|O|X| |O|X|X| | |X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X| | | |X|X| | |O| |O| |X|O| | |X|X| | 'X' Won! |O| |O| |X|O| | |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| | | | | |X|O| | |O| |X| | | | | |X|O| | |O|O|X| | | | | |X|O| | |O|O|X| | |X| | |X|O| | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | | |O| | |X|X|O| |O|X| | | |O| | |X|X|O| |O|X|O| | |O| | 'X' Won! |X|X|O| |O|X|O| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| | | | | |O|X| | |X|X|O| | | | | |O|X| | |X|X|O| | |O| | |O|X|X| |X|X|O| | |O| | |O|X|X| |X|X|O| |O|O| | It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| | | |O| |X|O| | | |O|X| |X| |O| |X|O| | |O|O|X| |X| |O| |X|O| | |O|O|X| |X| |O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X| |O| | |X| | |O|O| | |X| |O| |X|X| | |O|O| | 'O' Won! |X| |O| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | |O| | | |X|X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | |X| |X| |O| | | |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| |X| 'O' Won! |X| |X| |O|O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O| |X| | | |O| | |X|O| |O| |X| | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O|O|X| | |X|O| |X|X|O| |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | |O| | |O|X|X| |X| | | | |O| | |O|X|X| |X| | | | |O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X|O| | |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |X| |O| | |X| | |X|O|O| |X| |O| | |X|X| |X|O|O| |X| |O| 'O' Won! |O|X|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | | | |X| |O|O|X| |X| | | |O| |X| 'X' Won! |O|O|X| |X| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | |O|X| |X| | | |O| |O| |X|O|X| |X| | | |O| |O| |X|O|X| |X| |O| |O| |O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | |O| | |X| |X| |O| |O| 'X' Won! | |O| | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | | | |X| |O| | |X|O| | |X| | |X| |O| | |X|O| |O|X| | |X| |O| | |X|O| |O|X|X| |X| |O| | |X|O| 'O' Won! |O|X|X| |X|O|O| | |X|O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| |O| | | |O| |O| |X| |X| |O| |X| |O| |O| |X| 'O' Won! |X| |O| |X|O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | |O| | | | |X| |X| |O| | |O|X| | | |X| |X| |O| | |O|X| | |O|X| |X| |O| |X|O|X| | |O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| |O| |X| |O|X| | | |X|O| |O| |X| |O|X| | 'O' Won! | |X|O| |O|O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O|O| | |X| |X| | |O| | |O|O| | |X| |X| |X|O| | |O|O| | |X| |X| |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | | |X| |O|O| | 'X' Won! |O| |X| | | |X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | |O| | | |X|X| | |O|O| | |O| |X| |X|X| | |O|O| | 'O' Won! |O| |X| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | |O|X| |X| | | |O|X| | | |O|X| |X| | | |O|X| | | |O|X| |X|O| | |O|X| | |X|O|X| |X|O| | 'O' Won! |O|X| | |X|O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | | | | |X| | | |O|X|O| | | | | |X|O| | |O|X|O| | | | | |X|O|X| |O|X|O| | |O| | |X|O|X| |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| |X| | | | | |O|O| | |X| |X| | | | | |O|O|X| |X| |X| |O| | | |O|O|X| |X| |X| 'X' Won! |O| | | |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| |X| | | | |O| | |O| |X| |X| | | |O|O| | |O| |X| |X| |X| |O|O| | |O| |X| |X|O|X| |O|O| | |O| |X| 'X' Won! |X|O|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X| |X| | |O| | |O|X|O| 'X' Won! |X|X|X| | |O| | |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O|X| |X| |O| |O| | | | |O|X| 'X' Won! |X| |O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| | | |X|O| |O|X| | | |O| | | |X|O| |O|X| | | |O|X| | |X|O| |O|X| | |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | |X| |X|O| | | |O| | 'O' Won! | |O|X| |X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X|O| | | | |O| | | |X| |X|O| | | | |O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O|X| | |O|O| | |X|X| |X|O|X| | |O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| | |O|O| 'X' Won! |X| | | |X| |O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| | |X|O| |X| |X| | | |O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X|O|X| 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | |X|O| |O| | | | | |X| |O|X|O| |O| | | | | |X| |O|X|O| |O|X| | | | |X| |O|X|O| |O|X|O| | | |X| |O|X|O| |O|X|O| |X| |X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| | |X| |O| | |X|X| |O|O| | |X| |O| | |X|X| 'X' Won! |O|O| | |X| |O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| |O| |O| | | | | |X|X|O| |O| |O| | |X| | |X|X|O| |O| |O| |O|X| | 'X' Won! |X|X|O| |O|X|O| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | |O| | |X|O| | | | |X| | |O|X| |X|O| | | | |X| |O|O|X| |X|O| | | | |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O|O| |X| |X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | | | |O|X| | | |O|X| | |X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X|X| |O|X| | 'O' Won! |O|O|X| |O|X|X| |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| | | |O| |X| |X| | | |O| | |X|O| |X| |X| | |O|O| | |X|O| 'X' Won! |X|X|X| | |O|O| | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X|O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | |X|O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| | |X|O| 'X' Won! |X|O|X| |O|X|O| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X| |O| |X| | | |O|O| | |X| |O| |X| |X| |O|O| | |X| |O| |X|O|X| |O|O|X| |X| |O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| |O| |X| |X| | | |O| |O| |O| |X| |X| |X| |O| |O| |O|O|X| |X| |X| |O|X|O| |O|O|X| |X| |X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O|X| | | |X|O| |O| |X| |O|X| | | |X|O| |O| |X| |O|X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X| | |X|X| |O| |O| | |O|X| | |X|X| 'X' Won! |O| |O| | |O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |O| | |O| | |X|X|O| | |X|O| | |O| | 'O' Won! |X|X|O| | |X|O| | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | | |O|X| | | |O| |X| |X| | |O|X| | | |O| |X|O|X| | |O|X| | |X|O| |X|O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X|X| | |O| | | |O|O| | |X|X| | |O| | | 'X' Won! |O|O| | |X|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O| |X|X| | | |X| | | |O|O| |X|X| | | |X|O| |X|O|O| |X|X| | | |X|O| 'O' Won! |X|O|O| |X|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O|O| | |X| | | |X|O| | |O|O| | |X|X| | |X|O| | 'O' Won! |O|O|O| |X|X| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| |X| | | |X| |O| |O| 'O' Won! |X| |X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | 'X' Won! |O|X|O| | |X|O| |X|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X|O| |X|O| | | | |O| |X|X|O| |X|O| | |O| |O| |X|X|O| |X|O| | |O|X|O| |X|X|O| 'O' Won! |X|O|O| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| |X|X|O| | |X|X| 'O' Won! |O|O|O| |X|X|O| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | 'X' Won! | |X| | |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X|X| | |O|O| | | | |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X|O|O| |X|X| | |O|O| | 'X' Won! |X|O|O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| |X| | |O| | |X|X|O| |O| |X| | |O|X| |X|X|O| |O| |X| | |O|X| |X|X|O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O|X| |X| | | |O| | | | |O|X| |X| | | |O|O| | |X|O|X| |X| | | |O|O| | 'O' Won! |X|O|X| |X|O| | |O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O|O| | | | | |O|X|X| | |O|O| | | |X| |O|X|X| 'O' Won! | |O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |X| |O|O|X| | | | | | | |X| 'X' Won! |O|O|X| | | |X| | | |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| |X| | | |O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| |X| |X| |O| |X| | |O|O| |X|O|X| |O|X|X| | |O|O| |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X|O| | |X|X| |O| | | | |X|O| | |X|X| |O| | | |O|X|O| | |X|X| |O| |X| |O|X|O| | |X|X| |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | |O|O| |X| | | |X| | | 'X' Won! |X|O|O| |X| | | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| | | | |O|X| |O|O| | |X|X| | | |O|X| |O|O| | |X|X|O| | |O|X| |O|O| | |X|X|O| |X|O|X| 'O' Won! |O|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| |X| | | | |X| | |O|X|O| |X| | | |O|X| | |O|X|O| |X| |X| |O|X| | 'O' Won! |O|X|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | |X| |O| | |X|X| | |O| | |X| |O| |O|X|X| | |O| | |X| |O| |O|X|X| |X|O| | |X|O|O| |O|X|X| |X|O| | 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | | | | | 'X' Won! |O|X|O| | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| |X|X| | | |O|O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| | |O|O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |X| |O|O| | |X| |X| |O| |X| 'X' Won! |O|O| | |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | |O| | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| |X|O| | |X|O|X| |O| |O| |X|O| | |X|O|X| |O|X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| | | | | |O| |X|O|X| |O| | | | |X|O| |X|O|X| |O| | | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O|O|X| | | |O| | |X| | |O|O|X| | | |O| |X|X| | |O|O|X| | | |O| |X|X|O| |O|O|X| |X| |O| |X|X|O| 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| |X| |O| | | |X|X| | |O| |X| |O| |O| |X|X| | |O|X|X| |O| |O| 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | | |O|O| |X| | | |O|X| | |X|O|O| |X| | | 'O' Won! |O|X| | |X|O|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | |X|O| |X| | | | |O|O| | |X|O| |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X|O| | 'X' Won! |X|O|O| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | 'O' Won! |X|O| | |X|O| | | |O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| |X| | | |X|O|X| | | |O| |X| | | |X|O|X| | |O|O| 'X' Won! |X| | | |X|O|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | | |X|O| | |O| | |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X|O| | |O|X|O| | |O|X| |X|O|X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|X| | |O| | |X| | | |O|X|X| | |O|O| |X| | | |O|X|X| |X|O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| |X|O| | | | |O| | |X|X| |X|O| | | | |O| |O|X|X| 'X' Won! |X|O| | | |X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | |O|X|X| | |X| | | |O| | |O|X|X| 'O' Won! | |X|O| | |O| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| |O| | | |X|O| | 'X' Won! | | |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | |O| |X| | |O|X| | | | | |O|X|X| | |O|X| | | | | |O|X|X| |O|O|X| 'X' Won! |X| | | |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X| | |O|X|O| | | |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O| | | |X| |X| | |X|O| |O| |O| |X| |X| 'X' Won! | |X|O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | |O|X| | | | | |O|X| | | |O|X| | | | | |O|X|X| | |O|X| | |O| | |O|X|X| | |O|X| |X|O| | |O|X|X| |O|O|X| |X|O| | |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| |O| | | | | | |X|X| |O| |O| | | |X| 'O' Won! | |X|X| |O|O|O| | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| |X| | |O|X| |X|O| | |O| |X| | |O|X| |X|O| | |O|X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O| | |X| |O| |X| |X| | |O| | |X|O|O| 'X' Won! |X|X|X| | |O| | |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| |O| 'X' Won! |O| |X| |O|X| | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | |O| |X| |O|X| | | | | | |O|X|X| |O|X| | | | | | |O|X|X| |O|X| | | |O| | |O|X|X| |O|X|X| | |O| | |O|X|X| |O|X|X| | |O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | | | | |O|X|O| | |X|X| |O| | | |O|X|O| | |X|X| |O| |X| |O|X|O| | |X|X| |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | 'O' Won! |O| | | |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| |O| |X| |X| | | | | |O|X|O| |X| |X| |O| | | |O|X|O| |X| |X| 'X' Won! |O| |X| |O|X|O| |X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | |O| |O| |O|O|X| 'X' Won! |X|X|X| |O| |O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O| |O| |X| |X| |O| |X| |O| |O| |X| |X| |O|O|X| |O| |O| |X| |X| 'X' Won! |O|O|X| |O|X|O| |X| |X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| 'O' Won! |O|X| | | |O|X| | | |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O|O|X| | | | | |X| |X| |O|O|X| | | | | |X| |X| |O|O|X| | | |O| |X| |X| |O|O|X| |X| |O| |X| |X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| | | |X| |O| | |O|O| 'X' Won! |X| | | |X| |O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X|X| |O| |O| | |X| | | |X|X| |O| |O| |O|X| | 'X' Won! |X|X|X| |O| |O| |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| |X| 'O' Won! | | | | |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| |O| |X| | | | | | |X|O| |O| |X| |X| | | | |X|O| |O| |X| |X|O| | |X|X|O| |O| |X| |X|O| | |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | |O|X| |X| | | | | |O| | |O|X| |X|X| | | | |O| | |O|X| |X|X| | | |O|O| 'X' Won! | |O|X| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| |O| |X| |O|X| | | | |O| |O|X|X| |O|X| | | | |O| |O|X|X| |O|X|O| 'X' Won! | |X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | 'X' Won! | |X| | |O|X| | |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| |X| | | | |O|O| | |X|O| |X| | | | |O|O| | |X|O| |X| |X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'O' Won! |O|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | |O| |X| | | |O|X| | |X| |O| |X| | | |O|X| | |X| |O| |X| |O| |O|X| | |X|X|O| |X| |O| |O|X| | 'O' Won! |X|X|O| |X| |O| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | |X|X| | | | |O| |O|X| | |X|X|O| | | |O| |O|X| | |X|X|O| | | |O| |O|X|X| |X|X|O| |O| |O| |O|X|X| 'X' Won! |X|X|O| |O|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | |X| | | | | | |X|O|O| | |X| | | |O| | |X|O|O| | |X| | |X|O| | |X|O|O| |O|X| | |X|O| | 'X' Won! |X|O|O| |O|X| | |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | |O| |X| | |O|X| | |O| | |O| |X| |X|O|X| |O|O| | |O| |X| |X|O|X| 'X' Won! |O|O|X| |O| |X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | | | | |O|X|O| | |O|X| | | | | |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | |X| | | | |X| | |O| | | |X| | | | |X| | |O|O| | |X| |X| | |X| | |O|O| | |X|O|X| | |X| | |O|O| | |X|O|X| | |X|X| |O|O| | 'O' Won! |X|O|X| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | | |O|X| | | |X|O| |X| | | |O|X| | | |X|O| |X| | | |O|X|O| 'X' Won! | |X|O| |X|X| | |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | | |X| | |O|X|X| |O| | | | |X| | |O|X|X| |O| |O| | |X|X| |O|X|X| |O| |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| |O|X| | | | |O| 'X' Won! | | |X| |O|X| | |X| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | |X| | |O| |X| |O|O| | | |X| | |O| |X| |O|O| | |X|X| | |O|O|X| |O|O| | |X|X| | 'X' Won! |O|O|X| |O|O| | |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X| | | |O| | |X| |X| |O|X| | | |O|O| |X| |X| |O|X| | |X|O|O| |X| |X| |O|X| | |X|O|O| |X|O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X|X| | |O| | | | |O|O| |X|X| | |O| | | |X|O|O| |X|X| | 'O' Won! |O| | | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| | | |O|O| | | |O|X| |X| | | |O|O|X| 'O' Won! | |O|X| |X|O| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | |X|X| | |O| |O| | | | | |X|X| | 'X' Won! |O| |O| | | | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| | | |X| |O| |O|X| | |O| | | |X| |O| |O|X| | |O|X| | |X| |O| |O|X|O| |O|X| | |X| |O| 'X' Won! |O|X|O| |O|X| | |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| |O|O| | | |X| | | |X|X| |O|O| | | |X| | | |X|X| |O|O| | | |X|O| | |X|X| |O|O| | |X|X|O| 'O' Won! |O|X|X| |O|O| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O| | |O| |O| |X|X| | | |O| | |O|X|O| |X|X| | |O|O| | |O|X|O| 'X' Won! |X|X|X| |O|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| |O| |X| |O|X| | | | |O| |O|X|X| |O|X| | | |O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| |O|X| | It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| | | | |O|X| |X| |X| |O|O| | | |O|X| |X| |X| |O|O| | |X|O|X| |X| |X| 'O' Won! |O|O| | |X|O|X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O| | | |X|O| | |O|X| | |O| |X| |X|O| | |O|X|O| |O| |X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X|O| | |X| | |O| | | |O|X|O| |X|X| | |O| |O| |O|X|O| |X|X| | 'X' Won! |O| |O| |O|X|O| |X|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | |X| | |O|O| |X|X| | |O| |X| | |O|O| |X|X| | |O| |X| |X|O|O| |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | |X| | |O| |O| |X| |X| | |X|O| |O| |O| |X| |X| |X|X|O| |O| |O| |X| |X| 'O' Won! |X|X|O| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | |O|X| | | | | |X| |O| |X|O|X| | | | | |X| |O| |X|O|X| | |O| | |X| |O| 'X' Won! |X|O|X| |X|O| | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| |X| | | |X| |X|O| | |O|O|X| | |X|X| |X|O| | |O|O|X| | |X|X| |X|O|O| |O|O|X| 'X' Won! |X|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| |O| | | |O|O|X| 'X' Won! | | |X| |O| |X| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| | | | | | 'O' Won! |O|X| | |X|O| | | | |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | |O| | |O| | | 'X' Won! |X|X|X| | |O| | |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | | | |O| |X|O|O| |X| | | | |X|O| |X|O|O| |X|O| | | |X|O| |X|O|O| |X|O|X| | |X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | 'O' Won! | |X| | |O|O|O| | |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| |X|O| | 'O' Won! |O|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| | | | | | |X|O| |O| |X| | | | | | |X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | | | 'X' Won! |O|X|O| |O|X|X| | |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | |O|X| | |O| |X| | | | | |O|X| | |O| |X| |X| | | |O|X| | |O| |X| |X| |O| |O|X| | |O|X|X| |X| |O| |O|X| | |O|X|X| |X|O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | | |O|O| | 'X' Won! |X|X|X| | | | | |O|O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | |O| |X| |X| | | |O| | |X|O| |X| |X| | |O|O| | |X|O| |X| |X| | |O|O| |X|X|O| |X| |X| 'O' Won! |O|O|O| |X|X|O| |X| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | |X| |O|O| | |X|X| | | |O|X| |O|O| | |X|X| | |X|O|X| |O|O| | 'O' Won! |X|X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | |O| | |X| |X| | | |O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O| | |X|O|X| |X|X|O| 'O' Won! |O|O| | |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X| |O| | |X| | | |O| | |X| |O| |O|X| | | |O| | 'X' Won! |X| |O| |O|X| | | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |O| | |O| | | |X|X| | | |O| |O|O| | | |X|X| | | |O| |O|O| | | |X|X| | |X|O| |O|O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | | |O|O| |O|X|X| |X| | | | |O|O| 'O' Won! |O|X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | | | |O| |X|O|O| |X|X| | |O| |O| |X|O|O| |X|X| | |O|X|O| |X|O|O| |X|X| | 'O' Won! |O|X|O| |X|O|O| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X|O| |O| |X| | | |X| | |X|O| |O| |X| |O| |X| | |X|O| |O| |X| |O| |X| |X|X|O| 'O' Won! |O| |X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | | | | |X|X|O| | |X|O| |O| | | |X|X|O| |X|X|O| |O| | | |X|X|O| |X|X|O| |O|O| | |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| | | | |X|O| | |X| | |O| | | | |X|O| |O|X| | |O| | | | |X|O| |O|X| | |O| |X| 'O' Won! |O|X|O| |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | | | | |O| |X| 'X' Won! | |O|X| | | |X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X|O| | | |O| | |X|X| | |X|O| | | |O| | |X|X| | |X|O|O| 'X' Won! | |O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | |X| | |O| |O| |X|O|X| | |X| | |O|X|O| |X|O|X| | |X| | 'O' Won! |O|X|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | | |X|X| 'O' Won! |O| | | |O| | | |O|X|X| 1.1 Clean Data \u00b6 We will first need to organize the data into a parsable format. Q1 \u00b6 What is the object data and what does it contain? what are the keys of data? what are the keys of each game? # inspect data below by grabbing the first key in data # what are the three different keys within each game? data [ 'game 0' ] {'board': {1: 'X', 2: ' ', 3: 'O', 4: 'X', 5: 'X', 6: 'O', 7: 'O', 8: 'X', 9: 'O'}, 'starting player': 'X', 'winner': 'O'} Q2 \u00b6 Using those keys, iterate through every game in data and append the board, the winner, and the starting player to separate lists. Call these lists: boards, winners, and starters boards = [] winners = [] starters = [] for game in data : # YOUR CODE HERE Q3 \u00b6 Make a dataframe out of the list boards and call it df . Make a series out of the list winners . Make a series out of the list starters . Make a new column of df called \"Winner\" and set it equal to the pandas Series of the winners. Make a new column of df called \"Starter\" and set it equal to the pandas Series of the starters. # YOUR CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 Winner Starter 0 X O O O X O O 1 O O X O X X O X O O O 2 O X X X X O O X X 3 X X O O O X X O O Stalemate O 4 O O X O O X X X X O 1.2 Inferential Analysis \u00b6 We're going to use Bayes Rule or Bayesian Inference to make a probability of winning based on positions of the board. The formula is: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\). The example we will use is the following: what is the probability of 'O' being the winner, given that they've played the center piece. \\(B\\) = 'O' played the center piece \\(A\\) = 'O' won the game So what is probability? We will define it in terms of frequencies. So if we are for instance asking what is the probability of player 'O' being in the center piece, it would be defined as: P(B) = \\frac{|O_c|} {|O_c| + |X_c| + |empty|} Where the pipes, | | , or cardinality represent the count of the indicated observation or set. In this case \\(O_c\\) (O being in the center) and \\(X_c\\) (X being in the center). Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc_Xc_empty 1000 # example of assessing the probability of B, O playing the center piece player = 'O' Oc = ( df [ 5 ] == player ) . value_counts () Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc / Oc_Xc_empty False 0.577 True 0.423 Name: 5, dtype: float64 # we can also clean this up and replace the denominator with the whole # observation space (which is just the total number of games, df.shape[0]). # example of assesing probabiliy of A ( df [ 'Winner' ] == 'O' ) . value_counts () / df . shape [ 0 ] False 0.571 True 0.429 Name: Winner, dtype: float64 The \\(P(B|A) * P(A)\\) is the intersection of \\(B\\) and \\(A\\). The intersection is defined as the two events occuring together. Continuing with the example, the probablity of 'O' playing the center piece AND 'O' being the winner is the number of times these observations occured together divided by the whole observation space : # in this view, the total times A and B occured together is 247 player = 'O' df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] 247 # the total observation space is 1000 (1000 games) df . shape [ 0 ] 1000 And so we get: P(B|A) * P(A) = \\frac{247} {1000} = 0.247 In code: df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] / df . shape [ 0 ] 0.247 1.2.1 Behavioral Analysis of the Winner \u00b6 Q4 \u00b6 define the 3 different board piece types and label them middle , side , and corner . Middle should be an int and the other two should be lists. # define the 3 different board piece types # middle = # side = # corner = 1.2.1.1 What is the probability of winning after playing the middle piece? \u00b6 Q5: For player X \u00b6 # A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'X' # define the intersection of A AND B, A_B # A_B = # define prob B # B = # return A_B over B (The prob B given A) A_B / B 0.5732758620689655 Q6 For player O \u00b6 # A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'O' # define the intersection of A AND B, A_B # A_B = # define prob B # B = # return A_B over B (The prob B given A) A_B / B 0.5839243498817968 1.2.1.2 What is the probability of winning after playing a side piece? \u00b6 Q7 For player O \u00b6 # A intersect B: O played side and O won / tot games # B: O played side / tot games player = 'O' A_B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.4158609451385117 Q8 For player X \u00b6 # A intersect B: X played side and X won / tot games # B: X played side / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.38845460012026456 1.2.1.3 What is the probability of winning after playing a corner piece? \u00b6 Q9 For player O \u00b6 # A intersect B: O played corner and O won / tot games # B: O played corner / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.4779116465863454 Q10 For player X \u00b6 # A intersect B: X played corner and X won / tot games # B: X played corner / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.47386964180857316 Are these results surprising to you? Why? This resource may be illustrative. 1.3 Improving the Analysis \u00b6 In this analysis, we only tracked what moves were made, not the order they were made in. It really limited our assessment! How might we change our recording of the games to track order of moves as well? Do we need to track all the moves or just the first and the winner?","title":"Statistical Analysis of TicTacToe"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#data-science-foundations-project-part-1-statistical-analysis","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to perform statistical analysis on data generated from our tictactoe program!","title":"Data Science Foundations  Project Part 1: Statistical Analysis"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#10-preparing-environment-and-importing-data","text":"back to top","title":"1.0 Preparing Environment and Importing Data"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#101-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"1.0.1 Import Packages"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#102-load-dataset","text":"back to top data = {} for i in range ( 1000 ): game = GameEngine () game . setup_game () board = game . play_game () data [ 'game {} ' . format ( i )] = { 'board' : board . board , 'winner' : board . winner , 'starting player' : board . start_player } | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| |O| | | | | |O|X|X| |O| |O| |X| | | |O|X|X| 'O' Won! |O| |O| |X|O| | |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O| | | |X|X| 'O' Won! |X| |O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X| | | |O|X| | | |O|O| |X| |X| |O|X| | | |O|O| |X|O|X| |O|X| | | |O|O| |X|O|X| |O|X|X| | |O|O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | |X|X| | | |O| | |O|O| | |X|X| |X| |O| | |O|O| | |X|X| 'O' Won! |X| |O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | |O| | | |X|X| |O|X| | | |O| | | |X|X| |O|X| | | |O|O| | |X|X| |O|X|X| | |O|O| | |X|X| 'O' Won! |O|X|X| |O|O|O| | |X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | |O| | |O| |X| |X| | | | |O| | |O|X|X| |X| | | | |O|O| |O|X|X| |X| |X| | |O|O| |O|X|X| 'O' Won! |X| |X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | | | | |X| |X| |O| |O|O| | | | |X| |X|X|O| |O|O| | | | |X| 'O' Won! |X|X|O| |O|O| | |O| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X|X| |X| | | |O| |O| | |X|X| |X|O| | |O| |O| | |X|X| |X|O|X| |O| |O| 'O' Won! |O|X|X| |X|O|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | |O|O| | | |X| |X| |X| | |O|O| | | |X| |X|O|X| | |O|O| | | |X| |X|O|X| |X|O|O| | | |X| 'O' Won! |X|O|X| |X|O|O| | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| |X| | |O| | |O| |X| |O| |X| | |O| | 'X' Won! |O| |X| |O| |X| | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X|O| | | |O| |X|X| | 'X' Won! |O|X|O| | |X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| |X| | | | | | | | |O|X| |X|O| | | | | | | |O|X| 'X' Won! |X|O| | | |X| | | |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | |X|O| | | |X|O| | |X|O| |X|O| | | |X|O| |X|X|O| |X|O| | 'O' Won! | |X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X|O| | | |O|X| | | | | |X|O| | | |O|X| | | |X| |X|O|O| | |O|X| | | |X| |X|O|O| | |O|X| | |X|X| 'O' Won! |X|O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| |X| | | |X| | |O|O| |X| |X| |O| |X| |X|O|O| |X| |X| |O| |X| 'O' Won! |X|O|O| |X|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| |X|X| | | | |O| | |X|O| |X|X| | |O| |O| | |X|O| |X|X| | |O| |O| |X|X|O| 'O' Won! |X|X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | |X|O| | | | | |O| |X| | |X|O| |O| | | |O| |X| |X|X|O| |O| | | |O| |X| 'O' Won! |X|X|O| |O|O| | |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | |X| | | |O| |X| |O| | | |X|O| | |O| |X| |O| | | |X|O| | |O| |X| |O|X| | |X|O| | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | |X| | | | |O|O| |X|X| | |X|O| | | |O|O| 'X' Won! |X|X| | |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| | | | |X|O| |X| | | |O|X| | | |X|O| |X| | | |O|X| | |O|X|O| |X| | | |O|X|X| |O|X|O| |X|O| | |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| |X| |O| | |O|X| |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| | | |X| |O|X|O| |O|X|X| | | |X| |O|X|O| 'O' Won! |O|X|X| |O| |X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X|X| | |O|O| | |O| | | |X|X| | |O|O| | |O| | | |X|X| | |O|O|X| |O|O| | |X|X| | |O|O|X| |O|O|X| |X|X| | |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | |X| |X|O| | | |O| | | | |X| |X|O| | |X|O| | | | |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O|X| |O| |X| |X|O| | |X|O|X| |O| |X| |X|O|O| 'X' Won! |X|O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | |X| | | |O| |O| | |X| | |X|O| | |O| |O| | |X| | |X|O|X| |O| |O| 'O' Won! |O|X| | |X|O|X| |O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| 'O' Won! | |X|O| | |O| | |O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O| |X| |X|X| | |O| |O| |O| |X| |X|X| | 'X' Won! |O| |O| |O| |X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X| | | | |X| |O| |X| |O|X| | | | |X| |O|O|X| 'X' Won! |O|X|X| | | |X| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O|X| | | | | | | |X| |O|O|X| | |O| | | | |X| |O|O|X| | |O| | |X| |X| 'O' Won! |O|O|X| | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| |O| |X| | | | | | |O|X| |O| |X| |X| | | | |O|X| |O| |X| |X| | | |O|O|X| 'X' Won! |O| |X| |X| |X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | | |X|X| |O| | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| | | |X|O|O| |O|X|X| |O| |X| |X|O|O| 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O|O|X| | | | | | |X|X| |O|O|X| | | | | | |X|X| |O|O|X| | |O| | | |X|X| |O|O|X| |X|O| | | |X|X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| | |O|X| | |X|O| |O| |X| 'X' Won! |X|O|X| | |X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | | |O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| | |X|X| |X|O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | |X| |X| |O| |X| |O|O| | |X|O|X| |O|X|X| |O|O| | |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| 'O' Won! |X|O| | |X| |X| |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | |X| |O| | |X| | | |O|O| |X| |O| | |X| | | |O|O| |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X|O| | | | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| |X| | |O|O| |X|X| | |O| |X| 'X' Won! | |O|O| |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X|O| |O|X| | |X| | | | |X|O| |O|X| | |X| | | | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| 'X' Won! |X|X|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X|O| | | | |O| | | |X| |X|O|X| | | |O| | | |X| |X|O|X| |O| |O| | | |X| 'X' Won! |X|O|X| |O|X|O| | | |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | | |X| | |X|X| | |O|O| | | |X| |O|X|X| | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X|O| | |O|X| | | |X| | |X|O| | |O|X| |O| |X| | |X|O| | |O|X| 'X' Won! |O| |X| | |X|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| | |O|O|X| | |X|X| | |O| | 'X' Won! |O|O|X| | |X|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| | | |O| |O|O|X| | | |X| |X| |O| |O|O|X| | | |X| |X|O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | |X|X| | | | | |O| | | |O|X|X| | | | | |O| | | |O|X|X| |X| | | |O| | | |O|X|X| |X| | | |O| |O| |O|X|X| |X| |X| |O| |O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | |O| |X| | | |X| |X|O| | |O| |X| | | |X| |X|O|O| |O| |X| | | |X| 'X' Won! |X|O|O| |O|X|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | |X|X| |O| |O| | | | | |O|X|X| |O| |O| | |X| | |O|X|X| |O| |O| 'O' Won! | |X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| |X| | |X| | 'O' Won! |O| | | |O| |X| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | | | | | | |X|X| |O|O|X| | | |O| | |X|X| |O|O|X| | |X|O| | |X|X| |O|O|X| | |X|O| |O|X|X| |O|O|X| 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O|O| | | | |X| 'X' Won! | | |X| |O|O|X| | | |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | | |O|X| |X| | | |O| |O| | |O|X| |X| | | |O| |O| | |O|X| |X| |X| |O| |O| | |O|X| |X|O|X| |O| |O| |X|O|X| |X|O|X| 'O' Won! |O|O|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O| |O| | |X|X| | | | | |O| |O| | |X|X| | | |O| |O| |O| | |X|X| |X| |O| |O| |O| | |X|X| |X|O|O| |O| |O| 'X' Won! |X|X|X| |X|O|O| |O| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| |X| |X| | |X| | | |O|O| |X|O|X| | |X| | | |O|O| |X|O|X| |X|X| | | |O|O| |X|O|X| |X|X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | |X|X| |O| | | | | |O| | |X|X| |O| | | 'X' Won! | | |O| |X|X|X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| | |X|X| |O|O|X| | |O|O| | |X|X| 'X' Won! |O|O|X| | |O|O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | | |X|X| |O| |O| | | | | |O|X|X| |O| |O| |X| | | |O|X|X| 'O' Won! |O|O|O| |X| | | |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| | | |O| | |X| | | |X|O| |O| |O| | |X| | 'X' Won! | |X|O| |O|X|O| | |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | |X| | | |O|X| |O|X|O| | |X| | | |O|X| |O|X|O| | |X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | | | | |X| |X| |O|O| | | | | | |X|O|X| |O|O| | | | |X| |X|O|X| 'O' Won! |O|O| | | |O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | |X|O| | | |X|X| | |O| | |X|O| | | |X|X| | |O|O| |X|O|X| | |X|X| | |O|O| |X|O|X| |O|X|X| | |O|O| 'X' Won! |X|O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| |O|O| | | |X| | | | |X| |O|O| | |X|X| | | |O|X| |O|O| | |X|X| | |X|O|X| |O|O| | |X|X| | |X|O|X| |O|O| | |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| | | | | |X|X|O| |X|O|O| | | | | |X|X|O| |X|O|O| | |X| | |X|X|O| 'O' Won! |X|O|O| | |X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X|X| | |X|O| |O|O| | | |X|X| |X|X|O| |O|O| | | |X|X| 'O' Won! |X|X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | | |X| | | |O|O| |X| | | | |X| | | |O|O| |X| |X| |O|X| | | |O|O| |X| |X| |O|X| | |X|O|O| |X| |X| |O|X| | |X|O|O| |X|O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | |X| |X| |O|O| | | | | | 'O' Won! |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X|X| | |O| | |O|X|O| | |X|X| 'X' Won! | |O| | |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| |X| | |O|O| | |X|O| |X| |X| 'X' Won! | |O|O| | |X|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O|O| | |O|X| | |X| | | |O|O| |X|O|X| | |X| | | |O|O| |X|O|X| | |X|O| | |O|O| |X|O|X| |X|X|O| 'O' Won! |O|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | |X| | | |X| | |O|O| | 'O' Won! | |X| | | |X| | |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | | | | |O|O| |X| |X| |X| | | | |O|O| 'O' Won! |X| |X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| |X| | | | |O|O| |O|X|X| |X| | | | |O|O| |O|X|X| |X|X| | | |O|O| |O|X|X| |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O|O| |X|O| | 'X' Won! |X| | | |X|O|O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | 'X' Won! |X| | | |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | |O|X| |X|X| | | |O|O| | |O|X| 'O' Won! |X|X| | |O|O|O| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | | |X| | 'O' Won! |O|O|O| |X| | | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | | |O|O| | |X|X| |X| | | | |O|O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O| | |X|O|O| | |X|X| |X|O|O| |X|O|O| 'X' Won! |X|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| | | |O| |O|X|X| | | |O| | |X|O| |O|X|X| | | |O| | |X|O| |O|X|X| |O| |O| 'X' Won! | |X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X|X| | | |O|O| | | |O| |X|X| | | |O|O| | |X|O| |X|X| | 'O' Won! | |O|O| | |X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| |X| |O| |O| |X|O| | |X| |X| |O|X|O| |X|O| | |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| |X| | | | |X|O| |X|O|O| |X| | | | |X|O| 'X' Won! |X|O|O| |X| | | |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | | | |X|X|O| 'O' Won! |O| | | | |O| | |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| | | |O| | | | | |X|O|X| | | |O| |X| | | |X|O|X| | | |O| |X| |O| 'X' Won! |X|O|X| |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| 'O' Won! |O| |X| | |O| | | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| |O| 'X' Won! | | |X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O| | | |X| | |O|X|X| 'O' Won! |O|O|O| | |X| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| | |O| | |X|X| | |O| |X| | |O|O| |X|X| | 'X' Won! |O| |X| | |O|O| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| | | |O| |X|O|X| | | |X| | | |O| |X|O|X| |O| |X| | | |O| |X|O|X| |O|X|X| | | |O| |X|O|X| |O|X|X| |O| |O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | |O| | | |X| |O|X|O| | |X|O| | | |X| |O|X|O| 'O' Won! | |X|O| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | |X|O| | | | | | |O|X| |O|X|O| | | | | |X|O|X| |O|X|O| 'O' Won! | | |O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| | | |O| |X|O|X| | | |X| | | |O| |X|O|X| | | |X| |O| |O| |X|O|X| |X| |X| |O| |O| |X|O|X| |X|O|X| |O| |O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| |X| |X| |O| |X| 'O' Won! | |O|O| |X|O|X| |O| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| | | |O| |X| | | |X|X|O| | |O|O| |X| | | |X|X|O| | |O|O| |X| |X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | | | |O| |X| |X|X|O| | | |O| |O| |X| |X|X|O| |X| |O| |O| |X| |X|X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X|O|X| | | | | |O|X| | |X|O|X| | | |O| |O|X| | |X|O|X| | |X|O| |O|X| | |X|O|X| |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | | | |O| |X| | |X|O| | | | | |O|X|X| 'O' Won! | |X|O| | |O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| |X| | | | |O| | |O| |X| |X| | | |O|O| | |O| |X| |X| | | |O|O|X| |O| |X| |X| |O| |O|O|X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | | |X| | |O| | |X| |O| | | |X| | |O|O| |X| |O| |X| |X| | |O|O| |X|O|O| |X| |X| | |O|O| 'X' Won! |X|O|O| |X|X|X| | |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | |X| | | |X| |O| |O|X|O| |X| | | |X| |O| |O|X|O| |X| | | |X|X|O| 'O' Won! |O|X|O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O| |O| | |X| | | | |X| |O| |O| | |X|X| | | |X| |O| |O| |O|X|X| | | |X| |O| |O| |O|X|X| | |X|X| 'O' Won! |O|O|O| |O|X|X| | |X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | | | |X| | |X|O| |O|O| | | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| |O| |X| | |X|O| |O|O|X| |O|X|X| | |X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O| | | | |X|O| | | |X| |O| | | |X|X|O| | | |X| |O|O| | |X|X|O| |X| |X| |O|O| | |X|X|O| 'O' Won! |X| |X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| 'X' Won! |O|O|X| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | |X| | |O| | |X|O| | | | |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O|O| 'X' Won! |X|X|X| |O|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| |O| | | |X|O| | | |O|X| |O| | | |X|O|X| | |O|X| |O| | | |X|O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| |O| | |X| | | |O| | |X| |O| | |X| | |X|O| | |X| |O| |O|X| | |X|O| | 'X' Won! |X| |O| |O|X| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | |X| |X| |O|O| | | | |O| |X| |X| |O|O| | 'X' Won! | | |O| |X|X|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| |X|X| | |O| | | | |O|X| |X|X|O| |O| | | | |O|X| 'X' Won! |X|X|O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X| | | |X| |X| 'O' Won! |O|O|O| |X| | | |X| |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O| | |X|O| | | |X|X| | |O|O| |X|O| | 'X' Won! |X|X|X| | |O|O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| |O| | | | | |X| |O| 'X' Won! |X| |O| |X| | | |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| | |O| | |X|O|O| | |X|X| |X|O| | |X|O|O| |O|X|X| |X|O| | 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X| | | |O| | 'X' Won! |O| |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |X| | | |O| |O|X| | | | |X| | |X|O| |O|X| | | | |X| | |X|O| |O|X| | | |O|X| 'X' Won! |X|X|O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | | | |X| |X|O|O| | |X| | | |O|X| |X|O|O| | |X| | | |O|X| |X|O|O| | |X|X| |O|O|X| |X|O|O| | |X|X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | |X|O| | | |X| |O| | | | |X|O| | | |X| |O|O| | | |X|O| |X| |X| |O|O| | 'O' Won! | |X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| |O| | | | |X|X| | |X|O| |O| | | | |X|X| |O|X|O| |O| | | 'X' Won! |X|X|X| |O|X|O| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X|X| |O| | | | |O| | | |X|X| |O|O| | | |O| | | |X|X| |O|O|X| | |O| | |O|X|X| |O|O|X| 'X' Won! | |O|X| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | |X| | | |X| |O| |O|X|O| |X| | | |X| |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X| |X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| 'O' Won! |O|X| | | |O| | | |X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | | |O| |X|X| | |X|O| | | |O|O| |X|X| | 'X' Won! |X|O| | |X|O|O| |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | |O|X|X| | |O|X| |O|O| | |O|X|X| |X|O|X| 'O' Won! |O|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O|X| | | | | |O| |X| | |O|X| | |O| | |O|X|X| | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | 'X' Won! |O|X|X| | |O|X| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| |O| | | | | |X|O|X| |O|X|O| 'O' Won! |O| | | |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | |O|X| | | | | |X| |O| |X|O|X| | | | | |X| |O| |X|O|X| | | | | |X|O|O| |X|O|X| | | |X| |X|O|O| |X|O|X| |O| |X| |X|O|O| 'X' Won! |X|O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| |X| 'O' Won! | | | | |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| |X| |X| |O| | | | |O|X| |X|O|X| |O| | | | |O|X| 'X' Won! |X|O|X| |O|X| | | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| |X| |O| | | |O|X| | |X| |X| |O| |O| |O|X|X| |X| |X| |O| |O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | |X| | |X|O| | | |O| | | |X| | |X|O| | | |O|O| | |X| | |X|O| | |X|O|O| | |X| | |X|O|O| |X|O|O| 'X' Won! |X|X| | |X|O|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O| |O| 'X' Won! |O| | | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O|X|O| | | |X| | | |O| |O|X|O| | | |X| | |X|O| |O|X|O| | |O|X| | |X|O| |O|X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O| | 'O' Won! |X|X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X|O| | | |X| | | | | |O|X|O| | |X|X| | | | | |O|X|O| | |X|X| | | |O| |O|X|O| | |X|X| |X| |O| |O|X|O| |O|X|X| |X| |O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| |O| | |O|X| | |O|X| |X| |O| | |O|X| |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X| | | |O|O| 'X' Won! | |O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | |X| |O| | | |X| |O|O| | |X| |O| | | |X| |O|O| | |X|X|O| 'O' Won! | | |X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | |X| |O| | | | | |O|X| | 'O' Won! |X| |O| | |O| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O|X| |O| | | |X| | | | |O|X| |O| |O| |X|X| | | |O|X| |O| |O| 'O' Won! |X|X| | | |O|X| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | |X| |X| |O|X| | |O| | | |X| |X| |O|X| | |O| |O| |X| |X| |O|X| | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | |O|O| | | |X| | | |X|X| |O|O| | | |X| | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | |X|X| | | | | |O| |O| | |X|X| |O| | | |O| |O| | |X|X| |O|X| | |O| |O| | |X|X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | | | |X| |X| |O| | |O| | | |O|X| |X| |O| | |O| | |X|O|X| 'O' Won! |X|O|O| | |O| | |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O|X| | | |X| |X| |O| |O|O|X| | | |X| |X|O|O| |O|O|X| |X| |X| |X|O|O| 'O' Won! |O|O|X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X| | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | | |X|O| |O| |O| |X|X| | |X|X|O| 'O' Won! |O| |O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O|X| | | | | |O|X| | |O|O|X| | |X| | |O|X| | |O|O|X| | |X| | |O|X|O| |O|O|X| | |X|X| |O|X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X| |O| |X|O|O| | | |X| |X| |O| |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| |O| | | | | |X| 'X' Won! |X|O|O| |O|X| | | | |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | |O| | |X|X|O| |X| | | | |O| | |X|X|O| |X|O| | | |O| | 'X' Won! |X|X|O| |X|O| | |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | |X|X| | | |O| |O| |O| | |X|X| | | |O| |O|X|O| | |X|X| | |O|O| |O|X|O| | |X|X| 'X' Won! | |O|O| |O|X|O| |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | |O| | | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O|X| |X| | | |O| | | |X|O|X| |X|O| | |O| | | |X|O|X| |X|O|X| |O| | | |X|O|X| |X|O|X| |O| |O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| |O|O| | 'X' Won! |X| | | | |X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X|O| |X|O| | | |X|O| | |X|O| |X|O| | 'O' Won! | |X|O| | |X|O| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | |O| |O| | |X|X| |O| | | 'X' Won! |O| |O| |X|X|X| |O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| |X| |X| | | | | | |O|O| |X| |X| | |O| | |X|O|O| |X| |X| | |O| | |X|O|O| |X| |X| |O|O| | |X|O|O| |X| |X| |O|O|X| 'O' Won! |X|O|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| |O|O|X| |X| | | |O|X|O| |O|O|X| |X| |X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | | | |X|O|O| |X|O| | | | | | 'X' Won! |X|O|O| |X|O| | |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| |O| | | | | |X| |X| |O| |O|X| | | |O|X| |X| |O| |O|X| | | |O|X| |X|X|O| |O|X| | |O|O|X| |X|X|O| |O|X| | It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X|O| | |O| | | | |X|X| |X|O| | |O| | | |O|X|X| |X|O| | 'X' Won! |O| |X| |O|X|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | |O|O| |X| | | |X| |O| 'X' Won! |X|O|O| |X| | | |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | | |X| | | |O|X| |O| | | |O|X| | | |O|X| |O| |X| |O|X| | | |O|X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | | |O| | |X|X| |O|O| | |X| |O| | |X|X| |O|O| | 'O' Won! |X| |O| | |X|X| |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | |O|O| | |X| |X| | | | | |O|O|X| |X| |X| | | | | |O|O|X| |X|O|X| | | | | 'X' Won! |O|O|X| |X|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| |X| 'O' Won! |O| | | |O|X| | |O| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | | |O| |X|O| | |O|X|X| | | |O| |X|O| | |O|X|X| |O| |O| |X|O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | |O| | | |X| | |X|O|X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| |O|O| | |O| |X| | |X|X| |O|O| | |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | |O| | |O| |X| |X|X| | | |O| | |O| |X| |X|X| | | |O|O| |O| |X| |X|X| | |X|O|O| |O| |X| 'O' Won! |X|X|O| |X|O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X|O| |X| | | | |X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| |X| |O|X|O| | |X|O| |X|O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| | | |X| 'O' Won! |O|X|X| |O|O|O| | | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X|O| | |O| | | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O| |O|X| | | |X|X| | |O|O| |O|X| | 'O' Won! | |X|X| |O|O|O| |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | | | | | | |X|O| |O| | | |X| | | | |X|O| |O| | | |X| |O| | |X|O| |O| | | |X|X|O| 'O' Won! | |X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| | |X|X|O| | | |O| | |X| | 'O' Won! |X|X|O| | | |O| | |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| |O| | | | |X| | | |O|X| |O| | | | |X|X| | |O|X| |O| |O| | |X|X| | |O|X| |O|X|O| | |X|X| |O|O|X| |O|X|O| | |X|X| 'X' Won! |O|O|X| |O|X|O| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | | |X|O| 'O' Won! |O|X|X| | |O| | | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O|O| | |O|X| | | | |X| |O|O| | |O|X| | |X| |X| 'O' Won! |O|O|O| |O|X| | |X| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | | |X|X| | |O|O| |O| | | | |X|X| | |O|O| |O| |X| | |X|X| | |O|O| |O|O|X| | |X|X| 'X' Won! | |O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | |X|O| | | |O| |X| | | | |X|O| | |O|O| |X| | | | |X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X|X| | 'O' Won! |O|X|O| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | |X|O| |O| | | |X| | | | |X|O| |O|O| | 'X' Won! |X| | | | |X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | |O|X| |X| | | |O| |O| | |O|X| |X| | | |O|X|O| | |O|X| |X|O| | |O|X|O| |X|O|X| |X|O| | |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | 'X' Won! |X| | | |X|O| | |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | |O| | |X|O| | | |X|X| | |O| | |X|O| | | |X|X| | |O| | |X|O|O| 'X' Won! |X|X|X| | |O| | |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | |X| |O| | | |O|X| | | |O|X| |O| | | |O|X|X| | |O|X| |O| |O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | |X| | | | |O| | |X|O| |O|X| | | | |O| | |X|O| |O|X|X| | | |O| |O|X|O| |O|X|X| 'X' Won! |X| |O| |O|X|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X|O|O| |X| | | 'X' Won! |X|O| | |X|O|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X| | |O|O| | | | |X| |X|X| | |O|O| | | | |X| |X|X|O| |O|O| | | | |X| |X|X|O| |O|O| | | |X|X| 'O' Won! |X|X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | | |O| | | |X|X| |O|O| | 'X' Won! | |O| | |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | |O|X| |X| | | | |O|O| |X|O|X| |X| | | | |O|O| 'O' Won! |X|O|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | |X| |X|O|X| | | |O| 'O' Won! |O| |X| |X|O|X| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| |X| |O| | | |X| | | |O| |X| |O| |O| |X| | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| |O| |X|X| | 'X' Won! |O|X|O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |X| | |O| | |X|X|O| | |O|X| 'X' Won! |X|O| | |X|X|O| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | | 'O' Won! |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X|O| | |X| | | |O|X|O| |X|O| | |X| | | |O|X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X| |X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X|X| | |X|O|O| | | |O| |X|X| | 'X' Won! |X|O|O| | | |O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X|X| | | |O|O| |O| | | |X|X| | |X|O|O| |O| | | |X|X| | |X|O|O| |O|O| | |X|X| | 'X' Won! |X|O|O| |O|O| | |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | |X|X| | |O|O| | | |O| | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O|O| |X|X| | |O|O|X| |X|O|O| |X|X| | 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | |O|X| | | | | |O|X| | |X|O|X| | | | | |O|X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O|X| | |O|X| | |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | |O|X| |O| |X| | | | | | |O|X| |O|X|X| | | | | | |O|X| |O|X|X| | | |O| | |O|X| |O|X|X| | |X|O| |O|O|X| |O|X|X| | |X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| | |X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | |X| | |O|O| | | | |X| 'O' Won! | |X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X| | | |X|O| | | |O| |O|X| | | |X|O| | | |O| |O|X|X| 'O' Won! | |X|O| | |O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | |O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O|X| | | |X| | |O| |X| |O|X|O| | |X|X| |O| |X| |O|X|O| 'O' Won! |O|X|X| |O| |X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | |X|O| |O| | | | |X| | |O|X|O| |O| | | | |X| | 'X' Won! |O|X|O| |O|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| 'O' Won! |O| | | |X|O|X| | | |O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | | | |O|X| 'X' Won! |X| |O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X|X| | |X|O| | |O|O| | |X|X| | |X|O| |X|O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | |X|O| | | | | |X|O| | | |X|O| | | |O| |X|O| | | |X|O| | |X|O| |X|O| | | |X|O| |O|X|O| |X|O| | | |X|O| |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |X| | | |O|O|X| | | | | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| |X| |O| | | | | |X| |O| |X|O|O| | | | | |X| |O| 'X' Won! |X|O|O| |X| | | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O|O| | | | | | |X| |X| |O|O| | |X| | | |X| |X| |O|O| | |X| |O| |X| |X| |O|O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | |X| | | |X| |O| | |O| | |X|O| | |X| |O| | |O| | |X|O|X| |X| |O| |O|O| | |X|O|X| |X|X|O| |O|O| | |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O|X|O| 'O' Won! |O| | | |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | | |X| | |X| |O| |O|X| | | |X|O| |X| |O| |O|X| | |X|X|O| |X|O|O| |O|X| | |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X|X|O| 'O' Won! |O|O|O| | | |X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | | |X|X|O| | | | | |O| | | |X|X|O| | |O| | |O| | | |X|X|O| |X|O| | |O| | | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| |O| |O| | | | | | |X|X| |O|X|O| | | | | | |X|X| |O|X|O| | | |O| 'X' Won! | |X|X| |O|X|O| | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | | |O| |X| |O| |X|X| | | | |O| 'O' Won! |X| |O| |X|X|O| | | |O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | |X| |X| |X| |O| |O|O| | |X| |X| |X|X|O| |O|O| | |X| |X| |X|X|O| |O|O| | |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | | | | | |O| |X|X|O| | | | | |X| |O| |X|X|O| |O| | | |X| |O| |X|X|O| |O| |X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | | | | |O|X|O| | |X|X| | | | | |O|X|O| |O|X|X| | | |X| |O|X|O| |O|X|X| 'O' Won! |O| |X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| 'X' Won! | |O|X| | | |X| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X|O| | | |O| | |O|X| |X|X|O| | | |O| |O|O|X| |X|X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | |O| 'X' Won! | | |O| |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| |X| |O|O| | | |O|X| |X| |X| 'X' Won! |O|O| | | |O|X| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| | |O|X| |O|X| | | | |X| | |O|X| |O|X|O| | |X|X| | |O|X| |O|X|O| 'O' Won! |O|X|X| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | |X| |X|O| | | | |O| | | |X| |X|O| | |O| |O| | | |X| |X|O|X| |O| |O| 'O' Won! |O| |X| |X|O|X| |O| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | |O| |X| | | 'X' Won! | |O|X| | |X|O| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| 'X' Won! |X| | | | |X| | |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X|X| | | | | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O|X| | |O|O|X| |X|X| | |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O|O| | |X| | |X|O| | | |O|O| | |X| | |X|O|X| 'O' Won! |O|O|O| | |X| | |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | | |O|O| |X| |X| | | | | |X|O|O| |X|O|X| | | | | |X|O|O| 'X' Won! |X|O|X| |X| | | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | |X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O|X| |O|X| | |O|O|X| | |O|X| |O|X| | |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X| | | |X|O| |O| | | | |X| | |X|X|O| |O| | | | |X|O| |X|X|O| |O| | | | |X|O| |X|X|O| |O| |X| | |X|O| |X|X|O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | | | |X| |O|X|O| | | | | |X| |X| |O|X|O| | | | | |X| |X| |O|X|O| |O| | | 'X' Won! |X|X|X| |O|X|O| |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X|O| | |X| | | | |O| 'X' Won! |O|X|O| | |X| | | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X| | | | |O|O| |X| |X| |X| | | | |O|O| |X|O|X| |X| | | | |O|O| |X|O|X| |X| |X| 'O' Won! |O|O|O| |X|O|X| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | |X|X| |O|O| | |O| |X| | |X|X| 'X' Won! |O|O|X| |O| |X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| | | |O| | | |O|X|O| |X| | | |O|X| | |O|X|O| |X| | | |O|X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | |X| | | |O| | |X| |O| | |X| | |O|O| | |X| |O| | |X| | |O|O|X| |X| |O| 'O' Won! |O|X| | |O|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | |X| | |O| |X| |O|O| | | |X| | |O| |X| |O|O| | | |X| | |O|X|X| 'O' Won! |O|O| | |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| |X| |X| | | | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X|O|X| | |O|O| |O| |X| |X|O|X| | |O|O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| |X| |O| |X| | | |O|O|X| |X|O|O| |X| | | |O|O|X| 'X' Won! |X|O|O| |X|X| | |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | | |O|X| |O|X|X| |O| | | | |O|X| |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| | | |X|X| |O| |O| | |O| | | |X|X| |O|X|O| | |O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| |O|X|O| It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| |X|O| | | | | | |X| |O| |X|O|O| | | | | |X| |O| 'X' Won! |X|O|O| |X| | | |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | |O| 'X' Won! |X| | | |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | |O|O| | |X|X| | |X| | | |O|O| | |X|X| | |X| |O| |O|O| | |X|X| | |X| |O| |O|O|X| |X|X| | |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| 'O' Won! |O| |X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O| |X|X| | 'O' Won! | |X| | |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O|O| | |X| | |X|O| | |X|O|O| 'O' Won! |O|X| | |X|O| | |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | |X| | | | |O|X| |O|O| | |X| | | |X|O|X| |O|O| | |X| | | 'O' Won! |X|O|X| |O|O|O| |X| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | |O| |O|X|X| | | |X| | | |O| |O|X|X| | | |X| | |O|O| |O|X|X| | |X|X| | |O|O| 'O' Won! |O|X|X| | |X|X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| | | | | |X|X|O| | |O|X| | | | | 'O' Won! |X|X|O| | |O|X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | | | |O| |X| | |X|O| | | |X| |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |X|X| 'O' Won! |O| |X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | | |X| |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| |X|O|X| |X| | | 'O' Won! |O|X|O| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O|O| | |X| |X| | |X|O| |O|O| | 'O' Won! |X| |X| | |X|O| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | |X| | |O| |O| |O|X| | | |X| | |O| |O| |O|X| | | |X|X| 'O' Won! |O| |O| |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| |X| |X|O| | | | | | |O|O|X| |X|O| | |X| | | |O|O|X| 'O' Won! |X|O| | |X|O| | |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X| | |X|O|O| | | |X| | |X| | |X|O|O| |O| |X| | |X| | |X|O|O| |O| |X| |X|X| | |X|O|O| |O|O|X| |X|X| | 'X' Won! |X|O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | | |O| |O|X| | | | |X| | |X|O| |O|X| | | | |X| | |X|O| |O|X|O| | | |X| 'X' Won! |X|X|O| |O|X|O| | | |X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | |O| | |X| | | |O|O|X| | |O|X| |X| | | |O|O|X| |O|O|X| 'X' Won! |X| |X| |O|O|X| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| | | | | | |O|O| |O|X|X| |X| | | | |O|O| 'O' Won! |O|X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | | |X| | |X|O|O| |O|X|O| | |X| | |X|O|O| |O|X|O| | |X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O| |O| |X| |X| | |O| | |O|X|O| |X| |X| | |O|O| |O|X|O| |X| |X| 'X' Won! |X|O|O| |O|X|O| |X| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| | | | | | |X|X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| |X| | |X|X| |X|O|O| |O| |X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X|O| | | |X| | |O|X|O| |X|O| | |O|X| | |O|X|O| |X|O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | |O|O|X| | |X| | | |O| | 'X' Won! |O|O|X| | |X| | |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O|X| |X| | | | |O|X| |O|O|X| |X| | | 'X' Won! | |O|X| |O|O|X| |X| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| | |O| | 'X' Won! |O| |X| |O| |X| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | |X|O| |O|X|O| | | |X| | |X|O| |O|X|O| | | |X| |X|X|O| |O|X|O| |O| |X| |X|X|O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | |O|X| | |X|X| |O| | | | |O|X| | |X|X| |O| | | |O|O|X| 'X' Won! |X|X|X| |O| | | |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | |O| |X| | | | |X|O| | | |O| |X| |X| | |X|O| | |O|O| |X| |X| 'X' Won! | |X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| | | |O| |O|O| | | |X|X| | | |O| |O|O|X| | |X|X| |O| |O| |O|O|X| | |X|X| |O|X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | |O| | |O| | | | |X| |X| |O| | |O| | | | |X| |X|O|O| | |O| | | | |X| |X|O|O| | |O|X| | | |X| |X|O|O| |O|O|X| | | |X| |X|O|O| |O|O|X| | |X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X|O| | |X| |O| 'X' Won! |X| | | |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | |O| |O| |O|X|X| |X| | | |O| |O| |O|X|X| |X| |O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | 'O' Won! | | | | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | |O| | | |X| |O| |X|X| | |O| | | |X| |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X|X|O| |X|X|O| |O|O| | |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | | | | | |O| |X| | |X| | |O| | | |O| |X| | |X| | |O| |X| |O| |X| | |X|O| |O| |X| |O| |X| |X|X|O| |O| |X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | | | | | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| | |X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O|X| | | |X| | |O| |X| |O|X| | | |X| | |O|O|X| |O|X| | |X|X| | |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | |O|O| | |X| | |X|O|X| | |O|O| | |X| | |X|O|X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | |O|X| | |X| |O| | | |X| |O|X| | |X| |O| | | |X| |O|X|O| |X|X|O| | | |X| |O|X|O| |X|X|O| |O| |X| |O|X|O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O| | | |X| | 'O' Won! |O|X|X| |O|O|O| | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X| |O| |X|X|O| |O|O| | |X|X|O| |X|X|O| 'O' Won! |O|O|O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | | |O|O| |X| |X| |O| | | |X|O|O| |X| |X| |O|O| | |X|O|O| |X| |X| |O|O|X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O|X| | |X|X| | | | | |O|O|X| |O|X|X| |X| | | |O|O|X| |O|X|X| |X|O| | |O|O|X| |O|X|X| 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| | |O| | |O|O| | | |X|X| | |O| | |O|O|X| |O|X|X| | |O| | |O|O|X| |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | 'X' Won! |X| | | | |X|O| | |O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| |X| |X| |X| |O| 'O' Won! |O|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O|O| | | |X| |X|X| | | |O|O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| |X|O|O| | | |X| 'O' Won! |X|X|O| |X|O|O| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | | |O|X| |X|X| | |O|O| | | |O|X| 'O' Won! |X|X| | |O|O|O| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| |O| |X| |X| |X|O| | |O| |O| |X| |X| |X|O|O| |O| |O| |X| |X| 'X' Won! |X|O|O| |O|X|O| |X| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| |O|X| | | |O|X| | | |X| |O|X| | | |O|X| | | |X| |O|X|O| |X|O|X| | | |X| |O|X|O| |X|O|X| | |O|X| |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | |X|O| |O|X| | |X|O| | | |X|O| |O|X| | |X|O| | |X|X|O| |O|X| | |X|O|O| |X|X|O| |O|X| | 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| |X|O| | |X|O|O| | |X|X| |X|O|O| |X|O|O| 'X' Won! |X|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| |O| |O| | | |X| | | |X| |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |X|X| |O|X|O| |O|O|X| | |X|X| |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X| |O| | | | | |X|O| | |X|O|O| | | | | |X|O| | |X|O|O| | | | | |X|O|X| |X|O|O| |O| | | |X|O|X| |X|O|O| |O| |X| |X|O|X| 'O' Won! |X|O|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| |O|X| | | | |X| | | |O| |O|X|X| | |O|X| | | |O| |O|X|X| |X|O|X| | | |O| |O|X|X| |X|O|X| | |O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| | | | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| 'O' Won! |X| | | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| | |X|X| | | |O| | |O|O| | 'X' Won! |X|X|X| | |O| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| |X| |X|O| | | | | | |O| |X| |X|O|O| | |X| | |O| |X| |X|O|O| | |X|O| |O| |X| |X|O|O| |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | |O| |X| | | |X|O| | | | |O| |X| |X| |X|O| | |O| |O| |X| |X| |X|O|X| |O| |O| |X| |X| 'O' Won! |X|O|X| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X|O| | | |O| |O|O|X| 'X' Won! |X|X|O| | |X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | 'O' Won! |X| |O| | | |O| |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | |O| | | |X|X| |O|O|X| | |O| | | |X|X| |O|O|X| |O|O| | | |X|X| 'X' Won! |O|O|X| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| | |O|O| | |X| | | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | |X| | |O|O| | | | |X| | |X|O| |O|O| | | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| | |O|X| |X|X|O| |O|O|X| | |O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | |X| | |O| |X| | |O|O| | |X| | |O| |X| 'X' Won! |X|O|O| | |X| | |O| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | |O| | | |X| |O| |X| | | |O| |X| |X| |O| |X| | | |O|O|X| |X| |O| |X| | | |O|O|X| |X| |O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | | | |X|O|O| |X|X| | |O| | | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X| | 'O' Won! |O|X| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | |X|O| |X| | | |O|O| | | |X|O| |X| | | |O|O| | |X|X|O| |X|O| | |O|O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | |X| | |O|X| | |O| | | | |X|O| |O|X| | |O| |X| | |X|O| 'O' Won! |O|X| | |O| |X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| |O| |O| |X| | | | | |X|X|O| |O| |X| | | | | |X|X|O| |O| |X| | |O| | |X|X|O| |O| |X| |X|O| | |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | |O| |O| | | |X| | |X| | |O|X|O| | | |X| |O|X| | |O|X|O| |X| |X| |O|X| | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X|O| |O| | | |X| | | |X|X|O| |O| | | |X| | | |X|X|O| |O|O| | |X|X| | |X|X|O| |O|O| | |X|X|O| |X|X|O| |O|O| | 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| 'O' Won! | |O| | | |O|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O|O|X| 'X' Won! |X| | | | |X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| |O| | | | |X| | | | |O| |O| |X| | |X| | | | |O| |O| |X| | |X| | | |O|O| |O| |X| |X|X| | | |O|O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| | |O|O| | | | | |X|X|O| |X|O|O| | | | | 'O' Won! |X|X|O| |X|O|O| | | |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O|X| |O|X|O| |X| |X| |O|O|X| 'X' Won! |O|X|O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O| |X| |X| | | |O|X| | |O| |X| |X| |O| |O|X| | |O| |X| |X| |O| |O|X|X| |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | | | |O| |X|O|O| |X|X| | | |O|O| |X|O|O| 'X' Won! |X|X| | |X|O|O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X| | | |O| |O| |X|X| | |X| | | |O| |O| 'O' Won! |X|X| | |X| | | |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O| | | |X|X|O| | |X| | |O|O| | |X|X|O| | |X| | |O|O| | |X|X|O| | |X|X| |O|O| | |X|X|O| |O|X|X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | | |X|X| |O|X| | |O| | | | |X|X| |O|X|O| |O| | | | |X|X| |O|X|O| |O| |X| 'O' Won! |O|X|X| |O|X|O| |O| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | |X| | | | |O|O| |X|X|O| |X| | | | |O|O| 'X' Won! |X|X|O| |X| | | |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| |X| | |O| | |O|X|X| |O| |X| | |O|O| |O|X|X| |O|X|X| | |O|O| 'O' Won! |O|X|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| 'X' Won! |O| | | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | | | 'O' Won! |O| |X| |O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | |X| |X| | | 'O' Won! |O|O|O| | | |X| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | |X|X| | | | |O| | | |O| |X|X| | | | |O| | |X|O| |X|X| | | | |O| | |X|O| |X|X| | | |O|O| | |X|O| |X|X| | |X|O|O| |O|X|O| |X|X| | |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| |X| | |O|O| |X|X| | |O| |X| | |O|O| |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | |O| |X| |O|X| | | | |X| |O| |X| |O|X| | | | |X| |O|O|X| |O|X| | 'X' Won! | | |X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| |O| | | | |X|O| | | |X| |O|X| | | |X|O| | | |X| |O|X| | |O|X|O| 'X' Won! | |X|X| |O|X| | |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| | 'X' Won! |O|O| | |X|O| | |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | |O| | | |O| 'X' Won! |X|X|X| | | |O| | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| | |O|X| | |X| | | 'X' Won! |O|O|X| |O|X| | |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O|X| | | | |O| |X| |O| |O|X| | | | |O| |X|X|O| |O|X| | | | |O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X|X| | |O|O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O|X| 'O' Won! |X|O| | |X|O| | | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| | |O|O| |X| | | |X|O|X| | |O|O| |X| | | |X|O|X| | |O|O| |X| |O| |X|O|X| | |O|O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| | | |O| |X| | |O|O| |X| | | |O| |X| |X|O|O| |X|O| | |O| |X| |X|O|O| |X|O| | |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| | | | | |X| |O|X|O| |X| | | |O| |X| |O|X|O| |X|X| | |O| |X| |O|X|O| |X|X| | |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | | | |O|X|O| | | |X| |X| | | |O|X|O| |O| |X| |X| | | |O|X|O| |O| |X| |X| |X| |O|X|O| 'O' Won! |O| |X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | | |O| | |X| | |X|O| | | |X|O| | |X|O| |X|O| | | |X|O| |X|X|O| |X|O| | | |X|O| 'O' Won! |X|X|O| |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | |X| | | | |O| |O|X|X| | |X| | |O| |O| |O|X|X| 'X' Won! | |X| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| 'X' Won! |X| |X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| |X| 'O' Won! | | |O| |X|O| | |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| | |X| | | | |O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X|X| | |O|O| | |X|O|X| 'O' Won! |X|X| | |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | |X|O|X| |O| | | |O| | | |X|O|X| |O| | | |O| | | |X|O|X| |O| |X| |O| | | |X|O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O| |X| | |X| | |X|O|O| |O| |X| |O|X| | |X|O|O| |O|X|X| |O|X| | It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X|O| | | | | |X|X|O| |O|X|O| | | | | 'O' Won! |X|X|O| |O|X|O| | | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | | |O|X| | | | | |X|O|X| 'O' Won! | |O|X| | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | |X|X|O| |O| |O| | |X| | |X|X|O| 'X' Won! |O|X|O| | |X| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| |X|O| | | | |O| | |X|X| |X|O|O| | |X|O| | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O|X|O| | | | | | |X|O| |O|X|O| | | | | |X|X|O| |O|X|O| | | | | 'O' Won! |X|X|O| |O|X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | |X|O|X| | |O| | 'O' Won! |O|X| | |X|O|X| | |O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | | | |X|O|X| | |O|O| | | | | |X|O|X| | |O|O| | |X| | |X|O|X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'O' Won! |O|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| | | |O|X|O| |X|X| | 'X' Won! |O| |X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | |O|X| |X| | | |O| | | |X|O|X| |X| | | |O| | | |X|O|X| |X|O| | |O|X| | |X|O|X| |X|O| | 'O' Won! |O|X| | |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | |X|O|X| | | |O| |O| | | |X|O|X| | | |O| |O|X| | |X|O|X| |O| |O| |O|X| | |X|O|X| |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| 'X' Won! |X|O| | |O|X| | | | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O|O| | |X| | |X| | | |X|O|O| | |X| | |X| | | |X|O|O| |O|X| | |X| | | 'X' Won! |X|O|O| |O|X| | |X| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | | |X| | | |O| |O|X| | |O| |X| | | |O| |O|X|X| |O| |X| | |O|O| |O|X|X| |O| |X| | |O|O| |O|X|X| |O|X|X| 'O' Won! |O|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| |X| |O| |O| | | |X| |X| |X| |O| |O|O| | |X| |X| 'X' Won! |X| |O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | |X|O| |O| | | |O| |X| | |X|O| |O| |X| |O| |X| | |X|O| 'O' Won! |O| |X| |O| |X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | |X| | | | | |X|O|O| | | |X| | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X| |X| | |O|O| |X|O|O| 'X' Won! |X| |X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X|O|O| | | |X| | |X| | |X|O|O| | | |X| |O|X| | |X|O|O| | | |X| |O|X| | |X|O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O|X| |X| | | | |O|X| |O|O|X| 'X' Won! |X| |X| | |O|X| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | | | |X| | | |X| |O|O| | | |O|X| | | |X| |O|O| | |X|O|X| |O| |X| |O|O| | |X|O|X| 'X' Won! |O| |X| |O|O|X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X|O|O| | | |X| |O| |X| |X|O|O| | | |X| |O| |X| |X|O|O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| | |O|O| | 'X' Won! |X| | | |O|X| | |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X| | | | | |O| |O|O|X| |X| | | |X| |O| 'O' Won! |O|O|X| |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | 'O' Won! | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O| |X| |O|X| | |X|O| | |O|O|X| |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | |O|X| | |O|O| | | |X| | |O|X|X| |O|O| | | |X|O| |O|X|X| |O|O| | |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | |X| |O|X| | | |O|O| | | |X| |O|X| | | |O|O| |X| |X| |O|X|O| | |O|O| |X| |X| 'X' Won! |O|X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| |X| | |X| | | |O| | |O|O|X| |X|X| | | |O| | |O|O|X| |X|X| | |O|O| | |O|O|X| 'X' Won! |X|X|X| |O|O| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X| | | | |O|O| |X| |X| |X| | | | |O|O| |X|O|X| |X| |X| | |O|O| |X|O|X| 'O' Won! |X|O|X| | |O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| 'O' Won! |X| | | |X| | | |O|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | |O| | | |O|X| | |X|X| | |O| | | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | |O|O| |X|O|X| 'O' Won! | |X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X|O| | |X|O|O| | |X| | |X|O| | |X|O|O| | |X|O| |X|O| | 'X' Won! |X|O|O| | |X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| |X| | | |O| |O| 'O' Won! | |X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | |O| | |X| | | |X|O|O| | |O|X| |X| | | |X|O|O| | |O|X| |X| |O| |X|O|O| 'X' Won! | |O|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| | | | |O|X| |X|O|X| |O| | | | |O|X| |X|O|X| |O| | | |O|O|X| 'X' Won! |X|O|X| |O|X| | |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| |X| | | |O| |O| | |X|O| |X| | | |O| |O| | |X|O| |X| |X| |O| |O| |O|X|O| |X| |X| 'X' Won! |O| |O| |O|X|O| |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X|X| |O| | | |X| | | |O|X|X| |O| | | |X|O| | 'X' Won! |O|X|X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| |X| |X| | |O| | | |O|O| |X| |X| |X|O| | | |O|O| |X| |X| 'O' Won! |X|O| | | |O|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| |O| | | | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| |X| |X|X| | |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O| | | |X|O| |X|O|O| |X|O| | 'X' Won! |X|X|O| |X|O|O| |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O|X| | | | | | |O| | 'O' Won! |X|O|X| | |O| | | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| 'O' Won! | | |X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| |X| |O| | | | | | |X|O| |X|O|O| 'X' Won! | | |X| | |X|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | |X| |O| 'O' Won! |O|X| | | |O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X|X| | | | |O| |O|O| | |X|X| | | | |O| |O|O| | |X|X| | |X| |O| 'O' Won! |O|O|O| |X|X| | |X| |O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O| | |O| |O| |X| |X| | |O| | |O|X|O| |X| |X| | |O|O| |O|X|O| 'X' Won! |X|X|X| | |O|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| |X|X|O| | |O| | | |O|X| |X|X|O| | |O| | 'X' Won! | |O|X| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | | |O| |O|X|O| | | |X| |X| |O| |O|X|O| |O| |X| |X| |O| |O|X|O| |O|X|X| |X| |O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X|O| |O|X|X| | | | | | |X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | |X| |O|X|O| 'O' Won! |O|X|X| |O| |X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| |O| | | |O|X|X| |X| |O| |O| | | |O|X|X| |X| |O| |O|O| | |O|X|X| |X|X|O| |O|O| | 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X|X| |O| |O| | |O|X| | |X|X| 'X' Won! |O| |O| | |O|X| |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |O| 'X' Won! | |X| | |O|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X| |X| |X| |O|O| | |O| |X| |X| |X| |O|O| | |O|X|X| |X| |X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | |O| |X| |X| | |X|O| | | |O| |X|O|X| | |X|O| | | |O| |X|O|X| | |X|O| | |X|O| |X|O|X| |O|X|O| | |X|O| 'X' Won! |X|O|X| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X|X| | |O|O| |O| |X| | |X|X| | |O|O| |O|X|X| | |X|X| | |O|O| 'O' Won! |O|X|X| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X|O| | | | | |X| |O| |X|X|O| | | | | |X| |O| 'O' Won! |X|X|O| | | |O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | |X| | | |X| |O| |O|X| | |X| | | |X|O|O| |O|X| | |X|X| | |X|O|O| |O|X| | |X|X|O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| | | | |X|O| |O|O| | |X| |X| | |X|O| |O|O| | |X| |X| |O|X|O| |O|O| | 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| |X| |O|O| | 'O' Won! | | | | |X| |X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| | |X| | | |X| | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | |X|X| | |X|O|O| |O|X| | |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | | |X| |O| | | |X|O| | | |O|X| |O|X| | |X|O| | | |O|X| |O|X|O| |X|O| | | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | |O| |O| 'X' Won! | |X| | | |X| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| |O| | | | |X| | | |O|X| |O| | | | |X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O| | | |O|X|X| |X|O|X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | | |X| | | |O| |O|X|X| |O| |X| | | |O| |O|X|X| |O|X|X| | | |O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | |O| |O| | |X|X| |O| | | |O| |O| | |X|X| |O|X| | 'O' Won! |O| |O| |O|X|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| |O| |O| | | | | |X| 'X' Won! |X| |O| |O|X| | | | |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X| | | |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| | | |O| | | |O| |X|O|X| |X| |O| | | |O| |X|O|X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'O' Won! |O|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| |X| |O| | | |X| |O| |O| |X| |O| |X| |X| |O| |O| |X| |O|O|X| |X| |O| |O| |X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |O| | |O|X| | |O| | |X| |O| | |O|X| |X|O| | |X| |O| |O|O|X| |X|O| | |X|X|O| |O|O|X| |X|O| | It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O|X| | |O| | | |X|X| | |O|X| | |O|O| | |X|X| | |O|X| |X|O|O| | |X|X| |O|O|X| |X|O|O| | |X|X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| |O| | | | | |X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X| | |X|O|X| |O|X|O| |O|X| | It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | |X|X| | |O|O| | | |O| | |X|X| | |O|O| | | |O|X| |X|X| | 'O' Won! |O|O| | | |O|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X| | | | | | |O|X|X| 'O' Won! |O|X| | |O| | | |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | | | | | |X|O|O| |O|X| | | | | | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | |O|X| | 'X' Won! |X|O|O| |O|X| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| |O|X| | | | | | 'X' Won! | |O|X| |O|X| | |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| | |X| | |O| |X| |O|O|X| | |X| | |O| |X| |O|O|X| |X|X| | |O| |X| |O|O|X| |X|X| | |O|O|X| 'X' Won! |O|O|X| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X|X| | |O| | | |X| |O| |X|X|O| |O| | | |X| |O| |X|X|O| |O| |X| |X| |O| |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | |X|O| |O| |X| | | |O| | |X|O| |O| |X| 'X' Won! |X| |O| | |X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | |X| | | |X|X|O| | |O| | |X| | | |X|X|O| | |O| | |X|O| | |X|X|O| | |O|X| |X|O| | |X|X|O| |O|O|X| |X|O| | It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | | |O| | | |X| | |O|X|X| | |O| | | |X| | 'O' Won! |O|X|X| | |O| | | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| | | |X|O| | 'X' Won! |X| |O| |X| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | 'O' Won! | |X| | |O|O|O| | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| |X| | | |O| | |X| | |O|O|X| 'X' Won! |X| |O| | |X| | |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | |O|O| | 'X' Won! | | | | |X|X|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | | | | |O| | | |O|X|X| | | | | |O|X| | |O|X|X| | | | | |O|X|O| 'X' Won! |O|X|X| | |X| | |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X|O| | 'X' Won! |X|O|O| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | | |O| | | | |X| |X|O| | | |O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O| | 'O' Won! |X|O| | |O|O|X| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| | |O|O| | | | | |X| |X| 'X' Won! | |O|O| | | | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| |X| | |O| | |X| |O| |O| |X| | |O| | |X|X|O| |O| |X| | |O| | |X|X|O| |O| |X| |O|O| | |X|X|O| |O| |X| |O|O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | | | |O|X|O| |X| | | | |X| | |O|X|O| |X| | | | |X| | |O|X|O| |X| |O| |X|X| | |O|X|O| |X| |O| |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | |O| | |X| |O| | |O|X| | |O|X| |X| |O| |O|O|X| | |O|X| |X|X|O| |O|O|X| | |O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X|X| | | | |O| |O| | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O|O| | |X|X| | |O|X|O| |O|O|X| |X|X| | |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O|X|O| 'O' Won! |O|X| | |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | 'X' Won! | | |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| | | | | |X|O| | | |O|X| |O| | | |X|O| | |X|O|X| |O| | | 'O' Won! |X|O| | |X|O|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X|X| | | |O|O| 'O' Won! | | |X| |X|X| | |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | | |X| |O| | | |X|O|O| | | |X| |O|X| | |X|O|O| | | |X| |O|X| | |X|O|O| | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |O| |X|X| | | | |O| | | |O| |X|X| | | | |O| | |O|O| |X|X| | | |X|O| | |O|O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X|O|X| | | |O| |X|O|O| 'X' Won! |X|O|X| | |X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| | |O|X| |O| |O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| | |X| | |O| | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X|X| |O| | | |X|O|X| |O|X|X| |O| |O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | | |O|X| | |X|X| | |O| | | |O|X| | |X|X| |O|O| | |X|O|X| | |X|X| |O|O| | 'O' Won! |X|O|X| | |X|X| |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | | | | |X|X| |O|O| | | | | | |O|X|X| |O|O| | | | |X| |O|X|X| 'O' Won! |O|O| | |O| |X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| | | | |O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X|X| | |O|O| | |X|X|O| |X|X|O| 'X' Won! |O|O|X| |X|X|O| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | | | 'O' Won! |O|X|X| | |O| | | | |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| |X|X| | | |O|O| |X| |O| |X|X| | | |O|O| 'O' Won! |X| |O| |X|X|O| | |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | |O| | |X|X|O| | |X| | | |O| | |X|X|O| | |X|O| | |O| | 'X' Won! |X|X|O| | |X|O| | |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| |X|O| | |X|X| | | |O|O| |X|O|O| |X|X| | 'X' Won! | |O|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| |X|X| | |O| |O| 'X' Won! | | |O| |X|X|X| |O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| |X|X| | 'O' Won! | | |O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X|X| |X| | | |O| |O| |O|X|X| 'X' Won! |X| | | |O|X|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | | |O|O| | |X|X| | | | | | |O|O|X| |X|X| | |O| | | |O|O|X| 'X' Won! |X|X|X| |O| | | |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | |O| |X| |O| |X|O| | 'X' Won! |X| |O| |X| |O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | |O| | |O| | | |X| |X| |X|O| | |O| | | |X| |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| |X| |X| |X| |X|O|O| |O|O|X| 'X' Won! |X|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| | | |X|O| |X| | | |O|O|X| | |X|O| |X| |O| |O|O|X| | |X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X| |X| | |O|O| | | |X| |X| |X| | |O|O| | |O|X| 'X' Won! |X|X|X| | |O|O| | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X|O| |X|O| | |O|X| | |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X| | |O|X| |O| | | |O| |X| |X|O|X| |O| | | |O|O|X| |X|O|X| |O| | | |O|O|X| |X|O|X| |O|X| | 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | |X| | | | |X|O| |O| | | |X| | | |O|X|O| |O| | | 'X' Won! |X| | | |O|X|O| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | | |X|O| |X| | | |O| | | |O|X|O| |X| | | |O| | | |O|X|O| |X|X| | |O|O| | |O|X|O| |X|X| | 'X' Won! |O|O| | |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | |X| |O|X| | |O|O| | | | |X| |O|X|X| |O|O| | | | |X| 'O' Won! |O|X|X| |O|O| | |O| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | | | | | |O|O|X| |O|X| | | | |X| |O|O|X| |O|X| | | |O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | |X| | | | |X| | 'O' Won! |O|O|O| |X| | | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X|O| |O|X| | | | | | |X|X|O| |O|X| | | | | | |X|X|O| |O|X| | | | |O| |X|X|O| |O|X|X| | | |O| |X|X|O| |O|X|X| |O| |O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| |O| | | | | |X| |X| |O|X|O| | | | | |X| |X| |O|X|O| | | |O| 'X' Won! |X| |X| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O|X|O| |O| | | |X| |X| |O|X|O| |O|O| | 'X' Won! |X|X|X| |O|X|O| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| | |O| | 'X' Won! |X|O|O| | |X|X| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X|O| | |X|O| | | | | |O|X|O| | |X|O| | | |X| |O|X|O| | |X|O| |O| |X| 'X' Won! |O|X|O| | |X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| |X| |O|X|O| | |X|O| |X|O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | | |O| | | |X| | |X|O| | | |O| | |O|X| | |X|O| | | |O| | |O|X| | |X|O|X| | |O| | |O|X|O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|O| | |O| | |O|X|X| | |X|O| | |O|O| |O|X|X| | |X|O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X| | |X|X| | | | |O| |O|X| | |X|X| | | |O|O| |O|X| | |X|X| | |X|O|O| |O|X| | |X|X|O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X| |O| |O|O| | |X|O|X| |X| |O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X|O|X| | |O| | | | | | |X|O|X| |X|O| | | | | | 'O' Won! |X|O|X| |X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| | | |X| |O| | |O|O| 'X' Won! |X| | | |X| |O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| |O| |O| | | | | |X| |X|X|O| |O| | | | | |X| |X|X|O| |O| |O| |X| |X| |X|X|O| |O| |O| 'O' Won! |X| |X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X| | |X| |X| |O| | | |O|X|O| |X| |X| |O|X| | |O|X|O| |X| |X| |O|X| | |O|X|O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |X| |X| | | | |O| | |O| |X| |X| | | | |O|X| |O| |X| |X| | | | |O|X| |O| |X| |X| |O| |X|O|X| |O| |X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O| | | |X| | |O|X|X| |O|O| | | |X| | |O|X|X| |O|O| | | |X|X| 'O' Won! |O|X|X| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | |X|O|O| |X| |O| | | |X| |X|O|O| |X| |O| | | |X| |X|O|O| |X|O|O| | |X|X| |X|O|O| |X|O|O| 'O' Won! |O|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | | | |X|O|O| |X|X| | |O| | | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| |X| | |O| | | |O|X| |O| |X| | |O| | |X|O|X| |O| |X| |O|O| | |X|O|X| |O| |X| 'X' Won! |O|O|X| |X|O|X| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | |X| | |O|O| | | | |X| |O|X| | |O|O| | | | |X| |O|X| | |O|O|X| | | |X| |O|X| | |O|O|X| | |O|X| |O|X| | |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | | |O|X|O| |O|X| | | | |X| |O|X|O| |O|X| | 'O' Won! |O| |X| |O|X|O| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | |O|O| | |X|O| |X| |X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| | |O|O| 'O' Won! |X|X|O| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | |X|X| | | |O|X| | |O| | |X|X|O| | |O|X| | |O|X| |X|X|O| | |O|X| | |O|X| |X|X|O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| | | | | |O|X|X| | |O|O| | | | | |O|X|X| | |O|O| |X| | | |O|X|X| | |O|O| |X|O| | |O|X|X| | |O|O| |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | |X| | | |X| | |O| |O| | |X|O| | |X| | |O| |O| |X|X|O| 'O' Won! | |X| | |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X|X| | 'O' Won! |X| | | |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | | | |O| |X|O|O| |X|X| | |O| |O| |X|O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X|O| |O| | | | | |X| |X|X|O| 'O' Won! |O| | | | |O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X|X| | | | | |X|O|O| |O|X|X| | | |X| |X|O|O| |O|X|X| |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X|X| | |O|O| | | |X| | |X|X| | |O|O| | |O|X| | |X|X| |X|O|O| | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | |X| | |O|X| | | |O| 'O' Won! |O| |X| | |O|X| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |O|X| | | |X| | |O|O| | |O|X|X| 'O' Won! | |X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | |O| | |X| |O| | | |X| |X|O| | |X| |O| | | |X| |X|O| | |X|O|O| | | |X| |X|O|X| |X|O|O| | | |X| |X|O|X| |X|O|O| |O| |X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | | | |O| | |X|X| |O|O| | | |X|O| | |X|X| |O|O| | |O|X|O| | |X|X| |O|O| | |O|X|O| | |X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O| | | |O|X|X| |X|O|X| |O| | | |O|X|X| |X|O|X| |O|O| | 'X' Won! |O|X|X| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | | |O|X| |X| | | |O| | | | |O|X| |X| | | |O|O| | | |O|X| |X| |X| |O|O| | 'O' Won! | |O|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O| |X| | |X|X| |O| | | |O|O|X| | |X|X| 'X' Won! |O| |X| |O|O|X| | |X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X| | |X| | 'O' Won! |O| | | |O| |X| |O|X| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| |O|O| | |O|X|X| | |X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| |O| | | |X| | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| | |O| | 'X' Won! |X|O|O| | |X|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| | | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| |X|O| | |X| |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'X' Won! |X|O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O| |X| | |X| | |O| |X| 'O' Won! |O| |X| |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | | |O| |O|X|X| | |O| | | |X|O| |O|X|X| | |O| | |O|X|O| |O|X|X| 'X' Won! |X|O| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| | | |X|X|O| |O| |X| |O|O| | |X|X|O| |O| |X| |O|O| | |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O| |X| |O|X| | |X|O|O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| | | |X| | |O|X| | | |O|O| | |X| | |O|X| | |X|O|O| | |X| | |O|X|O| |X|O|O| | |X| | |O|X|O| |X|O|O| | |X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| |O| | | |X| |O| |X| |X| |O| | |O|X| |O| |X| |X| |O| | |O|X| |O| |X| |X|X|O| |O|O|X| |O| |X| |X|X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| 'O' Won! |X|X|O| | | |O| | | |O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | |O|X| | |X|O| |X|O| | | |O|X| | |X|O| |X|O|O| | |O|X| |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| |O| | | |X|X| | | |O|X| |O| | | |X|X| | | |O|X| |O| |O| |X|X| | |X|O|X| |O| |O| 'O' Won! |X|X| | |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | |X|O| | |X| | |X|O| | |O|X|O| | |X| | |X|O| | |O|X|O| |X|X| | |X|O| | |O|X|O| |X|X| | |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| |X|O| | |X| | | |O|O|X| |X|O| | |X| | | |O|O|X| |X|O|X| |X| | | 'O' Won! |O|O|X| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O|X| | |X| | 'O' Won! | | |O| | |O|X| |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O|X| 'O' Won! | |O| | | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | |X| | | |X| |O|O| | 'O' Won! | | |X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O|X| | |O| | | |X|X| |O|O|X| | |O| | | |X|X| 'X' Won! |O|O|X| | |O|X| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| |O|X|O| | | | | |X| |X| |O|X|O| | | | | |X|O|X| |O|X|O| | | |X| |X|O|X| |O|X|O| 'O' Won! |O| |X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X| |X| |O|X|X| 'O' Won! |O| |O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | |X| | | | |X| |O| |O| | |X|O| | | |X| |O| |O| |X|X|O| | | |X| |O| |O| |X|X|O| | |O|X| |O|X|O| |X|X|O| | |O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | |O| |X| |O| | |X|X| | | |O| |X|O|O| | |X|X| | | |O| 'X' Won! |X|O|O| |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | |X| | | |X|O|O| | |X| | |X| | | |X|O|O| |O|X| | |X| | | |X|O|O| |O|X|X| |X|O| | |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| | | |X|X| |O|O| | |X|O| | | |X|X| |O|O|X| |X|O| | | |X|X| |O|O|X| |X|O|O| | |X|X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | |X| |X| | |O|O| | |O| | |X| |X| |X|O|O| | |O| | |X| |X| 'O' Won! |X|O|O| | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | |X| | |X| | | |O| |O| | |X| | |X| |O| |O| |O| | |X| | |X| |O| |O|X|O| 'O' Won! | |X|O| |X| |O| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| | | |X| |X| | |X|O| 'O' Won! |O| | | |X|O|X| | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| |X| | |O| | |O|O|X| |X| |X| 'X' Won! | |O| | |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| |O| | | | | |X| |X|X|O| |O| | | |O| |X| |X|X|O| |O| | | |O|X|X| |X|X|O| |O| | | |O|X|X| |X|X|O| |O| |O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| | |O| | |X|O|X| | | |O| |X|O| | |X|O|X| | | |O| |X|O|O| |X|O|X| 'X' Won! |X| |O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X|O| | |O| | | |X| |X| |X|O| | |O| | | |X|O|X| |X|O| | |O| | | |X|O|X| |X|O|X| |O| | | 'O' Won! |X|O|X| |X|O|X| |O|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| | |X| | |X|O|O| | | |O| |X|X| | |X|O|O| 'O' Won! | | |O| |X|X|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| | | |O| | |X| | |O|O|X| | | |O| |X|X| | |O|O|X| |O| |O| |X|X| | 'X' Won! |O|O|X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| |X| | | | | |X| |O| |O| 'O' Won! |X| | | | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | | | |X| |O|O| | |O|X| | | |X|X| |O|O| | |O|X| | | |X|X| |O|O| | |O|X|O| 'X' Won! |X|X|X| |O|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | |O| | |X|O| | |O|X| |X| |O| | |X|O| | |O|X| |X| |O| |O|X|O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X|O| | | | | |O|O| | |X|X|O| | |X| | |O|O| | |X|X|O| |O|X| | |O|O| | 'X' Won! |X|X|O| |O|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O| |O| |X|O| | | |X| | |O|X|O| |X|O|O| | |X| | |O|X|O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| 'X' Won! | | |X| |O|X| | |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| |X| | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O|X|X| | | |O| |X|X|O| |O|X|X| |O| |O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | 'X' Won! |X| |O| |O|X| | | | |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | |O|X| | |X| |O| |O|X| | |O|X|X| |X| |O| |O|X| | |O|X|X| |X|O|O| |O|X| | It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| |O| |O| |X| |X| |X| |O| |O| 'O' Won! |O| |X| |X|O|X| |O| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | |X| | | | | |O| 'X' Won! |X|O| | |X| | | |X| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| |X| | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X| | |O| |O| |X|X| | |O|X|X| |O| |O| 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| |X| | |O| | | |X| | |O| |X| |O|O| | | |X| | |O| |X| |O|O|X| | |X| | |O|O|X| |O|O|X| |X|X| | |O|O|X| |O|O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | | | | |X| | | |O|X|O| | |O| | |X| | | |O|X|O| | |O| | |X|X| | |O|X|O| |O|O| | |X|X| | 'X' Won! |O|X|O| |O|O| | |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |O| | |X| | |O|X|O| | | |O| |X|X| | 'O' Won! |O|X|O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| |O| | |O|X| |X|X| | |O| |O| | |O|X| |X|X|O| |O| |O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X|O| |X| | | |O| | | | |X|O| |X|X| | |O| | | | |X|O| |X|X| | |O| |O| 'X' Won! | |X|O| |X|X| | |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| | |X|X| |O| |O| | | |O| | |X|X| |O|X|O| | | |O| |O|X|X| |O|X|O| 'X' Won! | |X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | |O|O|X| | |X| | | | | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O| |X| |O|O|X| |X|X| | |O|O|X| |O|O|X| |X|X| | 'X' Won! |O|O|X| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | |X| |X| | |X|O| |O|O| | |X| |X| | |X|O| |O|O| | |X| |X| |X|X|O| 'O' Won! |O|O| | |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | | |O| | |X|O|X| |O|X| | | |O|O| |X|O|X| |O|X|X| | |O|O| |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | |X| |O| |X| | |X|O| | | |X| |O| |X| |O|X|O| | | |X| |O|X|X| |O|X|O| 'O' Won! |O| |X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O| |O|X|O| | | | | | |X|O| |O|X|O| |X| | | 'O' Won! | |X|O| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | 'O' Won! |O| |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X| | | |O|X| |X| |O| | |X|O| | |O|X| |X| |O| | |X|O| | |O|X| |X|X|O| 'O' Won! |O|X|O| | |O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X|X| | | |O|O| | | |O| |X|X| | | |O|O| |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| |O| | | |X|X|O| | | |X| |O| | | 'O' Won! |X|X|O| | |O|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O| | | |X| | |X|O|O| | |O| | | |X|X| |X|O|O| | |O| | |O|X|X| |X|O|O| | |O| | |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | |O|X| |O|X| | | |X| | | |O|X| |O|X| | | |X| | |O|O|X| |O|X|X| | |X| | |O|O|X| |O|X|X| | |X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | | | |O| |X|O|O| |X| |X| | |O|O| |X|O|O| |X| |X| 'X' Won! |X|O|O| |X|O|O| |X| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O|O| | |X| |X| |O| | | |O|O|X| |X| |X| |O|O| | |O|O|X| |X| |X| 'X' Won! |O|O|X| |O|O|X| |X| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X|O|O| | |X|X| | | | | |X|O|O| |O|X|X| | | | | |X|O|O| 'X' Won! |O|X|X| | |X| | |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | |O| | | | | |X|X|O| | | |O| |X| | | |X|X|O| |O| |O| |X| | | |X|X|O| |O|X|O| |X| | | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | |X|O| |O|O| | |X| | | | |X|O| |O|O|X| |X| | | | |X|O| 'O' Won! |O|O|X| |X|O| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | |X|O| | | |X|O| | | | | |X|O|O| | |X|O| | | | | |X|O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X|O| | 'X' Won! |X|O|O| | |X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | | |X| | |X| | |O| |O| | | |X| | |X|O| |O| |O| | |X|X| | |X|O| 'O' Won! |O|O|O| | |X|X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X|O|O| 'X' Won! | |O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | |O| | |X|O| | |X|X| | | |O| | |X|O| | |X|X| | | |O| | |X|O|O| |X|X| | | |O|X| |X|O|O| |X|X| | |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | |O| | |X|X|O| | | |X| | |O| | |X|X|O| | | |X| |O|O| | |X|X|O| |X| |X| |O|O| | 'O' Won! |X|X|O| |X| |X| |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| | | |X|O| |X| |O| | |X| | 'O' Won! | |X|O| |X| |O| | |X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| |X| |O| | | | | |X| |O| |X| |O|O| | | | |X| |O|X|X| |O|O| | 'O' Won! |O| |X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | |X|X| |O| | | |O| | | 'O' Won! |O|X|X| |O| | | |O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | | |X|X| | |O| | |O| | | |O|X|X| | |O|X| |O| | | |O|X|X| | |O|X| |O| |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| | |X|X| | |O| | | |X|O| | |X|X|O| |O| | | |X|O| | |X|X|O| |O|X| | |X|O| | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | |O| | | |O|X|X| | |O| | |O|X| | |O|X|X| | |O| | |O|X|O| |O|X|X| 'X' Won! |X|O| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| |X|O| | | | |O| |X|X|O| |X|O| | | | |O| |X|X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'X' Won! |X|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| |O| |O|X| | |X|X| | |O| |O| |O|X|X| |X|X| | |O| |O| |O|X|X| |X|X|O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X| |X| | |O| | |X| |O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'X' Won! |X|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O|O| | | |X| | |X|O| | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | |X| | |O| | | |X|O| | |O|X| | |O| | | |X|O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| | |O|O| | | | | |X|X|O| |X|O|O| | | | | |X|X|O| |X|O|O| | |O| | |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | |X| | | |O|O| | | |X| | |X| |O| |O|O| | | |X| | |X|X|O| |O|O| | | |X| | |X|X|O| |O|O| | | |X|O| |X|X|O| |O|O|X| | |X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | | |O| | | |X| |X| |O|X| | |O|O| | |X| |X| |O|X| | |O|O| | |X| |X| |O|X|X| 'O' Won! |O|O|O| |X| |X| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | |O|X| |X| |O| | |X| | | |O|X| |X| |O| | |X| | | |O|X| |X|O|O| | |X| | |X|O|X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| | |O|O| | | |X| |X|O|X| | |O|O| | |X|X| |X|O|X| | |O|O| |O|X|X| |X|O|X| 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | 'O' Won! |O|X| | |O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O|X| | | |O| | |X| |X| |O|X| | | |O|O| |X| |X| 'X' Won! |O|X| | | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| 'O' Won! |X| | | | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| 'O' Won! |O| |X| |O| | | |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | | |X| |X|O| | | | |O| | | |X| |X|O| | |X| |O| | | |X| |X|O| | |X| |O| | |O|X| |X|O|X| |X| |O| | |O|X| |X|O|X| |X| |O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | |O| | |X| |X| | |O| | | |O|X| |X| |X| | |O| | | |O|X| |X| |X| |O|O| | 'X' Won! | |O|X| |X| |X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X|X| | |O| |O| | |X| | |X|X|O| |O| |O| 'X' Won! | |X| | |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X|X| | |O|O| | | |O| | |X|X| |X|O|O| | | |O| | |X|X| |X|O|O| | | |O| |O|X|X| |X|O|O| |X| |O| |O|X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O|O| |X|X|O| 'X' Won! |X|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O|X| | | |O| |X|X| | | |O|X| | |O|O| |X|X| | |X|O|X| | |O|O| |X|X| | 'O' Won! |X|O|X| |O|O|O| |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X|X| | | | | | |X|O|O| |X|X| | | | |O| 'X' Won! |X|O|O| |X|X| | |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |X|X| | |O| |O| | |X|O| |X|X| | |O| |O| |X|X|O| |X|X| | 'O' Won! |O| |O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | |X| | | |X| |O| |O|O| | |X| | | |X|X|O| |O|O| | |X| | | 'O' Won! |X|X|O| |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O| | | 'X' Won! |O|O|X| |X|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| |O| | | | | |X| |O|X|O| |O|X| | | | |X| |O|X|O| 'O' Won! |O|X| | |O| |X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |O| | | | | |X|O| | |X| |O| | | | | |X|O| | |X| |O| | | |O| 'X' Won! |X|O| | |X| |O| |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| | | | |O|X| | |X|O| |X| |O| | |O|X| | |X|O| |X| |O| | |O|X| |X|X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | | |X| | | | | |O|X|O| | |O|X| | | | | |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | |O| |X|X|O| | | | | | | |O| |X|X|O| | | |X| | | |O| |X|X|O| | |O|X| 'X' Won! |X| |O| |X|X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | |O|X| |X| | | | |O|O| | |O|X| |X| |X| | |O|O| 'O' Won! | |O|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| |O| | | | |X| | |X| |O| |O| |X| |O|X| | |X| |O| |O| |X| |O|X|X| |X| |O| |O| |X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | |O| | | |O| |X| |X|X| | |O| | | |O| |X| |X|X|O| |O| | | |O| |X| |X|X|O| |O| | | |O|X|X| 'O' Won! |X|X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X|X|O| |O| |O| | | |X| |X|X|O| |O| |O| | |O|X| 'X' Won! |X|X|O| |O|X|O| | |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | |O| |X| |O| | |O|X| | |X|O| 'O' Won! |X| |O| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | |X|X| | | | |O| | | | | |X|X| | | |O|O| | | |X| |X|X| | | |O|O| |O| |X| |X|X| | | |O|O| |O|X|X| |X|X| | | |O|O| 'O' Won! |O|X|X| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| | |O| | | |X| | |X|O|O| | |O|X| | |X|O| |X|O|O| | |O|X| | |X|O| |X|O|O| |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X|X|O| |O| | | |X|O| | |X|X|O| |O| | | 'X' Won! |X|O| | |X|X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X|O| |X| | | | |O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X| | | |O|O|X| |X|X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | | | | | |O|O|X| | |X|O| | | | | |O|O|X| |X|X|O| 'O' Won! |O| | | |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X|X| | |O|O| | | | | |O|X|X| |X|O|O| | | | | |O|X|X| 'O' Won! |X|O|O| | |O| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| 'X' Won! | |O|X| | |O|X| | | |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | |O| |O| |X| |O| |X| | | |O| |O| |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O| |O| |X| |X| | |O| | |O|X|O| |X| |X| | |O| | |O|X|O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| |X| | |O| | |O|X|X| |O| |X| | |O|O| |O|X|X| |O| |X| |X|O|O| 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X|X|O| | | | | |O|O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X|O| | |O|O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| |O|X|X| | |X| | | | |O| |O|X|X| | |X|O| | | |O| |O|X|X| | |X|O| |X| |O| |O|X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | |X|O| | | |X| | | |O| |O|X|O| | | |X| | | |O| |O|X|O| | | |X| |X| |O| 'O' Won! |O|X|O| | |O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | | | | | |X|O| |X|O| | | | | | |O|X|O| |X|O| | | | |X| |O|X|O| |X|O| | |O| |X| |O|X|O| |X|O|X| |O| |X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X| |X| |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X| |X|O|X| | | |O| |O| |X| |X|O|X| | | |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| |O| | | |X| | | | |X|O| |O| | | |X| |X| | |X|O| |O|O| | |X| |X| | |X|O| 'X' Won! |O|O| | |X|X|X| | |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| | |O| |O| | |X|X| | |O|X| |O| |O| | |X|X| |O|O|X| |O| |O| | |X|X| |O|O|X| |O|X|O| 'O' Won! |O|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X|X| | |O| | | |O|O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O| | | |O|O| |O|X|X| |X|O|X| | |O|O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| |X| |O| | | | |O|X| |O| |X| 'X' Won! |O| |X| | |O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X|O| | |O| | 'X' Won! |X|O|X| |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| 'O' Won! | | |X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| 'O' Won! |O| | | |X|O|X| | | |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | |X|O|X| |O|X| | |O|X|O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X| | | |O|X| |X| |O| | |X| | | |O|X| |X|O|O| | |X| | |X|O|X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | |X| |O| |X| |O| | |O| | 'X' Won! |X| |O| |X| |O| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| | |O| | |X|X| | 'O' Won! |O| |X| | |O| | |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X|X| | |O|O| | |O| | | |X|X| | 'X' Won! |O|O| | |O| | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | |O| | | |X|X| |X| |O| | |O| | | |X|X| |X| |O| | |O|O| | |X|X| |X|X|O| | |O|O| | |X|X| 'O' Won! |X|X|O| | |O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |X|X| | |O| | | | |X|O| |X|X|O| |O| | | | |X|O| |X|X|O| |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| | |O| | |O|X| | |X| |X| |O|O| | |O|X| | |X| |X| 'X' Won! |O|O|X| |O|X| | |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O|X|X| | |O| | |X| | | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | | | |X|O| | |X| |O| | | | | |X|O| | |X| |O| |O| | | |X|O| | |X| |O| |O| |X| |X|O|O| |X| |O| |O| |X| |X|O|O| |X| |O| |O|X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |X| | |O|X| |O|O| | |X| |X| 'X' Won! | |O|X| |O|O|X| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | |X| | | | |X|X| |O|O| | |X|O| | | |X|X| |O|O| | 'X' Won! |X|O| | | |X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |X| | |O|X| | |O|O| |X| |X| | |O|X| |X|O|O| |X| |X| | |O|X| 'O' Won! |X|O|O| |X|O|X| | |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O|X| | |O|O|X| |X|X|O| |O|X| | It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | |X|O| | | | |X| | |O| | |X|O| | | | |X| | |O|O| |X|O| | | | |X| |X|O|O| |X|O| | |O| |X| |X|O|O| |X|O|X| |O| |X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| |O| |O| | |X| | | |X|X| |O| |O| | |X| | | |X|X| |O| |O| | |X|O| 'X' Won! |X|X|X| |O| |O| | |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| |O| | | | |O| | | | |X| |O| | | |X|O| | | | |X| |O|O| | |X|O| | | | |X| |O|O| | |X|O| | |X| |X| 'O' Won! |O|O|O| |X|O| | |X| |X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | |X|O| |X| |O| | |O| | | |X|O| |X| |O| | |O|X| | |X|O| |X| |O| |O|O|X| | |X|O| |X|X|O| |O|O|X| | |X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| | |O|O| |X| | | |X|O|X| | |O|O| |X| | | 'O' Won! |X|O|X| | |O|O| |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | 'O' Won! |X| |O| |X| |O| | | |O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| |X| |X|O| | |O| | | |X|O|X| |X|O|X| |O| | | |X|O|X| |X|O|X| |O| |O| |X|O|X| 'X' Won! |X|O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X|O| | |X|O|O| | |X| | |X|O| | |X|O|O| |O|X| | |X|O| | |X|O|O| |O|X|X| |X|O| | It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X|O|X| |O| | | | | |X| |X|O|X| |O| | | |O| |X| |X|O|X| |O|X| | |O| |X| |X|O|X| |O|X|O| |O| |X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | |X| | | | |O|O| | |X| | 'O' Won! |X| | | |O|O|O| | |X| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | |X| |X|O|O| | | |O| | | |X| |X|O|O| | | |O| | |X|X| |X|O|O| | |O|O| | |X|X| |X|O|O| |X|O|O| | |X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | |X| | |O|X| | | |O| |O| |X| | |O|X| |X| |O| |O| |X| | |O|X| |X| |O| |O|O|X| |X|O|X| |X| |O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| |O| |X| |X| | |O|O| |X| |O| |X|O|X| | |O|O| |X| |O| 'X' Won! |X|O|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | | |X| |O| |O| | |O|X| | | |X| |O| |O| |X|O|X| | |O|X| |O| |O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X| |O| |X| |X|O|O| | | |X| |O| |X| |X|O|O| |O| |X| |O| |X| 'X' Won! |X|O|O| |O|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X| | | |O|O| | |O|X| | |X| | | |O|O| |X|O|X| | |X|O| | |O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| | |O| | | |X| | |O| |X| | |O| | | |X|O| |O| |X| | |O| | | |X|O| |O|X|X| | |O| | |O|X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X|X| 'O' Won! |O|O|O| | |X| | | |X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| |X| |O| |O| | | |X|O|X| |X| |O| |O| | | |X|O|X| |X|X|O| |O| | | |X|O|X| |X|X|O| |O|O| | 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| | | |O| | | |X| |O| |X| |X| |O| |O| |X| |O| |X| |X| |O| 'X' Won! |O| |X| |O|X|X| |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X| | | |X|X| | |O|O| |O|X| | | |X|X| 'X' Won! |X|O|O| |O|X| | | |X|X| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | |O| | | |O|X|X| |X| | | |O| | | |O|X|X| |X| |O| |O| | | |O|X|X| |X| |O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | 'X' Won! |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | |O| | | |X| |O| |X| |X| |O| | | |X| |O| |X| |X| |O| | |O|X| |O|X|X| |X| |O| | |O|X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| |O|X| | | |O|X| | |X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O| |X| |O|O| | |X|X| | |O| |X| |O|O|X| |X|X| | |O| |X| |O|O|X| |X|X| | |O|O|X| 'X' Won! |O|O|X| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| | | | | |X| | |O|O| |X| | | | |X|X| | |O|O| |X|O| | | |X|X| | |O|O| |X|O| | | |X|X| |X|O|O| |X|O|O| | |X|X| |X|O|O| 'X' Won! |X|O|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X|O| |X| | | |X|O| | | |X|O| |X| | | |X|O| | | |X|O| |X|O| | 'X' Won! |X|O| | | |X|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | |O|X| |O|X|O| | | |X| | |O|X| |O|X|O| | |O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X| |O| | | |O| | |O|X| |X| |O| | | |O| |X|O|X| |X|O|O| | | |O| |X|O|X| 'X' Won! |X|O|O| |X| |O| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| | |X|O| | |O|X|X| |X|O| | |X|O| | 'O' Won! |O|X|X| |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| |X| | | | |O| | |O| |X| |X| | | | |O| | |O|O|X| |X| | | |X|O| | |O|O|X| 'O' Won! |X| |O| |X|O| | |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | |O|X|O| | | | | 'X' Won! | |X| | |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X|O|O| | | |X| | | | | |X|O|O| | |X|X| | | |O| |X|O|O| | |X|X| |X| |O| |X|O|O| | |X|X| 'O' Won! |X| |O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | | | |X| | | | |X|O| | |O| | 'X' Won! |X| | | | |X|O| | |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| |O| |X| | | | | | |X|O| |O| |X| | | | | |X|X|O| 'O' Won! |O| |X| | |O| | |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | |O| | | |X|X| |X| |O| | |O| | |O|X|X| |X| |O| | |O| | |O|X|X| |X|X|O| | |O| | |O|X|X| |X|X|O| | |O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| |X| |X| | |O| | | |O|O| |X| |X| | |O| | 'X' Won! | |O|O| |X|X|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| | | |O| |O| |X|X| | |O| |X| |O| |O| |X|X| | |O|O|X| |O| |O| |X|X| | |O|O|X| |O|X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| |O| | | | | |X| |X| |O|X|O| | | | | |X| |X| |O|X|O| |O| | | 'X' Won! |X|X|X| |O|X|O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X| | | |X|O| |O| | | |O|X|X| | |X|O| |O|O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| |X| | | |X| | | |O| |O|O|X| |X| |X| | | |O| |O|O|X| |X| |X| |O| |O| |O|O|X| 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| |O| | | 'X' Won! | |O| | |X|X|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| |X| | | | | | |O|O| |X|O|X| | | |X| | |O|O| |X|O|X| 'O' Won! | |O|X| | |O|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| | | |X| |X| |O| |O| |X| | | |X|O|X| |O| |O| |X|X| | |X|O|X| |O| |O| 'O' Won! |X|X| | |X|O|X| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O| | | |X| |O|X|O| | | |O| | | |X| |O|X|O| | | |O| | |X|X| |O|X|O| | | |O| |O|X|X| |O|X|O| |X| |O| |O|X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O| |X| |X| |O| | | |X| |O|O|X| |X| |O| | | |X| |O|O|X| |X|X|O| | | |X| |O|O|X| |X|X|O| | |O|X| 'X' Won! |O|O|X| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X|O|O| | |X| | | | |X| |X|O|O| | |X|O| | | |X| |X|O|O| |X|X|O| | | |X| |X|O|O| |X|X|O| | |O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | | |X| | |X| |O| |O|X|O| | |X|X| |X| |O| |O|X|O| |O|X|X| |X| |O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | |X|X| | |O| |O| | | | | |X|X| | |O| |O| | | |O| |X|X| | |O| |O| |X| |O| |X|X| | |O| |O| |X|O|O| 'X' Won! |X|X|X| |O| |O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| | | |X|O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | |O|O| | | |O| |X| |X| |X|O|O| | |O|O| |X| |X| |X|O|O| 'X' Won! | |O|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | |X|O|O| | |X|O| | | | | |X|O|O| |X|X|O| | | | | |X|O|O| |X|X|O| | |O| | |X|O|O| |X|X|O| | |O|X| |X|O|O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| | | |O|O|X| |O| | | |X| |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| |O| 'X' Won! |X|X|X| |O|O|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X|X| 'O' Won! |O| |X| |O| | | |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|O| | | | | |O|X|X| |X|O|O| | | | | 'O' Won! |O|X|X| |X|O|O| | | |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X|O| |X| | | |O|X| | 'X' Won! |O|X|O| |X|X| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | |O|X| |O|X| | | | | | |X|O|X| |O|X| | | | | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X| | |O|X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | |O| |X|X| | | | |O| |X| |O| |X|X| | | | |O| 'O' Won! |X| |O| |X|X|O| | | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | | |X| |X|O| | |O| | | |X| |X| |X|O| | |O| | | |X| |X| |X|O| | |O| |O| |X| |X| |X|O|X| |O| |O| |X|O|X| |X|O|X| |O| |O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | | |O| | |X| | |O|X| | |O| |O| | |X| | |O|X| | |O| |O| |X|X| | |O|X| | |O| |O| |X|X|O| |O|X| | |O| |O| |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |O| |O|X|X| | | | | | | |O| 'O' Won! |O|X|X| | |O| | | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | | | | |X|X| 'O' Won! |O|O|O| | | | | | |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| | |O| | |X| | | |O| |X| | |O| | |X|O| | |O| |X| | |O|X| |X|O| | |O| |X| 'O' Won! | |O|X| |X|O| | |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | |X|O| |O| |X| |X| | | |O|X|O| |O| |X| |X|X| | |O|X|O| |O|O|X| |X|X| | |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | |O| |X| | | |X| |O|O| | |O| |X| |X| |X| 'O' Won! |O|O|O| |O| |X| |X| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| |O| |X| 'X' Won! | |O|X| |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O|O| |X| | | | |X| | | |O|O| |X|X| | | |X| | 'O' Won! |O|O|O| |X|X| | | |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | |O|X| | | |O|X| | | | | |O|X|X| | |O|X| | | | | |O|X|X| |O|O|X| 'X' Won! | | |X| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | |X| |X| | |O|O| | | | | |X| |X| | |O|O| | |O| | |X| |X| |X|O|O| | |O| | |X| |X| |X|O|O| | |O|O| 'X' Won! |X| |X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O| |O| |X| |O| | |X| | |O| |O| |X|X|O| |O|X| | |O| |O| |X|X|O| |O|X|X| |O| |O| |X|X|O| 'O' Won! |O|X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | |O| | |X|O| |X| | | | | |O| |X|X|O| |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X|X| | 'O' Won! |O|O|O| |X|X|O| |X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | | |O| |X| |X| |O|O|X| | | |O| |X|O|X| |O|O|X| | | |O| |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X|O| | |X|O| |X|O| | | |X|O| 'O' Won! | |X|O| |X|O|O| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X| |O| | | | | |X|O|O| |X|X|O| | | | | |X|O|O| |X|X|O| | |O| | 'X' Won! |X|O|O| |X|X|O| |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| | |O| | 'O' Won! | |O|X| |X|O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| |O| | |O| | | | |X| |X|X|O| | |O| | | | |X| |X|X|O| |O|O| | | | |X| |X|X|O| |O|O| | |X| |X| 'O' Won! |X|X|O| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X|X| |X| | | | | |O| |O|X|X| |X| | | | |O|O| |O|X|X| |X| |X| | |O|O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | |O| |X| | | 'X' Won! |X| |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |O|X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | | |X|X| |O|X| | |O| |O| | |X|X| |O|X| | 'X' Won! |O|X|O| | |X|X| |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | | | |O| |O| | |X|X| | |X| | |O| |O| 'O' Won! | |X|X| | |X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| |X|O|O| |O| |X| |X| |X| |X|O|O| |O|O|X| |X| |X| 'X' Won! |X|O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | |O| |O| |X| |X|X| | | | |O| |O| |X| |X|X|O| | | |O| 'X' Won! |O| |X| |X|X|O| |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | | |O|X| | | |O| |X| | | | |O|X| | |X|O| |X|O| | | |O|X| | |X|O| |X|O| | | |O|X| |X|X|O| |X|O|O| | |O|X| |X|X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O|X| | |X|O| |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | |O| | | |X|X| |X| |O| | |O| | | |X|X| |X|O|O| | |O| | | |X|X| |X|O|O| | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X|X| | | |O| |X| |O| |O|X|X| | | |O| |X|X|O| |O|X|X| | |O|O| |X|X|O| |O|X|X| 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X| |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |X|X| |O|X|O| | |O|X| |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | | |O| | |O|X| | |X| | | |X|O| | |O|X| | |X|O| | |X|O| |X|O|X| | |X|O| | |X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X|O|X| |X| | | |O|X|O| |X|O|X| |X| | | 'O' Won! |O|X|O| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| |O| | |X|X| |X|O| | |O| |O| 'O' Won! |O|X|X| |X|O| | |O| |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | |O| | |X|O| | |X|X| | | |O| | |X|O| | |X|X| | | |O| |O|X|O| 'X' Won! | |X|X| | |X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | |X| |O| |X| | | | |O|O| |X| |O| |X| |X| | |O|O| |X| |O| 'O' Won! |X| |X| |O|O|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | |O| | |O| | |X| |X| | |X|O| | |O| | |X| |X| | |X|O| | |O|O| |X| |X| | |X|O| |X|O|O| |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| |O|X| | | | |O| |X|O|X| |O|X| | | | |O| |X|O|X| |O|X| | |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | |X| |X| | | |O| | |O| | |X| |X| | | |O| | |O|O| 'X' Won! |X|X|X| | | |O| | |O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X|X| | | | |O| |O| | | |X|X| | | | |O| |O| |X| |X|X| | | |O|O| |O| |X| |X|X| | 'X' Won! | |O|O| |O| |X| |X|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O|O| | | | | |X| |X| |X|O|O| | | | | |X|O|X| |X|O|O| | | | | |X|O|X| |X|O|O| | | |X| |X|O|X| |X|O|O| |O| |X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| |X|O| | | | | | |O| |X| |X|O| | |O| | | |O| |X| |X|O| | |O| | | |O|X|X| |X|O|O| |O| | | |O|X|X| 'X' Won! |X|O|O| |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | | | |X|O|X| | | |O| | | |O| |X|O|X| | | |O| | | |O| |X|O|X| |X| |O| | |O|O| |X|O|X| |X| |O| 'X' Won! |X|O|O| |X|O|X| |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| | |X| | | |O| | |O|X|X| | |X| | |O|O| | |O|X|X| | |X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | | |O|O| |X| |X| | | | | |X|O|O| |X| |X| |O| | | |X|O|O| |X| |X| |O|X| | |X|O|O| |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | |X| | |X| | |O|O| | | | |X| | |X|O| |O|O|X| | | |X| | |X|O| 'O' Won! |O|O|X| | |O|X| | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| |O| | | | | |X| |X|O|X| |O| | | | | |X| |X|O|X| |O| |O| | | |X| |X|O|X| |O|X|O| | |O|X| |X|O|X| |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | |O| | 'X' Won! |X| | | |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| | |O|O|X| | | | | |X|X| | |O|O|X| | | | | |X|X| | |O|O|X| | |O| | 'X' Won! |X|X|X| |O|O|X| | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | | |O|O| | |X|X| |O| | | | |O|O| | |X|X| |O|X| | | |O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| |O|X| | It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X| | | |O|O| 'X' Won! | |O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O|X| | |X| |X| |O| |O| |O|X| | |X|O|X| |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O|X| | |O|O| |X| |X| | |O|X| 'X' Won! | |O|O| |X|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O| | |O| |X| |X| | | |X|O| | |O| |X| |X| | | |X|O| | |O|O|X| |X| |X| |X|O| | |O|O|X| |X| |X| |X|O|O| |O|O|X| 'X' Won! |X|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | | | 'O' Won! |O| |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| | |O|X| |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X|O| | |X| | | | |O| |O|X|O| |X|X| | |O| |O| |O|X|O| |X|X| | 'X' Won! |O|X|O| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| |O|X| | | |O| | | |X|X| |O|X| | | |O|O| | |X|X| |O|X| | | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O|O|X| | | |X| | | |O| |O|O|X| |X| |X| | | |O| |O|O|X| |X| |X| | |O|O| 'X' Won! |O|O|X| |X|X|X| | |O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O| | |X|X| |O| | | |X| |O| | |X|X| |O|O| | |X|X|O| | |X|X| |O|O| | 'O' Won! |X|X|O| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|O| |X| | | | |X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| 'X' Won! |X| |X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X|X| |O| | | |O|O| | | |X|X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O| |O| |O|O|X| | |X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | |X|O|X| | | | | | |O| | 'O' Won! |X|O|X| | |O| | | |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | |X| | | | |O| |X|O| | | |X| | |O| |O| |X|O| | | |X| | |O|X|O| |X|O| | | |X|O| |O|X|O| |X|O| | |X|X|O| |O|X|O| |X|O| | 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| | | |X| | 'O' Won! |O| | | |X|O| | | |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | |O|O| | | |X|O| | |X| | |O|O|X| | |X|O| |O|X| | |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X| | | |O|X| |O|O|X| | |X| | | |O|X| 'X' Won! |O|O|X| | |X|X| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| |X| | |O| | |X|O| | |O| |X| | |O| | |X|O| | |O|X|X| | |O| | |X|O|O| |O|X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O|O|X| |X| |X| |O| | | |O|O|X| |X| |X| |O| |O| |O|O|X| 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| |X| |O| | | |X| |O| |O| |X| |O| |X| |X| |O| |O| |X| |O|O|X| |X| |O| |O| |X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | |O| | |X|X| | | |O|X| | |O| | |X|X| | |O|O|X| | |O| | |X|X| | 'X' Won! |O|O|X| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |O| |O| | |X|X| |X|O| | |O| |O| 'O' Won! |O|X|X| |X|O| | |O| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X| | |X|X| | | | |O| |O|X| | |X|X|O| | | |O| |O|X|X| |X|X|O| |O| |O| |O|X|X| |X|X|O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | | | | | |O|X|O| |O|X|X| | | | | |O|X|O| 'O' Won! |O|X|X| | |O| | |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | |X|X| | | |O| |O| | | | |X|X| | | |O| |O| |O| | |X|X| | |X|O| |O| |O| 'O' Won! | |X|X| | |X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| | | |X|X| |X| | | |O|O| | | |X|X| |X|O| | 'X' Won! |O|O| | |X|X|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O|X| | | |O| | |X| | |X|O|X| |O| |O| | |X| | |X|O|X| |O| |O| | |X|X| 'O' Won! |X|O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O|X|O| | |X| | | | |O| |O|X|O| | |X| | | | |O| |O|X|O| | |X|X| |O| |O| |O|X|O| | |X|X| 'X' Won! |O| |O| |O|X|O| |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| | | | | |O| |O|X|O| |X|X| | | | |O| 'O' Won! |O|X|O| |X|X|O| | | |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O|X| | | |O| | |X|X| | |O|X|O| | |O| | |X|X| | 'X' Won! |O|X|O| | |O| | |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | |X| | | | |X| | |O|O| |O|X| | | | |X| | |O|O| |O|X|X| | | |X| | |O|O| |O|X|X| | |O|X| 'X' Won! |X|O|O| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O|O| | | |X| | | | | |X|O|O| | |X|X| | | | | |X|O|O| |O|X|X| | |X| | |X|O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| | |X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O|O| |O| | | |X|X| | |X|O|O| |O| | | |X|X| | |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | |O|X| | |X| | |O|O|X| |X|O|X| | |X| | 'O' Won! |O|O|X| |X|O|X| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | |X|X| | | | | 'O' Won! |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | | |X| |X|O| | | |O|O| | | |X| |X|O| | |X|O|O| | | |X| |X|O| | |X|O|O| |O| |X| |X|O| | |X|O|O| |O|X|X| |X|O| | It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| |X| |X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | | |X| | | |X| |O|X|O| |O| |X| | | |X| |O|X|O| |O|X|X| | | |X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | |O|X| | |O|O| |X| |X| | |O|X| 'O' Won! | |O|O| |X|O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| | | |O| |O| | |X|O| |X| | | |O|X|O| | |X|O| 'O' Won! |X| |O| |O|X|O| | |X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | | |O|X|O| |O|X| | 'X' Won! | |X| | |O|X|O| |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| |O|X| | | |O|X| | |O|X| |O|X| | | |O|X| | |O|X| |O|X|O| | |O|X| |X|O|X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | |X|O|X| | |O| | 'O' Won! | |O| | |X|O|X| | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X|O| | | | | |X|O| | | |X|O| |X| | | |X|O| | | |X|O| |X|O| | |X|O| | 'X' Won! |X|X|O| |X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | |O| |O| | | |X|X| | 'X' Won! | | |O| |O| | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X| |O| |O|O| | |X| | | |X|X|O| |O|O| | |X| |O| |X|X|O| |O|O| | 'X' Won! |X| |O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | | |O| | |X| |X| |O|X| | | |O| | |X| |X| |O|X| | |O|O| | 'X' Won! |X|X|X| |O|X| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O| | |O|X|O| | |X|X| | |O| | |O|X|O| | |X|X| | |O|O| |O|X|O| | |X|X| |X|O|O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | | | |X| | 'O' Won! |O|X| | |O| | | |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | |X|O|X| |O| |O| | |X| | |X|O|X| |O| |O| | |X|X| |X|O|X| |O| |O| |O|X|X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | | | | | |X|X|O| 'O' Won! |O|X| | | |O| | |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | |X|O| |X| | | | | |O| |O|X|O| |X| | | | |X|O| |O|X|O| |X| | | |O|X|O| |O|X|O| |X| | | |O|X|O| |O|X|O| |X| |X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | |X| | | | | | |X|O|O| | |X| | | | |O| |X|O|O| | |X| | | |X|O| |X|O|O| |O|X| | | |X|O| |X|O|O| 'X' Won! |O|X|X| | |X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| |O|X| | |X| | | | |O|O| |O|X|X| 'O' Won! |X| |O| | |O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| | |O|X| |X|O| | | | |X| | |O|X| |X|O| | |O| |X| | |O|X| |X|O| | |O|X|X| |O|O|X| |X|O| | |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| | | | | |X| |X|O|X| |O|O| | | | |X| 'X' Won! |X|O|X| |O|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | | |O|X| | | |X| |O|O| | |X|O|X| | | |X| |O|O| | 'O' Won! |X|O|X| | |O|X| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O|O| 'X' Won! | | |X| | |X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| |O| |X|X| | | |O| | |O|X|O| |X|X| | | |O| | |O|X|O| |X|X|O| | |O| | |O|X|O| |X|X|O| |X|O| | 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O|X| |O| | | | | |X| |O|O|X| |O| | | | | |X| |O|O|X| |O|X| | | | |X| 'O' Won! |O|O|X| |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | |X|X| | | |O| |O| |O| | |X|X| | | |O| |O|X|O| | |X|X| | |O|O| |O|X|O| | |X|X| 'X' Won! |X|O|O| |O|X|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X|O| | |X| | | |O| |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O|X| 'O' Won! |X|O| | |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X|O| | |X| |O| | |O| | |X|O| | |X|X|O| | |O| | |X|O|O| |X|X|O| | |O| | 'X' Won! |X|O|O| |X|X|O| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| |O| | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O|O| | | | |X| | |X|O| |O|O|X| |O| |X| | |X|O| |O|O|X| |O|X|X| | |X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X| | | |X|X| | |O| |O| |X|O| | |X|X| | 'X' Won! |O| |O| |X|O| | |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| | | | | |X|O| | |O| |X| | | | | |X|O| | |O|O|X| | | | | |X|O| | |O|O|X| | |X| | |X|O| | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | | |O| | |X|X|O| |O|X| | | |O| | |X|X|O| |O|X|O| | |O| | 'X' Won! |X|X|O| |O|X|O| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| | | | | |O|X| | |X|X|O| | | | | |O|X| | |X|X|O| | |O| | |O|X|X| |X|X|O| | |O| | |O|X|X| |X|X|O| |O|O| | It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| | | |O| |X|O| | | |O|X| |X| |O| |X|O| | |O|O|X| |X| |O| |X|O| | |O|O|X| |X| |O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X| |O| | |X| | |O|O| | |X| |O| |X|X| | |O|O| | 'O' Won! |X| |O| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | |O| | | |X|X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | |X| |X| |O| | | |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| |X| 'O' Won! |X| |X| |O|O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O| |X| | | |O| | |X|O| |O| |X| | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O|O|X| | |X|O| |X|X|O| |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | |O| | |O|X|X| |X| | | | |O| | |O|X|X| |X| | | | |O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X|O| | |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |X| |O| | |X| | |X|O|O| |X| |O| | |X|X| |X|O|O| |X| |O| 'O' Won! |O|X|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | | | |X| |O|O|X| |X| | | |O| |X| 'X' Won! |O|O|X| |X| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | |O|X| |X| | | |O| |O| |X|O|X| |X| | | |O| |O| |X|O|X| |X| |O| |O| |O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | |O| | |X| |X| |O| |O| 'X' Won! | |O| | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | | | |X| |O| | |X|O| | |X| | |X| |O| | |X|O| |O|X| | |X| |O| | |X|O| |O|X|X| |X| |O| | |X|O| 'O' Won! |O|X|X| |X|O|O| | |X|O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| |O| | | |O| |O| |X| |X| |O| |X| |O| |O| |X| 'O' Won! |X| |O| |X|O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | |O| | | | |X| |X| |O| | |O|X| | | |X| |X| |O| | |O|X| | |O|X| |X| |O| |X|O|X| | |O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| |O| |X| |O|X| | | |X|O| |O| |X| |O|X| | 'O' Won! | |X|O| |O|O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O|O| | |X| |X| | |O| | |O|O| | |X| |X| |X|O| | |O|O| | |X| |X| |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | | |X| |O|O| | 'X' Won! |O| |X| | | |X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | |O| | | |X|X| | |O|O| | |O| |X| |X|X| | |O|O| | 'O' Won! |O| |X| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | |O|X| |X| | | |O|X| | | |O|X| |X| | | |O|X| | | |O|X| |X|O| | |O|X| | |X|O|X| |X|O| | 'O' Won! |O|X| | |X|O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | | | | |X| | | |O|X|O| | | | | |X|O| | |O|X|O| | | | | |X|O|X| |O|X|O| | |O| | |X|O|X| |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| |X| | | | | |O|O| | |X| |X| | | | | |O|O|X| |X| |X| |O| | | |O|O|X| |X| |X| 'X' Won! |O| | | |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| |X| | | | |O| | |O| |X| |X| | | |O|O| | |O| |X| |X| |X| |O|O| | |O| |X| |X|O|X| |O|O| | |O| |X| 'X' Won! |X|O|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X| |X| | |O| | |O|X|O| 'X' Won! |X|X|X| | |O| | |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O|X| |X| |O| |O| | | | |O|X| 'X' Won! |X| |O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| | | |X|O| |O|X| | | |O| | | |X|O| |O|X| | | |O|X| | |X|O| |O|X| | |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | |X| |X|O| | | |O| | 'O' Won! | |O|X| |X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X|O| | | | |O| | | |X| |X|O| | | | |O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O|X| | |O|O| | |X|X| |X|O|X| | |O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| | |O|O| 'X' Won! |X| | | |X| |O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| | |X|O| |X| |X| | | |O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X|O|X| 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | |X|O| |O| | | | | |X| |O|X|O| |O| | | | | |X| |O|X|O| |O|X| | | | |X| |O|X|O| |O|X|O| | | |X| |O|X|O| |O|X|O| |X| |X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| | |X| |O| | |X|X| |O|O| | |X| |O| | |X|X| 'X' Won! |O|O| | |X| |O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| |O| |O| | | | | |X|X|O| |O| |O| | |X| | |X|X|O| |O| |O| |O|X| | 'X' Won! |X|X|O| |O|X|O| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | |O| | |X|O| | | | |X| | |O|X| |X|O| | | | |X| |O|O|X| |X|O| | | | |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O|O| |X| |X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | | | |O|X| | | |O|X| | |X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X|X| |O|X| | 'O' Won! |O|O|X| |O|X|X| |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| | | |O| |X| |X| | | |O| | |X|O| |X| |X| | |O|O| | |X|O| 'X' Won! |X|X|X| | |O|O| | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X|O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | |X|O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| | |X|O| 'X' Won! |X|O|X| |O|X|O| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X| |O| |X| | | |O|O| | |X| |O| |X| |X| |O|O| | |X| |O| |X|O|X| |O|O|X| |X| |O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| |O| |X| |X| | | |O| |O| |O| |X| |X| |X| |O| |O| |O|O|X| |X| |X| |O|X|O| |O|O|X| |X| |X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O|X| | | |X|O| |O| |X| |O|X| | | |X|O| |O| |X| |O|X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X| | |X|X| |O| |O| | |O|X| | |X|X| 'X' Won! |O| |O| | |O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |O| | |O| | |X|X|O| | |X|O| | |O| | 'O' Won! |X|X|O| | |X|O| | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | | |O|X| | | |O| |X| |X| | |O|X| | | |O| |X|O|X| | |O|X| | |X|O| |X|O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X|X| | |O| | | |O|O| | |X|X| | |O| | | 'X' Won! |O|O| | |X|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O| |X|X| | | |X| | | |O|O| |X|X| | | |X|O| |X|O|O| |X|X| | | |X|O| 'O' Won! |X|O|O| |X|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O|O| | |X| | | |X|O| | |O|O| | |X|X| | |X|O| | 'O' Won! |O|O|O| |X|X| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| |X| | | |X| |O| |O| 'O' Won! |X| |X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | 'X' Won! |O|X|O| | |X|O| |X|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X|O| |X|O| | | | |O| |X|X|O| |X|O| | |O| |O| |X|X|O| |X|O| | |O|X|O| |X|X|O| 'O' Won! |X|O|O| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| |X|X|O| | |X|X| 'O' Won! |O|O|O| |X|X|O| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | 'X' Won! | |X| | |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X|X| | |O|O| | | | |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X|O|O| |X|X| | |O|O| | 'X' Won! |X|O|O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| |X| | |O| | |X|X|O| |O| |X| | |O|X| |X|X|O| |O| |X| | |O|X| |X|X|O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O|X| |X| | | |O| | | | |O|X| |X| | | |O|O| | |X|O|X| |X| | | |O|O| | 'O' Won! |X|O|X| |X|O| | |O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O|O| | | | | |O|X|X| | |O|O| | | |X| |O|X|X| 'O' Won! | |O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |X| |O|O|X| | | | | | | |X| 'X' Won! |O|O|X| | | |X| | | |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| |X| | | |O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| |X| |X| |O| |X| | |O|O| |X|O|X| |O|X|X| | |O|O| |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X|O| | |X|X| |O| | | | |X|O| | |X|X| |O| | | |O|X|O| | |X|X| |O| |X| |O|X|O| | |X|X| |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | |O|O| |X| | | |X| | | 'X' Won! |X|O|O| |X| | | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| | | | |O|X| |O|O| | |X|X| | | |O|X| |O|O| | |X|X|O| | |O|X| |O|O| | |X|X|O| |X|O|X| 'O' Won! |O|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| |X| | | | |X| | |O|X|O| |X| | | |O|X| | |O|X|O| |X| |X| |O|X| | 'O' Won! |O|X|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | |X| |O| | |X|X| | |O| | |X| |O| |O|X|X| | |O| | |X| |O| |O|X|X| |X|O| | |X|O|O| |O|X|X| |X|O| | 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | | | | | 'X' Won! |O|X|O| | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| |X|X| | | |O|O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| | |O|O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |X| |O|O| | |X| |X| |O| |X| 'X' Won! |O|O| | |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | |O| | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| |X|O| | |X|O|X| |O| |O| |X|O| | |X|O|X| |O|X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| | | | | |O| |X|O|X| |O| | | | |X|O| |X|O|X| |O| | | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O|O|X| | | |O| | |X| | |O|O|X| | | |O| |X|X| | |O|O|X| | | |O| |X|X|O| |O|O|X| |X| |O| |X|X|O| 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| |X| |O| | | |X|X| | |O| |X| |O| |O| |X|X| | |O|X|X| |O| |O| 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | | |O|O| |X| | | |O|X| | |X|O|O| |X| | | 'O' Won! |O|X| | |X|O|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | |X|O| |X| | | | |O|O| | |X|O| |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X|O| | 'X' Won! |X|O|O| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | 'O' Won! |X|O| | |X|O| | | |O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| |X| | | |X|O|X| | | |O| |X| | | |X|O|X| | |O|O| 'X' Won! |X| | | |X|O|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | | |X|O| | |O| | |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X|O| | |O|X|O| | |O|X| |X|O|X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|X| | |O| | |X| | | |O|X|X| | |O|O| |X| | | |O|X|X| |X|O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| |X|O| | | | |O| | |X|X| |X|O| | | | |O| |O|X|X| 'X' Won! |X|O| | | |X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | |O|X|X| | |X| | | |O| | |O|X|X| 'O' Won! | |X|O| | |O| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| |O| | | |X|O| | 'X' Won! | | |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | |O| |X| | |O|X| | | | | |O|X|X| | |O|X| | | | | |O|X|X| |O|O|X| 'X' Won! |X| | | |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X| | |O|X|O| | | |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O| | | |X| |X| | |X|O| |O| |O| |X| |X| 'X' Won! | |X|O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | |O|X| | | | | |O|X| | | |O|X| | | | | |O|X|X| | |O|X| | |O| | |O|X|X| | |O|X| |X|O| | |O|X|X| |O|O|X| |X|O| | |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| |O| | | | | | |X|X| |O| |O| | | |X| 'O' Won! | |X|X| |O|O|O| | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| |X| | |O|X| |X|O| | |O| |X| | |O|X| |X|O| | |O|X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O| | |X| |O| |X| |X| | |O| | |X|O|O| 'X' Won! |X|X|X| | |O| | |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| |O| 'X' Won! |O| |X| |O|X| | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | |O| |X| |O|X| | | | | | |O|X|X| |O|X| | | | | | |O|X|X| |O|X| | | |O| | |O|X|X| |O|X|X| | |O| | |O|X|X| |O|X|X| | |O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | | | | |O|X|O| | |X|X| |O| | | |O|X|O| | |X|X| |O| |X| |O|X|O| | |X|X| |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | 'O' Won! |O| | | |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| |O| |X| |X| | | | | |O|X|O| |X| |X| |O| | | |O|X|O| |X| |X| 'X' Won! |O| |X| |O|X|O| |X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | |O| |O| |O|O|X| 'X' Won! |X|X|X| |O| |O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O| |O| |X| |X| |O| |X| |O| |O| |X| |X| |O|O|X| |O| |O| |X| |X| 'X' Won! |O|O|X| |O|X|O| |X| |X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| 'O' Won! |O|X| | | |O|X| | | |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O|O|X| | | | | |X| |X| |O|O|X| | | | | |X| |X| |O|O|X| | | |O| |X| |X| |O|O|X| |X| |O| |X| |X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| | | |X| |O| | |O|O| 'X' Won! |X| | | |X| |O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X|X| |O| |O| | |X| | | |X|X| |O| |O| |O|X| | 'X' Won! |X|X|X| |O| |O| |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| |X| 'O' Won! | | | | |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| |O| |X| | | | | | |X|O| |O| |X| |X| | | | |X|O| |O| |X| |X|O| | |X|X|O| |O| |X| |X|O| | |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | |O|X| |X| | | | | |O| | |O|X| |X|X| | | | |O| | |O|X| |X|X| | | |O|O| 'X' Won! | |O|X| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| |O| |X| |O|X| | | | |O| |O|X|X| |O|X| | | | |O| |O|X|X| |O|X|O| 'X' Won! | |X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | 'X' Won! | |X| | |O|X| | |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| |X| | | | |O|O| | |X|O| |X| | | | |O|O| | |X|O| |X| |X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'O' Won! |O|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | |O| |X| | | |O|X| | |X| |O| |X| | | |O|X| | |X| |O| |X| |O| |O|X| | |X|X|O| |X| |O| |O|X| | 'O' Won! |X|X|O| |X| |O| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | |X|X| | | | |O| |O|X| | |X|X|O| | | |O| |O|X| | |X|X|O| | | |O| |O|X|X| |X|X|O| |O| |O| |O|X|X| 'X' Won! |X|X|O| |O|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | |X| | | | | | |X|O|O| | |X| | | |O| | |X|O|O| | |X| | |X|O| | |X|O|O| |O|X| | |X|O| | 'X' Won! |X|O|O| |O|X| | |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | |O| |X| | |O|X| | |O| | |O| |X| |X|O|X| |O|O| | |O| |X| |X|O|X| 'X' Won! |O|O|X| |O| |X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | | | | |O|X|O| | |O|X| | | | | |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | |X| | | | |X| | |O| | | |X| | | | |X| | |O|O| | |X| |X| | |X| | |O|O| | |X|O|X| | |X| | |O|O| | |X|O|X| | |X|X| |O|O| | 'O' Won! |X|O|X| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | | |O|X| | | |X|O| |X| | | |O|X| | | |X|O| |X| | | |O|X|O| 'X' Won! | |X|O| |X|X| | |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | | |X| | |O|X|X| |O| | | | |X| | |O|X|X| |O| |O| | |X|X| |O|X|X| |O| |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| |O|X| | | | |O| 'X' Won! | | |X| |O|X| | |X| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | |X| | |O| |X| |O|O| | | |X| | |O| |X| |O|O| | |X|X| | |O|O|X| |O|O| | |X|X| | 'X' Won! |O|O|X| |O|O| | |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X| | | |O| | |X| |X| |O|X| | | |O|O| |X| |X| |O|X| | |X|O|O| |X| |X| |O|X| | |X|O|O| |X|O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X|X| | |O| | | | |O|O| |X|X| | |O| | | |X|O|O| |X|X| | 'O' Won! |O| | | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| | | |O|O| | | |O|X| |X| | | |O|O|X| 'O' Won! | |O|X| |X|O| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | |X|X| | |O| |O| | | | | |X|X| | 'X' Won! |O| |O| | | | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| | | |X| |O| |O|X| | |O| | | |X| |O| |O|X| | |O|X| | |X| |O| |O|X|O| |O|X| | |X| |O| 'X' Won! |O|X|O| |O|X| | |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| |O|O| | | |X| | | |X|X| |O|O| | | |X| | | |X|X| |O|O| | | |X|O| | |X|X| |O|O| | |X|X|O| 'O' Won! |O|X|X| |O|O| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O| | |O| |O| |X|X| | | |O| | |O|X|O| |X|X| | |O|O| | |O|X|O| 'X' Won! |X|X|X| |O|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| |O| |X| |O|X| | | | |O| |O|X|X| |O|X| | | |O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| |O|X| | It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| | | | |O|X| |X| |X| |O|O| | | |O|X| |X| |X| |O|O| | |X|O|X| |X| |X| 'O' Won! |O|O| | |X|O|X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O| | | |X|O| | |O|X| | |O| |X| |X|O| | |O|X|O| |O| |X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X|O| | |X| | |O| | | |O|X|O| |X|X| | |O| |O| |O|X|O| |X|X| | 'X' Won! |O| |O| |O|X|O| |X|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | |X| | |O|O| |X|X| | |O| |X| | |O|O| |X|X| | |O| |X| |X|O|O| |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | |X| | |O| |O| |X| |X| | |X|O| |O| |O| |X| |X| |X|X|O| |O| |O| |X| |X| 'O' Won! |X|X|O| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | |O|X| | | | | |X| |O| |X|O|X| | | | | |X| |O| |X|O|X| | |O| | |X| |O| 'X' Won! |X|O|X| |X|O| | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| |X| | | |X| |X|O| | |O|O|X| | |X|X| |X|O| | |O|O|X| | |X|X| |X|O|O| |O|O|X| 'X' Won! |X|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| |O| | | |O|O|X| 'X' Won! | | |X| |O| |X| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| | | | | | 'O' Won! |O|X| | |X|O| | | | |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | |O| | |O| | | 'X' Won! |X|X|X| | |O| | |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | | | |O| |X|O|O| |X| | | | |X|O| |X|O|O| |X|O| | | |X|O| |X|O|O| |X|O|X| | |X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | 'O' Won! | |X| | |O|O|O| | |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| |X|O| | 'O' Won! |O|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| | | | | | |X|O| |O| |X| | | | | | |X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | | | 'X' Won! |O|X|O| |O|X|X| | |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | |O|X| | |O| |X| | | | | |O|X| | |O| |X| |X| | | |O|X| | |O| |X| |X| |O| |O|X| | |O|X|X| |X| |O| |O|X| | |O|X|X| |X|O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | | |O|O| | 'X' Won! |X|X|X| | | | | |O|O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | |O| |X| |X| | | |O| | |X|O| |X| |X| | |O|O| | |X|O| |X| |X| | |O|O| |X|X|O| |X| |X| 'O' Won! |O|O|O| |X|X|O| |X| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | |X| |O|O| | |X|X| | | |O|X| |O|O| | |X|X| | |X|O|X| |O|O| | 'O' Won! |X|X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | |O| | |X| |X| | | |O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O| | |X|O|X| |X|X|O| 'O' Won! |O|O| | |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X| |O| | |X| | | |O| | |X| |O| |O|X| | | |O| | 'X' Won! |X| |O| |O|X| | | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |O| | |O| | | |X|X| | | |O| |O|O| | | |X|X| | | |O| |O|O| | | |X|X| | |X|O| |O|O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | | |O|O| |O|X|X| |X| | | | |O|O| 'O' Won! |O|X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | | | |O| |X|O|O| |X|X| | |O| |O| |X|O|O| |X|X| | |O|X|O| |X|O|O| |X|X| | 'O' Won! |O|X|O| |X|O|O| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X|O| |O| |X| | | |X| | |X|O| |O| |X| |O| |X| | |X|O| |O| |X| |O| |X| |X|X|O| 'O' Won! |O| |X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | | | | |X|X|O| | |X|O| |O| | | |X|X|O| |X|X|O| |O| | | |X|X|O| |X|X|O| |O|O| | |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| | | | |X|O| | |X| | |O| | | | |X|O| |O|X| | |O| | | | |X|O| |O|X| | |O| |X| 'O' Won! |O|X|O| |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | | | | |O| |X| 'X' Won! | |O|X| | | |X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X|O| | | |O| | |X|X| | |X|O| | | |O| | |X|X| | |X|O|O| 'X' Won! | |O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | |X| | |O| |O| |X|O|X| | |X| | |O|X|O| |X|O|X| | |X| | 'O' Won! |O|X|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | | |X|X| 'O' Won! |O| | | |O| | | |O|X|X|","title":"1.0.2 Load Dataset"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#11-clean-data","text":"We will first need to organize the data into a parsable format.","title":"1.1 Clean Data"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q1","text":"What is the object data and what does it contain? what are the keys of data? what are the keys of each game? # inspect data below by grabbing the first key in data # what are the three different keys within each game? data [ 'game 0' ] {'board': {1: 'X', 2: ' ', 3: 'O', 4: 'X', 5: 'X', 6: 'O', 7: 'O', 8: 'X', 9: 'O'}, 'starting player': 'X', 'winner': 'O'}","title":"Q1"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q2","text":"Using those keys, iterate through every game in data and append the board, the winner, and the starting player to separate lists. Call these lists: boards, winners, and starters boards = [] winners = [] starters = [] for game in data : # YOUR CODE HERE","title":"Q2"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q3","text":"Make a dataframe out of the list boards and call it df . Make a series out of the list winners . Make a series out of the list starters . Make a new column of df called \"Winner\" and set it equal to the pandas Series of the winners. Make a new column of df called \"Starter\" and set it equal to the pandas Series of the starters. # YOUR CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 Winner Starter 0 X O O O X O O 1 O O X O X X O X O O O 2 O X X X X O O X X 3 X X O O O X X O O Stalemate O 4 O O X O O X X X X O","title":"Q3"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#12-inferential-analysis","text":"We're going to use Bayes Rule or Bayesian Inference to make a probability of winning based on positions of the board. The formula is: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\). The example we will use is the following: what is the probability of 'O' being the winner, given that they've played the center piece. \\(B\\) = 'O' played the center piece \\(A\\) = 'O' won the game So what is probability? We will define it in terms of frequencies. So if we are for instance asking what is the probability of player 'O' being in the center piece, it would be defined as: P(B) = \\frac{|O_c|} {|O_c| + |X_c| + |empty|} Where the pipes, | | , or cardinality represent the count of the indicated observation or set. In this case \\(O_c\\) (O being in the center) and \\(X_c\\) (X being in the center). Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc_Xc_empty 1000 # example of assessing the probability of B, O playing the center piece player = 'O' Oc = ( df [ 5 ] == player ) . value_counts () Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc / Oc_Xc_empty False 0.577 True 0.423 Name: 5, dtype: float64 # we can also clean this up and replace the denominator with the whole # observation space (which is just the total number of games, df.shape[0]). # example of assesing probabiliy of A ( df [ 'Winner' ] == 'O' ) . value_counts () / df . shape [ 0 ] False 0.571 True 0.429 Name: Winner, dtype: float64 The \\(P(B|A) * P(A)\\) is the intersection of \\(B\\) and \\(A\\). The intersection is defined as the two events occuring together. Continuing with the example, the probablity of 'O' playing the center piece AND 'O' being the winner is the number of times these observations occured together divided by the whole observation space : # in this view, the total times A and B occured together is 247 player = 'O' df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] 247 # the total observation space is 1000 (1000 games) df . shape [ 0 ] 1000 And so we get: P(B|A) * P(A) = \\frac{247} {1000} = 0.247 In code: df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] / df . shape [ 0 ] 0.247","title":"1.2 Inferential Analysis"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#121-behavioral-analysis-of-the-winner","text":"","title":"1.2.1 Behavioral Analysis of the Winner"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q4","text":"define the 3 different board piece types and label them middle , side , and corner . Middle should be an int and the other two should be lists. # define the 3 different board piece types # middle = # side = # corner =","title":"Q4"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#1211-what-is-the-probability-of-winning-after-playing-the-middle-piece","text":"","title":"1.2.1.1 What is the probability of winning after playing the middle piece?"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q5-for-player-x","text":"# A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'X' # define the intersection of A AND B, A_B # A_B = # define prob B # B = # return A_B over B (The prob B given A) A_B / B 0.5732758620689655","title":"Q5: For player X"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q6-for-player-o","text":"# A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'O' # define the intersection of A AND B, A_B # A_B = # define prob B # B = # return A_B over B (The prob B given A) A_B / B 0.5839243498817968","title":"Q6 For player O"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#1212-what-is-the-probability-of-winning-after-playing-a-side-piece","text":"","title":"1.2.1.2 What is the probability of winning after playing a side piece?"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q7-for-player-o","text":"# A intersect B: O played side and O won / tot games # B: O played side / tot games player = 'O' A_B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.4158609451385117","title":"Q7 For player O"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q8-for-player-x","text":"# A intersect B: X played side and X won / tot games # B: X played side / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.38845460012026456","title":"Q8 For player X"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#1213-what-is-the-probability-of-winning-after-playing-a-corner-piece","text":"","title":"1.2.1.3 What is the probability of winning after playing a corner piece?"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q9-for-player-o","text":"# A intersect B: O played corner and O won / tot games # B: O played corner / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.4779116465863454","title":"Q9 For player O"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q10-for-player-x","text":"# A intersect B: X played corner and X won / tot games # B: X played corner / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.47386964180857316 Are these results surprising to you? Why? This resource may be illustrative.","title":"Q10 For player X"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#13-improving-the-analysis","text":"In this analysis, we only tracked what moves were made, not the order they were made in. It really limited our assessment! How might we change our recording of the games to track order of moves as well? Do we need to track all the moves or just the first and the winner?","title":"1.3 Improving the Analysis"},{"location":"project/P2_Heuristical_TicTacToe_Agents/","text":"Data Science Foundations Project Part 2: Heuristical Agents \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com We makin' some wack AI today 2.0 Preparing Environment and Importing Data \u00b6 back to top 2.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 2.0.2 Load Dataset \u00b6 back to top 2.1 AI Heuristics \u00b6 Develop a better AI based on your analyses of game play so far. Q1 \u00b6 In our groups, let's discuss what rules we would like to hard code in. Harsha, Varsha and I will help you with the flow control to program these rules # we will define some variables to help us define the types of positions middle = 5 side = [ 2 , 4 , 6 , 8 ] corner = [ 1 , 3 , 7 , 9 ] # recall that our board is a dictionary tictactoe = TicTacToe () tictactoe . board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} # and we have a win_patterns object to help us with the algorithm tictactoe . win_patterns [[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 4, 7], [2, 5, 8], [3, 6, 9], [1, 5, 9], [7, 5, 3]] for example, if we want to check if the middle piece is available, and play it if it is. How do we do that? # set some key variables player = 'X' opponent = 'O' avail_moves = [ i for i in tictactoe . board . keys () if tictactoe . board [ i ] == ' ' ] # a variable that will keep track if we've found a move we like or not move_found = False # <- some other moves we might want to make would go here -> # # and now for our middle piece play if move_found == False : # if no other move has been found yet if middle in avail_moves : # if middle is available move_found = True # then change our move_found status move = middle # update our move Note: in the following when I say return a move I mean when we wrap this up in a function we will want the return to be for a move. For now I just mean that the result of your code in Q3 is to take the variable name move and set it equal to the tic-tac-toe board piece the AI will play Our standard approach will be to always return a move by the agent . Whether the agent is heruistical or from some other ML framework we always want to return a move Q2 \u00b6 Write down your algorithm steps in markdown. i.e. play a corner piece play to opposite corner from the opponent, etc. ....etc. Q3 \u00b6 Begin to codify your algorithm from Q3. Make sure that no matter what, you return a move # some starting variables for you self = TicTacToe () # this is useful cheat for when we actually put this in as a method player_label = 'X' opponent = 'O' avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] # temp board will allow us to play hypothetical moves and see where they get us # in case you need it temp_board = self . board . copy () 2.2 Wrapping our Agent \u00b6 Now that we've created a conditional tree for our AI to make a decision, we need to integrate this within the gaming framework we've made so far. How should we do this? Let's define this thought pattern or tree as an agent. Recall our play_game function within GameEngine def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break ######################################################################## ##################### WE WANT TO CHANGE THESE LINES #################### ######################################################################## else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 2.2.1 Redefining the Random Agent \u00b6 In particular, we want to change lines 30-37 to take our gaming agent in as a parameter to make decisions. Let's try this. In setup_game we want to have the option to set the AI type/level. In play_game we want to make a call to that AI to make the move. For instance, our random AI will go from: while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break to: def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ########## our fresh off the assembly line tictactoe playing robot ########### ############################################################################## def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : pass self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Let's test that our random ai works now in this format random . seed ( 12 ) game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| <__main__.GameEngine at 0x7fadbea428d0> Let's try it with a user player: random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2)2 | | | | | | | | | | | | X, what's your move?q quiting the game <__main__.GameEngine at 0x7fadbea25e90> Q4 \u00b6 Now let's fold in our specialized AI agent. Add your code under the heurstic_ai function. Note that the player_label is passed as an input parameter now class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ################### YOUR BADASS HEURISTIC AGENT GOES HERE #################### ############################################################################## def heuristic_ai ( self , player_label ): # SOME HELPER VARIABLES IF YOU NEED THEM opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () ################## YOUR CODE GOES HERE, RETURN THAT MOVE! ################## while True : # DELETE LINES 20 - 25, USED FOR TESTING PURPOSES ONLY move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break ############################################################################ return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Q5 \u00b6 And we'll test that it works! random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2)1 select AI level (1, 2)2 who will go first? (X, (AI), or O (Player))O | | | | | | | | | | | | O, what's your move?5 | | | | | |O| | | | | | | | | | | |O| | | |X| | O, what's your move?9 | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| O, what's your move?1 'O' Won! |O| | | | |O|X| | |X|O| <__main__.GameEngine at 0x7fadbe93f610> Q6 \u00b6 Test the autorun feature! game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| 'O' Won! |O|X| | |O| | | |O| |X| <__main__.GameEngine at 0x7fadbe8cc050>","title":"Heuristical TicTacToe Agents"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#data-science-foundations-project-part-2-heuristical-agents","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com We makin' some wack AI today","title":"Data Science Foundations  Project Part 2: Heuristical Agents"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#20-preparing-environment-and-importing-data","text":"back to top","title":"2.0 Preparing Environment and Importing Data"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#201-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"2.0.1 Import Packages"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#202-load-dataset","text":"back to top","title":"2.0.2 Load Dataset"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#21-ai-heuristics","text":"Develop a better AI based on your analyses of game play so far.","title":"2.1 AI Heuristics"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q1","text":"In our groups, let's discuss what rules we would like to hard code in. Harsha, Varsha and I will help you with the flow control to program these rules # we will define some variables to help us define the types of positions middle = 5 side = [ 2 , 4 , 6 , 8 ] corner = [ 1 , 3 , 7 , 9 ] # recall that our board is a dictionary tictactoe = TicTacToe () tictactoe . board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} # and we have a win_patterns object to help us with the algorithm tictactoe . win_patterns [[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 4, 7], [2, 5, 8], [3, 6, 9], [1, 5, 9], [7, 5, 3]] for example, if we want to check if the middle piece is available, and play it if it is. How do we do that? # set some key variables player = 'X' opponent = 'O' avail_moves = [ i for i in tictactoe . board . keys () if tictactoe . board [ i ] == ' ' ] # a variable that will keep track if we've found a move we like or not move_found = False # <- some other moves we might want to make would go here -> # # and now for our middle piece play if move_found == False : # if no other move has been found yet if middle in avail_moves : # if middle is available move_found = True # then change our move_found status move = middle # update our move Note: in the following when I say return a move I mean when we wrap this up in a function we will want the return to be for a move. For now I just mean that the result of your code in Q3 is to take the variable name move and set it equal to the tic-tac-toe board piece the AI will play Our standard approach will be to always return a move by the agent . Whether the agent is heruistical or from some other ML framework we always want to return a move","title":"Q1"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q2","text":"Write down your algorithm steps in markdown. i.e. play a corner piece play to opposite corner from the opponent, etc. ....etc.","title":"Q2"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q3","text":"Begin to codify your algorithm from Q3. Make sure that no matter what, you return a move # some starting variables for you self = TicTacToe () # this is useful cheat for when we actually put this in as a method player_label = 'X' opponent = 'O' avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] # temp board will allow us to play hypothetical moves and see where they get us # in case you need it temp_board = self . board . copy ()","title":"Q3"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#22-wrapping-our-agent","text":"Now that we've created a conditional tree for our AI to make a decision, we need to integrate this within the gaming framework we've made so far. How should we do this? Let's define this thought pattern or tree as an agent. Recall our play_game function within GameEngine def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break ######################################################################## ##################### WE WANT TO CHANGE THESE LINES #################### ######################################################################## else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"2.2 Wrapping our Agent"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#221-redefining-the-random-agent","text":"In particular, we want to change lines 30-37 to take our gaming agent in as a parameter to make decisions. Let's try this. In setup_game we want to have the option to set the AI type/level. In play_game we want to make a call to that AI to make the move. For instance, our random AI will go from: while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break to: def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ########## our fresh off the assembly line tictactoe playing robot ########### ############################################################################## def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : pass self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Let's test that our random ai works now in this format random . seed ( 12 ) game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| <__main__.GameEngine at 0x7fadbea428d0> Let's try it with a user player: random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2)2 | | | | | | | | | | | | X, what's your move?q quiting the game <__main__.GameEngine at 0x7fadbea25e90>","title":"2.2.1 Redefining the Random Agent"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q4","text":"Now let's fold in our specialized AI agent. Add your code under the heurstic_ai function. Note that the player_label is passed as an input parameter now class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ################### YOUR BADASS HEURISTIC AGENT GOES HERE #################### ############################################################################## def heuristic_ai ( self , player_label ): # SOME HELPER VARIABLES IF YOU NEED THEM opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () ################## YOUR CODE GOES HERE, RETURN THAT MOVE! ################## while True : # DELETE LINES 20 - 25, USED FOR TESTING PURPOSES ONLY move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break ############################################################################ return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"Q4"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q5","text":"And we'll test that it works! random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2)1 select AI level (1, 2)2 who will go first? (X, (AI), or O (Player))O | | | | | | | | | | | | O, what's your move?5 | | | | | |O| | | | | | | | | | | |O| | | |X| | O, what's your move?9 | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| O, what's your move?1 'O' Won! |O| | | | |O|X| | |X|O| <__main__.GameEngine at 0x7fadbe93f610>","title":"Q5"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q6","text":"Test the autorun feature! game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| 'O' Won! |O|X| | |O| | | |O| |X| <__main__.GameEngine at 0x7fadbe8cc050>","title":"Q6"},{"location":"project/P3_1_Step_Look_Ahead_Agents/","text":"Data Science Foundations Project Part 3: 1-Step Look Ahead \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we're working on a more advanced AI structure: 1-step lookahead. 3.0 Preparing Environment and Importing Data \u00b6 back to top 3.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## self . ai_level = 2 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 3.0.2 Load Dataset \u00b6 back to top 3.1 Rethinking gameplay \u00b6 To implement the broader strategies used in game theory and machine learning, we need to rebroadcast our approach to creating our AI agent. In the heurstical agent model, we thought in terms of checking for specific move types, defined by what kind of advantage they give us during game play, i.e. see if a winning move is available, a blocking move, if a corner place is free, etc. Rather than thinking with this look and check mindset that is centered around specific strategies and our own prior knowledge about the game (we know that a center piece is statistically likely to give us a higher chance of winning) we will evaluate every available move to the AI, and rate them quantitatively. switching from ordinal to interval Notice the datatype change when we move from giving simple preferences of moves to actual scores of moves. Catalog this in your mind for future reference when considering datatypes! 3.1.1 One-Step Look Ahead \u00b6 For now, when we rate our boards, we will only look 1-step ahead in gameplay. Hence the name we give this AI strategy, 1-step lookahead The beginning portion of our code will look about the same as the heuristic AI model. Recall: def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() but now, instead of searching progressively through our preferred move-types (winning, middle, etc.) . We are going to give every available move (1, 3, 7, etc.) a score. Our score regimen will look like the following: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move Q1 Rewrite avail_moves \u00b6 define avail_moves as a dictionary of available moves with scores for each move as empty strings. We will update this dictionary with numerical scores in the next step # we're going to steal the parameter names to # prototype our new function self = TicTacToe () player_label = 'X' opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] # instead of a list, we want avail_moves to now be a dictionary that will # contain the move and its score # avail_moves = # YOUR CODE HERE temp_board = self . board . copy () Q2 Score each move in avail_moves \u00b6 Now let's fold this into our new one_step_ai function. Remember: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## # avail_moves = # temp_board = self . board . copy () # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score temp_board [ move ] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## ############################################################################## ################### All remaining moves receive a score of 1 ############################################################################## return avail_moves one_step_ai ( self , 'X' ) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} Q3 Test one_step_ai \u00b6 That's great, but how do we check that our code will work when a winning move is available, or a losing move is just around the corner? let's create a unit test for these! # just defining a new game self = TicTacToe () player_label = 'X' # seeding the board with some X's self . board [ 1 ] = 'X' self . board [ 2 ] = 'X' self . board {1: 'X', 2: 'X', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} Now test the winning move. Your code should return 100 at move 3 and 1 everywhere else one_step_ai ( self , player_label ) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} We can test the losing move by reversing the players player_label = 'O' one_step_ai ( self , player_label ) {3: 10, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} great! Let's keep these shorthand codes in mind when we go to write actual unit tests with the one_step_ai function embedded in the GameEngine module. We're not done yet, recall that our other ai agents returned the actual selected move, not a dictionary of the moves with scores. We need to create a move from this dictionary and return it. Here's what the general procedure will look like: Grab the maximum score (after assigning scores to all of avail_moves) Select all moves that have this maximum score Return a random selection of the moves with the max score and then in code format: avail_moves = one_step_ai ( self , player_label ) # 1. grab the maximum score max_score = max ( avail_moves . values ()) # 2. select all moves that have this maximum score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # 3. return a random selection of the moves with the max score move = random . choice ( valid ) move 3 3.2 Putting it all together \u00b6 Q4 Finish one_step_ai to return a move \u00b6 Let's see if we can rewrite our game engine to take new AI models in as a passable parameter. This way our base module will be much cleaner, and allow us to continue to write new functions for the base engine as long as they pass along the same variables. # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( board , win_patterns , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] temp_board = board . copy () # define avail_moves # avail_moves = # your code for giving scores for avail_moves # first grab max score # then select all moves that have this max score # return a random selection of the moves with the max score return move 3.2.1 Allow GameEngine to take an ai agent as a passable parameter \u00b6 Let's rewrite our GameEngine to take an ai agent as a passable parameter under user_ai . The default value will be None Additional user_ai criteria will be that user_ai receives board , win_patterns and player_label and returns move . class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' , user_ai = None ): super () . __init__ () self . setup = setup self . user_ai = user_ai def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self . user_ai == None : level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) else : self . ai_level = 3 if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## if self . user_ai == None : self . ai_level = 2 else : self . ai_level = 3 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) ########## # Our user-defined AI agent ########## elif self . ai_level == 3 : move = self . user_ai ( self . board , self . win_patterns , player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Test the auto and user functions game = GameEngine ( setup = 'user' , user_ai = one_step_ai ) game . setup_game () How many Players? (type 0, 1, or 2)1 who will go first? (X, (AI), or O (Player))X game . play_game () | | | | | | | | | | | | | | | | | | | | | |X| | O, what's your move?5 | | | | | |O| | | |X| | | | |X| | |O| | | |X| | O, what's your move?4 | | |X| |O|O| | | |X| | | | |X| |O|O|X| | |X| | O, what's your move?1 |O| |X| |O|O|X| | |X| | 'X' Won! |O| |X| |O|O|X| | |X|X| <__main__.GameEngine at 0x7f1f812e7d50> 3.3 Write Unit Tests for the New Code \u00b6 def test_user_ai (): random . seed ( 42 ) game = GameEngine ( setup = 'auto' , user_ai = one_step_ai ) game . setup_game () outcome = game . play_game () assert outcome . winner == 'X' , 'X should have won!' test_user_ai () | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| | |X|O| |O| | | 'X' Won! |X|O|X| | |X|O| |O| |X|","title":"1-Step Look Ahead Agents"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#data-science-foundations-project-part-3-1-step-look-ahead","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we're working on a more advanced AI structure: 1-step lookahead.","title":"Data Science Foundations  Project Part 3: 1-Step Look Ahead"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#30-preparing-environment-and-importing-data","text":"back to top","title":"3.0 Preparing Environment and Importing Data"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#301-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## self . ai_level = 2 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"3.0.1 Import Packages"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#302-load-dataset","text":"back to top","title":"3.0.2 Load Dataset"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#31-rethinking-gameplay","text":"To implement the broader strategies used in game theory and machine learning, we need to rebroadcast our approach to creating our AI agent. In the heurstical agent model, we thought in terms of checking for specific move types, defined by what kind of advantage they give us during game play, i.e. see if a winning move is available, a blocking move, if a corner place is free, etc. Rather than thinking with this look and check mindset that is centered around specific strategies and our own prior knowledge about the game (we know that a center piece is statistically likely to give us a higher chance of winning) we will evaluate every available move to the AI, and rate them quantitatively. switching from ordinal to interval Notice the datatype change when we move from giving simple preferences of moves to actual scores of moves. Catalog this in your mind for future reference when considering datatypes!","title":"3.1 Rethinking gameplay"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#311-one-step-look-ahead","text":"For now, when we rate our boards, we will only look 1-step ahead in gameplay. Hence the name we give this AI strategy, 1-step lookahead The beginning portion of our code will look about the same as the heuristic AI model. Recall: def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() but now, instead of searching progressively through our preferred move-types (winning, middle, etc.) . We are going to give every available move (1, 3, 7, etc.) a score. Our score regimen will look like the following: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move","title":"3.1.1 One-Step Look Ahead"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#q1-rewrite-avail_moves","text":"define avail_moves as a dictionary of available moves with scores for each move as empty strings. We will update this dictionary with numerical scores in the next step # we're going to steal the parameter names to # prototype our new function self = TicTacToe () player_label = 'X' opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] # instead of a list, we want avail_moves to now be a dictionary that will # contain the move and its score # avail_moves = # YOUR CODE HERE temp_board = self . board . copy ()","title":"Q1 Rewrite avail_moves"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#q2-score-each-move-in-avail_moves","text":"Now let's fold this into our new one_step_ai function. Remember: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## # avail_moves = # temp_board = self . board . copy () # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score temp_board [ move ] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## ############################################################################## ################### All remaining moves receive a score of 1 ############################################################################## return avail_moves one_step_ai ( self , 'X' ) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}","title":"Q2 Score each move in avail_moves"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#q3-test-one_step_ai","text":"That's great, but how do we check that our code will work when a winning move is available, or a losing move is just around the corner? let's create a unit test for these! # just defining a new game self = TicTacToe () player_label = 'X' # seeding the board with some X's self . board [ 1 ] = 'X' self . board [ 2 ] = 'X' self . board {1: 'X', 2: 'X', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} Now test the winning move. Your code should return 100 at move 3 and 1 everywhere else one_step_ai ( self , player_label ) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} We can test the losing move by reversing the players player_label = 'O' one_step_ai ( self , player_label ) {3: 10, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} great! Let's keep these shorthand codes in mind when we go to write actual unit tests with the one_step_ai function embedded in the GameEngine module. We're not done yet, recall that our other ai agents returned the actual selected move, not a dictionary of the moves with scores. We need to create a move from this dictionary and return it. Here's what the general procedure will look like: Grab the maximum score (after assigning scores to all of avail_moves) Select all moves that have this maximum score Return a random selection of the moves with the max score and then in code format: avail_moves = one_step_ai ( self , player_label ) # 1. grab the maximum score max_score = max ( avail_moves . values ()) # 2. select all moves that have this maximum score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # 3. return a random selection of the moves with the max score move = random . choice ( valid ) move 3","title":"Q3 Test one_step_ai"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#32-putting-it-all-together","text":"","title":"3.2 Putting it all together"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#q4-finish-one_step_ai-to-return-a-move","text":"Let's see if we can rewrite our game engine to take new AI models in as a passable parameter. This way our base module will be much cleaner, and allow us to continue to write new functions for the base engine as long as they pass along the same variables. # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( board , win_patterns , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] temp_board = board . copy () # define avail_moves # avail_moves = # your code for giving scores for avail_moves # first grab max score # then select all moves that have this max score # return a random selection of the moves with the max score return move","title":"Q4 Finish one_step_ai to return a move"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#321-allow-gameengine-to-take-an-ai-agent-as-a-passable-parameter","text":"Let's rewrite our GameEngine to take an ai agent as a passable parameter under user_ai . The default value will be None Additional user_ai criteria will be that user_ai receives board , win_patterns and player_label and returns move . class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' , user_ai = None ): super () . __init__ () self . setup = setup self . user_ai = user_ai def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self . user_ai == None : level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) else : self . ai_level = 3 if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## if self . user_ai == None : self . ai_level = 2 else : self . ai_level = 3 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) ########## # Our user-defined AI agent ########## elif self . ai_level == 3 : move = self . user_ai ( self . board , self . win_patterns , player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Test the auto and user functions game = GameEngine ( setup = 'user' , user_ai = one_step_ai ) game . setup_game () How many Players? (type 0, 1, or 2)1 who will go first? (X, (AI), or O (Player))X game . play_game () | | | | | | | | | | | | | | | | | | | | | |X| | O, what's your move?5 | | | | | |O| | | |X| | | | |X| | |O| | | |X| | O, what's your move?4 | | |X| |O|O| | | |X| | | | |X| |O|O|X| | |X| | O, what's your move?1 |O| |X| |O|O|X| | |X| | 'X' Won! |O| |X| |O|O|X| | |X|X| <__main__.GameEngine at 0x7f1f812e7d50>","title":"3.2.1 Allow GameEngine to take an ai agent as a passable parameter"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#33-write-unit-tests-for-the-new-code","text":"def test_user_ai (): random . seed ( 42 ) game = GameEngine ( setup = 'auto' , user_ai = one_step_ai ) game . setup_game () outcome = game . play_game () assert outcome . winner == 'X' , 'X should have won!' test_user_ai () | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| | |X|O| |O| | | 'X' Won! |X|O|X| | |X|O| |O| |X|","title":"3.3 Write Unit Tests for the New Code"},{"location":"project/P4_N_Step_Look_Ahead_Agents/","text":"Data Science Foundations Project Part 4: N-Step Look Ahead \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com For this lesson, we will be adding N-step Look Ahead algorithm to our ensemble of AI agents! 4.0 Preparing Environment and Importing Data \u00b6 back to top 4.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt def one_step_ai ( board , win_patterns , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = { i : 1 for i in board . keys () if board [ i ] == ' ' } temp_board = board . copy () ######################################## # we're going to change the following lines, instead of caring # whether we've found the best move, we want to update the move # with a score ######################################## # check if the opponent has a winning move first, we will overwrite # the score for this move if it is also a winning move for the current # player for move in avail_moves . keys (): temp_board [ move ] = opponent for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: avail_moves [ move ] = 10 temp_board [ move ] = ' ' for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: avail_moves [ move ] = 100 temp_board [ move ] = ' ' # first grab max score max_score = max ( avail_moves . values ()) # then select all moves that have this max score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # return a random selection of the moves with the max score move = random . choice ( valid ) return move class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' , user_ai = None ): super () . __init__ () self . setup = setup self . user_ai = user_ai def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self . user_ai == None : level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) else : self . ai_level = 3 if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## if self . user_ai == None : self . ai_level = 2 else : self . ai_level = 3 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) ########## # Our user-defined AI agent ########## elif self . ai_level == 3 : move = self . user_ai ( self . board , self . win_patterns , player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 4.1 N-Step Look Ahead and Minimax \u00b6 In the previous tic tac toe module, our AI only looked 1 step ahead, and we can probably see how this has disadvantages. When we play strategy games ourselves, we often do better by looking a number of steps into the future. One new idea that this requires, is how we will anticipate our opponents move. This gets us into game theory . We're not going to borrow a whole lot from here, just the following: we will assume our opponent will work to minimize our score This switching from us wanting to maximize our score to the opponent wanting to minimize our score is called the minimax algorithm. As well look ahead into the future possibility of moves, we will use minimax to set our hypothetical behavior as well as our opponents You can investigate the pseudocode for minmax on wiki. # we're going to pull out and reformat some of our helper functions in the # TicTacToe class win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] def check_winning ( board , win_patterns ): for pattern in win_patterns : values = [ board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ] or values == [ 'O' , 'O' , 'O' ]: return True return False def check_stalemate ( board , win_patterns ): if ( ' ' not in board . values ()) and ( check_winning ( board , win_patterns ) == '' ): return True return False def minimax ( depth , board , maximizing_player , player_label , verbiose = False ): # infer the opponent opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] # set the available moves avail_moves = [ i for i in board . keys () if board [ i ] == ' ' ] # check if the depth is 0, or stalemate/winner has been reached # if so this is the basecase and we want to return get_score() terminal_move = is_terminal_node ( board , avail_moves ) if terminal_move or depth == 0 : score = get_score ( board , player_label , win_patterns ) if verbiose : print ( ' {} score: {} . depth: {} ' . format ( board , score , depth )) return score ### in the following we want to search through every possible board at the ### current level (the possible moves for the current player, given that the ### player is either the one whose turn it is or the imagined opponent) # call minimax where it is the current players turn and so we want to # maximize the score if maximizing_player : score = - np . Inf for move in avail_moves : new_board = board . copy () new_board [ move ] = player_label score = max ( score , minimax ( depth - 1 , new_board , False , player_label , verbiose )) if verbiose : print ( ' {} max. score: {} . depth: {} ' . format ( board , score , depth )) return score # call minimax where it is the opponent players turn and so we want to # minimize the score elif not maximizing_player : score = np . Inf for move in avail_moves : new_board = board . copy () new_board [ move ] = opponent score = min ( score , minimax ( depth - 1 , new_board , True , player_label , verbiose )) if verbiose : print ( ' {} min. score: {} . depth: {} ' . format ( board , score , depth )) return score def is_terminal_node ( board , avail_moves ): if check_winning ( board , win_patterns ): return True elif check_stalemate ( board , win_patterns ): return True else : return False def get_score ( board , player_label , win_patterns ): # this will look somewhat similar to our 1-step lookahead algorithm opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] score = 0 for pattern in win_patterns : values = [ board [ i ] for i in pattern ] # if the opponent wins, the score is -100 if values == [ opponent , opponent , opponent ]: score = - 100 elif values == [ player_label , player_label , player_label ]: score = 100 return score board = TicTacToe () . board board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} minimax ( depth = 1 , board = board , maximizing_player = True , player_label = 'O' ) 0 Finally, we need a couple wrapper functions to handle this. The first is a handler for the top level of the game tree (we want to see the minmax result for every possible move at the current place in the game) verbiose = True verbiose True def get_minimax ( depth , board , player_label , verbiose = False ): score = minimax ( depth - 1 , board , False , player_label , verbiose = verbiose ) return score def n_step_ai_temp ( board , win_patterns , player_label , n_steps , verbiose = False ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = { i : 1 for i in board . keys () if board [ i ] == ' ' } for move in avail_moves . keys (): temp_board = board . copy () temp_board [ move ] = player_label score = get_minimax ( n_steps , temp_board , player_label , verbiose = verbiose ) avail_moves [ move ] = score return avail_moves Let's test our n_step_ai while we're still returning the dictionary of available moves. Does this make sense? board = TicTacToe () . board board [ 1 ] = 'X' board [ 5 ] = 'O' board [ 2 ] = 'X' # with this setup we should see that a good move will be to play 3 # be sure to vary the number of lookahead steps n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 2 ) {3: 100, 4: 0, 6: 0, 7: 0, 8: 0, 9: 0} Looks like it's making sense. Let's also try when the opponent looks like they've got a winning move to be made board = TicTacToe () . board board [ 1 ] = 'X' board [ 5 ] = 'O' board [ 2 ] = 'X' board [ 4 ] = 'O' # with this setup we should see that a good move will be to play 6 or 3 # be sure to vary the number of lookahead steps n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 3 ) Let's look at the following as well. This should be an interesting game situation to you. It is \"X's\" move. What do you notice? Does this outcome make sense? X cannot win this game board = TicTacToe () . board board [ 1 ] = 'O' board [ 5 ] = 'O' board [ 2 ] = 'X' board [ 8 ] = 'X' # be sure to vary the number of lookahead steps n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 4 , verbiose = False ) # as a side note, sometimes it helps to interpret the outcome if you discount # the results from looking further down the game tree. (maybe O will make a mistake) # how would you encode this in your n-step look ahead algorithm? {3: -100, 4: -100, 6: -100, 7: -100, 9: -100} 4.2 Packaging for GameEngine \u00b6 Nice. Let's finish packaging our n_steps_ai so we can feed it to our game engine. def n_step_ai ( board , win_patterns , player_label , n_steps = 3 ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = { i : 1 for i in board . keys () if board [ i ] == ' ' } for move in avail_moves . keys (): temp_board = board . copy () temp_board [ move ] = player_label score = get_minimax ( n_steps , temp_board , player_label ) avail_moves [ move ] = score ########################################## ### The rest of our ai agent harness is the same ########################################## # first grab max score max_score = max ( avail_moves . values ()) # then select all moves that have this max score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # return a random selection of the moves with the max score move = random . choice ( valid ) return move game = GameEngine ( setup = 'user' , user_ai = n_step_ai ) game . setup_game () How many Players? (type 0, 1, or 2)1 who will go first? (X, (AI), or O (Player))X game . play_game () | | | | | | | | | | | | | | | | |X| | | | | | | O, what's your move?5 | | | | |X|O| | | | | | | | | | |X|O| | | |X| | O, what's your move?2 | |O| | |X|O| | | |X| | | |O| | |X|O| | |X|X| | O, what's your move?1 |O|O| | |X|O| | |X|X| | 'X' Won! |O|O| | |X|O| | |X|X|X| <__main__.GameEngine at 0x7fe529a0d850> Let's investigate the behavior of our AI and double check that it makes sense board = game . board board [ 9 ] = ' ' board {1: 'O', 2: 'O', 3: ' ', 4: 'X', 5: 'O', 6: ' ', 7: 'X', 8: 'X', 9: ' '} game . visualize_board () |O|O| | |X|O| | |X|X| | n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 3 ) {3: -100, 6: -100, 9: 100} 4.3 Writing Tests \u00b6 def test_n_step_ai (): random . seed ( 42 ) game = GameEngine ( setup = 'auto' , user_ai = n_step_ai ) game . setup_game () game . play_game () # check that the winner is X assert game . winner == 'X' , \"Winner should be X!\" # check that the ai level is set to 3 which means our engine is properly # accessing the user defined ai assert game . ai_level == 3 , \"The engine is not using the user defined AI!\" test_n_step_ai () | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | 'X' Won! |X|O|X| |O|X|O| | | |X|","title":"N-Step Look Ahead Agents"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#data-science-foundations-project-part-4-n-step-look-ahead","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com For this lesson, we will be adding N-step Look Ahead algorithm to our ensemble of AI agents!","title":"Data Science Foundations  Project Part 4: N-Step Look Ahead"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#40-preparing-environment-and-importing-data","text":"back to top","title":"4.0 Preparing Environment and Importing Data"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#401-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt def one_step_ai ( board , win_patterns , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = { i : 1 for i in board . keys () if board [ i ] == ' ' } temp_board = board . copy () ######################################## # we're going to change the following lines, instead of caring # whether we've found the best move, we want to update the move # with a score ######################################## # check if the opponent has a winning move first, we will overwrite # the score for this move if it is also a winning move for the current # player for move in avail_moves . keys (): temp_board [ move ] = opponent for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: avail_moves [ move ] = 10 temp_board [ move ] = ' ' for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: avail_moves [ move ] = 100 temp_board [ move ] = ' ' # first grab max score max_score = max ( avail_moves . values ()) # then select all moves that have this max score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # return a random selection of the moves with the max score move = random . choice ( valid ) return move class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' , user_ai = None ): super () . __init__ () self . setup = setup self . user_ai = user_ai def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self . user_ai == None : level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) else : self . ai_level = 3 if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## if self . user_ai == None : self . ai_level = 2 else : self . ai_level = 3 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) ########## # Our user-defined AI agent ########## elif self . ai_level == 3 : move = self . user_ai ( self . board , self . win_patterns , player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"4.0.1 Import Packages"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#41-n-step-look-ahead-and-minimax","text":"In the previous tic tac toe module, our AI only looked 1 step ahead, and we can probably see how this has disadvantages. When we play strategy games ourselves, we often do better by looking a number of steps into the future. One new idea that this requires, is how we will anticipate our opponents move. This gets us into game theory . We're not going to borrow a whole lot from here, just the following: we will assume our opponent will work to minimize our score This switching from us wanting to maximize our score to the opponent wanting to minimize our score is called the minimax algorithm. As well look ahead into the future possibility of moves, we will use minimax to set our hypothetical behavior as well as our opponents You can investigate the pseudocode for minmax on wiki. # we're going to pull out and reformat some of our helper functions in the # TicTacToe class win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] def check_winning ( board , win_patterns ): for pattern in win_patterns : values = [ board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ] or values == [ 'O' , 'O' , 'O' ]: return True return False def check_stalemate ( board , win_patterns ): if ( ' ' not in board . values ()) and ( check_winning ( board , win_patterns ) == '' ): return True return False def minimax ( depth , board , maximizing_player , player_label , verbiose = False ): # infer the opponent opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] # set the available moves avail_moves = [ i for i in board . keys () if board [ i ] == ' ' ] # check if the depth is 0, or stalemate/winner has been reached # if so this is the basecase and we want to return get_score() terminal_move = is_terminal_node ( board , avail_moves ) if terminal_move or depth == 0 : score = get_score ( board , player_label , win_patterns ) if verbiose : print ( ' {} score: {} . depth: {} ' . format ( board , score , depth )) return score ### in the following we want to search through every possible board at the ### current level (the possible moves for the current player, given that the ### player is either the one whose turn it is or the imagined opponent) # call minimax where it is the current players turn and so we want to # maximize the score if maximizing_player : score = - np . Inf for move in avail_moves : new_board = board . copy () new_board [ move ] = player_label score = max ( score , minimax ( depth - 1 , new_board , False , player_label , verbiose )) if verbiose : print ( ' {} max. score: {} . depth: {} ' . format ( board , score , depth )) return score # call minimax where it is the opponent players turn and so we want to # minimize the score elif not maximizing_player : score = np . Inf for move in avail_moves : new_board = board . copy () new_board [ move ] = opponent score = min ( score , minimax ( depth - 1 , new_board , True , player_label , verbiose )) if verbiose : print ( ' {} min. score: {} . depth: {} ' . format ( board , score , depth )) return score def is_terminal_node ( board , avail_moves ): if check_winning ( board , win_patterns ): return True elif check_stalemate ( board , win_patterns ): return True else : return False def get_score ( board , player_label , win_patterns ): # this will look somewhat similar to our 1-step lookahead algorithm opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] score = 0 for pattern in win_patterns : values = [ board [ i ] for i in pattern ] # if the opponent wins, the score is -100 if values == [ opponent , opponent , opponent ]: score = - 100 elif values == [ player_label , player_label , player_label ]: score = 100 return score board = TicTacToe () . board board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} minimax ( depth = 1 , board = board , maximizing_player = True , player_label = 'O' ) 0 Finally, we need a couple wrapper functions to handle this. The first is a handler for the top level of the game tree (we want to see the minmax result for every possible move at the current place in the game) verbiose = True verbiose True def get_minimax ( depth , board , player_label , verbiose = False ): score = minimax ( depth - 1 , board , False , player_label , verbiose = verbiose ) return score def n_step_ai_temp ( board , win_patterns , player_label , n_steps , verbiose = False ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = { i : 1 for i in board . keys () if board [ i ] == ' ' } for move in avail_moves . keys (): temp_board = board . copy () temp_board [ move ] = player_label score = get_minimax ( n_steps , temp_board , player_label , verbiose = verbiose ) avail_moves [ move ] = score return avail_moves Let's test our n_step_ai while we're still returning the dictionary of available moves. Does this make sense? board = TicTacToe () . board board [ 1 ] = 'X' board [ 5 ] = 'O' board [ 2 ] = 'X' # with this setup we should see that a good move will be to play 3 # be sure to vary the number of lookahead steps n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 2 ) {3: 100, 4: 0, 6: 0, 7: 0, 8: 0, 9: 0} Looks like it's making sense. Let's also try when the opponent looks like they've got a winning move to be made board = TicTacToe () . board board [ 1 ] = 'X' board [ 5 ] = 'O' board [ 2 ] = 'X' board [ 4 ] = 'O' # with this setup we should see that a good move will be to play 6 or 3 # be sure to vary the number of lookahead steps n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 3 ) Let's look at the following as well. This should be an interesting game situation to you. It is \"X's\" move. What do you notice? Does this outcome make sense? X cannot win this game board = TicTacToe () . board board [ 1 ] = 'O' board [ 5 ] = 'O' board [ 2 ] = 'X' board [ 8 ] = 'X' # be sure to vary the number of lookahead steps n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 4 , verbiose = False ) # as a side note, sometimes it helps to interpret the outcome if you discount # the results from looking further down the game tree. (maybe O will make a mistake) # how would you encode this in your n-step look ahead algorithm? {3: -100, 4: -100, 6: -100, 7: -100, 9: -100}","title":"4.1 N-Step Look Ahead and Minimax"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#42-packaging-for-gameengine","text":"Nice. Let's finish packaging our n_steps_ai so we can feed it to our game engine. def n_step_ai ( board , win_patterns , player_label , n_steps = 3 ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = { i : 1 for i in board . keys () if board [ i ] == ' ' } for move in avail_moves . keys (): temp_board = board . copy () temp_board [ move ] = player_label score = get_minimax ( n_steps , temp_board , player_label ) avail_moves [ move ] = score ########################################## ### The rest of our ai agent harness is the same ########################################## # first grab max score max_score = max ( avail_moves . values ()) # then select all moves that have this max score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # return a random selection of the moves with the max score move = random . choice ( valid ) return move game = GameEngine ( setup = 'user' , user_ai = n_step_ai ) game . setup_game () How many Players? (type 0, 1, or 2)1 who will go first? (X, (AI), or O (Player))X game . play_game () | | | | | | | | | | | | | | | | |X| | | | | | | O, what's your move?5 | | | | |X|O| | | | | | | | | | |X|O| | | |X| | O, what's your move?2 | |O| | |X|O| | | |X| | | |O| | |X|O| | |X|X| | O, what's your move?1 |O|O| | |X|O| | |X|X| | 'X' Won! |O|O| | |X|O| | |X|X|X| <__main__.GameEngine at 0x7fe529a0d850> Let's investigate the behavior of our AI and double check that it makes sense board = game . board board [ 9 ] = ' ' board {1: 'O', 2: 'O', 3: ' ', 4: 'X', 5: 'O', 6: ' ', 7: 'X', 8: 'X', 9: ' '} game . visualize_board () |O|O| | |X|O| | |X|X| | n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 3 ) {3: -100, 6: -100, 9: 100}","title":"4.2 Packaging for GameEngine"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#43-writing-tests","text":"def test_n_step_ai (): random . seed ( 42 ) game = GameEngine ( setup = 'auto' , user_ai = n_step_ai ) game . setup_game () game . play_game () # check that the winner is X assert game . winner == 'X' , \"Winner should be X!\" # check that the ai level is set to 3 which means our engine is properly # accessing the user defined ai assert game . ai_level == 3 , \"The engine is not using the user defined AI!\" test_n_step_ai () | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | 'X' Won! |X|O|X| |O|X|O| | | |X|","title":"4.3 Writing Tests"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/","text":"Data Science Foundations, Lab 1: Data Hunt I \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's a data hunt. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from ipywidgets import interact df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_explorers/main/assets/imdb_movies.csv\" ) # converting years to numbers for easy conditionals df [ 'year' ] = pd . to_numeric ( df [ 'year' ], errors = 'coerce' ) df . shape /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) (85855, 22) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics 0 tt0000009 Miss Jerry Miss Jerry 1894.0 1894-10-09 Romance 45 USA None Alexander Black ... Blanche Bayliss, William Courtenay, Chauncey D... The adventures of a female reporter in the 1890s. 5.9 154 NaN NaN NaN NaN 1.0 2.0 1 tt0000574 The Story of the Kelly Gang The Story of the Kelly Gang 1906.0 1906-12-26 Biography, Crime, Drama 70 Australia None Charles Tait ... Elizabeth Tait, John Tait, Norman Campbell, Be... True story of notorious Australian outlaw Ned ... 6.1 589 $ 2250 NaN NaN NaN 7.0 7.0 2 tt0001892 Den sorte dr\u00f8m Den sorte dr\u00f8m 1911.0 1911-08-19 Drama 53 Germany, Denmark NaN Urban Gad ... Asta Nielsen, Valdemar Psilander, Gunnar Helse... Two men of high rank are both wooing the beaut... 5.8 188 NaN NaN NaN NaN 5.0 2.0 3 rows \u00d7 22 columns Q1 What american director has the highest mean avg_vote? \u00b6 df . groupby ( 'director' )[ 'avg_vote' ] . mean () . sort_values ( ascending = False ) director Msn Surya 9.9 Aalmist Subba 9.8 Sampath Rudra 9.8 Basheed S.K. 9.8 Abner Official 9.8 ... Ramana Reddy B.V. 1.0 Tam\u00e1s Gerencs\u00e9r 1.0 Tommy Yu 1.0 G\u00f6khan G\u00f6k 1.0 Yasutake Torii 1.0 Name: avg_vote, Length: 34733, dtype: float64 Q2 What american director with more than 5 movies, has the highest mean avg_vote? \u00b6 df . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 5 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . mean () . sort_values ( ascending = False ) director Christopher Nolan 8.218182 Satyajit Ray 8.025000 Andrei Tarkovsky 8.014286 Hayao Miyazaki 8.008333 Sergio Leone 7.928571 ... Bill Zebub 2.483333 Mark Polonia 2.433333 Paul T.T. Easter 2.383333 Christopher Forbes 2.000000 Brett Kelly 1.533333 Name: avg_vote, Length: 3047, dtype: float64 Q3 What director has the largest variance in avg_vote? \u00b6 df . groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) director Deniz Denizciler 4.030509 Rudi Lagemann 3.747666 Emilio Ruiz Barrachina 3.676955 Krishna Ghattamaneni 3.676955 Milos Avramovic 3.606245 ... \u00dcmit Degirmenci NaN \u00dcmit El\u00e7i NaN \u00dcmit K\u00f6reken NaN \u00deorsteinn Gunnar Bjarnason NaN \u00de\u00f3rhildur \u00deorleifsd\u00f3ttir NaN Name: avg_vote, Length: 34733, dtype: float64 Q4 What director with more than 10 movies has the largest variance in avg_vote? \u00b6 df . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 10 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) director Harry Baweja 1.869954 Shaji Kailas 1.854502 Zdenek Troska 1.775984 Adam Rifkin 1.711251 Ram Gopal Varma 1.687850 ... Ford Beebe 0.224343 Ray Nazarro 0.210311 Jean Gr\u00e9millon 0.196946 Louis Feuillade 0.156428 Tsutomu Shibayama 0.126121 Name: avg_vote, Length: 1135, dtype: float64 Q5 What american directors with more than 5 movies have the largest variance in avg_vote? \u00b6 df . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 5 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) director Jorge Ameer 2.200606 Tigran Keosayan 2.188150 Nikos Zervos 2.093243 Kundan Shah 2.060502 Feroz Khan 2.036220 ... Sang-il Lee 0.132916 Nate Watt 0.129099 Daisuke Nishio 0.127242 Tsutomu Shibayama 0.126121 Pierre Chenal 0.103280 Name: avg_vote, Length: 3047, dtype: float64 Q6 Where does M. Night Shyamalan fall on this rank scale? \u00b6 (He's number 36/859) var_rank = df . loc [ df [ 'country' ] == 'USA' ] . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 5 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) . reset_index () display ( var_rank . loc [ var_rank [ 'director' ] == 'M. Night Shyamalan' ]) print ( var_rank . shape [ 0 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } director avg_vote 36 M. Night Shyamalan 1.258791 859 what happens when you only include directors who, on average (based on mean), have made most their movies after 1990 and have produced 10 or more movies? (Shyamalan rises to 3/83) var_rank = df . loc [ df [ 'country' ] == 'USA' ] \\ . groupby ( 'director' ) . filter ( lambda x : ( x [ 'year' ] . mean () > 1990 ) & ( x . shape [ 0 ] > 9 )) \\ . groupby ( 'director' )[[ 'avg_vote' , 'votes' ]] . describe () . sort_values ( by = ( 'avg_vote' , 'std' ), ascending = False ) display ( var_rank . iloc [: 10 ]) print ( var_rank . shape [ 0 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } avg_vote votes count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max director Adam Rifkin 15.0 5.053333 1.711251 1.3 4.200 5.80 6.100 6.9 15.0 4417.400000 9414.430237 124.0 525.50 1084.0 1782.50 34958.0 Mark L. Lester 19.0 4.768421 1.262296 2.3 4.200 4.70 5.800 6.7 19.0 11479.052632 32768.240173 298.0 549.00 1219.0 4405.50 143443.0 M. Night Shyamalan 10.0 5.970000 1.258791 4.0 4.975 6.05 6.650 8.1 10.0 239264.500000 262079.154005 731.0 97982.50 169426.0 308493.25 894385.0 Sean McNamara 12.0 5.216667 1.252513 2.9 4.725 5.60 5.950 7.0 12.0 9221.166667 13933.853515 365.0 1085.25 1416.0 12191.50 44808.0 Sam Firstenberg 10.0 4.550000 1.174970 2.8 3.475 4.85 5.325 6.2 10.0 1890.400000 1552.704107 153.0 713.50 1282.0 3317.25 4330.0 Gus Van Sant 12.0 6.583333 1.169952 4.4 6.025 6.95 7.300 8.3 12.0 112868.166667 232569.906962 3674.0 16191.75 38458.5 82270.25 837379.0 John Lyde 16.0 4.937500 1.159813 3.6 3.900 4.80 5.500 7.0 16.0 1021.937500 911.075516 113.0 409.50 802.0 1360.00 3270.0 Michael Polish 12.0 5.458333 1.154011 3.4 4.700 5.45 6.300 7.2 12.0 4396.833333 5662.740034 528.0 1698.75 3395.5 3624.00 21873.0 Randal Kleiser 12.0 5.708333 1.126102 3.5 5.050 5.60 6.750 7.2 12.0 35908.916667 65204.420315 1030.0 2458.75 11245.5 41162.50 232940.0 Brian Brough 12.0 5.575000 1.096378 3.5 5.225 5.80 6.350 7.0 12.0 675.500000 590.071721 104.0 134.00 606.5 1048.75 1842.0 83 var_rank = df . loc [ df [ 'country' ] == 'USA' ] \\ . groupby ( 'director' ) . filter ( lambda x : ( x [ 'avg_vote' ] . max () > 8 ) & ( x [ 'votes' ] . mean () > 1e3 ) & ( x . shape [ 0 ] > 2 )) \\ . groupby ( 'director' )[[ 'avg_vote' , 'votes' ]] . describe () . sort_values ( by = ( 'avg_vote' , 'std' ), ascending = False ) display ( var_rank . iloc [: 10 ]) print ( var_rank . shape [ 0 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } avg_vote votes count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max director Peter Bogdanovich 13.0 6.446154 1.564510 2.9 6.000 7.00 7.600 8.1 13.0 12023.307692 1.510756e+04 870.0 1262.00 2426.0 19253.00 41283.0 Francis Ford Coppola 18.0 6.777778 1.444077 3.1 6.225 6.65 7.550 9.2 18.0 226387.555556 4.392702e+05 199.0 5078.25 23681.5 164311.25 1572674.0 Richard Marquand 5.0 6.320000 1.375500 4.5 5.800 6.50 6.500 8.3 5.0 188499.800000 4.134480e+05 411.0 560.00 598.0 12894.00 928036.0 Curtis Hanson 6.0 6.150000 1.361984 4.3 5.350 6.40 6.550 8.2 6.0 100734.333333 2.069594e+05 209.0 1345.00 20931.0 39724.25 521530.0 Sean Penn 4.0 6.525000 1.322561 4.9 5.950 6.55 7.125 8.1 4.0 157547.000000 2.695851e+05 4409.0 10514.00 32543.5 179576.50 560692.0 Timothy A. Chey 6.0 6.250000 1.291124 4.4 5.775 6.05 6.850 8.2 6.0 1447.500000 5.878295e+02 788.0 923.50 1510.5 1812.50 2235.0 M. Night Shyamalan 10.0 5.970000 1.258791 4.0 4.975 6.05 6.650 8.1 10.0 239264.500000 2.620792e+05 731.0 97982.50 169426.0 308493.25 894385.0 Stanley Kubrick 5.0 7.280000 1.202913 5.5 6.600 7.90 8.000 8.4 5.0 80954.400000 6.855686e+04 9649.0 20806.00 79652.0 121994.00 172671.0 Gus Van Sant 12.0 6.583333 1.169952 4.4 6.025 6.95 7.300 8.3 12.0 112868.166667 2.325699e+05 3674.0 16191.75 38458.5 82270.25 837379.0 Frank Darabont 4.0 7.975000 1.164403 6.9 7.050 7.85 8.775 9.3 4.0 929718.000000 1.008586e+06 51763.0 219886.75 694132.0 1403963.25 2278845.0 66 Q7 How many movies were made each year in US from 2000-2020 \u00b6 df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 )][[ 'year' ]] . value_counts ( sort = False ) year 2000.0 1345 2001.0 1447 2002.0 1405 2003.0 1496 2004.0 1681 2005.0 1827 2006.0 2063 2007.0 2074 2008.0 2175 2009.0 2298 2010.0 2281 2011.0 2429 2012.0 2560 2013.0 2783 2014.0 2942 2015.0 2977 2016.0 3138 2017.0 3329 2018.0 3257 2019.0 2841 2020.0 789 dtype: int64 Q8 Visualize The Results of Q7! \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 5 )) df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 )][[ 'year' ]] . value_counts ( sort = False ) . plot ( kind = 'bar' , ax = ax ) <AxesSubplot:xlabel='year'> Q9 For single country movies, how many movies were made each year in each country from 2000-2020, only include countries that made more than 1000 movies in that timeframe \u00b6 df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 ) & ( ~ df [ 'country' ] . str . contains ( ',' , na = False ))] \\ . groupby ( 'country' ) . filter ( lambda x : x . shape [ 0 ] > 1000 ) \\ . groupby ([ 'country' , 'year' ])[[ 'title' ]] . count () . reset_index () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year title 0 Canada 2000.0 39 1 Canada 2001.0 51 2 Canada 2002.0 49 3 Canada 2003.0 38 4 Canada 2004.0 52 ... ... ... ... 163 USA 2016.0 869 164 USA 2017.0 905 165 USA 2018.0 886 166 USA 2019.0 700 167 USA 2020.0 276 168 rows \u00d7 3 columns Q10 Visualize the results from Q9! \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) countries = df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 ) & ( ~ df [ 'country' ] . str . contains ( ',' , na = False ))] \\ . groupby ( 'country' ) . filter ( lambda x : x . shape [ 0 ] > 1000 ) \\ . groupby ([ 'country' , 'year' ])[[ 'title' ]] . count () . reset_index () for country in countries . groupby ( 'country' ): country [ 1 ] . plot ( x = 'year' , y = 'title' , ax = ax , label = country [ 0 ])","title":"SOLN L1 Descriptive Statistics Data Hunt"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#data-science-foundations-lab-1-data-hunt-i","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's a data hunt. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from ipywidgets import interact df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_explorers/main/assets/imdb_movies.csv\" ) # converting years to numbers for easy conditionals df [ 'year' ] = pd . to_numeric ( df [ 'year' ], errors = 'coerce' ) df . shape /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) (85855, 22) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics 0 tt0000009 Miss Jerry Miss Jerry 1894.0 1894-10-09 Romance 45 USA None Alexander Black ... Blanche Bayliss, William Courtenay, Chauncey D... The adventures of a female reporter in the 1890s. 5.9 154 NaN NaN NaN NaN 1.0 2.0 1 tt0000574 The Story of the Kelly Gang The Story of the Kelly Gang 1906.0 1906-12-26 Biography, Crime, Drama 70 Australia None Charles Tait ... Elizabeth Tait, John Tait, Norman Campbell, Be... True story of notorious Australian outlaw Ned ... 6.1 589 $ 2250 NaN NaN NaN 7.0 7.0 2 tt0001892 Den sorte dr\u00f8m Den sorte dr\u00f8m 1911.0 1911-08-19 Drama 53 Germany, Denmark NaN Urban Gad ... Asta Nielsen, Valdemar Psilander, Gunnar Helse... Two men of high rank are both wooing the beaut... 5.8 188 NaN NaN NaN NaN 5.0 2.0 3 rows \u00d7 22 columns","title":"Data Science Foundations, Lab 1: Data Hunt I"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q1-what-american-director-has-the-highest-mean-avg_vote","text":"df . groupby ( 'director' )[ 'avg_vote' ] . mean () . sort_values ( ascending = False ) director Msn Surya 9.9 Aalmist Subba 9.8 Sampath Rudra 9.8 Basheed S.K. 9.8 Abner Official 9.8 ... Ramana Reddy B.V. 1.0 Tam\u00e1s Gerencs\u00e9r 1.0 Tommy Yu 1.0 G\u00f6khan G\u00f6k 1.0 Yasutake Torii 1.0 Name: avg_vote, Length: 34733, dtype: float64","title":"Q1 What american director has the highest mean  avg_vote?"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q2-what-american-director-with-more-than-5-movies-has-the-highest-mean-avg_vote","text":"df . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 5 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . mean () . sort_values ( ascending = False ) director Christopher Nolan 8.218182 Satyajit Ray 8.025000 Andrei Tarkovsky 8.014286 Hayao Miyazaki 8.008333 Sergio Leone 7.928571 ... Bill Zebub 2.483333 Mark Polonia 2.433333 Paul T.T. Easter 2.383333 Christopher Forbes 2.000000 Brett Kelly 1.533333 Name: avg_vote, Length: 3047, dtype: float64","title":"Q2 What american director with more than 5 movies, has the highest mean avg_vote?"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q3-what-director-has-the-largest-variance-in-avg_vote","text":"df . groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) director Deniz Denizciler 4.030509 Rudi Lagemann 3.747666 Emilio Ruiz Barrachina 3.676955 Krishna Ghattamaneni 3.676955 Milos Avramovic 3.606245 ... \u00dcmit Degirmenci NaN \u00dcmit El\u00e7i NaN \u00dcmit K\u00f6reken NaN \u00deorsteinn Gunnar Bjarnason NaN \u00de\u00f3rhildur \u00deorleifsd\u00f3ttir NaN Name: avg_vote, Length: 34733, dtype: float64","title":"Q3 What director has the largest variance in avg_vote?"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q4-what-director-with-more-than-10-movies-has-the-largest-variance-in-avg_vote","text":"df . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 10 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) director Harry Baweja 1.869954 Shaji Kailas 1.854502 Zdenek Troska 1.775984 Adam Rifkin 1.711251 Ram Gopal Varma 1.687850 ... Ford Beebe 0.224343 Ray Nazarro 0.210311 Jean Gr\u00e9millon 0.196946 Louis Feuillade 0.156428 Tsutomu Shibayama 0.126121 Name: avg_vote, Length: 1135, dtype: float64","title":"Q4 What director with more than 10 movies has the largest variance in avg_vote?"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q5-what-american-directors-with-more-than-5-movies-have-the-largest-variance-in-avg_vote","text":"df . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 5 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) director Jorge Ameer 2.200606 Tigran Keosayan 2.188150 Nikos Zervos 2.093243 Kundan Shah 2.060502 Feroz Khan 2.036220 ... Sang-il Lee 0.132916 Nate Watt 0.129099 Daisuke Nishio 0.127242 Tsutomu Shibayama 0.126121 Pierre Chenal 0.103280 Name: avg_vote, Length: 3047, dtype: float64","title":"Q5 What american directors with more than 5 movies have the largest variance in avg_vote?"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q6-where-does-m-night-shyamalan-fall-on-this-rank-scale","text":"(He's number 36/859) var_rank = df . loc [ df [ 'country' ] == 'USA' ] . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 5 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) . reset_index () display ( var_rank . loc [ var_rank [ 'director' ] == 'M. Night Shyamalan' ]) print ( var_rank . shape [ 0 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } director avg_vote 36 M. Night Shyamalan 1.258791 859 what happens when you only include directors who, on average (based on mean), have made most their movies after 1990 and have produced 10 or more movies? (Shyamalan rises to 3/83) var_rank = df . loc [ df [ 'country' ] == 'USA' ] \\ . groupby ( 'director' ) . filter ( lambda x : ( x [ 'year' ] . mean () > 1990 ) & ( x . shape [ 0 ] > 9 )) \\ . groupby ( 'director' )[[ 'avg_vote' , 'votes' ]] . describe () . sort_values ( by = ( 'avg_vote' , 'std' ), ascending = False ) display ( var_rank . iloc [: 10 ]) print ( var_rank . shape [ 0 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } avg_vote votes count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max director Adam Rifkin 15.0 5.053333 1.711251 1.3 4.200 5.80 6.100 6.9 15.0 4417.400000 9414.430237 124.0 525.50 1084.0 1782.50 34958.0 Mark L. Lester 19.0 4.768421 1.262296 2.3 4.200 4.70 5.800 6.7 19.0 11479.052632 32768.240173 298.0 549.00 1219.0 4405.50 143443.0 M. Night Shyamalan 10.0 5.970000 1.258791 4.0 4.975 6.05 6.650 8.1 10.0 239264.500000 262079.154005 731.0 97982.50 169426.0 308493.25 894385.0 Sean McNamara 12.0 5.216667 1.252513 2.9 4.725 5.60 5.950 7.0 12.0 9221.166667 13933.853515 365.0 1085.25 1416.0 12191.50 44808.0 Sam Firstenberg 10.0 4.550000 1.174970 2.8 3.475 4.85 5.325 6.2 10.0 1890.400000 1552.704107 153.0 713.50 1282.0 3317.25 4330.0 Gus Van Sant 12.0 6.583333 1.169952 4.4 6.025 6.95 7.300 8.3 12.0 112868.166667 232569.906962 3674.0 16191.75 38458.5 82270.25 837379.0 John Lyde 16.0 4.937500 1.159813 3.6 3.900 4.80 5.500 7.0 16.0 1021.937500 911.075516 113.0 409.50 802.0 1360.00 3270.0 Michael Polish 12.0 5.458333 1.154011 3.4 4.700 5.45 6.300 7.2 12.0 4396.833333 5662.740034 528.0 1698.75 3395.5 3624.00 21873.0 Randal Kleiser 12.0 5.708333 1.126102 3.5 5.050 5.60 6.750 7.2 12.0 35908.916667 65204.420315 1030.0 2458.75 11245.5 41162.50 232940.0 Brian Brough 12.0 5.575000 1.096378 3.5 5.225 5.80 6.350 7.0 12.0 675.500000 590.071721 104.0 134.00 606.5 1048.75 1842.0 83 var_rank = df . loc [ df [ 'country' ] == 'USA' ] \\ . groupby ( 'director' ) . filter ( lambda x : ( x [ 'avg_vote' ] . max () > 8 ) & ( x [ 'votes' ] . mean () > 1e3 ) & ( x . shape [ 0 ] > 2 )) \\ . groupby ( 'director' )[[ 'avg_vote' , 'votes' ]] . describe () . sort_values ( by = ( 'avg_vote' , 'std' ), ascending = False ) display ( var_rank . iloc [: 10 ]) print ( var_rank . shape [ 0 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } avg_vote votes count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max director Peter Bogdanovich 13.0 6.446154 1.564510 2.9 6.000 7.00 7.600 8.1 13.0 12023.307692 1.510756e+04 870.0 1262.00 2426.0 19253.00 41283.0 Francis Ford Coppola 18.0 6.777778 1.444077 3.1 6.225 6.65 7.550 9.2 18.0 226387.555556 4.392702e+05 199.0 5078.25 23681.5 164311.25 1572674.0 Richard Marquand 5.0 6.320000 1.375500 4.5 5.800 6.50 6.500 8.3 5.0 188499.800000 4.134480e+05 411.0 560.00 598.0 12894.00 928036.0 Curtis Hanson 6.0 6.150000 1.361984 4.3 5.350 6.40 6.550 8.2 6.0 100734.333333 2.069594e+05 209.0 1345.00 20931.0 39724.25 521530.0 Sean Penn 4.0 6.525000 1.322561 4.9 5.950 6.55 7.125 8.1 4.0 157547.000000 2.695851e+05 4409.0 10514.00 32543.5 179576.50 560692.0 Timothy A. Chey 6.0 6.250000 1.291124 4.4 5.775 6.05 6.850 8.2 6.0 1447.500000 5.878295e+02 788.0 923.50 1510.5 1812.50 2235.0 M. Night Shyamalan 10.0 5.970000 1.258791 4.0 4.975 6.05 6.650 8.1 10.0 239264.500000 2.620792e+05 731.0 97982.50 169426.0 308493.25 894385.0 Stanley Kubrick 5.0 7.280000 1.202913 5.5 6.600 7.90 8.000 8.4 5.0 80954.400000 6.855686e+04 9649.0 20806.00 79652.0 121994.00 172671.0 Gus Van Sant 12.0 6.583333 1.169952 4.4 6.025 6.95 7.300 8.3 12.0 112868.166667 2.325699e+05 3674.0 16191.75 38458.5 82270.25 837379.0 Frank Darabont 4.0 7.975000 1.164403 6.9 7.050 7.85 8.775 9.3 4.0 929718.000000 1.008586e+06 51763.0 219886.75 694132.0 1403963.25 2278845.0 66","title":"Q6 Where does M. Night Shyamalan fall on this rank scale?"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q7-how-many-movies-were-made-each-year-in-us-from-2000-2020","text":"df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 )][[ 'year' ]] . value_counts ( sort = False ) year 2000.0 1345 2001.0 1447 2002.0 1405 2003.0 1496 2004.0 1681 2005.0 1827 2006.0 2063 2007.0 2074 2008.0 2175 2009.0 2298 2010.0 2281 2011.0 2429 2012.0 2560 2013.0 2783 2014.0 2942 2015.0 2977 2016.0 3138 2017.0 3329 2018.0 3257 2019.0 2841 2020.0 789 dtype: int64","title":"Q7 How many movies were made each year in US from 2000-2020"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q8-visualize-the-results-of-q7","text":"fig , ax = plt . subplots ( figsize = ( 10 , 5 )) df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 )][[ 'year' ]] . value_counts ( sort = False ) . plot ( kind = 'bar' , ax = ax ) <AxesSubplot:xlabel='year'>","title":"Q8 Visualize The Results of Q7!"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q9-for-single-country-movies-how-many-movies-were-made-each-year-in-each-country-from-2000-2020-only-include-countries-that-made-more-than-1000-movies-in-that-timeframe","text":"df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 ) & ( ~ df [ 'country' ] . str . contains ( ',' , na = False ))] \\ . groupby ( 'country' ) . filter ( lambda x : x . shape [ 0 ] > 1000 ) \\ . groupby ([ 'country' , 'year' ])[[ 'title' ]] . count () . reset_index () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year title 0 Canada 2000.0 39 1 Canada 2001.0 51 2 Canada 2002.0 49 3 Canada 2003.0 38 4 Canada 2004.0 52 ... ... ... ... 163 USA 2016.0 869 164 USA 2017.0 905 165 USA 2018.0 886 166 USA 2019.0 700 167 USA 2020.0 276 168 rows \u00d7 3 columns","title":"Q9 For single country movies, how many movies were made each year in each country from 2000-2020, only include countries that made more than 1000 movies in that timeframe"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q10-visualize-the-results-from-q9","text":"fig , ax = plt . subplots ( figsize = ( 10 , 10 )) countries = df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 ) & ( ~ df [ 'country' ] . str . contains ( ',' , na = False ))] \\ . groupby ( 'country' ) . filter ( lambda x : x . shape [ 0 ] > 1000 ) \\ . groupby ([ 'country' , 'year' ])[[ 'title' ]] . count () . reset_index () for country in countries . groupby ( 'country' ): country [ 1 ] . plot ( x = 'year' , y = 'title' , ax = ax , label = country [ 0 ])","title":"Q10 Visualize the results from Q9!"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/","text":"Data Science Foundations Lab 2: Data Hunt II \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's the data hunt part TWO. Preparing Environment and Importing Data \u00b6 Import Packages \u00b6 ! pip install - U plotly Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1) Collecting plotly Downloading plotly-5.1.0-py2.py3-none-any.whl (20.6 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.6 MB 1.3 MB/s \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0) Collecting tenacity>=6.2.0 Downloading tenacity-8.0.1-py3-none-any.whl (24 kB) Installing collected packages: tenacity, plotly Attempting uninstall: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-5.1.0 tenacity-8.0.1 # our standard libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns from ipywidgets import interact # our stats libraries import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy # our scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score Import and Clean Data \u00b6 df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_fundamentals/main/assets/truffle_rates.csv\" ) df = df . loc [ df [ 'rate' ] > 0 ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group rate 0 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.167097 1 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.153827 2 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.100299 3 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.333008 4 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.078108 df . shape (9210, 6) Exploratory Data Analysis \u00b6 Q1 Finding Influential Features \u00b6 Which of the five features (base_cake, truffle_type, primary_flavor, secondary_flavor, color_group) of the truffles is most influential on production rate? Back your answer with both a visualization of the distributions (boxplot, kernel denisty estimate, histogram, violin plot) and a statistical test (moods median, ANOVA, t-test) Be sure: everything is labeled (can you improve your labels with additional descriptive statistical information e.g. indicate mean, std, etc.) you meet the assumptions of your statistical test Q1.1 Visualization \u00b6 Use any number of visualizations. Here is an example to get you started: # Example: a KDE of the truffle_type and base_cake columns fig , ax = plt . subplots ( 2 , 1 , figsize = ( 12 , 12 )) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'truffle_type' ], fill = True , ax = ax [ 0 ]) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'base_cake' ], fill = True , ax = ax [ 1 ]) <AxesSubplot:xlabel='rate', ylabel='Density'> fig , ax = plt . subplots ( 5 , 1 , figsize = ( 10 , 40 )) for idx , col in enumerate ( df . columns [: - 1 ]): df . boxplot ( by = col , column = 'rate' , ax = ax [ idx ]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 9 ( ) missing from current font. fig.canvas.print_figure(bytes_io, **kw) Q1.2 Statistical Analysis \u00b6 What statistical tests can you perform to evaluate your hypothesis from the visualizations (maybe you think one particular feature is significant). Here's an ANOVA on the truffle_type column to get you started: model = ols ( 'rate ~ C( {} )' . format ( 'truffle_type' ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Is this P value significant? What is the null hypothesis? How do we check the assumptions of ANOVA? Answer: All groups fail for normal distribution of residuals and homogeneity of variances. So we cannot use ANOVA with this data: confidence_level = 0.05 for idx , col in enumerate ( df . columns [: - 1 ]): model = ols ( 'rate ~ C( {} )' . format ( col ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) print ( \"Shapiro-Wilk\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( * stats . shapiro ( model . resid ))) if stats . shapiro ( model . resid ) . pvalue < confidence_level : shapiro_rej = True print ( f \" \\t reject: { shapiro_rej } \" ) print ( \"Bartlett\" ) gb = df . groupby ( col )[ 'rate' ] w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( w , pvalue )) if pvalue < confidence_level : shapiro_rej = True print ( f \" \\t reject: { shapiro_rej } \" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(base_cake) 331.373550 5.0 2349.684756 0.0 Residual 259.606073 9204.0 NaN NaN Shapiro-Wilk statistic=0.93, pvalue=0.00e+00 reject: True Bartlett statistic=619.37, pvalue=1.32e-131 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Shapiro-Wilk statistic=0.96, pvalue=1.37e-42 reject: True Bartlett statistic=533.02, pvalue=1.80e-116 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(primary_flavor) 159.105452 47.0 71.815842 0.0 Residual 431.874171 9162.0 NaN NaN Shapiro-Wilk statistic=0.97, pvalue=6.49e-38 reject: True Bartlett /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") statistic=1609.00, pvalue=1.85e-306 reject: True .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(secondary_flavor) 115.773877 28.0 79.884192 0.0 Residual 475.205747 9181.0 NaN NaN Shapiro-Wilk statistic=0.97, pvalue=4.34e-39 reject: True Bartlett statistic=1224.49, pvalue=3.55e-240 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(color_group) 33.878491 11.0 50.849974 1.873235e-109 Residual 557.101132 9198.0 NaN NaN Shapiro-Wilk statistic=0.96, pvalue=1.40e-44 reject: True Bartlett statistic=298.64, pvalue=1.69e-57 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") Instead we might use Moods Median Q2 Finding Best and Worst Groups \u00b6 Q2.1 Compare Every Group to the Whole \u00b6 Of the primary flavors (feature), what 5 flavors (groups) would you recommend Truffletopia discontinue? Iterate through every level (i.e. pound, cheese, sponge cakes) of every category (i.e. base cake, primary flavor, secondary flavor) and use moods median testing to compare the group distribution to the grand median rate. moodsdf = pd . DataFrame () target = 'rate' for col in df . columns [: - 1 ]: for truff in df [ col ] . unique (): group = df . loc [ df [ col ] == truff ][ target ] pop = df . loc [ ~ ( df [ col ] == truff )][ target ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , table ]) . T ]) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'table' ] moodsdf [ 'p_value' ] = moodsdf [ 'p_value' ] . astype ( float ) print ( moodsdf . shape ) (98, 9) After you've computed a moods median test on every group, filter any data above a significance level of 0.05 moodsdf = moodsdf . loc [( moodsdf [ 'p_value' ] < confidence_level )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( moodsdf . shape ) (83, 9) Return the groups with the lowest median performance (your table need not look exactly like the one I've created) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 0 secondary_flavor Wild Cherry Cream 56.867453 4.661980e-14 0.310345 0.139998 0.085628 100 [[12, 4593], [88, 4517]] 1 primary_flavor Coconut 56.867453 4.661980e-14 0.310345 0.139998 0.085628 100 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.556345 4.302530e-15 0.310345 0.129178 0.092878 85 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.32026 7.846169e-13 0.310345 0.145727 0.095758 91 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.545249 4.142688e-11 0.310345 0.148964 0.10588 70 [[7, 4598], [63, 4542]] ... ... ... ... ... ... ... ... ... ... 78 secondary_flavor Vanilla 40.484134 1.982191e-10 0.310345 0.559808 0.664252 200 [[145, 4460], [55, 4550]] 79 primary_flavor Orange 60.723646 6.567649e-15 0.310345 0.580579 0.681157 200 [[155, 4450], [45, 4560]] 80 primary_flavor Plum 308.037116 5.845614e-69 0.310345 0.669126 0.681309 300 [[300, 4305], [0, 4605]] 81 primary_flavor Cheesecake 99.085851 2.417896e-23 0.310345 0.745813 0.689802 100 [[100, 4505], [0, 4605]] 82 base_cake Sponge 2107.437614 0.000000e+00 0.310345 0.711797 0.699679 1800 [[1774, 2831], [26, 4579]] 83 rows \u00d7 9 columns We would want to cut the following primary flavors. Check to see that you get a similar answer. rip wild cherry cream. ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] moodsdf . loc [ moodsdf [ 'descriptor' ] == 'primary_flavor' ][: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 1 primary_flavor Coconut 56.867453 4.661980e-14 0.310345 0.139998 0.085628 100 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.556345 4.302530e-15 0.310345 0.129178 0.092878 85 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.32026 7.846169e-13 0.310345 0.145727 0.095758 91 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.545249 4.142688e-11 0.310345 0.148964 0.10588 70 [[7, 4598], [63, 4542]] 10 primary_flavor Gingersnap 131.113519 2.338438e-30 0.310345 0.159268 0.143347 192 [[17, 4588], [175, 4430]] Q2.2 Beyond Statistical Testing: Using Reasoning \u00b6 Let's look at the total profile of the products associated with the five worst primary flavors. Given the number of different products made with any of these flavors, would you alter your answer at all? Answer: We would opt to only discontinue the gingersnap, and wild cherry cream flavors. The other flavors, Pink Lemonade, Chocolate, and Coconut may be subject to Simpson's Paradox since we do not have adequate sampling of them with other categories # 1. filter df for only bottom five flavors # 2. groupby all columns besides rate # 3. describe the rate column. # by doing this we can evaluate just how much sampling variety we have for the # worst performing flavors. bottom_five = [ 'Coconut' , 'Pink Lemonade' , 'Chocolate' , 'Wild Cherry Cream' , 'Gingersnap' ] df . loc [ df [ 'primary_flavor' ] . isin ( bottom_five )] . groupby ( list ( df . columns [: - 1 ]))[ 'rate' ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max base_cake truffle_type primary_flavor secondary_flavor color_group Butter Jelly Filled Pink Lemonade Butter Rum Rose 85.0 0.129178 0.137326 0.000061 0.032887 0.092878 0.171350 0.860045 Chiffon Candy Outer Wild Cherry Cream Rock and Rye Olive 17.0 0.094287 0.059273 0.010464 0.053976 0.077098 0.120494 0.229933 Chocolate Outer Gingersnap Dill Pickle Burgundy 59.0 0.133272 0.080414 0.021099 0.069133 0.137972 0.172066 0.401387 Jelly Filled Chocolate Tutti Frutti Burgundy 91.0 0.145727 0.135230 0.000033 0.044847 0.095758 0.185891 0.586570 Pound Candy Outer Coconut Wild Cherry Cream Taupe 100.0 0.139998 0.147723 0.000705 0.036004 0.085628 0.187318 0.775210 Chocolate Outer Gingersnap Rock and Rye Black 67.0 0.156160 0.110666 0.002846 0.074615 0.139572 0.241114 0.551898 Jelly Filled Gingersnap Kiwi Taupe 66.0 0.185662 0.132272 0.000014 0.086377 0.166340 0.247397 0.593016 Wild Cherry Cream Mango Taupe 53.0 0.166502 0.160090 0.001412 0.056970 0.108918 0.207306 0.787224 Q2.3 The Jelly Filled Conundrum \u00b6 Your boss notices the Jelly filled truffles are being produced much faster than the candy outer truffles and suggests expanding into this product line. What is your response? Use the visualization tool below to help you think about this problem, then create any visualizations or analyses of your own. sunburst charts Answer: Another case of Simpson's Paradox. The real boost is due to the sponge cake base cake type. It is simply that we have been producing more of the sponge cakes that are jelly filled. In fact, jelly filled has a slightly worse performance than chocolate outer when paired with sponge cake. This can be visually verified by switching the menu item in interact to observe the effect of Jelly Filled on all its constituent products def sun ( path = [[ 'base_cake' , 'truffle_type' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ], [ 'truffle_type' , 'base_cake' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ]]): fig = px . sunburst ( df , path = path , color = 'rate' , color_continuous_scale = 'viridis' , ) fig . update_layout ( margin = dict ( l = 20 , r = 20 , t = 20 , b = 20 ), height = 650 ) fig . show () interact ( sun ) interactive(children=(Dropdown(description='path', options=(['base_cake', 'truffle_type', 'primary_flavor', 's\u2026 <function __main__.sun(path=[['base_cake', 'truffle_type', 'primary_flavor', 'secondary_flavor', 'color_group'], ['truffle_type', 'base_cake', 'primary_flavor', 'secondary_flavor', 'color_group']])> This negative impact of Jelly filled is still obscured in the median analysis: moodsdf . loc [ moodsdf [ 'descriptor' ] == 'truffle_type' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 32 truffle_type Candy Outer 369.023049 3.054270e-82 0.310345 0.289822 0.249572 3352 [[1232, 3373], [2120, 2485]] 52 truffle_type Jelly Filled 95.107432 1.803284e-22 0.310345 0.414813 0.365576 3450 [[1952, 2653], [1498, 3107]] 55 truffle_type Chocolate Outer 105.424685 9.857421e-25 0.310345 0.427535 0.380527 2408 [[1421, 3184], [987, 3618]] So we must account for the affect of the sponge cake: fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = 'truffle_type' , y = 'rate' , data = df . loc [ ~ ( df [ 'base_cake' ] == 'Sponge' )]) print ( df . loc [ ~ ( df [ 'base_cake' ] == 'Sponge' )] . groupby ( 'truffle_type' )[[ 'rate' ]] . mean ()) rate truffle_type Candy Outer 0.289822 Chocolate Outer 0.320189 Jelly Filled 0.266877 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = 'truffle_type' , y = 'rate' , data = df . loc [( df [ 'base_cake' ] == 'Sponge' )]) print ( df . loc [( df [ 'base_cake' ] == 'Sponge' )] . groupby ( 'truffle_type' )[[ 'rate' ]] . mean ()) rate truffle_type Chocolate Outer 0.751005 Jelly Filled 0.692193","title":"SOLN L2 Inferential Statistics Data Hunt"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#data-science-foundations-lab-2-data-hunt-ii","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's the data hunt part TWO.","title":"Data Science Foundations  Lab 2: Data Hunt II"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#preparing-environment-and-importing-data","text":"","title":"Preparing Environment and Importing Data"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#import-packages","text":"! pip install - U plotly Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1) Collecting plotly Downloading plotly-5.1.0-py2.py3-none-any.whl (20.6 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.6 MB 1.3 MB/s \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0) Collecting tenacity>=6.2.0 Downloading tenacity-8.0.1-py3-none-any.whl (24 kB) Installing collected packages: tenacity, plotly Attempting uninstall: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-5.1.0 tenacity-8.0.1 # our standard libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns from ipywidgets import interact # our stats libraries import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy # our scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score","title":"Import Packages"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#import-and-clean-data","text":"df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_fundamentals/main/assets/truffle_rates.csv\" ) df = df . loc [ df [ 'rate' ] > 0 ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group rate 0 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.167097 1 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.153827 2 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.100299 3 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.333008 4 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.078108 df . shape (9210, 6)","title":"Import and Clean Data"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#exploratory-data-analysis","text":"","title":"Exploratory Data Analysis"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q1-finding-influential-features","text":"Which of the five features (base_cake, truffle_type, primary_flavor, secondary_flavor, color_group) of the truffles is most influential on production rate? Back your answer with both a visualization of the distributions (boxplot, kernel denisty estimate, histogram, violin plot) and a statistical test (moods median, ANOVA, t-test) Be sure: everything is labeled (can you improve your labels with additional descriptive statistical information e.g. indicate mean, std, etc.) you meet the assumptions of your statistical test","title":"Q1 Finding Influential Features"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q11-visualization","text":"Use any number of visualizations. Here is an example to get you started: # Example: a KDE of the truffle_type and base_cake columns fig , ax = plt . subplots ( 2 , 1 , figsize = ( 12 , 12 )) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'truffle_type' ], fill = True , ax = ax [ 0 ]) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'base_cake' ], fill = True , ax = ax [ 1 ]) <AxesSubplot:xlabel='rate', ylabel='Density'> fig , ax = plt . subplots ( 5 , 1 , figsize = ( 10 , 40 )) for idx , col in enumerate ( df . columns [: - 1 ]): df . boxplot ( by = col , column = 'rate' , ax = ax [ idx ]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 9 ( ) missing from current font. fig.canvas.print_figure(bytes_io, **kw)","title":"Q1.1 Visualization"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q12-statistical-analysis","text":"What statistical tests can you perform to evaluate your hypothesis from the visualizations (maybe you think one particular feature is significant). Here's an ANOVA on the truffle_type column to get you started: model = ols ( 'rate ~ C( {} )' . format ( 'truffle_type' ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Is this P value significant? What is the null hypothesis? How do we check the assumptions of ANOVA? Answer: All groups fail for normal distribution of residuals and homogeneity of variances. So we cannot use ANOVA with this data: confidence_level = 0.05 for idx , col in enumerate ( df . columns [: - 1 ]): model = ols ( 'rate ~ C( {} )' . format ( col ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) print ( \"Shapiro-Wilk\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( * stats . shapiro ( model . resid ))) if stats . shapiro ( model . resid ) . pvalue < confidence_level : shapiro_rej = True print ( f \" \\t reject: { shapiro_rej } \" ) print ( \"Bartlett\" ) gb = df . groupby ( col )[ 'rate' ] w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( w , pvalue )) if pvalue < confidence_level : shapiro_rej = True print ( f \" \\t reject: { shapiro_rej } \" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(base_cake) 331.373550 5.0 2349.684756 0.0 Residual 259.606073 9204.0 NaN NaN Shapiro-Wilk statistic=0.93, pvalue=0.00e+00 reject: True Bartlett statistic=619.37, pvalue=1.32e-131 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Shapiro-Wilk statistic=0.96, pvalue=1.37e-42 reject: True Bartlett statistic=533.02, pvalue=1.80e-116 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(primary_flavor) 159.105452 47.0 71.815842 0.0 Residual 431.874171 9162.0 NaN NaN Shapiro-Wilk statistic=0.97, pvalue=6.49e-38 reject: True Bartlett /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") statistic=1609.00, pvalue=1.85e-306 reject: True .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(secondary_flavor) 115.773877 28.0 79.884192 0.0 Residual 475.205747 9181.0 NaN NaN Shapiro-Wilk statistic=0.97, pvalue=4.34e-39 reject: True Bartlett statistic=1224.49, pvalue=3.55e-240 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(color_group) 33.878491 11.0 50.849974 1.873235e-109 Residual 557.101132 9198.0 NaN NaN Shapiro-Wilk statistic=0.96, pvalue=1.40e-44 reject: True Bartlett statistic=298.64, pvalue=1.69e-57 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") Instead we might use Moods Median","title":"Q1.2 Statistical Analysis"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q2-finding-best-and-worst-groups","text":"","title":"Q2 Finding Best and Worst Groups"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q21-compare-every-group-to-the-whole","text":"Of the primary flavors (feature), what 5 flavors (groups) would you recommend Truffletopia discontinue? Iterate through every level (i.e. pound, cheese, sponge cakes) of every category (i.e. base cake, primary flavor, secondary flavor) and use moods median testing to compare the group distribution to the grand median rate. moodsdf = pd . DataFrame () target = 'rate' for col in df . columns [: - 1 ]: for truff in df [ col ] . unique (): group = df . loc [ df [ col ] == truff ][ target ] pop = df . loc [ ~ ( df [ col ] == truff )][ target ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , table ]) . T ]) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'table' ] moodsdf [ 'p_value' ] = moodsdf [ 'p_value' ] . astype ( float ) print ( moodsdf . shape ) (98, 9) After you've computed a moods median test on every group, filter any data above a significance level of 0.05 moodsdf = moodsdf . loc [( moodsdf [ 'p_value' ] < confidence_level )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( moodsdf . shape ) (83, 9) Return the groups with the lowest median performance (your table need not look exactly like the one I've created) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 0 secondary_flavor Wild Cherry Cream 56.867453 4.661980e-14 0.310345 0.139998 0.085628 100 [[12, 4593], [88, 4517]] 1 primary_flavor Coconut 56.867453 4.661980e-14 0.310345 0.139998 0.085628 100 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.556345 4.302530e-15 0.310345 0.129178 0.092878 85 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.32026 7.846169e-13 0.310345 0.145727 0.095758 91 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.545249 4.142688e-11 0.310345 0.148964 0.10588 70 [[7, 4598], [63, 4542]] ... ... ... ... ... ... ... ... ... ... 78 secondary_flavor Vanilla 40.484134 1.982191e-10 0.310345 0.559808 0.664252 200 [[145, 4460], [55, 4550]] 79 primary_flavor Orange 60.723646 6.567649e-15 0.310345 0.580579 0.681157 200 [[155, 4450], [45, 4560]] 80 primary_flavor Plum 308.037116 5.845614e-69 0.310345 0.669126 0.681309 300 [[300, 4305], [0, 4605]] 81 primary_flavor Cheesecake 99.085851 2.417896e-23 0.310345 0.745813 0.689802 100 [[100, 4505], [0, 4605]] 82 base_cake Sponge 2107.437614 0.000000e+00 0.310345 0.711797 0.699679 1800 [[1774, 2831], [26, 4579]] 83 rows \u00d7 9 columns We would want to cut the following primary flavors. Check to see that you get a similar answer. rip wild cherry cream. ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] moodsdf . loc [ moodsdf [ 'descriptor' ] == 'primary_flavor' ][: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 1 primary_flavor Coconut 56.867453 4.661980e-14 0.310345 0.139998 0.085628 100 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.556345 4.302530e-15 0.310345 0.129178 0.092878 85 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.32026 7.846169e-13 0.310345 0.145727 0.095758 91 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.545249 4.142688e-11 0.310345 0.148964 0.10588 70 [[7, 4598], [63, 4542]] 10 primary_flavor Gingersnap 131.113519 2.338438e-30 0.310345 0.159268 0.143347 192 [[17, 4588], [175, 4430]]","title":"Q2.1 Compare Every Group to the Whole"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q22-beyond-statistical-testing-using-reasoning","text":"Let's look at the total profile of the products associated with the five worst primary flavors. Given the number of different products made with any of these flavors, would you alter your answer at all? Answer: We would opt to only discontinue the gingersnap, and wild cherry cream flavors. The other flavors, Pink Lemonade, Chocolate, and Coconut may be subject to Simpson's Paradox since we do not have adequate sampling of them with other categories # 1. filter df for only bottom five flavors # 2. groupby all columns besides rate # 3. describe the rate column. # by doing this we can evaluate just how much sampling variety we have for the # worst performing flavors. bottom_five = [ 'Coconut' , 'Pink Lemonade' , 'Chocolate' , 'Wild Cherry Cream' , 'Gingersnap' ] df . loc [ df [ 'primary_flavor' ] . isin ( bottom_five )] . groupby ( list ( df . columns [: - 1 ]))[ 'rate' ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max base_cake truffle_type primary_flavor secondary_flavor color_group Butter Jelly Filled Pink Lemonade Butter Rum Rose 85.0 0.129178 0.137326 0.000061 0.032887 0.092878 0.171350 0.860045 Chiffon Candy Outer Wild Cherry Cream Rock and Rye Olive 17.0 0.094287 0.059273 0.010464 0.053976 0.077098 0.120494 0.229933 Chocolate Outer Gingersnap Dill Pickle Burgundy 59.0 0.133272 0.080414 0.021099 0.069133 0.137972 0.172066 0.401387 Jelly Filled Chocolate Tutti Frutti Burgundy 91.0 0.145727 0.135230 0.000033 0.044847 0.095758 0.185891 0.586570 Pound Candy Outer Coconut Wild Cherry Cream Taupe 100.0 0.139998 0.147723 0.000705 0.036004 0.085628 0.187318 0.775210 Chocolate Outer Gingersnap Rock and Rye Black 67.0 0.156160 0.110666 0.002846 0.074615 0.139572 0.241114 0.551898 Jelly Filled Gingersnap Kiwi Taupe 66.0 0.185662 0.132272 0.000014 0.086377 0.166340 0.247397 0.593016 Wild Cherry Cream Mango Taupe 53.0 0.166502 0.160090 0.001412 0.056970 0.108918 0.207306 0.787224","title":"Q2.2 Beyond Statistical Testing: Using Reasoning"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q23-the-jelly-filled-conundrum","text":"Your boss notices the Jelly filled truffles are being produced much faster than the candy outer truffles and suggests expanding into this product line. What is your response? Use the visualization tool below to help you think about this problem, then create any visualizations or analyses of your own. sunburst charts Answer: Another case of Simpson's Paradox. The real boost is due to the sponge cake base cake type. It is simply that we have been producing more of the sponge cakes that are jelly filled. In fact, jelly filled has a slightly worse performance than chocolate outer when paired with sponge cake. This can be visually verified by switching the menu item in interact to observe the effect of Jelly Filled on all its constituent products def sun ( path = [[ 'base_cake' , 'truffle_type' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ], [ 'truffle_type' , 'base_cake' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ]]): fig = px . sunburst ( df , path = path , color = 'rate' , color_continuous_scale = 'viridis' , ) fig . update_layout ( margin = dict ( l = 20 , r = 20 , t = 20 , b = 20 ), height = 650 ) fig . show () interact ( sun ) interactive(children=(Dropdown(description='path', options=(['base_cake', 'truffle_type', 'primary_flavor', 's\u2026 <function __main__.sun(path=[['base_cake', 'truffle_type', 'primary_flavor', 'secondary_flavor', 'color_group'], ['truffle_type', 'base_cake', 'primary_flavor', 'secondary_flavor', 'color_group']])> This negative impact of Jelly filled is still obscured in the median analysis: moodsdf . loc [ moodsdf [ 'descriptor' ] == 'truffle_type' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 32 truffle_type Candy Outer 369.023049 3.054270e-82 0.310345 0.289822 0.249572 3352 [[1232, 3373], [2120, 2485]] 52 truffle_type Jelly Filled 95.107432 1.803284e-22 0.310345 0.414813 0.365576 3450 [[1952, 2653], [1498, 3107]] 55 truffle_type Chocolate Outer 105.424685 9.857421e-25 0.310345 0.427535 0.380527 2408 [[1421, 3184], [987, 3618]] So we must account for the affect of the sponge cake: fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = 'truffle_type' , y = 'rate' , data = df . loc [ ~ ( df [ 'base_cake' ] == 'Sponge' )]) print ( df . loc [ ~ ( df [ 'base_cake' ] == 'Sponge' )] . groupby ( 'truffle_type' )[[ 'rate' ]] . mean ()) rate truffle_type Candy Outer 0.289822 Chocolate Outer 0.320189 Jelly Filled 0.266877 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = 'truffle_type' , y = 'rate' , data = df . loc [( df [ 'base_cake' ] == 'Sponge' )]) print ( df . loc [( df [ 'base_cake' ] == 'Sponge' )] . groupby ( 'truffle_type' )[[ 'rate' ]] . mean ()) rate truffle_type Chocolate Outer 0.751005 Jelly Filled 0.692193","title":"Q2.3 The Jelly Filled Conundrum"},{"location":"solutions/SOLN_L3_Feature_Engineering/","text":"Data Science Foundations, Lab 3: Practice with Feature Engineering and Pipelines \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines and feature engineering. We will use the wine dataset. import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn.model_selection import train_test_split from statsmodels.stats.outliers_influence import variance_inflation_factor wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) On Wine Density \u00b6 L1 Q1: Feature Derivation \u00b6 Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 One-Hot encode categorical variables in the wine dataset # Code Cell for L1 Q1 display ( wine . head ()) print ( wine . shape ) str_cols = [ 'type' ] enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () X . pop ( 'type' ) y = X . pop ( 'density' ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) print ( y . shape ) print ( X . shape ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 (6497, 13) (6497,) (6497, 13) L1 Q2: Feature Transformation \u00b6 Use StandardScaler on the input data and evaluate how this affects VIF, kurtosis, and skew You should ignore the one-hot encoded column(s) for this section # Non-one hot encoded columns cols = list ( wine . columns ) cols . remove ( 'density' ) cols . remove ( 'type' ) # Code Cell for L1 Q2 kurt1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . kurt () skew1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . skew () vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( X [:, 2 :], i ) for i in range ( X [:, 2 :] . shape [ 1 ])] vif . index = cols vif [ \"kurtosis\" ] = kurt1 vif [ \"skew\" ] = skew1 vif .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 41.790949 5.070143 1.724131 volatile acidity 9.482732 2.834263 1.496433 citric acid 9.344218 2.404077 0.473142 residual sugar 3.336944 4.360399 1.435221 chlorides 5.398369 50.911457 5.400680 free sulfur dioxide 8.529778 7.906238 1.220066 total sulfur dioxide 13.448130 -0.371664 -0.001177 pH 149.003349 0.374743 0.387234 sulphates 18.402953 8.667071 1.799021 alcohol 114.836088 -0.531687 0.565718 quality 63.455488 0.232322 0.189623 scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) kurt1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . kurt () skew1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . skew () vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( X [:, 2 :], i ) for i in range ( X [:, 2 :] . shape [ 1 ])] vif . index = cols vif [ \"kurtosis\" ] = kurt1 vif [ \"skew\" ] = skew1 vif .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 1.781336 5.070143 1.724131 volatile acidity 1.808525 2.834263 1.496433 citric acid 1.606484 2.404077 0.473142 residual sugar 1.533403 4.360399 1.435221 chlorides 1.564413 50.911457 5.400680 free sulfur dioxide 2.156598 7.906238 1.220066 total sulfur dioxide 2.872586 -0.371664 -0.001177 pH 1.413100 0.374743 0.387234 sulphates 1.364157 8.667071 1.799021 alcohol 1.696986 -0.531687 0.565718 quality 1.408210 0.232322 0.189623 L1 Q3: Modeling \u00b6 Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using logistic regression: dependent variable: wine quality If you are using linear regression: dependent variable: wine density # Code Cell for L1 Q3 model = LinearRegression () X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.963') On Wine Quality \u00b6 L1 Q1: Feature Derivation \u00b6 Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 One-Hot encode categorical variables in the wine dataset # Code Cell for L1 Q1 display ( wine . head ()) print ( wine . shape ) str_cols = [ 'type' ] enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () X . pop ( 'type' ) y = X . pop ( 'quality' ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) print ( y . shape ) print ( X . shape ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 (6497, 13) (6497,) (6497, 13) L1 Q2: Feature Transformation \u00b6 Use StandardScaler on the input data and evaluate how this affects VIF, kurtosis, and skew You should ignore the one-hot encoded column(s) for this section scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) L1 Q3: Modeling \u00b6 Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using logistic regression: dependent variable: wine quality If you are using linear regression: dependent variable: wine density # Code Cell for L1 Q3 model = LogisticRegression ( max_iter = 1e4 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.60 0.07 0.12 46 5 0.58 0.61 0.59 420 6 0.52 0.68 0.59 579 7 0.44 0.19 0.26 221 8 0.00 0.00 0.00 32 accuracy 0.54 1300 macro avg 0.36 0.26 0.26 1300 weighted avg 0.51 0.54 0.50 1300 sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True ) <AxesSubplot:>","title":"SOLN L3 Feature Engineering"},{"location":"solutions/SOLN_L3_Feature_Engineering/#data-science-foundations-lab-3-practice-with-feature-engineering-and-pipelines","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines and feature engineering. We will use the wine dataset. import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn.model_selection import train_test_split from statsmodels.stats.outliers_influence import variance_inflation_factor wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" )","title":"Data Science Foundations, Lab 3: Practice with Feature Engineering and Pipelines"},{"location":"solutions/SOLN_L3_Feature_Engineering/#on-wine-density","text":"","title":"On Wine Density"},{"location":"solutions/SOLN_L3_Feature_Engineering/#l1-q1-feature-derivation","text":"Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 One-Hot encode categorical variables in the wine dataset # Code Cell for L1 Q1 display ( wine . head ()) print ( wine . shape ) str_cols = [ 'type' ] enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () X . pop ( 'type' ) y = X . pop ( 'density' ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) print ( y . shape ) print ( X . shape ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 (6497, 13) (6497,) (6497, 13)","title":"L1 Q1: Feature Derivation"},{"location":"solutions/SOLN_L3_Feature_Engineering/#l1-q2-feature-transformation","text":"Use StandardScaler on the input data and evaluate how this affects VIF, kurtosis, and skew You should ignore the one-hot encoded column(s) for this section # Non-one hot encoded columns cols = list ( wine . columns ) cols . remove ( 'density' ) cols . remove ( 'type' ) # Code Cell for L1 Q2 kurt1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . kurt () skew1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . skew () vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( X [:, 2 :], i ) for i in range ( X [:, 2 :] . shape [ 1 ])] vif . index = cols vif [ \"kurtosis\" ] = kurt1 vif [ \"skew\" ] = skew1 vif .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 41.790949 5.070143 1.724131 volatile acidity 9.482732 2.834263 1.496433 citric acid 9.344218 2.404077 0.473142 residual sugar 3.336944 4.360399 1.435221 chlorides 5.398369 50.911457 5.400680 free sulfur dioxide 8.529778 7.906238 1.220066 total sulfur dioxide 13.448130 -0.371664 -0.001177 pH 149.003349 0.374743 0.387234 sulphates 18.402953 8.667071 1.799021 alcohol 114.836088 -0.531687 0.565718 quality 63.455488 0.232322 0.189623 scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) kurt1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . kurt () skew1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . skew () vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( X [:, 2 :], i ) for i in range ( X [:, 2 :] . shape [ 1 ])] vif . index = cols vif [ \"kurtosis\" ] = kurt1 vif [ \"skew\" ] = skew1 vif .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 1.781336 5.070143 1.724131 volatile acidity 1.808525 2.834263 1.496433 citric acid 1.606484 2.404077 0.473142 residual sugar 1.533403 4.360399 1.435221 chlorides 1.564413 50.911457 5.400680 free sulfur dioxide 2.156598 7.906238 1.220066 total sulfur dioxide 2.872586 -0.371664 -0.001177 pH 1.413100 0.374743 0.387234 sulphates 1.364157 8.667071 1.799021 alcohol 1.696986 -0.531687 0.565718 quality 1.408210 0.232322 0.189623","title":"L1 Q2: Feature Transformation"},{"location":"solutions/SOLN_L3_Feature_Engineering/#l1-q3-modeling","text":"Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using logistic regression: dependent variable: wine quality If you are using linear regression: dependent variable: wine density # Code Cell for L1 Q3 model = LinearRegression () X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.963')","title":"L1 Q3: Modeling"},{"location":"solutions/SOLN_L3_Feature_Engineering/#on-wine-quality","text":"","title":"On Wine Quality"},{"location":"solutions/SOLN_L3_Feature_Engineering/#l1-q1-feature-derivation_1","text":"Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 One-Hot encode categorical variables in the wine dataset # Code Cell for L1 Q1 display ( wine . head ()) print ( wine . shape ) str_cols = [ 'type' ] enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () X . pop ( 'type' ) y = X . pop ( 'quality' ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) print ( y . shape ) print ( X . shape ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 (6497, 13) (6497,) (6497, 13)","title":"L1 Q1: Feature Derivation"},{"location":"solutions/SOLN_L3_Feature_Engineering/#l1-q2-feature-transformation_1","text":"Use StandardScaler on the input data and evaluate how this affects VIF, kurtosis, and skew You should ignore the one-hot encoded column(s) for this section scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :])","title":"L1 Q2: Feature Transformation"},{"location":"solutions/SOLN_L3_Feature_Engineering/#l1-q3-modeling_1","text":"Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using logistic regression: dependent variable: wine quality If you are using linear regression: dependent variable: wine density # Code Cell for L1 Q3 model = LogisticRegression ( max_iter = 1e4 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.60 0.07 0.12 46 5 0.58 0.61 0.59 420 6 0.52 0.68 0.59 579 7 0.44 0.19 0.26 221 8 0.00 0.00 0.00 32 accuracy 0.54 1300 macro avg 0.36 0.26 0.26 1300 weighted avg 0.51 0.54 0.50 1300 sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True ) <AxesSubplot:>","title":"L1 Q3: Modeling"},{"location":"solutions/SOLN_L4_Supervised_Learners/","text":"Data Science Foundations Lab 4: Practice with Supervised Learners \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines, feature engineering, and applying learning algorithms. Now that we have covered supervised learning methods, and we've covered Grid Search, we will use these tools to do a sophisticated, search of hyperparameter optimization. import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression , LinearRegression import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.model_selection import train_test_split from sklearn import metrics wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'quality' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ target ] = y to compare, here is our results performing classification on this set of data with just logistic regression: model = LogisticRegression ( max_iter = 1e4 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.60 0.07 0.12 46 5 0.58 0.61 0.59 420 6 0.52 0.68 0.59 579 7 0.44 0.19 0.26 221 8 0.00 0.00 0.00 32 accuracy 0.54 1300 macro avg 0.36 0.26 0.26 1300 weighted avg 0.51 0.54 0.50 1300 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) <AxesSubplot:> \ud83c\udfce\ufe0f Q1: \u00b6 Evaluate the performance of a Random Forest on classifying wine quality # Code Cell for L1 Q2 model = RandomForestClassifier () X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 1.00 0.15 0.26 46 5 0.72 0.77 0.75 420 6 0.67 0.78 0.72 579 7 0.71 0.51 0.59 221 8 1.00 0.22 0.36 32 accuracy 0.70 1300 macro avg 0.68 0.41 0.45 1300 weighted avg 0.71 0.70 0.68 1300 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) <AxesSubplot:> \ud83d\udd2c Q2: \u00b6 Do a grid search to optimize your Random Forest model, use whatever hyperparameters you would like RandomForestClassifier () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Code Cell for L1 Q3 from sklearn.model_selection import GridSearchCV param_grid = { 'bootstrap' : [ True , False ], 'criterion' : [ 'gini' , 'entropy' ], 'min_samples_split' : [ 2 , 4 , 6 ], 'min_samples_leaf' : [ 1 , 3 , 5 ], 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'class_weight' : [ 'balanced' , 'balanced_subsample' , None ]} grid = GridSearchCV ( RandomForestClassifier ( n_jobs =- 1 ), param_grid , cv = 7 ) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=7. warnings.warn( {'bootstrap': True, 'class_weight': None, 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2} model = grid . best_estimator_ model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.75 0.13 0.22 46 5 0.70 0.76 0.73 420 6 0.66 0.78 0.72 579 7 0.73 0.48 0.58 221 8 1.00 0.25 0.40 32 accuracy 0.69 1300 macro avg 0.64 0.40 0.44 1300 weighted avg 0.70 0.69 0.67 1300 <AxesSubplot:>","title":"SOLN L4 Supervised Learners"},{"location":"solutions/SOLN_L4_Supervised_Learners/#data-science-foundations-lab-4-practice-with-supervised-learners","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines, feature engineering, and applying learning algorithms. Now that we have covered supervised learning methods, and we've covered Grid Search, we will use these tools to do a sophisticated, search of hyperparameter optimization. import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression , LinearRegression import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.model_selection import train_test_split from sklearn import metrics wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'quality' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ target ] = y to compare, here is our results performing classification on this set of data with just logistic regression: model = LogisticRegression ( max_iter = 1e4 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.60 0.07 0.12 46 5 0.58 0.61 0.59 420 6 0.52 0.68 0.59 579 7 0.44 0.19 0.26 221 8 0.00 0.00 0.00 32 accuracy 0.54 1300 macro avg 0.36 0.26 0.26 1300 weighted avg 0.51 0.54 0.50 1300 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) <AxesSubplot:>","title":"Data Science Foundations  Lab 4: Practice with Supervised Learners"},{"location":"solutions/SOLN_L4_Supervised_Learners/#q1","text":"Evaluate the performance of a Random Forest on classifying wine quality # Code Cell for L1 Q2 model = RandomForestClassifier () X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 1.00 0.15 0.26 46 5 0.72 0.77 0.75 420 6 0.67 0.78 0.72 579 7 0.71 0.51 0.59 221 8 1.00 0.22 0.36 32 accuracy 0.70 1300 macro avg 0.68 0.41 0.45 1300 weighted avg 0.71 0.70 0.68 1300 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) <AxesSubplot:>","title":"\ud83c\udfce\ufe0f Q1:"},{"location":"solutions/SOLN_L4_Supervised_Learners/#q2","text":"Do a grid search to optimize your Random Forest model, use whatever hyperparameters you would like RandomForestClassifier () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Code Cell for L1 Q3 from sklearn.model_selection import GridSearchCV param_grid = { 'bootstrap' : [ True , False ], 'criterion' : [ 'gini' , 'entropy' ], 'min_samples_split' : [ 2 , 4 , 6 ], 'min_samples_leaf' : [ 1 , 3 , 5 ], 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'class_weight' : [ 'balanced' , 'balanced_subsample' , None ]} grid = GridSearchCV ( RandomForestClassifier ( n_jobs =- 1 ), param_grid , cv = 7 ) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=7. warnings.warn( {'bootstrap': True, 'class_weight': None, 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2} model = grid . best_estimator_ model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.75 0.13 0.22 46 5 0.70 0.76 0.73 420 6 0.66 0.78 0.72 579 7 0.73 0.48 0.58 221 8 1.00 0.25 0.40 32 accuracy 0.69 1300 macro avg 0.64 0.40 0.44 1300 weighted avg 0.70 0.69 0.67 1300 <AxesSubplot:>","title":"\ud83d\udd2c Q2:"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/","text":"Data Science Foundations Lab 5: Writing Unit Tests \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab, we will try our hand at writing unit tests Import Libraries \u00b6 import random import numpy as np from contextlib import contextmanager import sys , os @contextmanager def suppress_stdout (): with open ( os . devnull , \"w\" ) as devnull : old_stdout = sys . stdout sys . stdout = devnull try : yield finally : sys . stdout = old_stdout Types of Tests \u00b6 There are two main types of tests we want to distinguish: * Unit test : an automatic test to test the internal workings of a class or function. It should be a stand-alone test which is not related to other resources. * Integration test : an automatic test that is done on an environment, it tests the coordination of different classes and functions as well as with the running environment. This usually precedes sending code to a QA team. To this I will add: Acid test : extremely rigorous tests that push beyond the intended use cases for your classes/functions. Written when you, like me, cannot afford QA employees to actually test your code. (word origin: gold acid tests in the 1850s , acid tests in the 70's ) EDIT : you could also call this a corner, or an edge case In this lab we will focus on unit tests . Unit Tests \u00b6 Each unit test should test the smallest portion of your code possible, i.e. a single method or function. Any random number generators should be seeded so that they run the exact same way every time. Unit tests should not rely on any local files or the local environment. Why bother with Unit Tests when we have Integration tests? A major challenge with integration testing is when an integration test fails. It\u2019s very hard to diagnose a system issue without being able to isolate which part of the system is failing. Here comes the unit test to the rescue. Let's take a simple example. If I wanted to test that the sume of two numbers is correct assert sum ([ 2 , 5 ]) == 7 , \"should be 7\" Nothing is sent to the print out because the condition is satisfied. If we run, however: assert sum([2, 4]) == 7, \"should be 7\" we get an error message: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-3-d5724b127818> in <module>() ----> 1 assert sum([2, 4]) == 7, \"should be 7\" AssertionError: should be 7 To make this a Unit Test, you will want to wrap it in a function def test_sum (): assert sum ([ 1 , 2 , 3 ]) == 6 , \"Should be 6\" test_sum () print ( \"Everything passed\" ) Everything passed And if we include a test that does not pass: def test_sum(): assert sum([3, 3]) == 6, \"Should be 6\" def test_my_broken_func(): assert sum([1, 2]) == 5, \"Should be 5\" test_sum() test_my_broken_func() print(\"Everything passed\") Here our test fails, because the sum of 1 and 2 is 3 and not 5. We get a traceback that tells us the source of the error: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-13-8a552fbf52bd> in <module>() 6 7 test_sum() ----> 8 test_my_broken_func() 9 print(\"Everything passed\") <ipython-input-13-8a552fbf52bd> in test_my_broken_func() 3 4 def test_my_broken_func(): ----> 5 assert sum([1, 2]) == 5, \"Should be 5\" 6 7 test_sum() AssertionError: Should be 5 Before sending us on our merry way to practice writing unit tests, we will want to ask, what do I want to write a test about? Here, we've been testing sum(). There are many behaviors in sum() we could check, such as: Does it sum a list of whole numbers (integers)? Can it sum a tuple or set? Can it sum a list of floats? What happens if one of the numbers is negative? etc.. In the end, what you test is up to you, and depends on your intended use cases. As a general rule of thumb, your unit test should test what is relevant. The only caveat to that, is that many continuous integration services (like TravisCI ) will benchmark you based on the percentage of lines of code you have that are covered by your unit tests (ex: 85% coverage ). \u270d\ud83c\udffd Q1 Write a Unit Test \u00b6 Remember our Pokeball discussion in Python Foundations ? We'll return to that here. This time writing unit tests for our classes. Sometimes when writing unit tests, it can be more complicated than checking the return value of a function. Think back on our pokemon example: class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None If I wanted to write a unit test for the release method, I couldn't directly check for the output of a function. I'll have to check for a side effect , in this case, the change of an attribute belonging to a pokeball object; that is the change to the attribute contains . class Pokeball : def __init__ ( self , contains = None , type_name = \"poke ball\" ): self . contains = contains self . type_name = type_name self . catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch ( self , pokemon ): if self . contains == None : if random . random () < self . catch_rate : self . contains = pokemon print ( f \" { pokemon } captured!\" ) else : print ( f \" { pokemon } escaped!\" ) pass else : print ( \"pokeball is not empty!\" ) def release ( self ): if self . contains == None : print ( \"Pokeball is already empty\" ) else : print ( self . contains , \"has been released\" ) self . contains = None In the following cell, finish the code to test the functionality of the release method: def test_release (): ball = Pokeball () ball . contains = 'Pikachu' ball . release () # turn the pseudo code below into an assert statement ### YOUR CODE HERE ### assert ball . contains == None , \"ball is not empty!\" test_release () Pikachu has been released \u26f9\ufe0f Q2 Write a Unit Test for the Catch Rate \u00b6 First, we will check that the succcessful catch is operating correctly. Remember that we depend on random.random and condition our success on whether that random value is less than the catch_rate of the pokeball: if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon so to test whether the successful catch is working we will seed our random number generator with a value that returns less than the catch_rate of the pokeball and then write our assert statement: def test_successful_catch (): # choose a random seed such that # we know the catch call should succeed ### YOUR CODE BELOW ### random . seed ( 1 ) ball = Pokeball () ball . catch ( 'Psyduck' ) # Someone's fave pokemon (bless 'em) ### YOUR CODE BELOW ### assert ball . contains == 'Psyduck' , \"ball did not catch as expected\" NICE. Now we will do the same thing again, this time testing for an unsuccessful catch. SO in order to do this, we need to choose a random seed that will cause our catch to fail: def test_unsuccessful_catch (): # choose a random seed such that # we know the catch call should FAIL ### YOUR CODE BELOW ### random . seed ( 0 ) ball = Pokeball () ball . catch ( 'Psyduck' ) ### YOUR CODE BELOW ### assert ball . contains == None , \"ball did not fail as expected\" When you are finished test your functions below test_unsuccessful_catch () Psyduck escaped! test_successful_catch () Psyduck captured! \u2696\ufe0f Q3 Write a Unit Test that Checks Whether the Overall Catch Rate is 50/50 \u00b6 For this one, we're going to take those same ideas around seeding the random number generator. However, here we'd like to run the catch function multiple times to check whether it is truly creating a 50/50 catch rate situation. Here's a pseudo code outline: seed the random number generator for 100 iterations: create a pokeball try to catch something log whether it was successful check that for the 100 attempts the success was approximately 50/50 note: you can use my suppress stdout() function to suppress the print statements from ball.catch ex: with suppress_stdout(): print(\"HELLO OUT THERE!\") quick segway : what is the actual behavior of random.seed() ? Does it produce the same number every time we call random.random() now? Check for yourself: random . seed ( 42 ) [ random . random () for i in range ( 5 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124] We see that it still produces random numbers with each call to random.random . However, those numbers are the same with every execution of the cell. What happens when we do this: [ random . random () for i in range ( 5 )] [0.5449414806032167, 0.2204406220406967, 0.5892656838759087, 0.8094304566778266, 0.006498759678061017] The numbers are different. BUT: random . seed ( 42 ) [ random . random () for i in range ( 10 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124, 0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] We see them here in the bottom half of the list again. So, random.seed() is seeding the random number generator such that it will produce the same sequence of random numbers every time from the given seed. This will reset whenever random.seed() is set again. This behavior is useful because it allows us to continue using random number generation in our code, (for testing, creating examples and demos, etc.) but it will be reproducable each time. End Segway # 1. seed the random number generator # 2. for 100 iterations: # * create a pokeball # * try to catch something # * log whether it was successful # 3. check that for the 100 attempts the success was approximately 50/50 def test_catch_rate (): ### YOUR CODE HERE ### results = 0 random . seed ( 42 ) for i in range ( 100 ): ball = Pokeball () with suppress_stdout (): ball . catch ( \"Charzard\" ) if ball . contains != None : results += 1 results = results / 100 ### END YOUR CODE ### assert np . abs ( np . mean ( results ) - 0.5 ) < 0.1 , \"catch rate not 50/50\" test_catch_rate () Test Runners \u00b6 When we start to create many tests like this, it can be cumbersome to run them all at once and log which ones fail. To handle our unit tests we use what are called test runners . We won't dedicate time to any single one here but the three most common are: unittest nose2 pytest unittest is built into python. I don't like it because you have to follow a strict class/method structure when writing the tests. nose2 is popular with many useful features and is generally good for high volumes of tests. My favorite is pytest, it's flexible and has an ecosystem of plugins for extensibility.","title":"SOLN L5 Writing Unit Tests"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#data-science-foundations-lab-5-writing-unit-tests","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab, we will try our hand at writing unit tests","title":"Data Science Foundations  Lab 5: Writing Unit Tests"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#import-libraries","text":"import random import numpy as np from contextlib import contextmanager import sys , os @contextmanager def suppress_stdout (): with open ( os . devnull , \"w\" ) as devnull : old_stdout = sys . stdout sys . stdout = devnull try : yield finally : sys . stdout = old_stdout","title":"Import Libraries"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#types-of-tests","text":"There are two main types of tests we want to distinguish: * Unit test : an automatic test to test the internal workings of a class or function. It should be a stand-alone test which is not related to other resources. * Integration test : an automatic test that is done on an environment, it tests the coordination of different classes and functions as well as with the running environment. This usually precedes sending code to a QA team. To this I will add: Acid test : extremely rigorous tests that push beyond the intended use cases for your classes/functions. Written when you, like me, cannot afford QA employees to actually test your code. (word origin: gold acid tests in the 1850s , acid tests in the 70's ) EDIT : you could also call this a corner, or an edge case In this lab we will focus on unit tests .","title":"Types of Tests"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#unit-tests","text":"Each unit test should test the smallest portion of your code possible, i.e. a single method or function. Any random number generators should be seeded so that they run the exact same way every time. Unit tests should not rely on any local files or the local environment. Why bother with Unit Tests when we have Integration tests? A major challenge with integration testing is when an integration test fails. It\u2019s very hard to diagnose a system issue without being able to isolate which part of the system is failing. Here comes the unit test to the rescue. Let's take a simple example. If I wanted to test that the sume of two numbers is correct assert sum ([ 2 , 5 ]) == 7 , \"should be 7\" Nothing is sent to the print out because the condition is satisfied. If we run, however: assert sum([2, 4]) == 7, \"should be 7\" we get an error message: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-3-d5724b127818> in <module>() ----> 1 assert sum([2, 4]) == 7, \"should be 7\" AssertionError: should be 7 To make this a Unit Test, you will want to wrap it in a function def test_sum (): assert sum ([ 1 , 2 , 3 ]) == 6 , \"Should be 6\" test_sum () print ( \"Everything passed\" ) Everything passed And if we include a test that does not pass: def test_sum(): assert sum([3, 3]) == 6, \"Should be 6\" def test_my_broken_func(): assert sum([1, 2]) == 5, \"Should be 5\" test_sum() test_my_broken_func() print(\"Everything passed\") Here our test fails, because the sum of 1 and 2 is 3 and not 5. We get a traceback that tells us the source of the error: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-13-8a552fbf52bd> in <module>() 6 7 test_sum() ----> 8 test_my_broken_func() 9 print(\"Everything passed\") <ipython-input-13-8a552fbf52bd> in test_my_broken_func() 3 4 def test_my_broken_func(): ----> 5 assert sum([1, 2]) == 5, \"Should be 5\" 6 7 test_sum() AssertionError: Should be 5 Before sending us on our merry way to practice writing unit tests, we will want to ask, what do I want to write a test about? Here, we've been testing sum(). There are many behaviors in sum() we could check, such as: Does it sum a list of whole numbers (integers)? Can it sum a tuple or set? Can it sum a list of floats? What happens if one of the numbers is negative? etc.. In the end, what you test is up to you, and depends on your intended use cases. As a general rule of thumb, your unit test should test what is relevant. The only caveat to that, is that many continuous integration services (like TravisCI ) will benchmark you based on the percentage of lines of code you have that are covered by your unit tests (ex: 85% coverage ).","title":"Unit Tests"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#q1-write-a-unit-test","text":"Remember our Pokeball discussion in Python Foundations ? We'll return to that here. This time writing unit tests for our classes. Sometimes when writing unit tests, it can be more complicated than checking the return value of a function. Think back on our pokemon example: class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None If I wanted to write a unit test for the release method, I couldn't directly check for the output of a function. I'll have to check for a side effect , in this case, the change of an attribute belonging to a pokeball object; that is the change to the attribute contains . class Pokeball : def __init__ ( self , contains = None , type_name = \"poke ball\" ): self . contains = contains self . type_name = type_name self . catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch ( self , pokemon ): if self . contains == None : if random . random () < self . catch_rate : self . contains = pokemon print ( f \" { pokemon } captured!\" ) else : print ( f \" { pokemon } escaped!\" ) pass else : print ( \"pokeball is not empty!\" ) def release ( self ): if self . contains == None : print ( \"Pokeball is already empty\" ) else : print ( self . contains , \"has been released\" ) self . contains = None In the following cell, finish the code to test the functionality of the release method: def test_release (): ball = Pokeball () ball . contains = 'Pikachu' ball . release () # turn the pseudo code below into an assert statement ### YOUR CODE HERE ### assert ball . contains == None , \"ball is not empty!\" test_release () Pikachu has been released","title":"\u270d\ud83c\udffd Q1 Write a Unit Test"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#q2-write-a-unit-test-for-the-catch-rate","text":"First, we will check that the succcessful catch is operating correctly. Remember that we depend on random.random and condition our success on whether that random value is less than the catch_rate of the pokeball: if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon so to test whether the successful catch is working we will seed our random number generator with a value that returns less than the catch_rate of the pokeball and then write our assert statement: def test_successful_catch (): # choose a random seed such that # we know the catch call should succeed ### YOUR CODE BELOW ### random . seed ( 1 ) ball = Pokeball () ball . catch ( 'Psyduck' ) # Someone's fave pokemon (bless 'em) ### YOUR CODE BELOW ### assert ball . contains == 'Psyduck' , \"ball did not catch as expected\" NICE. Now we will do the same thing again, this time testing for an unsuccessful catch. SO in order to do this, we need to choose a random seed that will cause our catch to fail: def test_unsuccessful_catch (): # choose a random seed such that # we know the catch call should FAIL ### YOUR CODE BELOW ### random . seed ( 0 ) ball = Pokeball () ball . catch ( 'Psyduck' ) ### YOUR CODE BELOW ### assert ball . contains == None , \"ball did not fail as expected\" When you are finished test your functions below test_unsuccessful_catch () Psyduck escaped! test_successful_catch () Psyduck captured!","title":"\u26f9\ufe0f Q2 Write a Unit Test for the Catch Rate"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#q3-write-a-unit-test-that-checks-whether-the-overall-catch-rate-is-5050","text":"For this one, we're going to take those same ideas around seeding the random number generator. However, here we'd like to run the catch function multiple times to check whether it is truly creating a 50/50 catch rate situation. Here's a pseudo code outline: seed the random number generator for 100 iterations: create a pokeball try to catch something log whether it was successful check that for the 100 attempts the success was approximately 50/50 note: you can use my suppress stdout() function to suppress the print statements from ball.catch ex: with suppress_stdout(): print(\"HELLO OUT THERE!\") quick segway : what is the actual behavior of random.seed() ? Does it produce the same number every time we call random.random() now? Check for yourself: random . seed ( 42 ) [ random . random () for i in range ( 5 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124] We see that it still produces random numbers with each call to random.random . However, those numbers are the same with every execution of the cell. What happens when we do this: [ random . random () for i in range ( 5 )] [0.5449414806032167, 0.2204406220406967, 0.5892656838759087, 0.8094304566778266, 0.006498759678061017] The numbers are different. BUT: random . seed ( 42 ) [ random . random () for i in range ( 10 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124, 0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] We see them here in the bottom half of the list again. So, random.seed() is seeding the random number generator such that it will produce the same sequence of random numbers every time from the given seed. This will reset whenever random.seed() is set again. This behavior is useful because it allows us to continue using random number generation in our code, (for testing, creating examples and demos, etc.) but it will be reproducable each time. End Segway # 1. seed the random number generator # 2. for 100 iterations: # * create a pokeball # * try to catch something # * log whether it was successful # 3. check that for the 100 attempts the success was approximately 50/50 def test_catch_rate (): ### YOUR CODE HERE ### results = 0 random . seed ( 42 ) for i in range ( 100 ): ball = Pokeball () with suppress_stdout (): ball . catch ( \"Charzard\" ) if ball . contains != None : results += 1 results = results / 100 ### END YOUR CODE ### assert np . abs ( np . mean ( results ) - 0.5 ) < 0.1 , \"catch rate not 50/50\" test_catch_rate ()","title":"\u2696\ufe0f Q3 Write a Unit Test that Checks Whether the Overall Catch Rate is 50/50"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#test-runners","text":"When we start to create many tests like this, it can be cumbersome to run them all at once and log which ones fail. To handle our unit tests we use what are called test runners . We won't dedicate time to any single one here but the three most common are: unittest nose2 pytest unittest is built into python. I don't like it because you have to follow a strict class/method structure when writing the tests. nose2 is popular with many useful features and is generally good for high volumes of tests. My favorite is pytest, it's flexible and has an ecosystem of plugins for extensibility.","title":"Test Runners"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/","text":"Data Science Foundations, Project Part 1: Statistical Analysis \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to perform statistical analysis on data generated from our tictactoe program! 1.0 Preparing Environment and Importing Data \u00b6 back to top 1.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 1.0.2 Load Dataset \u00b6 back to top data = {} for i in range ( 1000 ): game = GameEngine () game . setup_game () board = game . play_game () data [ 'game {} ' . format ( i )] = { 'board' : board . board , 'winner' : board . winner , 'starting player' : board . start_player } | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | |X|X| | | | | |O|O| | |O|X|X| | | | | |O|O| | |O|X|X| | |X| | |O|O| | 'O' Won! |O|X|X| | |X| | |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X|X| | |O| | | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |O| | | | |X|X| |O| | | |O| | | 'O' Won! |O|X|X| |O| | | |O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |O| | |O| | |X| |X| | | |O| | |O| | |X| |X| | | |O| |O|O| | |X| |X| | |X|O| |O|O| | |X|O|X| | |X|O| |O|O| | |X|O|X| |X|X|O| |O|O| | 'O' Won! |X|O|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| |O| |O| 'X' Won! |X|X|X| | | |O| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| |X|O| | | | |O| |O|X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X|O| | |X|O|O| | | | | |X|O|X| |X|O|O| | | |O| |X|O|X| 'X' Won! |X|O|O| | |X|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X|X| |O|O| | |O| | | | |X|X| 'X' Won! |O|O| | |O| | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| | | | |O|X| |X| |O| |X| | | | |O|X| |X| |O| |X| | | |O|O|X| 'X' Won! |X| |O| |X|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X|O|X| | |O| | | |X| | |X|O|X| |O|O| | | |X| | |X|O|X| |O|O| | |X|X| | |X|O|X| 'O' Won! |O|O|O| |X|X| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X| |X| | | | | |X|O|O| |X|O|X| | |X| | |X|O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| |X| |X| | |X|O| |O|O|X| |X| |X| |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| | |O| | |X|X| | |X|O|O| |O|O| | |X|X| | |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | |X| | | |O|X|O| | | |O| |X| | | |O|X|O| |X| |O| |X| | | |O|X|O| 'O' Won! |X| |O| |X| |O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | |X|O| | |X| | |X|O| | |O|X|O| | |X| | |X|O|X| |O|X|O| 'O' Won! | |X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| |O| |X| | | | | |X| |O| |O|X|X| | | | | |X| |O| |O|X|X| |O| | | |X| |O| 'X' Won! |O|X|X| |O|X| | |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X|X| | |X| |O| 'O' Won! | | |O| |X|X|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| |X|O| | | | | | | |X|O| |X|O| | | | |O| | |X|O| |X|O|X| | | |O| | |X|O| |X|O|X| | |O|O| | |X|O| |X|O|X| |X|O|O| 'O' Won! |O|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O| |O| |X|X| | |X|O|O| |O| |O| |X|X| | |X|O|O| |O|X|O| |X|X| | 'O' Won! |X|O|O| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | |X| | |O|X|O| | |O| | | |X| | |O|X|O| | |O| | |X|X| | |O|X|O| | |O| | |X|X|O| |O|X|O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | |O| | |X| |X| 'X' Won! |O| |O| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X|X| | |O| |O| | | | | |X|X| | |O| |O| | | |O| |X|X| | |O| |O| |X| |O| 'O' Won! |X|X|O| |O| |O| |X| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X|O| | | | | | |O|O|X| |X|O|X| | | | | |O|O|X| |X|O|X| |O| | | |O|O|X| |X|O|X| |O|X| | 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | |O| | | |X| | |O|X| |X| |O| | | |X| |O|O|X| |X| |O| | | |X| |O|O|X| |X| |O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | |X| | |O| |O| |X|O| | | |X| | |O| |O| |X|O| | |X|X| | |O| |O| |X|O|O| |X|X| | 'X' Won! |O| |O| |X|O|O| |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | | | | | | |X|X| |O|O| | | | |X| | |X|X| |O|O| | | |O|X| 'X' Won! |X|X|X| |O|O| | | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| | | |O| | | |X| |X|O|O| |X| |O| | |O|X| |X|O|O| |X| |O| 'X' Won! |X|O|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O| |O|X|O| | | | | 'X' Won! | |X|O| |O|X|O| | |X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X|O| | | |O|X| | | |O| |X|O| | | |O|X| | |X|O| |X|O| | |O|O|X| | |X|O| |X|O| | |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| |X|O| | |X|X|O| | |O|O| |X|O| | |X|X|O| | |O|O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X|X| | |O|O| | | | |O| |X|X| | |O|O| | |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | |O|O| | | |X| |X| | | |X|O|O| | | |X| |X| | | |X|O|O| | |O|X| 'X' Won! |X| | | |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O| | 'O' Won! |X|X| | |O|O|O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | |O| |X| |O|X|O| | | | | |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |O|X| 'X' Won! |O| |X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X|X| |X| | | | | |O| |O|X|X| |X| | | | |O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X| |O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X| |O| | |O|X| |O| | | |X| |O| |X|O|X| |O| | | |X|O|O| |X|O|X| |O| |X| |X|O|O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | | |X| | |X| |O| | |O| | |O|X| | |X| |O| | |O| | |O|X|X| |X| |O| | |O| | |O|X|X| |X|O|O| 'X' Won! | |O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | 'O' Won! |X| |X| | | | | |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | | | | |X|O|X| |O| |O| | | | | |X|O|X| |O| |O| | | |X| |X|O|X| |O| |O| | |O|X| |X|O|X| |O| |O| |X|O|X| 'O' Won! |X|O|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X|X| | |O| | |O| |O| | |X|X| 'X' Won! | |O| | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | |O|O| | | |X|X| | |O| | |O|O| | | |X|X| |X|O| | |O|O| | | |X|X| |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| |X| | | | |X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| |X| |O|X|O| 'O' Won! | |X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | |X|O| | | |X| |X| |O| |O|X|O| | |X|X| |X| |O| |O|X|O| | |X|X| |X|O|O| |O|X|O| 'X' Won! |X|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | |O|X| | | |O|X| | |X| | |O|X| | | |O|X| | |X| | |O|X|O| | |O|X| | |X|X| |O|X|O| | |O|X| |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| |O| |O| |X|X|O| |X| |X| |O| |O| 'O' Won! |X|X|O| |X| |X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| |O| | | |X| | | |O| |X|X|O| | | |X| | | |O| |X|X|O| | | |X| |O| |O| |X|X|O| | |X|X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X|X| |X|O| | |O|O| | | |X|X| |X|O| | |O|O| | | |X|X| |X|O|X| 'O' Won! |O|O|O| | |X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| | | |X| |O|O| | | |X|O| | |X|X| |O|O| | | |X|O| 'O' Won! |O|X|X| |O|O| | | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O| | 'X' Won! |X|O|X| | |X|O| |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O| |O| | |X| | | | |X| 'O' Won! |O|O|O| | |X| | | | |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |O| | | |O|X|X| | | | | |O| | | |O|X|X| | | | | |O| |O| |O|X|X| | |X| | |O| |O| 'O' Won! |O|X|X| |O|X| | |O| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X|O| | |X| | |O| | | |O|X|O| |X|X| | |O|O| | |O|X|O| |X|X| | 'X' Won! |O|O|X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X|O|X| | | |X| |O| |O| |X|O|X| | |O|X| |O| |O| |X|O|X| |X|O|X| |O| |O| |X|O|X| 'O' Won! |X|O|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| | | |X|X|O| |O|O|X| |X| |O| |X|X|O| 'X' Won! |O|O|X| |X|X|O| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O|X| | | |X| |O|X|O| | |O|X| 'X' Won! | | |X| |O|X|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | |X|O| | | |X| |O|O| | | |X|O| | | |X| |O|O|X| | |X|O| | | |X| |O|O|X| |O|X|O| | | |X| |O|O|X| |O|X|O| | |X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | |X| | | |O|X|O| |X| | | |X| |O| |O|X|O| |X| | | |X|X|O| |O|X|O| |X| | | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | |O|O|X| | | |X| |X| | | |O|O|X| |O| |X| |X| | | |O|O|X| |O|X|X| |X| | | |O|O|X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | | |O| | | |O| |X|O|X| | |X|O| | | |O| |X|O|X| | |X|O| |O| |O| |X|O|X| |X|X|O| |O| |O| 'O' Won! |X|O|X| |X|X|O| |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O| |O| |O|X|X| | |X| | |O| |O| |O|X|X| | |X|O| |O| |O| 'X' Won! |O|X|X| | |X|O| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | |O|X| |X|O| | | | |X| | |O|X| |X|O| | | | |X| | |O|X| |X|O|O| |X| |X| | |O|X| |X|O|O| 'O' Won! |X|O|X| | |O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | |O| |X| |X| | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| | |O|O| 'X' Won! |X| |O| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | |X|O| | | | |O| |X| | | |X|O| | | | |O| |X| |O| |X|O| | | | |O| |X|X|O| |X|O| | | |O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X| | | 'O' Won! | |X| | |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X| |O| |X| | | |O| |O| |X| |O| |X| | | |O|X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | |O| |X| | |O| | |O|X| | |O| |X| | |O|X| |O|X| | 'O' Won! |O| |X| |O|O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| |O| | | |X| | |O|O| |X|X|O| | | |X| | |O|O| 'O' Won! |X|X|O| | | |X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X| |X| |X|O|O| | |O| | |X| |X| 'X' Won! |X|O|O| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | | |O| | | 'O' Won! |X| |O| |X|O| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | |O| | |O|X|X| | |X| | | |O| | |O|X|X| | |X| | | |O|O| |O|X|X| | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| |O| |X| | |O| | | |O|X| 'X' Won! |O| |X| | |O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | |X|X| | |O| |X| |O| | | |X|X| | |O|O|X| 'X' Won! |O| | | |X|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | | | | |X|O| 'X' Won! | |X|O| | |X| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | |O| |X| |O|X|O| | | | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | |O| | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |O|X| | |O| | | | |X|O| |O|X|X| |O| | | 'O' Won! |O|X|O| |O|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| |X|X| | | |O|O| | | |X| 'O' Won! |X|X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O| |O| |X|O| | |X|X| | |O| |O| |X|O|O| |X|X| | |O| |O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | | |X| | |O|X|O| | | | | |X|X| | |O|X|O| | | | | |X|X| | |O|X|O| | |O| | 'X' Won! |X|X|X| |O|X|O| | |O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | | | |X|X| | 'O' Won! |O|O|O| | | | | |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| | | | | |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| | |X| |O| |O|X| | |X|O| | 'X' Won! |X| |O| |O|X| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O| |X| | | |O| |O|X| | |O| |X| | | |O| |O|X|X| |O| |X| | | |O| 'O' Won! |O|X|X| |O| |X| |O| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| |X| |X| | | | | 'O' Won! |O|O|O| |X| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | |X| | | |X|O|O| |O|X| | |X| | | |X|O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X| |O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | | |X| |O|O| | |X| | | | |X|X| |O|O| | |X| | | | |X|X| |O|O| | |X|O| | 'X' Won! |X|X|X| |O|O| | |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | |X| | 'O' Won! |O|O|O| | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O|O| |X| |O| |X| |X| | |O|O| |X| |O| |X|O|X| | |O|O| 'X' Won! |X| |O| |X|O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | |X|X| | |O| | |O| | | | |X|X| |O|O| | |O| | | | |X|X| |O|O|X| |O|O| | | |X|X| |O|O|X| 'X' Won! |O|O| | |X|X|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | |X| | |O| |O| | |O|X| | |X| | |O| |O| | |O|X| | |X|X| 'O' Won! |O|O|O| | |O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| | |X| | |X|O|O| | | |O| | |X|X| |X|O|O| | | |O| |O|X|X| |X|O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X| |O|X|O| | | |X| | | |X| |O|X|O| | | |X| | |O|X| |O|X|O| | | |X| |X|O|X| |O|X|O| |O| |X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | |O|O| | | | | |X|O|X| | |O|O| | |X| | |X|O|X| 'O' Won! |O|O|O| | |X| | |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| | | |O| | | |X|X|O| |O| |X| |O| | | |X|X|O| |O| |X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| |O| |X| |X|X| | 'X' Won! |O| |O| |O| |X| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | | | | |O|X| | | |O|X| | | |O| |O|X| | | |O|X| |X| |O| |O|X| | | |O|X| |X| |O| |O|X|O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | |O|X| | |O| | |X| | | |X|O|X| | |O| | |X| | | |X|O|X| | |O| | |X| |O| 'X' Won! |X|O|X| |X|O| | |X| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X|O| |X|O| | | |X| | | |X|O| |X|O| | |O|X| | | |X|O| |X|O| | |O|X| | | |X|O| |X|O|X| |O|X| | |O|X|O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | |O| | |X| | |O| |X| | | |O| | |X|X| |O| |X| |O| |O| | |X|X| 'X' Won! |O| |X| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | |X| |O|O| | | |X| | | |O|X| |O|O| | |X|X| | | |O|X| |O|O| | 'O' Won! |X|X|O| | |O|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | |O| |O| | | | |X|X| | | |O| |O|X| | | |X|X| | |O|O| |O|X| | | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |O| | |O| | |X|X|O| | |X|O| | |O| | |X|X|O| |O|X|O| | |O| | |X|X|O| |O|X|O| |X|O| | 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | 'O' Won! |X| | | | | |X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| |O|O| | |O| |X| | | |X| 'X' Won! |O|O|X| |O| |X| | | |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | |O| |X|X|O| | | | | | |X|O| |X|X|O| 'O' Won! | | |O| | |X|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |X| | |X| | |O|X|O| |O| |X| 'X' Won! | |X| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | |O| |X| |O| |X|O| | | | |O| |X|X|O| |X|O| | 'O' Won! | | |O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X|X| | |O| |O| | |O| | |X|X| | |O| |O| | |O|X| |X|X| | |O| |O| |O|O|X| |X|X| | |O|X|O| |O|O|X| |X|X| | 'O' Won! |O|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X| | | | |X|O| |O| |O| |X| |X| | |X|O| |O| |O| |X| |X| |O|X|O| |O| |O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | |X| | | |O|O| |X|X| | |O|X| | 'X' Won! | |O|O| |X|X|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| |X|O| | | | | | | |X|O| |X|O| | | | | | |O|X|O| |X|O|X| | | | | |O|X|O| |X|O|X| |O| | | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | | | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| | | |X|X|O| 'X' Won! |O|X|O| |O|X| | |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | |O| | | |O|O|X| | |X| | |O|X| | |O|O|X| | |X| | |O|X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| |X|X| | 'O' Won! |O|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| |X| | | | | |X|O|O| |X| |X| |O| | | |X|O|O| 'X' Won! |X|X|X| |O| | | |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | |X|X| | | |O| | |O|O| | |X|X| | | |O| | |O|O| | |X|X| | |X|O| | |O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| |O|X|O| It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X| |X| 'O' Won! |O|O|O| |X|X|O| |X| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|O| |X|O| | |O| |X| | |X|O| |X|O|O| |O| |X| | |X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | |O|O| |X| | | |X| |O| | |O|O| |X| |X| |X| |O| | |O|O| |X| |X| |X|O|O| 'X' Won! |X|O|O| |X| |X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | 'X' Won! | | |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X|O| | | | | |X|X|O| | |X|O| | | | | |X|X|O| |O|X|O| | | |X| |X|X|O| |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | |X|X|O| | |O|O| | |X| | |X|X|O| | |O|O| |X|X| | |X|X|O| 'O' Won! |O|O|O| |X|X| | |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X|X| | | | | | |O| |O| |X|X| | | | |O| |O| |O| |X|X| | | |X|O| 'O' Won! |O| |O| |X|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | |O| |O| | | |X| | |X| | |O| |O| | | |X| | |X|O| |O| |O| | |X|X| | |X|O| |O| |O| | |X|X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| | | |O|O|X| |O| | | |X|X| | |O|O|X| |O| | | |X|X|O| |O|O|X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X|X| | |O| | | | |X| |O|X|X| |O|O| | | | |X| |O|X|X| |O|O| | |X| |X| 'O' Won! |O|X|X| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X| | | |X| | | |O|O| |O|X| | |X|X| | | |O|O| |O|X| | 'O' Won! |X|X| | |O|O|O| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | |O| | | |O|X| |X| | | | |O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X| |X| 'O' Won! |O|O| | |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| | | | | |X|O| | |O| |X| |O| | | |X|O| | |O| |X| |O|X| | |X|O| | |O| |X| 'O' Won! |O|X|O| |X|O| | |O| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | |O|X| | |X| |O| | | | | |O|X|X| 'O' Won! |X| |O| | |O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O|X| | |X| | | |O| | |O|O|X| | |X| | |X|O| | |O|O|X| | |X| | |X|O|O| |O|O|X| | |X|X| |X|O|O| |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | |X|X| | | | | |O|O|X| |O|X|X| | | | | |O|O|X| |O|X|X| | |X| | |O|O|X| |O|X|X| | |X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| | | |O| | | |X| |X|O|O| | |X|O| | | |X| |X|O|O| |O|X|O| |X| |X| |X|O|O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| |O| |O| | |X| | |X|O|X| |O| |O| 'O' Won! | |X|O| |X|O|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | |X| | |O| | | |O| |X| | |X|O| |O| | | |O| |X| | |X|O| |O| |X| |O| |X| 'O' Won! | |X|O| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| | | |O| |O| |X| |O| |X| |X| |O| |O| |X| |O| |X|O|X| |O| |O| |X| |O| |X|O|X| |O| |O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | |X| | | | | | |O|X|O| | |X| | | |O| | |O|X|O| | |X| | |X|O| | |O|X|O| 'O' Won! |O|X| | |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| |O|O| | |X| | | |X| |O| |O|O|X| |X|O| | |X| |O| |O|O|X| 'X' Won! |X|O| | |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| |O|O| | | | | | |X| |X| 'X' Won! |O|O| | | | | | |X|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| |X|O| | |O| | | |X|O|X| |X|O| | |O| |X| |X|O|X| |X|O| | 'O' Won! |O| |X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X| | |O| | | |O|O| 'X' Won! |X|X|X| | |O| | | |O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | |X| | | |O|O| |X|X| | | |X|O| | |O|O| |X|X| | |X|X|O| | |O|O| |X|X| | 'O' Won! |X|X|O| |O|O|O| |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | |O| | |X| |X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | |X| | |O|O| | |X|X| | |O|X| | |O|O| | |X|X| | |O|X|X| |O|O| | |X|X|O| |O|X|X| |O|O| | 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | |O| | |X|O| | |X|X| | | |O| | |X|O| | |X|X| | | |O|O| 'X' Won! |X|O| | |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O|X| | | | | | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | 'O' Won! | |X|O| |X|O|O| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | |O|X| | | | |X| | |O|X| |O|X| | | | |X| | |O|X| |O|X| | |O| |X| 'X' Won! | |O|X| |O|X|X| |O| |X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |O| | | | |X|O| | | | | |O|X| | |O|X|O| | | | | |O|X| | |O|X|O| | | |X| |O|X| | 'O' Won! |O|X|O| |O| |X| |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X| | | 'X' Won! |X|O|O| |X| | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | |O| | |O| | | |X|X| | | |O| | |O| |X| |X|X| | |O|O| | |O| |X| |X|X| | |O|O|X| |O| |X| |X|X| | |O|O|X| |O|O|X| 'X' Won! |X|X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | |X| | | | |O| |X|O|O| | |X| | | | |O| |X|O|O| |X|X| | 'O' Won! | | |O| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X| |O| | | |O| | |O|X| |X| |O| | | |O| |X|O|X| |X| |O| | |O|O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | |O|O| | | |X| | |X|X| | |O|O| | | |X| | |X|X|O| |O|O| | | |X| | |X|X|O| |O|O| | |X|X| | 'O' Won! |X|X|O| |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | |O| | |X| | | |O|X|X| | |O| | |X| | | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|X| |X| | | | |X|O| | |O|X| |X|O| | | |X|O| | |O|X| |X|O|X| | |X|O| | |O|X| |X|O|X| |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| 'O' Won! |O| | | |O| |X| |O| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | |X|O|X| | | | | |O|O| | |X|O|X| | | |X| 'O' Won! |O|O| | |X|O|X| | |O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X|O| | | | | | |O|O|X| |X|O| | | |X| | |O|O|X| |X|O| | |O|X| | |O|O|X| |X|O| | |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| | | |X| |X|X|O| |O| |O| | |X|X| |X|X|O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | |X| | | | |X| |O|O| | | |X| | | |O|X| |O|O| | | |X| | |X|O|X| |O|O| | |O|X| | |X|O|X| |O|O| | |O|X|X| |X|O|X| |O|O| | 'O' Won! |O|X|X| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O|X|O| |O| | | |X| |X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | | |O|X| | |X| | |X|O| | | |O|X| |O|X| | |X|O| | | |O|X| |O|X|X| |X|O| | | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | |O| | | |X|X| | |O|O| | |O|X| 'O' Won! | |X|X| |O|O|O| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X|O| |X| |X| |O|X| | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| |X| |O|X| | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X|X| |O|O| | |X|O|X| |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| |X| |X| | | |O| |O| |O| |X| 'X' Won! |X| | | |O|X|O| |O| |X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | |X|X| | | |O| |O| |X| | |X|X| |O| |O| |O| |X| | |X|X| |O|X|O| |O|O|X| | |X|X| |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| |O| |X|O| | |O| | | |X|X|O| |X|O| | |O| | | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| |O| |O| |X| | | |X|O|X| |O| |O| |X| | | |X|O|X| |O| |O| |X|O| | 'X' Won! |X|O|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X|X| | | | | |O|O| | | |X|X| | | |X| |O|O| | | |X|X| | | |X| |O|O| | |O|X|X| | |X|X| |O|O| | |O|X|X| 'O' Won! |O|X|X| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X|O| | |X| | | | |O| |X|X|O| | |X| | | | |O| |X|X|O| |O|X| | |X| |O| |X|X|O| |O|X| | |X|O|O| |X|X|O| |O|X| | 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | |X| |X| | |O| | |O|O| | |X| |X| | |O| | |O|O|X| |X| |X| | |O|O| |O|O|X| |X| |X| 'X' Won! | |O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| | |O| |X| | |O|O| | |X| | |O| |X| |X|O|O| | |X| | |O|O|X| |X|O|O| |X|X| | |O|O|X| |X|O|O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| | |X| | |O| |O| |X|O|X| | |X| | |O| |O| |X|O|X| |X|X| | 'O' Won! |O|O|O| |X|O|X| |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | |O| | | |X|O| | |O|X| | |O|X| | |X|O| | |O|X|O| |O|X| | |X|O| | |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O| | | |X|X| |X| |O| |O|O| | | |X|X| |X|O|O| 'X' Won! |O|O|X| | |X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| | | |X| |X| |O| | |O|X| | |O|X| |X| |O| | |O|X| |X|O|X| |X| |O| 'O' Won! |O|O|X| |X|O|X| |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O|X| |X| | | |O| | | |O|O|X| |X| | | |O| | | |O|O|X| |X| | | |O| |X| |O|O|X| |X|O| | |O| |X| |O|O|X| |X|O| | |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O| | | |X| |O|X|O| | | |O| | | |X| |O|X|O| | | |O| | |X|X| |O|X|O| | |O|O| | |X|X| 'X' Won! |O|X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | |X| | |O| | | |O|X|O| | |X| | 'X' Won! |O|X| | |O|X|O| | |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| | | |X|O|O| 'X' Won! |O|X|X| |O|X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O|O| | |X|X|O| | | |X| |O|O| | 'O' Won! |X|X|O| | | |X| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | 'O' Won! |X| |O| | |O|X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O|X| |X| | | | | |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | |X|X| | |O| |X| |O| |O| |X|X| | |O| |X| 'X' Won! |O| |O| |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O|X| | |X| | |X| | | |O|O|X| | |X|O| |X| | | |O|O|X| |X|X|O| |X|O| | |O|O|X| |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| |X| | | | | |X|O|O| |X| |X| | |O| | |X|O|O| 'X' Won! |X|X|X| | |O| | |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| |X| | |X|O| | | | | |O|O|X| | |X|O| | | | | |O|O|X| |X|X|O| 'O' Won! |O| | | |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X|X| |O|X| | | |O| | |O|X|X| |O|X| | | |O|X| |O|X|X| 'O' Won! |O|X| | |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X| |O| |O| | |X|X| | | |X| |O| |O| | |X|X| | |O|X| |O|X|O| | |X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | |X| | |X|O|X| | | |O| | |X|O| |X|O|X| | | |O| | |X|O| |X|O|X| | |X|O| 'O' Won! |O|X|O| |X|O|X| | |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | |O|X| | |O|X|X| | | | | |O|X| | |O|X|X| | | |O| |O|X| | 'X' Won! |O|X|X| | |X|O| |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X|X| | | |X| | 'O' Won! |O|O|O| |X|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O| | |O| |X| |X| | | | |O|X| |O| |X| |X| | | | |O|X| |O|O|X| |X|X| | | |O|X| |O|O|X| 'O' Won! |X|X|O| | |O|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X|O| | | | | | | |O| |X|X|O| | | | | |X| |O| |X|X|O| |O| | | |X| |O| |X|X|O| |O| | | |X|X|O| 'O' Won! |X|X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | | | | |X|O| | |X|O|O| | | | | |X|O|X| |X|O|O| | | | | |X|O|X| |X|O|O| |O| | | |X|O|X| |X|O|O| |O|X| | It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | | |O| | |O|X| |X|X| | | | |O| | |O|X| |X|X| | | |O|O| 'X' Won! | |O|X| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X| |X| | |O|O| |X|O|O| |X| |X| 'X' Won! | |O|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| |O| | | |X| | | | |X|O| |O| |X| |X| | | | |X|O| |O| |X| |X| | | |O|X|O| |O|X|X| |X| | | |O|X|O| 'O' Won! |O|X|X| |X|O| | |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | |X|O| | | | | |X|O| | | |X|O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | 'X' Won! |X|O|X| |O|X|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X| |X| |O| |O| 'X' Won! |X|O|O| |X|X|X| |O| |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X|O| | | | | |O|X| | | |X|O| | | |X| |O|X| | |O|X|O| | | |X| |O|X| | 'X' Won! |O|X|O| | |X|X| |O|X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O|X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| |O| | | | | | | |X|X|O| |O| | | | |O| | |X|X|O| |O| | | | |O| | |X|X|O| |O|X| | | |O| | |X|X|O| |O|X|O| |X|O| | |X|X|O| |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O|X| | |O| | | | | | |X|O|X| |O|O| | | | | | |X|O|X| |O|O|X| | | | | |X|O|X| |O|O|X| | | |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | |O| | |X|X| |X| |O| | | |O| | |X|X| |X| |O| |O| |O| 'X' Won! |X|X|X| |X| |O| |O| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X|X| | |X|O| | |O|O| | |X|X| | 'X' Won! |X|O| | |O|O| | |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | | | |X| | |O|X| |O| | | | |O|X| 'X' Won! | |O|X| |O| |X| | |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| |X| | | | | |X|O|X| |O| |X| | |O| | 'X' Won! |X|O|X| |O| |X| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | |X| |X| | | |O| |O|O| | |X| |X| |X| |O| 'O' Won! |O|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| | | |X|X| | | |O| | |O|O| | |X|X| | | |O| | |O|O|X| |X|X| | | |O|O| |O|O|X| |X|X| | |X|O|O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | |O| | |X| |O| |X| |X| | |O| | |X| |O| |X| |X| | |O|O| |X| |O| |X| |X| | |O|O| |X|X|O| 'O' Won! |X| |X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X| | | 'O' Won! |X| | | |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| |O| | | |X| | |X| | |O| |O| | | |X| | |X|X| |O| |O| | |O|X| | |X|X| |O| |O| |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | | |X| | |O| |O| |X|X|O| | |X|X| |O| |O| |X|X|O| 'O' Won! | |X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X|X| | 'O' Won! |O|X|O| | |O|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | | |O| | |X|X| |X| |O| | | |O| | |X|X| |X|O|O| | | |O| 'X' Won! |X|X|X| |X|O|O| | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X|O| | |O|X| | | |O| |X|X|O| |O|O|X| | | |O| |X|X|O| |O|O|X| |X| |O| |X|X|O| 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | |O| |X| | |X|O| |X| | | |O| |X| | |X|O| |X| |O| |O| |X| | |X|O| |X| |O| |O|X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | |X|O| |O| | | |X| | | |O|X|O| |O| | | |X| | | |O|X|O| |O|X| | |X| | | |O|X|O| |O|X|O| 'X' Won! |X|X| | |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| | | |O|X| | | | | |X|O| | |O|O|X| | | | | |X|O|X| |O|O|X| | | | | 'O' Won! |X|O|X| |O|O|X| | |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | |X| | | |X|O| |O| |O| | |X| | 'X' Won! | |X|O| |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| | | | |O|O| 'X' Won! |X|X|X| |O| | | | |O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | |O| | | | |X| | |O|X| |X|O| | | | |X| | |O|X| |X|O| | |O| |X| |X|O|X| |X|O| | |O| |X| |X|O|X| |X|O|O| |O| |X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O|X|O| |X| |X| | | |O| |O|X|O| 'X' Won! |X|X|X| | | |O| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | |O|X| | 'X' Won! |X|O|O| |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X|O| | | | |O| |X| | | |X|O| | | | |O| |X|X| | |X|O| | | | |O| |X|X|O| |X|O|X| | | |O| |X|X|O| |X|O|X| |O| |O| |X|X|O| 'X' Won! |X|O|X| |O|X|O| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O|O| | |O|X| | |X| | | |O|O|X| |O|X|O| |X| | | |O|O|X| |O|X|O| |X|X| | |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | |X| |X| | |O| | |O|O|X| |X| |X| | |O| | |O|O|X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| 'O' Won! |O|O|X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X| | |O| |O| | |X| | | |X|X| |O| |O| | |X| | |O|X|X| |O| |O| |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| |O| | | | | |X|O|X| |X| |O| | | | | |X|O|X| |X| |O| | |O| | 'X' Won! |X|O|X| |X| |O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | |X|O|X| | |O| | |O|X|O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X|O| | | |X| | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O|O| | |X| | |X|O|O| |X|O|O| | |X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X|O| | | | | |X|O|X| |O|X|O| | | | | |X|O|X| |O|X|O| | | |O| 'X' Won! |X|O|X| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|O| | |O| | 'X' Won! |X| | | |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | | |X|X|O| | | | | |O| | | |X|X|O| | | |O| |O|X| | |X|X|O| | | |O| |O|X| | |X|X|O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| | | |X|O| | | |X|O| |X| |O| |X|O| | 'X' Won! |X|X|O| |X| |O| |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | |O|O| |O|O|X| 'X' Won! |X|X|X| | |O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | |X|O|O| | |X|O| | | | | |X|O|O| | |X|O| | |X| | |X|O|O| | |X|O| |O|X| | |X|O|O| |X|X|O| |O|X| | |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X|X| | |O|O| | |X| | 'O' Won! | |X|X| |O|O|O| | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | 'O' Won! |O|O|O| | |X| | | |X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X|O| | | |O| 'X' Won! | | |X| |O|X|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | 'X' Won! |X| |O| |X|O|O| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X| |X| |X|O| | |O| |O| |X| |X| |X|O| | |O|X|O| |X| |X| |X|O|O| |O|X|O| 'X' Won! |X|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X|O| | |X| | | |O| |O| |X|O| | |X| |X| |O| |O| |X|O| | |X|O|X| |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | | | |X| | |X|O| |O|X| | |O| |X| | |X|O| |O|X|X| |O| |X| | |X|O| |O|X|X| |O|O|X| 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O| | | | |X| |O|X|X| |O|O| | | | |X| |O|X|X| 'X' Won! |O|O|X| | | |X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| |O| | |X| | | |X| | 'X' Won! |O|X|O| | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | |X| |O|O| | |O|X| | | | |X| |O|O| | |O|X| | |X| |X| |O|O| | |O|X| | |X|O|X| |O|O| | |O|X|X| |X|O|X| |O|O| | 'O' Won! |O|X|X| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| |X| | | |O| |X| | | |O|O|X| | | |O| |X| | | |O|O|X| | | |O| |X|X| | |O|O|X| |O| |O| |X|X| | 'X' Won! |O|O|X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | | |X| |O|O| | | |X| | |X| |X| |O|O| | | |X| | |X|O|X| |O|O| | | |X| | |X|O|X| |O|O| | | |X|X| 'O' Won! |X|O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | |O|O|X| | |X| | | |O| | |O|O|X| | |X|X| | |O|O| |O|O|X| | |X|X| |X|O|O| |O|O|X| | |X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O| | | | |X|X| | |O|X| |O|O| | | |X|X| | |O|X| 'X' Won! |O|O|X| | |X|X| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| | |X|X| | | |O| | | |O|O| |X|X| | | |O| | |X|O|O| |X|X|O| | |O| | |X|O|O| 'X' Won! |X|X|O| |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O| | | |X|O| |X|X| | | |O|O| | |X|O| |X|X| | |X|O|O| | |X|O| |X|X| | 'O' Won! |X|O|O| | |X|O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | |X|O| |O| |O| | | |X| | |X|O| |O|X|O| | | |X| | |X|O| 'O' Won! |O|X|O| | |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| |O| |O| |X| | | |X| |O| |O| |O| |X| | | |X|X|O| 'O' Won! |O| |O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | | | |X| |O|O|X| |X| |O| | | |X| |O|O|X| |X|X|O| | | |X| |O|O|X| |X|X|O| |O| |X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| |O| |X| | |O|O| | | |X| |O|X|X| 'O' Won! | |O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O| |O| |X|X| | 'O' Won! | | | | |O|O|O| |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X| |X| |O|X| | | | |O| |X| |X| |O|X| | | |O|O| 'X' Won! |X| |X| |O|X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | | |O| |X| |X| |X| |O| | |O|O| |X| |X| |X| |O| 'X' Won! |X|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O|O| | | | |X| |X|O| | |O|O|X| |O| |X| |X|O| | |O|O|X| 'X' Won! |O| |X| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |X| | |O| | |X|X|O| | |O|X| 'X' Won! |X|O| | |X|X|O| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| |O| |X| |O| | | | | |X|X|O| |X| |O| | | | | |X|X|O| |X| |O| |O| | | |X|X|O| |X| |O| |O| |X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| | | | |O| | | |X|X| |O| | | |X|O| | | |X|X| |O| | | |X|O|O| | |X|X| |O| |X| |X|O|O| |O|X|X| |O| |X| |X|O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| |X| |O|O| | | | |X| |X| |X| |O|O| | | |O|X| |X| |X| |O|O| | |X|O|X| 'O' Won! |X|O|X| |O|O| | |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X| | | |O|X| |O| | | |O|X|X| | |O|X| 'O' Won! |O| | | |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| |X| | | | | |O|X| | |O|O|X| | | | | |O|X|X| |O|O|X| | | | | |O|X|X| |O|O|X| | |O| | 'X' Won! |O|X|X| |O|O|X| | |O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X|O| | |O| | |O|X|X| | |X|O| | |O| | |O|X|X| |O|X|O| | |O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | | |X| |O| |O| |O| |X| |X| |X| |O| |O| |O| 'O' Won! |X| |X| |X| |O| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | |X| | | | |O|O| |X|O| | |X| | | | |O|O| |X|O|X| |X| | | 'O' Won! | |O|O| |X|O|X| |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| | |O| | |O|O| | 'X' Won! |X|X|X| | |O| | |O|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| 'O' Won! | |X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | |O| |O| |X| |X| | |X| | |O| |O| |X| |X| | |X|O| |O| |O| |X| |X| | |X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| 'X' Won! |X|X|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | |O| | |X| |O| | |O|X| |X|O| | 'O' Won! |X|O|O| | |O|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O| | | | |X| | |O|X|X| 'O' Won! |O| | | |O|X| | |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |O| | | | | |O| | |X|X| |O| | | | | |O| | |X|X| |O| |O| |X| |O| | |X|X| |O| |O| 'O' Won! |X| |O| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | | |X|O| |O|O| | | |X| | | |X|O| |O|O| | | |X|X| |O|X|O| |O|O| | | |X|X| 'X' Won! |O|X|O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | | | | | |X| |O| | |X|O| | | | | |X| |O| | |X|O| | |X| | |X|O|O| | |X|O| | |X| | |X|O|O| | |X|O| |X|X| | |X|O|O| |O|X|O| |X|X| | 'X' Won! |X|O|O| |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | |X| | |X|O| | | |O| |O| |X| | |X|O| | | |O| 'X' Won! |O| |X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|X| |O|O| | |X| | | |O|X|X| |O|O|X| |X| | | |O|X|X| |O|O|X| |X|O| | |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | |X|O| | |O| | | |X|O| | |X|O| | |O| | |X|X|O| | |X|O| 'O' Won! | |O|O| |X|X|O| | |X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | |O| | | | |O| |X| |X| |X|O| | | | |O| |X|O|X| |X|O| | | | |O| |X|O|X| |X|O| | | |X|O| |X|O|X| |X|O| | |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| |O|X| | |X| |O| | | |O| |O|X|X| |X| |O| | |O|O| |O|X|X| |X| |O| |X|O|O| |O|X|X| |X| |O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| |X| |O| | | | |O|O| |X| |X| 'X' Won! |O| | | | |O|O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X|X| | | |O| |X| |O| |O|X|X| | |X|O| |X| |O| |O|X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| |O| | | | | |X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| | | |O|X|X| |X|O|O| |O|X| | |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | | |X| |X|O| | | |O|O| | | |X| |X|O|X| | |O|O| 'O' Won! | |O|X| |X|O|X| | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | | |O|X|O| |X|O| | |X| | | |O|X|O| |X|O| | |X|X| | |O|X|O| |X|O| | |X|X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X|X| | | | | |O|X|O| |O|X|X| 'X' Won! | |X| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X|O| | |O|O| | |X| | |X|X|O| | |O|O| |O|X| | |X|X|O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| | | |X| |X|X|O| |O| |O| | | |X| |X|X|O| |O| |O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |X| | | | | |X|O| | | |O|X| | | | | |X|O|X| | |O|X| | | |O| |X|O|X| | |O|X| | | |O| |X|O|X| |X|O|X| 'O' Won! | |O|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X|O| | | | | |X|O| | | |X|O| |X| | | |X|O| | | |X|O| |X| | | |X|O|O| | |X|O| |X| |X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O|X| | 'O' Won! | | |O| | |O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | 'X' Won! |X| | | |X| |O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| |O| |O|X| | | | |X| |O| |O| |O|X|X| 'O' Won! |O| |X| |O| |O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| |O| |X| |X| |X|O|O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| |O|X| | | |O|X| | |O|X| |O|X| | | |O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| |O| |X| | | |O| | |O|X| |O|X|X| | | |O| | |O|X| |O|X|X| |O| |O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | |O|X| | |X| | | |X|O| | |O|X| | |X| | | |X|O|O| |O|X| | |X|X| | |X|O|O| |O|X| | 'O' Won! |X|X|O| |X|O|O| |O|X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| 'O' Won! |O| |X| | |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X|O| | | | |X| | |O| | |X|O|X| | | |X| |O|O| | |X|O|X| | | |X| |O|O| | |X|O|X| |X| |X| 'O' Won! |O|O|O| |X|O|X| |X| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | | |X| |X|O| | |O| |X| | | |X| |X|O|O| 'X' Won! |O| |X| | |X|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X|X| | | |O|O| |X| | | |X|X|O| | |O|O| 'X' Won! |X| | | |X|X|O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O|X| | | | |X| |O|O|X| |O|X| | |X| |X| |O|O|X| |O|X| | |X|O|X| |O|O|X| |O|X| | 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | |O| | |X|O|X| |X| | | | |O| | |X|O|X| |X| |O| | |O| | |X|O|X| 'X' Won! |X| |O| |X|O| | |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | | | |O|X|X| 'O' Won! |X| |O| | |O| | |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O|O| | | |X|X| |X|O| | |O|O| | | |X|X| |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X|X| | |O| | | |O|O| | |X|X| | |O| | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O|O| | |O|O|X| |X|X| | |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | |X|X| | | | |O| | |O| | |X|X| | | | |O| |O|O| | |X|X| | |X| |O| |O|O| | |X|X|O| |X| |O| |O|O| | |X|X|O| |X| |O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O|X| |O|X|X| | | | | |O|O|X| |O|X|X| | |X| | |O|O|X| |O|X|X| | |X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | |X| | |O| |O| | |X|X| 'O' Won! | |X| | |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| |O|X| | |O|O|X| | | |X| 'X' Won! |O|X|X| |O|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | |X|X| |O| |O| |O| | | | |X|X| |O| |O| |O| | | | |X|X| |O|X|O| |O| |O| | |X|X| |O|X|O| 'X' Won! |O|X|O| | |X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | |O| |X| | |X|O| | |X| | |O| |X| | |X|O| |O|X| | |O| |X| | |X|O| 'X' Won! |O|X| | |O|X|X| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| |O| | | | |X|O| | | |X| |O| | | | |X|O| | |X|X| |O|O| | | |X|O| | |X|X| |O|O|X| | |X|O| | |X|X| |O|O|X| | |X|O| |O|X|X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | | | |O| |X| |X| |O|X|O| | | |O| |X| |X| |O|X|O| |X| |O| |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X| |O| | |O| | |X|X|O| |X| |O| | |O| | |X|X|O| |X| |O| |O|O| | |X|X|O| |X| |O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O| | | | |X| | | |O| |X|O|X| | | |X| | | |O| |X|O|X| | | |X| | |O|O| |X|O|X| | | |X| |X|O|O| 'O' Won! |X|O|X| | |O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | |X| | |O|X| | | | |O| | |X|O| |O|X| | | | |O| | |X|O| |O|X|X| | | |O| |O|X|O| |O|X|X| | | |O| |O|X|O| |O|X|X| |X| |O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| |X| |O| | |X| | | |O|X| |X|O|O| | |X| | | |O|X| |X|O|O| | |X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O| | |X|O|X| | |X| | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X|X| 'O' Won! | |O|O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| | |O|O| |O|X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| |O|O| | |O| | | |X| |X| |O|O| | |O|X| | |X|O|X| |O|O| | |O|X| | |X|O|X| |O|O|X| |O|X| | It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| |O| |X| | | |X|O| | |O| |O| |X|X| | |X|O| | |O| |O| |X|X|O| |X|O|X| |O| |O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | |O| | |O| |X| |X| |X| | |O| | |O| |X| |X| |X| | |O|O| |O|X|X| |X| |X| | |O|O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | |O| | |X| |X| |O|X|O| | |O| | |X|O|X| |O|X|O| | |O| | 'X' Won! |X|O|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | | | | | |O|X| |X| |O| | | |O| | |O|X| |X|X|O| | | |O| | |O|X| |X|X|O| |O| |O| | |O|X| |X|X|O| |O| |O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| |X| | | |O| | |X| | |O| |X| | |O|O| | |X| | |O| |X| | |O|O| | |X|X| 'O' Won! |O| |X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | |X|X| |O| | | | | | | |O|X|X| |O| | | |X| | | |O|X|X| |O| | | |X| | | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | |O|X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|O| |X|O|X| | |X|O| | |O|O| |X|O|X| |X|X|O| 'O' Won! |O|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O|X| |O|X| | 'O' Won! |O|X| | |O|O|X| |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X|X| | |X| | | |O|O| | |X|X| | |X|O| | |O|O| | |X|X| |X|X|O| | |O|O| 'O' Won! | |X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X| | |X| | | |O|O| | |X|X| |O|X| | | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X| | |O| |O| |X| |X| |O|X| | |O| |O| |X| |X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | |X| | |O| | | |O|X| | 'O' Won! |O|X| | |O| | | |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| 'O' Won! |X|X|O| | | |O| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | |X| | | |O| |X| |O| | | |X|O| | 'X' Won! |O| |X| |O|X| | |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |X| |X| |O| |O|O|X| |X| |X| |X|O|O| 'X' Won! |O|O|X| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| |O| |X| | | | |O|O| |X|X|O| 'O' Won! |X| |O| | |O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | |X|X| | | | | |O| |O| | |X|X| 'X' Won! | | | | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | |X|O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| |X| | | |X| | | |O|X|O| |X| | | |X| |O| |O|X|O| |X| | | |X|X|O| 'O' Won! |O|X|O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| | | | | | |X|O| |O| |X| | |X| | | |X|O| |O| |X| | |X| | |O|X|O| |O| |X| | |X|X| |O|X|O| |O| |X| 'O' Won! |O|X|X| |O|X|O| |O| |X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X|O| | | | |X| |O|O| | |X|O|X| 'O' Won! | |O|X| |O|O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| |O| | | |O| |X|X| | |X| |O| | |O|O| |X|X| | |X| |O| 'X' Won! |X|O|O| |X|X| | |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | |O| | | |O|X| | | |X|X| |O| | | |O|X| | | |X|X| |O| |O| |O|X| | | |X|X| |O| |O| |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X|X| | | |O| | | |X| |O|X|X| | |O|O| | | |X| |O|X|X| |X|O|O| | | |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| |X| |X| | | | |O|O| |X|O|X| |X|X| | | |O|O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X|X| |X| | | |O|O| | |O|X|X| |X| | | |O|O| | |O|X|X| |X|X| | |O|O| | |O|X|X| |X|X|O| |O|O| | It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | |X| | | |O| |X| |O| | | |X| | | |O|O|X| |O| | | |X|X| | |O|O|X| |O| | | |X|X| | |O|O|X| |O| |O| 'X' Won! |X|X|X| |O|O|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X|O|X| |O| | | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X| | | |O|O| |X|O|X| |O|X|X| | |O|O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | |X| | | | |O| | |O|X|O| |X| | | | |O| | |O|X|O| |X| | | |X|O| | |O|X|O| |X|O| | |X|O| | |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X|X| | |O| |O| | |X| | |X|X|O| |O| |O| | |X| | |X|X|O| |O| |O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| |X| |O| | |O|X| | | |X| 'O' Won! |X| |O| | |O|X| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O|X|O| | |X| | |X|O| | |O|X|O| | |X|O| |X|O|X| |O|X|O| | |X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | | | | |X|O|X| |O| | | | | |O| |X|O|X| |O| |X| | | |O| |X|O|X| |O|O|X| | | |O| |X|O|X| |O|O|X| |X| |O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| |X| |X| |O| | | | | |O|O|X| |X| |O| | | | | |O|O|X| |X| |O| |X| | | |O|O|X| |X|O|O| |X| | | |O|O|X| |X|O|O| |X| |X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| |O| |O| | | | | |X| 'X' Won! |X| |O| |O|X| | | | |X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| | |X| | | 'X' Won! |X|O| | |X|O| | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | |O| | | | |O|X| |O|X| | |O| |X| | |O|X| |O|X| | |O|O|X| | |O|X| 'X' Won! |O|X|X| |O|O|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| | | |O| | | |X| |X|X|O| | | |O| | |O|X| |X|X|O| | | |O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X| |X| | |O|O| | |X| | |X|O|X| 'X' Won! |X|O|O| | |X| | |X|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O|X| | | | |X| |O|O|X| |O|X| | 'X' Won! | | |X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| | |X| | |O|O| | | |X|X| | |X|O| 'X' Won! |O|O| | |X|X|X| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | |X|O|X| | | | | | |O| | |X|O|X| |O| | | |X|O| | |X|O|X| |O| | | 'O' Won! |X|O|O| |X|O|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| |X|O| | | | | | | |O|X| |X|O|O| | | | | |X|O|X| |X|O|O| 'O' Won! |O| | | |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| |X|X|O| 'O' Won! |O|O|X| | |O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X|O| | | | | | |X|O| |O|X|O| | | |X| | |X|O| |O|X|O| |O| |X| 'X' Won! |X|X|O| |O|X|O| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X|O| | |X|X| |O| | | |O|X|O| | |X|X| |O|O| | |O|X|O| | |X|X| |O|O|X| |O|X|O| 'O' Won! |O|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O|X| | |O| | |X| |O| |X|O|X| | |O| | 'X' Won! |X| |O| |X|O|X| |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| |X| |X| | | |O|O| | |O| |X| |X| |X| |O|O| | |O| |X| 'O' Won! |X| |X| |O|O|O| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| | | | |O| | |X|X|O| |O|X| | | |O| | |X|X|O| |O|X|O| | |O| | |X|X|O| |O|X|O| |X|O| | 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X|X| | |O|O| | | | | | |X|X| |X|O|O| | | | | | |X|X| |X|O|O| | | | | |O|X|X| |X|O|O| |X| | | |O|X|X| |X|O|O| |X| |O| |O|X|X| 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | |X| | | |X|O|O| | |X| | |X| |O| |X|O|O| | |X|X| |X| |O| |X|O|O| | |X|X| |X|O|O| |X|O|O| 'X' Won! |X|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | |X|X| | |O| |O| | | | | |X|X| | |O| |O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X|X| | | |X| | |O| |O| 'O' Won! |X|X| | | |X| | |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | |O| | |O| | | | |X|X| |X|O| | |O| |O| | |X|X| |X|O| | |O|X|O| | |X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O| | 'O' Won! |X|O| | | |O| | |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | 'O' Won! |X| |O| |X| |O| | | |O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | |O| |X| |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| 'X' Won! |X|O| | | |X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X|X| |O|O| | | | |X| |O|X|X| 'X' Won! |O|O|X| | | |X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| 'O' Won! | | |X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | |X|X| | | |O|O| |O|X| | |X|X|O| | |O|O| |O|X|X| |X|X|O| | |O|O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | | | | |O| |X| | |O|X| | | | | 'O' Won! |O| |X| | |O|X| | | |O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | |X| |O| |O| | | | |X| | |X| |O| |O|O| | | |X|X| |X| |O| |O|O| | 'O' Won! | |X|X| |X| |O| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | |X| |O|O|X| 'X' Won! |O| |X| | | |X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | |X| | | | |X| |O| |O| | |X| | | |O|X| |O| |O| |X|X| | | |O|X| 'O' Won! |O|O|O| |X|X| | | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | |X| | | 'O' Won! | |O|X| | |O| | |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | |X| | | | |O|O| |X| | | 'O' Won! |X| | | |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | |O| | | |X| |O|X|X| | | |O| | | |X| |O|X|X| |O| |O| |X| |X| |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|O| | |O| | |O| |X| | |X|O| | |O|X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| | |O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | |O|O| | |X| | |X| |X| | |O|O| 'O' Won! | |X| | |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | |X| | |X| | |O| |O| | | |X| | |X| | |O| |O| | |X|X| | |X| | 'O' Won! |O|O|O| | |X|X| | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | |X| | |X| |X| |O|O| | | |X| | |X|O|X| |O|O| | | |X|X| |X|O|X| |O|O| | 'O' Won! | |X|X| |X|O|X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| |O| | | |X|X| | | |O|X| |O| | | |X|X| | | |O|X| |O|O| | 'X' Won! |X|X|X| | |O|X| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X| | | |X|O| |O| | | | |X| | |X|X|O| |O| | | | |X|O| |X|X|O| |O| | | | |X|O| |X|X|O| |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X|X| | | | | |O|O| | |O|X|X| | | | | |O|O|X| |O|X|X| 'O' Won! |O| | | |O|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | |O|O| | |X|X| | | |X| | |O|O| | |X|X| |O| |X| | |O|O| 'X' Won! |X|X|X| |O| |X| | |O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| | | |X|X|O| | | | | |O| |O| |X|X|O| | | | | |O|X|O| |X|X|O| 'O' Won! | | |O| |O|X|O| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | |X| |X| | | |X|O|O| |O| |X| |X| | | |X|O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X| |O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | |O| | |O| | | | |X|X| | |O|X| |O| | | | |X|X| |O|O|X| |O| | | 'X' Won! | |X|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X|X| | | |X| | 'O' Won! |O|O|O| |X|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| |O|O| | | | |O| |X| |X| |O|O| | |X| |O| |X|O|X| |O|O| | |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| |O| | | |X|X| | |X| |O| |O| | | |X|X|O| |X| |O| |O| | | |X|X|O| |X| |O| |O| |X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | |O|O| | |X| | |X|O| | | |O|O| |X|X| | |X|O| | | |O|O| |X|X|O| |X|O|X| | |O|O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| |O| |O| | | | |X| | |X| |O| |O|X| | |O|X| | |X| |O| |O|X| | |O|X| | |X| |O| |O|X|X| |O|X|O| |X| |O| |O|X|X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O| | |X|O| |O| | | | |X|O| | |X|O| |O| |X| | |X|O| | |X|O| |O|O|X| | |X|O| |X|X|O| |O|O|X| | |X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | | | |O| |O| |X| |X|X| | | | |O| |O| |X| |X|X| | | | |O| |O|O|X| 'X' Won! |X|X| | | |X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| | | |O|O|X| |X| | | |X| | | |O|O|X| |X| |O| |X| | | |O|O|X| |X|X|O| |X| | | |O|O|X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | |X|O| |O| | | | |X| | | |X|O| |O|O| | | |X| | | |X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| 'X' Won! |X|X| | |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | | |O| | | |X| |X|X|O| | | |O| |O| |X| |X|X|O| |X| |O| |O| |X| |X|X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | |O| | | |X|O|X| | | | | |O| | | |X|O|X| |X| | | 'O' Won! |O| | | |X|O|X| |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | | |O| | |X| | |X|O|X| | | |O| 'O' Won! |O|X| | |X|O|X| | | |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | |X| | | | |X| |O| |O| 'O' Won! | |X| | | | |X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | | |O| | |X|O|X| |O|X| | |O|O| | |X|O|X| |O|X| | |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | |O| |O| 'X' Won! |X|X|X| |O| | | |O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| | | |O| | | |O|X|O| |X| | | |O|X| | 'O' Won! |O|X|O| |X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O|X| | | | | |O| |X| | |O|X| | |O| | |O| |X| |X|O|X| | |O| | |O| |X| |X|O|X| |O|O| | |O| |X| 'X' Won! |X|O|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | | | | |O|X| | |O| |X| 'X' Won! |X| | | |O|X| | |O| |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O|O| |X| | | |X| |O| | |O|O| |X| |X| 'O' Won! |X| |O| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X|O| | |O|X| | | | | |X|X|O| | |O|X| | |O| | |X|X|O| | |O|X| | |O|X| |X|X|O| 'O' Won! |O|O|X| | |O|X| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O|O|X| | | | | | |X|X| |O|O|X| | | | | | |X|X| |O|O|X| | |O| | | |X|X| |O|O|X| |X|O| | | |X|X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | |X| |O| | |O|O| 'O' Won! |X|X| | |X| |O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | | | |X| |O|X|O| | |X| | | | |X| |O|X|O| | |X|O| |X| |X| |O|X|O| | |X|O| |X| |X| |O|X|O| |O|X|O| 'X' Won! |X|X|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | | | | | |X|O|O| | |X|X| | | | | |X|O|O| | |X|X| | | |O| |X|O|O| | |X|X| | |X|O| |X|O|O| |O|X|X| | |X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | |X| |X|O| | |O|O| | | | |X| |X|O| | |O|O| | | | |X| |X|O|X| 'O' Won! |O|O| | | |O|X| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| |X|X|O| | |X| | | |O|O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X|X| | | | | | | |O|O| 'O' Won! |X|X| | | | | | |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | | |O| |X| |O| | |X| | |O| |O| |X| |O| | |X| | |O| |O| |X| |O| |X|X| | 'O' Won! |O| |O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X|X| | |O| | |O|O| | | |X|X| | |O| | |O|O|X| | |X|X| | |O|O| |O|O|X| | |X|X| |X|O|O| |O|O|X| | |X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| |X| |X| |O|X| | | |O|O| |X| |X| |O|X|X| | |O|O| |X| |X| |O|X|X| | |O|O| |X|O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O|X| | | | |X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X|X| 'O' Won! |O|X|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| | | |O| |X| | | |X|O|X| | | |O| |X| | | |X|O|X| | |O|O| 'X' Won! |X| | | |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | | |O| |O| | | | |X|X| | |O|O| |O| | | | |X|X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| |O| | |X|X| 'X' Won! |X|O|O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | |X|O| 'O' Won! | | |O| | |X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O| | |X|O| | | | |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O|X| |X|O| | 'O' Won! |O|O|X| |X|O|X| |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | | |X| |O| |O| |O|X|X| | | |X| |O| |O| |O|X|X| | |X|X| 'O' Won! |O| |O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X|X| | |O| | |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| | |O|X| |O|O| | |O|X|X| 'X' Won! | |O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | | |O| |X|O| | |X|X| | | | |O| |X|O| | |X|X|O| | | |O| 'X' Won! |X|O| | |X|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| | | |X|X|O| |O| |X| |O| |O| |X|X|O| |O|X|X| |O| |O| |X|X|O| 'O' Won! |O|X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X|X| |O| | | |X| |O| |O|X|X| |O| | | |X|X|O| |O|X|X| |O| | | |X|X|O| |O|X|X| |O|O| | 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| |O| | | |O| | | |X| |X|X|O| | | |O| | |O|X| |X|X|O| | | |O| |X|O|X| |X|X|O| | |O|O| |X|O|X| |X|X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X|O| |X|O| | | | | | | |X|O| |X|O| | | |X| | | |X|O| |X|O| | | |X|O| | |X|O| |X|O| | |X|X|O| 'O' Won! |O|X|O| |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| |X| | | |O|O| | |O|X|X| |X| | | |O|O| | |O|X|X| |X| |X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|O| |X| | | |O|X|X| | |O|O| |X| |O| |O|X|X| | |O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | |O| | |O|X|O| | |X| | | |O|X| |O|X|O| | |X| | | |O|X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O|X| | |O|X| | | | | | 'O' Won! |O|X| | |O|X| | |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | | | |X|X|O| | | |O| |O| | | |X|X|O| | | |O| |O| | | |X|X|O| |X| |O| |O| | | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| | |O| | | | |X| |X|O|O| | |O| | |X| |X| |X|O|O| |O|O| | |X| |X| |X|O|O| |O|O|X| |X| |X| 'O' Won! |X|O|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| | | |O|O| |X| | | | |X| | | |O|O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| |X|O|O| |X|X| | 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | | | | |X|O| | |O|X| | | |X| | |X|O| | |O|X| | | |X| | |X|O|O| |O|X| | | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X|X| | |O| | | | |O|O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X|O| | |O|O| |X|X| | 'X' Won! |O|X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X|O| |X|O| | | | |X| |O|X|O| |X|O|O| | | |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O|X| | |X|O|O| | |X| | |O|X| | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X|O| |O|X| | 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | |O| |O| | |X|X| | |X| | |O| |O| | |X|X| |O|X| | |O| |O| 'X' Won! | |X|X| |O|X| | |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O|X| |X| | | |X| |O| | |O|X| |X|O| | |X|X|O| | |O|X| |X|O| | |X|X|O| | |O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | |O| |O| |X| | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X|O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |O| 'X' Won! | |O| | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X|X| | |O| | | |O|O| | |X|X| | 'X' Won! |O| | | |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | | | | |O| | | | |O|X| |X| | | |O| | | | |O|X| |X|O| | |O|X| | | |O|X| |X|O| | 'O' Won! |O|X| | | |O|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | |O| | |O|X|X| |X| | | | |O| | |O|X|X| |X| | | |O|O| | |O|X|X| |X|X| | |O|O| | 'O' Won! |O|X|X| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X| | |O| | | | |O|X| |X|X| | |O| | | | |O|X| |X|X| | |O|O| | | |O|X| 'X' Won! |X|X|X| |O|O| | | |O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | |X| | |O|O|X| | | | | | |X|O| |O|O|X| | | | | |X|X|O| |O|O|X| | |O| | |X|X|O| |O|O|X| | |O|X| |X|X|O| |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| |X| |O|O| | | | |X| 'O' Won! |X| |X| |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O|O| 'X' Won! |O|X|X| |O|X| | |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | |O| |X|X| | | |O| | |X| |O| |X|X| | | |O| | |X| |O| |X|X|O| | |O| | 'X' Won! |X| |O| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X|O|X| |O| |X| | | | | |X|O|X| |O| |X| |O| | | |X|O|X| |O|X|X| |O| | | |X|O|X| |O|X|X| |O|O| | |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| |X| |X|O| | | |O| | |X| |X| |X|O| | | |O|O| |X| |X| |X|O|X| | |O|O| 'O' Won! |X| |X| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | | |X|X| | |X|O| | |O|O| | |X|X| |X|X|O| | |O|O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X|O|X| 'X' Won! |X| |O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| 'O' Won! | |O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O|X| | | |O|X| | | |X| |O|X|O| | |O|X| 'X' Won! |X| |X| |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | |O|O| | |X|O| | | |X| | |O|O| | |X|O|X| | |X|O| |O|O| | |X|O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | | |O| |O| | | | |X|X| | | |O| |O|X| | | |X|X| |O| |O| |O|X| | | |X|X| |O| |O| |O|X|X| | |X|X| 'O' Won! |O| |O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| |O|O| | 'X' Won! |O| |X| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | |O| | | |X|O| | |O|X| | |O| | | |X|O| | |O|X|X| 'O' Won! |O| |O| |X|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| |O| | | | | |O| |X|O|X| |O| | | | |X|O| |X|O|X| |O|O| | | |X|O| |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O|X| |O|X| | 'O' Won! |O|X|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | |O| | |X|O| | |X| | | | |O| | |X|O| | |X|X| | | |O|O| |X|O| | |X|X| | | |O|O| |X|O|X| |X|X| | 'O' Won! |O|O|O| |X|O|X| |X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O|X| | | | | | |O| | |X|O|X| |O| | | | |O| | |X|O|X| |O| |X| 'O' Won! | |O| | |X|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X|X|O| 'O' Won! |O|O|O| | | |X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X|O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | |O| | |X|X| | |O|X| | | |O| | |X|X| | |O|X| | |O|O| | 'X' Won! |X|X| | |O|X| | |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X|X| | |O| | |O|O|X| |O|X|X| 'X' Won! | |O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X|X| | | |O|O| | | |X| |X|X| | | |O|O| | |O|X| |X|X| | | |O|O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X|X| | | | | |O|O|X| |O|X|X| | |O| | 'X' Won! |O|O|X| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| |O|X|O| | | | | | |X|O| |O|X|O| |X| | | | |X|O| 'O' Won! |O|X|O| |X| |O| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | |X|X| | |O| |X| |O| | | |X|X|O| |O| |X| |O|X| | |X|X|O| |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | |O| | | |O| |X| |X| |X| |O| | | |O| |X| |X| |X| |O| | | |O| |X|O|X| 'X' Won! |X| |O| |X| |O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O| | | |X| |X| |O| | |O|O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| | |O|O| | |O|X| |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | |X| | |X|O|O| | | |O| | |X| | |X|O|O| | |X|O| | |X| | |X|O|O| | |X|O| |O|X| | |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| |O| |O|X| | | |O| | |X| |O| |O|X|X| |O|O| | |X| |O| |O|X|X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X|X| | | | | 'O' Won! |O|O|X| |O|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| 'X' Won! | |O|O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| 'X' Won! |X|X|X| | |O| | | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | |O| | | |X| |O| |X|X| | |O| | | |X| |O| |X|X|O| |O| | | 'X' Won! |X| |O| |X|X|O| |O| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X|O| |X|O| | 'X' Won! |X|O|X| |O|X|O| |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| |X| |O| | | |X|X| | |O| |X| |O|O| | |X|X| | |O| |X| |O|O|X| |X|X|O| |O| |X| |O|O|X| 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X| | | | |O| | |O| | |X|X| | | |X|O| | |O| | |X|X| | | |X|O| |O|O| | 'X' Won! |X|X|X| | |X|O| |O|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | |O| | |X| |X| |O|X|O| | |O| | |X| |X| |O|X|O| |O|O| | 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| | | |O| |X| |O| |X| |X| | | |O|O|X| |O| |X| |X| | | 'X' Won! |O|O|X| |O|X|X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | |X| | |O| | | |O|X|O| | |X| | |O| | | |O|X|O| | |X|X| |O|O| | |O|X|O| | |X|X| 'X' Won! |O|O| | |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| | | |X| 'X' Won! |X|O|O| |X|X|O| | | |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| |O| |O| 'X' Won! |X| |O| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| 'O' Won! |O|X|X| |O| | | |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O|O| | |X| | | |X|O| | |O|O| | |X|X| | |X|O| | |O|O| | |X|X| | |X|O|O| 'X' Won! |O|O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X|X| | | |O|O| | |X|O| |X|X| | | |O|O| |X|X|O| |X|X| | 'O' Won! | |O|O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X|O| | | | | |X| |O| | |X|O| | | |X| |X| |O| |O|X|O| | | |X| |X| |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X| |X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X| |O| | |O|O| | |X|X| |X| |O| | |O|O| |O|X|X| |X| |O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| |O| |X| |X|X| | |O| |O| 'X' Won! |O| |X| |X|X|X| |O| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X|O| | |O| |X| | | | | |X|O| | |O|X|X| | | | | |X|O|O| |O|X|X| | | | | 'X' Won! |X|O|O| |O|X|X| | | |X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| |X| | | |X|O| | | |O|O| |X| | | |X|O|X| | |O|O| |X| | | 'O' Won! |X|O|X| | |O|O| |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O|O| |X| | | |X|O|X| | |O|O| |X| | | 'O' Won! |X|O|X| | |O|O| |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| | | |X|O|X| | | |X| |O|O| | |X|O|X| | | |X| |O|O| | |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | |X|O| |O| | | | |X| | | |X|O| |O|O| | | |X|X| | |X|O| |O|O| | | |X|X| |O|X|O| |O|O| | | |X|X| |O|X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X|O| | |O|O| | |X|X| | |X|O| | |O|O| | 'O' Won! |X|X|O| |X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| |X| | | |X| | | |O|O|X| |X| | | |X|O| | 'X' Won! |O|O|X| |X|X| | |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | |X| |X| | |O|O| | | | | |X| |X| | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | |O|O| | |X|O| | |X|X| | |O|O| | |X|O| | 'O' Won! |X|X| | |O|O|O| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| |O| | | | | |X|O| | |X|X|O| | | | | |X|O| | |X|X|O| |O| | | |X|O| | |X|X|O| |O| |X| |X|O| | |X|X|O| |O|O|X| |X|O| | It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O|X|X| | | | | |O| | | 'O' Won! |O|X|X| |O| | | |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| |O| | | |X|X| | |X| |O| |O| | | |X|X| | |X| |O| |O|O| | |X|X| | |X|X|O| |O|O| | 'O' Won! |X|X| | |X|X|O| |O|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | | | |O|O|X| | | |X| | | | | 'X' Won! |O|O|X| | | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | | |X| | | |O| |X|O| | | | |X| |O| |O| |X|O|X| | | |X| |O| |O| |X|O|X| |O| |X| |O| |O| |X|O|X| |O| |X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X|O| | |O| | |X| |O| |X|X|O| | |O| | |X|O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | |O| | | | |X| |O| |X| | |O| | | |X|X| |O| |X| | |O| | |O|X|X| 'X' Won! |O| |X| | |O|X| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X|X| | |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X|O| |X| | | 'X' Won! |O| |X| | |X|O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | 'O' Won! |O| |X| |O|X| | |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | | |O| | | |O| |O|X|X| |X| |O| | | |O| |O|X|X| |X| |O| | |O|O| |O|X|X| |X|X|O| | |O|O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| |X| | | | | |O| | |O|X| |X| | | | |O|O| | |O|X| |X| | | |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | | |X|X| | |O|X| | |O|O| | |X|X| | |O|X| | |O|O| | |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| 'X' Won! |X|O| | |O|X| | | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X|O| | |O| | | |X| |O| |X|O| | |O|X| | |X| |O| |X|O| | |O|X|O| |X| |O| |X|O| | |O|X|O| |X|X|O| 'O' Won! |X|O|O| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| |O| | | |X| |O| |X| |X| |O| | | |X| |O|O|X| |X| |O| |X| |X| |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X|X| | |X|O| | |O|O| | |X|X| 'X' Won! | |X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | |X| | |X| | |O|O| | |O| |X| | |X|X| |O|O| | |O| |X| 'O' Won! |O|X|X| |O|O| | |O| |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X|X| | |O| |O| 'O' Won! | |X| | |X|X| | |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X| | |O|X| | | | |O| 'X' Won! |O|X| | |O|X| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O|O| | | |X|X| | |X|O| |O|O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| |O|O| | 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X| | |O| |O| | | | | 'X' Won! |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| |O| |O|O|X| | | |X| |X| |O| |O|O|X| | | |X| |X|O|O| |O|O|X| |X| |X| |X|O|O| 'O' Won! |O|O|X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |X| |O|O|X| | |O| | |X| |X| 'X' Won! |O|O|X| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | | | |O|X| | | |X|O| | | | | |O|X| | |X|X|O| | | |O| |O|X| | |X|X|O| 'X' Won! | |X|O| |O|X| | |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X|X| | |X| |O| |O| |O| |X|X| | 'X' Won! |X| |O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X| | |X| | | | |O|O| | |X| | |X|X| | | |O|O| 'O' Won! | |X| | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | |O| |X| |X| | | |O|O| | |O|X|X| 'O' Won! |X| |O| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | |O|O| | | | |O| 'X' Won! |X|X|X| |O|O| | | | |O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | |O|X| | | |X| 'X' Won! | |O|X| | |O|X| | | |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| | | | |O| | | |X|X| |O|X| | | |O| | | |X|X| |O|X| | |O|O| | | |X|X| |O|X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| | | | | |O|X| | |O| |X| | | | | |O|X| | |O|X|X| | | | | |O|X| | |O|X|X| | | |O| |O|X|X| |O|X|X| | | |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| |O| |O| |O| |X| | | |X| |O| |O| |O| |X| | |X|X| 'O' Won! |O|O|O| |O| |X| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| |O| | | | | |X| |O| |X| 'X' Won! |O| |X| | | |X| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | |X| | |O| |O| |X|O| | | |X| | |O| |O| |X|O| | | |X|X| |O| |O| |X|O|O| | |X|X| |O| |O| 'X' Won! |X|O|O| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | | |X| | | |O| 'O' Won! |O|X| | | |O|X| | | |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| | | |O|X| | |X|X| |O|O| | | |O|X| 'X' Won! |X|X|X| |O|O| | | |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | |O|X| | |X| |O| 'X' Won! |O|X| | |O|X| | |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| | | | | |X|X| | |O|O|X| | |O| | |X|X| | |O|O|X| |X|O| | |X|X| | 'O' Won! |O|O|X| |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X| | | |O|O| | |X| |O| |X| | | |O|O|X| |X| |O| |X|O| | |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X| | | |X|O| |O| | | |O|X| | | |X|O| |O| |X| |O|X| | 'O' Won! | |X|O| |O|O|X| |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O|O| | |O| |X| | | |X| 'X' Won! |O|O|X| |O| |X| | | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | 'X' Won! |O|O| | |O|X| | |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | |X|X| | | |O| |O| |X| | |X|X| | |O|O| |O| |X| | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | |O| | |X| | | |O|X| | |X|O| | |X| | | |O|X|O| |X|O| | |X|X| | |O|X|O| |X|O| | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O|X|X| | | | | | |O|O| |O|X|X| | | | | |X|O|O| |O|X|X| | | | | |X|O|O| |O|X|X| | |O| | 'X' Won! |X|O|O| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| |X| | | |X| 'O' Won! |O|O|O| |X| |X| | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| |X|X|O| | |O| | | | |X| |X|X|O| | |O|O| 'X' Won! | | |X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O|X|X| | | |X| |O| | | |O|X|X| | | |X| |O| |O| |O|X|X| | | |X| |O|X|O| 'O' Won! |O|X|X| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | |O| |X| | | | | |O|X| | |O| |X| | | | | |O|X|X| 'O' Won! |O| |X| |O| | | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | | |X| | |X|X| 'O' Won! |O|O|O| | | |X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| | |X|O| | |X| | |X|O|O| |O|X|O| | |X| | |X|O|O| |O|X|O| | |X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | 'O' Won! |X|O| | | |O| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O|X| |X| | | |O| | | |O|O|X| |X| | | |O|X| | |O|O|X| |X| | | |O|X| | |O|O|X| |X|O| | |O|X|X| |O|O|X| |X|O| | 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| | | |X|O| | | |O|X| |X| |O| |X|O| | 'X' Won! |X|O|X| |X| |O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | |X| | |X| |O| |O| |O| | |X| | |X|X|O| |O| |O| | |X| | |X|X|O| |O| |O| |O|X| | |X|X|O| |O| |O| |O|X|X| 'O' Won! |X|X|O| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| |O| | | | | |X| |X|O|O| |O|X| | |O| |X| |X|O|O| |O|X| | |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| |X| |O| |O| | | |X| |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | | |X| |O|O|X| |O|X|O| |X| |X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |O| | |O| | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O| | | |X|X| |X| |O| |O|O| | | |X|X| |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | | | |O| |O|O|X| | |X| | | |X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| |X|X| | 'O' Won! |O|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | | | |X| |O|O| | |X| | | |O| |X| |O|O| | |X| |X| |O| |X| 'O' Won! |O|O|O| |X| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | |O| | | |O| | |X| |X| | |O| | | |O|X| |X| |X| 'O' Won! | |O| | | |O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X|O| |X| | | | | |O| |O|X|O| |X| | | | |X|O| |O|X|O| |X| | | |O|X|O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | |O|O| | |O|X|X| |X|X| | 'O' Won! |O|O|O| |O|X|X| |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | | | |O|X|O| | | |X| | |X| | |O|X|O| | | |X| | |X|O| |O|X|O| | | |X| |X|X|O| |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X|O|X| | |O| | |O| | | |X|O|X| |X|O| | |O| | | 'O' Won! |X|O|X| |X|O| | |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| |X| | | | |X| | |O|O|X| |X|O| | | |X| | |O|O|X| |X|O|X| |O|X| | |O|O|X| |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| |X| |X| |O| | | | | |O|O|X| 'X' Won! |X| |O| | |X| | |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |O| |X| |O| |O|O|X| |X|X|O| |X| |O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | |O| |O| | | | |X|X| 'X' Won! | | |O| |O| | | |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | |X| | | |X| |O| | |O|O| |X| | | |X| |O| | |O|O| |X|X| | 'O' Won! |X| |O| | |O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X|O| | |X| | |X| |O| |O|X|O| | |X| | |X|O|O| |O|X|O| |X|X| | |X|O|O| |O|X|O| 'O' Won! |X|X|O| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X|X| | | |O|O| 'O' Won! | | | | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X| |O| |X|O| | | |O| | 'X' Won! |X| |O| |X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | | |X| |X|O| | | |O| | |O| |X| |X|O| | | |O| | |O|X|X| |X|O| | | |O| | |O|X|X| |X|O| | |O|O| | |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |X| | |O| | | |O| |X| | |X| | |O| |X| |O| |X| | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O| |X| |X|X| | |O|O|X| |O| |X| |X|X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| |X| | | |O| |X|X|O| |O| |X| 'X' Won! |X| |O| |X|X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| |O|X| | | |O|X| |X|X|O| |O|X| | | |O|X| |X|X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | |O| |O| |O|X|X| | | | | |O|X|O| |O|X|X| | | | | 'O' Won! |O|X|O| |O|X|X| |O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | |O|X| | | | |O| | |X| | |O|X| | | | |O| | |X|X| |O|X| | | | |O| |O|X|X| |O|X|X| | | |O| |O|X|X| 'O' Won! |O|X|X| |O| |O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | |O| | | | |X| |X| |O| |X|O| | | | |X| |X| |O| |X|O| | |O| |X| |X| |O| |X|O|X| |O| |X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | |O|X| | |X| | |O|O|X| | |O|X| |O|X| | |O|O|X| | |O|X| 'X' Won! |O|X|X| |O|O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| |O|X| | | | |O| | |O|X| |O|X| | | |X|O| |O|O|X| |O|X| | | |X|O| |O|O|X| |O|X|X| | |X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| | |X|X| 'O' Won! | |X| | |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| | |O| | |O| |X| |X|O|X| |X|O| | |O| |X| |X|O|X| 'O' Won! |X|O| | |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O| |X| | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O| |X| |X|X| | |O|O|X| |O| |X| |X|X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| |O| |X| | | | |O| | |X|X|O| |X|O| | | |O| | |X|X|O| 'X' Won! |X|O| | |X|O| | |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | |O| |X|O| | |X|X| | | | |O| |X|O| | |X|X|O| | | |O| 'X' Won! |X|O| | |X|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X|X| |O| | | |O|O| | | |X|X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O|O| | 'X' Won! |O|O|X| |X|X|X| |O|O| | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | 'O' Won! |X| |O| | |O|X| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | | |X|O| 'O' Won! |O| |X| |X|O| | | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| |X| |X| |O| |O| | | |O| |X| |X| |O| |O| |X| |O| |X| |X| 'O' Won! |O|O|O| |X| |O| |X| |X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| | | |O|O| 'X' Won! |X|X|X| | |O| | | |O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X|X| | |O| |O| 'O' Won! | | | | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O|X| |O|X| | |O|X|O| | |O|X| |O|X| | |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X|X| | | |O|O| |X| |O| |X|X| | | |O|O| |X|X|O| |X|X| | | |O|O| 'O' Won! |X|X|O| |X|X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | |X|O|X| | | | | | |O| | |X|O|X| | | | | | |O|X| |X|O|X| | | | | |O|O|X| |X|O|X| | |X| | |O|O|X| |X|O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O|X| | | | | | | |X| |O|O|X| |O| | | 'X' Won! | | |X| |O|O|X| |O| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O|O| | | |X| | | | |X| |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| 'X' Won! | |O|X| |O|O| | |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| |X| |X| |O| 'O' Won! |O| |X| |O|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | |O| |X| | | |O|X|O| | | |O| |X| |X| |O|X|O| | |O|O| |X| |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | | |X| | |X| |O| |O|X| | |O|X| | 'X' Won! |X|X|O| |O|X| | |O|X| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | 'X' Won! | |O|X| | |X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X| |O| | |O| | | |X|O| |X|X|O| | |O| | |O|X|O| |X|X|O| | |O| | |O|X|O| |X|X|O| |X|O| | 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O|X| |X| | | |X|O|O| | |O|X| |X| | | 'X' Won! |X|O|O| |X|O|X| |X| | | | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | |O|X| |O|X| | | | |X| | |O|X| |O|X| | |O| |X| | |O|X| |O|X| | |O| |X| |X|O|X| |O|X| | |O|O|X| |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | |X| |O|X| | | |O|O| | | |X| |O|X| | | |O|O| | | |X| |O|X|X| 'O' Won! |O|O|O| | | |X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | | |X| 'O' Won! |X|X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | |O| |O|X| | | | |X| |X| |O| |O|X| | | | |X| |X| |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X|O| | | |O| | |X| |O| |X|O|X| | |O| | |X| |O| |X|O|X| | |O|O| |X| |O| 'X' Won! |X|O|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O| | |X|X| | |X| |O| | |O| | |X|X|O| |X|X|O| | |O| | |X|X|O| 'O' Won! |X|X|O| | |O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X|O| |X|O| | |X|O| | |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | |O| | |O|X|X| |X|O| | | |O| | |O|X|X| |X|O| | | |O|O| |O|X|X| |X|O|X| | |O|O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | | | |O| | | |X|X|O| | | | | |O|X| | |X|X|O| | | | | |O|X|O| 'X' Won! |X|X|O| | |X| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X|X| | |X| | | |O|O| |O|X|X| | |X|X| | |O|O| |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | |X| |O| | |O| | | |X| | |X| |O| | |O| | | |X|O| |X|X|O| | |O| | | |X|O| |X|X|O| |O|O| | | |X|O| |X|X|O| |O|O| | |X|X|O| 'O' Won! |X|X|O| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| |X| | | |X| |O| | | |O| 'O' Won! |X| |O| |X| |O| | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | |X|O| | | | | |X| | | | |X|O| | | |O| |X| |X| | |X|O| | | |O| |X| |X| | |X|O| |O| |O| |X| |X| |X|X|O| |O| |O| |X|O|X| |X|X|O| |O| |O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | |X| | | |O|X|O| |X|O| | |X| | | |O|X|O| |X|O| | |X| |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'X' Won! |X|O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| | | |O|O| | | |O|X| |X| |X| |O|O| | |O|O|X| |X| |X| |O|O| | 'X' Won! |O|O|X| |X| |X| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| |O|O| | | | |X| |X|O|X| |O|O| | | |X|X| |X|O|X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | |O| | |O| | | |X| |X| | |O|X| |O| | | |X| |X| | |O|X| |O|O| | 'X' Won! |X| |X| | |O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | |O| | |X|O| |O|X| | | | |O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | |O| |O| |O|X|X| | | | | |O| |O| |O|X|X| |X| | | |O| |O| |O|X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X|X|O| |O| | | | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O| |X| |O| |O| |X|X|O| |O|X|X| |O| |O| |X|X|O| 'O' Won! |O|X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | |O| | | | |X| |X|O|O| |X|O| | | | |X| |X|O|O| |X|O| | |O| |X| |X|O|O| |X|O|X| |O| |X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | |O|X| | |X| | |O| | | |X|O|X| | |X| | |O| | | |X|O|X| | |X| | |O| |O| |X|O|X| | |X| | |O|X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | | |X| | | |O| |X|O| | | | |X| | |O|O| |X|O| | |X| |X| | |O|O| 'O' Won! |X|O| | |X| |X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O|O| |X| | | |O|X| | |X|O|O| |X| |X| |O|X| | 'O' Won! |X|O|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| | |O|X| |X|X|O| | |O|O| | |O|X| |X|X|O| | |O|O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O|O| | | | |X| |X| |X| |O|O| | | | |X| |X|O|X| |O|O| | | | |X| |X|O|X| |O|O| | |X| |X| 'O' Won! |X|O|X| |O|O| | |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | |X|O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| |X|O| | | |O|X| 'O' Won! | |X|O| |X|O| | |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| |O|O| | |X| | | |O| |X| |O|O| | |X|X| | |O|O|X| |O|O| | |X|X| | |O|O|X| |O|O|X| |X|X| | 'O' Won! |O|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| |O| | |X|X| |O|O| | |X| |O| | |X|X| |O|O| | |X|O|O| | |X|X| |O|O|X| |X|O|O| 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | | |O|O|X| |X| |X| |O| | | |O|O|X| |X| |X| |O|O| | |O|O|X| 'X' Won! |X| |X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | |X| | |O|X| | | |O|O| | |X| | |O|X| | | |O|O| | |X|X| |O|X| | | |O|O| | |X|X| |O|X|O| 'X' Won! | |O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O|O| | | |X| | |X| |X| |O|O| | | |X| | |X|O|X| |O|O| | |X|X| | |X|O|X| |O|O| | 'O' Won! |X|X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | |X| | |X| | | |O| |O| | |X| | |X| |O| |O| |O| | |X| | |X|X|O| 'O' Won! |O|O|O| | |X| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| | | |O| | | |X| |O|X|X| | | |O| | | |X| |O|X|X| | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | | |O| |X| | | |O|X|O| | |X|O| |X| | | |O|X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| |O| |X| | |O|O| | |X|X| |O| |X| | |O|O| | |X|X| |O|O|X| | |O|O| 'X' Won! |X|X|X| |O|O|X| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O|X| | |X| |X| |O| |O| |O|X|O| |X| |X| |O| |O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X|X| |X|O| | |O| | | |O|X|X| |X|O| | |O| |X| |O|X|X| |X|O| | |O|O|X| |O|X|X| 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | 'O' Won! | | |X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O| | | |X|O|X| | |O| | |O| | | |X|O|X| | |O| | |O|X| | |X|O|X| | |O| | |O|X|O| |X|O|X| |X|O| | |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O|X|X| | |X| | 'O' Won! |O| | | |O|X|X| |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| | 'X' Won! |X| | | |X|O| | |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| | |O| | |X| | | |X|O|X| | |O| | 'O' Won! |X|O| | |X|O|X| | |O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | |O| |X| |O| | |O|X| |X| |O| |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | |O| |X| | |O| | |O|X| | |O| |X| |X|O| | |O|X| | |O| |X| |X|O|O| |O|X| | |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O|O| | | |O|X| | |X|X| |O|O| | | |O|X| | |X|X| |O|O| | |O|O|X| 'X' Won! | |X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| |X| |O| |X| | | |O| |O| |X| |O| |X| | | |O| |O|X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | | | |X| |O| | |X|O| |X| | | |X| |O| |O|X|O| |X| | | |X| |O| |O|X|O| |X|X| | |X| |O| 'O' Won! |O|X|O| |X|X|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X| |O| | |O| | |X| |O| |X|X|O| | |O| | |X| |O| |X|X|O| |O|O| | 'X' Won! |X| |O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | |X|O| | |X| | | | |O| |O|X|O| | |X| | | | |O| |O|X|O| |X|X| | | | |O| |O|X|O| |X|X| | |O| |O| 'X' Won! |O|X|O| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O|X| | | | | | |X|O|O| |O|X| | | | | | |X|O|O| |O|X|X| | | | | |X|O|O| |O|X|X| | | |O| |X|O|O| |O|X|X| | |X|O| It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | |O| | |O| | |X|X| | | |X|O| | |O| | |X|X|O| | |X|O| |X|O| | |X|X|O| | |X|O| 'O' Won! |X|O|O| |X|X|O| | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| |X|O|X| |X| |X| |O| |O| |X|O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| | | |X|X| | | |O| |O|O| | 'X' Won! |X|X|X| | | |O| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X|O| | | |X| | | |O| | 'X' Won! |X|O| | | |X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | | | | | |X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | |X| |O|X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| |O| |X| |O| | | | |X|X| |O| |X| |O| | | | |X|X| |O|O|X| |O| | | 'X' Won! | |X|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | |O|X| | | | |X| 'X' Won! |X|O| | |O|X| | | | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| |O| | |X| | |O|O|X| |X| |O| | |X|O| |O|O|X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | |X| |O|O| | |X| | | 'O' Won! | | |X| |O|O|O| |X| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| | | | | |O|X| | |X| |O| |X| | | |O|X| | |X| |O| |X| |O| |O|X| | |X| |O| |X|X|O| 'O' Won! |O|X|O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | |O|O|X| | |X| | | |O|X| |O|O|X| | |X| | | |O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| |O|X| | It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| 'O' Won! |O|O|O| | | | | |X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| |O| | | |X| |X|O|O| |X| |O| 'O' Won! |O| |X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| |X| | | | |O|X| | | |O| |X|O| | | |O|X| | | |O| |X|O| | | |O|X| | |X|O| |X|O| | | |O|X| |O|X|O| |X|O|X| | |O|X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| |O| |O| | | | | | |X|X| |O| |O| | | |X| | |X|X| |O| |O| | | |X| |O|X|X| |O| |O| |X| |X| |O|X|X| 'O' Won! |O|O|O| |X| |X| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | |X|O| | | | |X| | |O| | |X|O|O| | | |X| | |O|X| |X|O|O| | | |X| |O|O|X| |X|O|O| | | |X| |O|O|X| |X|O|O| | |X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | | |X|X| | |O| | |O| |O| | |X|X| | |O| | |O| |O| | |X|X| |X|O| | |O| |O| | |X|X| |X|O|O| 'X' Won! |O| |O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | | |O|X| |X| | | | | |O| | |O|X| |X| |X| | | |O| | |O|X| |X|O|X| | | |O| | |O|X| |X|O|X| | |X|O| 'O' Won! |O|O|X| |X|O|X| | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | | | | |O|O| 'X' Won! |X|X|X| | | | | | |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | | |X| |X| | | |O|O|X| |O| |X| |X| | | 'X' Won! |O|O|X| |O| |X| |X| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| | |O|X| | |X| | | |O|O| |X|O|X| | |X| | | |O|O| |X|O|X| | |X|O| | |O|O| 'X' Won! |X|O|X| | |X|O| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O|O| | | | | |X| |X| |X|O|O| | | | | |X| |X| |X|O|O| | | |O| 'X' Won! |X|X|X| |X|O|O| | | |O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| |X| |X| |X| |O| |O| |O|O|X| 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | | |X| | | |O|X| |O| |O| | |X| | | |O|X| |O|X|O| | |X| | | |O|X| |O|X|O| | |X|O| | |O|X| |O|X|O| |X|X|O| | |O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| |X| |O| | | |X| |O| |X| |X| |O| |O| |X| |O| 'X' Won! |X| |X| |O|X|O| |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | |O|O| | |X| |X| |O| | | |O|O| | |X| |X| |O| |X| |O|O| | |X| |X| |O|O|X| 'X' Won! |O|O| | |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O| | 'O' Won! |X|O|X| |O|O| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| |X|O| | | | |O| |O|X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | |X| |X| |O| |X| 'O' Won! |O|O|O| |X| |X| |O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X|X| 'O' Won! |O| | | |O| |X| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O|X|X| | | |O| | | | | |O|X|X| | | |O| | | |O| |O|X|X| |X| |O| | | |O| |O|X|X| |X| |O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| |O| | | | |X| | | | |O| |O| | | | |X| | |X| |O| |O| | | | |X|O| |X| |O| 'X' Won! |O| |X| | |X|O| |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| |X|O| | | | |O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O| | |X|O|O| 'O' Won! |O|X|X| |X|O| | |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | | | | |O|X|X| |O|O| | | | |X| |O|X|X| |O|O| | | |O|X| |O|X|X| 'X' Won! |O|O|X| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X|O| |O| |X| |X| |O| | |X|O| |O| |X| |X| |O| |O|X|O| |O|X|X| |X| |O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | |X|X| | | | | |O| |O| | |X|X| | | |O| |O| |O| | |X|X| |X| |O| |O| |O| | |X|X| |X|O|O| 'X' Won! |O| |O| |X|X|X| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | |O| | |X| |X| |O|X| | | |O| | |X| |X| |O|X| | | |O|O| |X| |X| |O|X|X| | |O|O| 'O' Won! |X| |X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X|O| | | |X| |O|X| | | |X|O| | | |X| |O|X| | | |X|O| |O| |X| |O|X|X| | |X|O| |O| |X| |O|X|X| | |X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| |O| |X| |X| | | |O| |O| |O|X|X| |X|O| | |O| |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| |O|O| | | |O| | | |X|X| |O|O| | |X|O| | | |X|X| |O|O| | |X|O| | |O|X|X| |O|O| | |X|O|X| |O|X|X| 'O' Won! |O|O|O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | |O| | | |X| | |X| |O| |X|O| | | |X| | |X| |O| |X|O| | | |X| | |X|O|O| 'X' Won! |X|O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X|X|O| 'O' Won! | | |O| | | |O| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|X| | |O| | |O| |X| |O|X|X| | |O| | |O|X|X| |O|X|X| | |O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | |O| | | | |X|X| | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| | |O|O| |O|X|X| | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X|O| | |X|O| 'X' Won! | |X| | |O|X|O| | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X| | | |X| | |O|O|X| |O|X| | | |X|O| 'X' Won! |O|O|X| |O|X| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O|O|X| | |X| | |X|O| | |O|O|X| | |X| | |X|O|O| |O|O|X| |X|X| | |X|O|O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X| |X| | |O| | |O|O| | |X| |X| | |O|X| |O|O| | |X| |X| 'O' Won! | |O|X| |O|O|O| |X| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| |X| |O| | |O|X| |O| |X| |X| |O| | |O|X| |O| |X| |X|X|O| | |O|X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| |O| |O| | |X|O| | | |X| |O|X|O| |O|X|O| | | |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O|X|X| | | | | |O| | | |O|X|X| | | | | |O| |O| |O|X|X| |X| | | |O| |O| 'O' Won! |O|X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X|O| | |O|O| |X| | | |X|X|O| | |O|O| 'O' Won! |X| |O| |X|X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O|O| |X| | | | |X| | |X|O|O| |X| |O| | |X| | |X|O|O| 'X' Won! |X| |O| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| | |O|X| | | | | |X|X|O| |O|O|X| | | |X| |X|X|O| |O|O|X| | |O|X| |X|X|O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O| | |O|X|X| 'O' Won! |O|X|O| | |O| | |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O| |O| |X| |X| 'X' Won! |O| | | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | |X| |X| |O|O| | | | |O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O| | |O|X|O| |X| |X| |O|O| | |O|X|O| |X| |X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | |X| | | | |O| | |X|O| |O|X| | | | |O| 'X' Won! | |X|O| |O|X| | | |X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | |O|X|X| |X|O|X| |O|O| | |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O|O| |X| | | |O| | | |X|O|O| |X|X| | |O| | | |X|O|O| |X|X| | |O|O| | 'X' Won! |X|O|O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | |O|X|O| | | |X| | | | | |O|X|O| | | |X| | | |X| |O|X|O| |O| |X| | | |X| |O|X|O| |O| |X| |X| |X| |O|X|O| |O| |X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O| |X| |O|X| | | |X|O| |O| |X| |O|X| | | |X|O| |O| |X| |O|X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | | |X|O|O| |O|X|X| | | | | |X|O|O| |O|X|X| |X| | | |X|O|O| 'O' Won! |O|X|X| |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | | |O|O| | | | | |X|X| | |X|O|O| | | | | |X|X| | |X|O|O| | | |O| |X|X| | 'X' Won! |X|O|O| |X| |O| |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | |O| | |X| | |X|O| | | | |O| |X|X| | |X|O| | | |O|O| |X|X| | |X|O| | 'X' Won! | |O|O| |X|X|X| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| | | |O|O| |X| |X| |O|X| | | |O|O| |X| |X| |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | |X| | |O|X|O| | | | | |O|X| | |O|X|O| | | | | 'X' Won! |O|X| | |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | |O| | |X|X| |X| |O| | | |O| | |X|X| |X| |O| |O| |O| | |X|X| 'X' Won! |X| |O| |O|X|O| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| |X| | | | |X| | | | |O| |X|O| | | |X| | | | |O| |X|O|X| | |X| | | | |O| |X|O|X| | |X|O| | | |O| 'X' Won! |X|O|X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | |O|X| | |X| |O| | | | | |O|X|X| |X| |O| | | | | |O|X|X| |X| |O| | |O| | |O|X|X| |X| |O| |X|O| | |O|X|X| |X| |O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | |X| | | | |O|O| |X| | | |X| |O| | |O|O| |X|X| | |X| |O| | |O|O| 'O' Won! |X|X| | |X| |O| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | |O|O|X| | |X| | | | |X| |O|O|X| | |X| | | |O|X| |O|O|X| | |X| | | |O|X| |O|O|X| |X|X| | | |O|X| |O|O|X| |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X|O| | |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| 'O' Won! |O| | | |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | |X| | |X| |O| |O| |O| | |X| | |X| |O| |O| |O| | |X|X| |X| |O| |O| |O| |O|X|X| |X| |O| |O|X|O| |O|X|X| |X| |O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | |O| | | | |X|X| |X|O| | |O| | | | |X|X| |X|O| | |O| |O| | |X|X| |X|O|X| |O| |O| | |X|X| |X|O|X| |O| |O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O|X| | |O|O| | | | |X| |O|X| | |O|O|X| | | |X| |O|X| | |O|O|X| | |O|X| |O|X| | 'X' Won! |O|O|X| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X| | |X| | | | |X|O| |O|X| | |X| |O| | |X|O| 'X' Won! |O|X| | |X|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| |O| |X| | | | | |X|X|O| |O| |X| | |O| | |X|X|O| |O| |X| | |O|X| |X|X|O| |O| |X| |O|O|X| |X|X|O| |O| |X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | |O| | |X|X| | | |O| | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O|X| |X|X| | 'O' Won! |O|O| | |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X|O|O| |X| | | | | |X| |X|O|O| |X| | | | |O|X| |X|O|O| |X| | | | |O|X| |X|O|O| |X|X| | | |O|X| |X|O|O| |X|X|O| 'X' Won! |X|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X| | | | |X| |O|X| | |O|X|O| | | |X| 'X' Won! |O|X| | |O|X|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O| | | | |X|X| |X|O|O| |O| | | |O|X|X| |X|O|O| |O|X| | |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | |X|O| |X|O| | | | | | |O|X|O| |X|O| | | | | | |O|X|O| |X|O|X| | | | | |O|X|O| |X|O|X| | |O| | |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| | |X| | |O|X| | | |O|X| | |X| | |O|X| | |O|O|X| 'X' Won! |X|X| | |O|X| | |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | | | | | |X|X| 'O' Won! |O|O|O| | | | | | |X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| |X| | |O| | |O|X| | |O| |X| | |O| | |O|X| | |O|X|X| 'O' Won! |O|O| | |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | |O|X| | |X| | |O| | | |X|O|X| | |X| | |O| | | |X|O|X| | |X|O| |O| | | |X|O|X| | |X|O| |O|X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X| |O| |O| | | | |O|X| |X|X|O| |O| | | | |O|X| |X|X|O| |O| |O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | | | |X| | |O|X|O| 'X' Won! |O|X| | | |X| | |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| |O| |X|O| | | |O|X| |X| |O| |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| 'O' Won! | |O| | | |O|X| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | | | |O| | | |X|X|O| | | |X| |O| | | |X|X|O| |O| |X| |O| | | |X|X|O| |O|X|X| |O| | | |X|X|O| |O|X|X| |O|O| | 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| |X| | |O|X| |O| | | |X| |X| |O|O|X| 'X' Won! |O| | | |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |X| |O| |O| |X| | | | | |X| |O| |O| |X| |X| | | |X| |O| |O| |X| |X| | |O|X| |O|X|O| |X| |X| | |O|X| |O|X|O| |X|O|X| | |O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | | |X|X| |X| |O| |O|O| | | |X|X| 'X' Won! |X| |O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| |O| |O|X| | | | |X| |O|X|O| 'O' Won! |O|X| | | |O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | | | | |X| |X| |O|O| | | |X| | |X| |X| |O|O| | |O|X| | |X| |X| |O|O| | |O|X|X| |X|O|X| |O|O| | |O|X|X| 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| |O|X| | | |O|O| | | |X| |O|X| | | |O|O| | |X|X| 'O' Won! |O|X| | |O|O|O| | |X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| |X| |X| |X| | |O|O| |O| |X| 'X' Won! |X|X|X| | |O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | |O| |X| | | | |O|X| | |X|O| |X| | | | |O|X| |O|X|O| |X| | | | |O|X| |O|X|O| |X|X| | | |O|X| |O|X|O| |X|X| | |O|O|X| 'X' Won! |O|X|O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | | | |X|O|O| |X|O| | 'X' Won! |X| | | |X|O|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | |X|O| | |O| | |O| |X| | |X|O| | |O| | |O| |X| |X|X|O| |O|O| | |O| |X| |X|X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | 'X' Won! | | | | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | | | | | |X|X| |O|O| | |O| | | 'X' Won! |X|X|X| |O|O| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | | | |X| |O| |O| |X|X| | |O| |X| |O| |O| |X|X| | 'X' Won! |O| |X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| | | |X| |X|O|O| |X|X|O| 'O' Won! |O| |X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | |X| | |X| |O| | |O| | | |X| | |X|O|O| | |O| | | |X| | |X|O|O| |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| |X|O| | 'O' Won! |O|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| |O| | |O|X| |X| |X| |O| |O| |O|O|X| |X| |X| |O| |O| 'X' Won! |O|O|X| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| | |X|O|X| |X| | | |O|O| | 'O' Won! |X|O|X| |X| | | |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X|O| | | |X| | |O|O| | |X|O| | |X|X| | 'O' Won! |O|O| | |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O|O| |O| | | |X| |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| |O| |X| |X| |X|O|O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | | | |X| | |O|X|O| |O| | | | |X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O|O| | |X|X| | 'X' Won! |O|X|O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | |X| |O|O| | |X|X| | 'O' Won! | | |X| |O|O|O| |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| |X| | |O| | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | |X| |X| |O|O|X| |O|O| | |X| |X| 'X' Won! |O|O|X| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | | |O| |X| |O|X|X| | | | | |O| |X| |O|X|X| | |O| | |O| |X| 'X' Won! |O|X|X| | |O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X|X| | | | |O| |O| | | |X|X| | |X| |O| |O| | | |X|X| | |X| |O| |O| |O| |X|X| | |X|X|O| |O| |O| |X|X| | 'O' Won! |X|X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O|X| | |X| |O| | |X| | |O|X| | |X|O|O| | |X| | |O|X| | |X|O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | |X| |O| | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| | |X| | | |X|O| |X|O|O| | |X| | | |X|O| |X|O|O| | |X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O| | |X| |O| | |X|O| |X|O| | |X| |O| 'O' Won! |O|X|O| |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X|O| | 'X' Won! |X| |O| |X| |O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X| | | | |X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X| |X| |O|X|X| |O| |O| 'O' Won! |X| |X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| |O| |X|O| | | | | | |X|X|O| |X|O| | | | | | |X|X|O| |X|O|O| 'X' Won! |X| | | |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| |O| | | |O|X|X| | | |X| |O| | | |O|X|X| | | |X| |O|O| | |O|X|X| | |X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | 'X' Won! | |O| | |X|X|X| | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O|X| |O| | | | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O|O|X| | |X| | |O|O|X| 'X' Won! |O|O|X| | |X|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| | |O| | |X|O|X| | | |O| | |O|X| |X|O|X| |O| |O| | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X|O| | | | | |X|O|O| |X|X|O| | |O| | |X|O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| | | |X| 'O' Won! |O|O|O| | | |X| | | |X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X|O|X| | |O| | | |X| | |X|O|X| | |O|O| | |X| | |X|O|X| 'X' Won! |X|O|O| | |X| | |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | |O| |X| |X| |X|O|O| | | |O| |X| |X| |X|O|O| |O| |O| 'X' Won! |X|X|X| |X|O|O| |O| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O| | | | |X| | | |O| |X|O| | | | |X| | |X|O| |X|O| | |O| |X| | |X|O| |X|O|X| |O| |X| | |X|O| |X|O|X| |O| |X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| |X|X| | | |O|X| | |O|O| |X|X| | 'X' Won! | |O|X| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O| |X| | | |O| | |X| | |O|O|X| | | |O| | |X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| |X|X|O| | |X|O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | | | | |O|X| | | |O|X| | | | | |O|X|O| | |O|X| 'X' Won! |X| | | |O|X|O| | |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | |X|O| | |O|X|O| | | | | |X|O| | |O|X|O| | | |X| |X|O| | |O|X|O| | |O|X| |X|O| | |O|X|O| |X|O|X| |X|O| | 'O' Won! |O|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | | |X| |X| | | |O|O| | |O| |X| |X| | | |O|O| | |O| |X| |X|X| | |O|O| | |O| |X| |X|X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | |O|O|X| 'X' Won! |X| | | | |X| | |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | |O| |O| | | |X| | |X| | |O| |O| | |X|X| | |X| | 'O' Won! |O|O|O| | |X|X| | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X|O| | |O| | | |X| | |X|X|O| | |O| | | |X| | 'O' Won! |X|X|O| | |O| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | |O| | | | |X| | |O|X|X| |O| | | | |X| | |O|X|X| |O| |O| | |X|X| |O|X|X| |O| |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| |X|O| | | |O|O| |O|X|X| |X|O| | |X|O|O| |O|X|X| |X|O| | It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O|X| | |O|X| | |X|O| | |O|X| 'X' Won! |X|O|X| | |X|O| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X|X| | |O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | |O|X| |O| |O| |X| |X| | |O|X| |O| |O| |X|O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | |X| |X| | | |O| |X|O| | |X| |X| |O| |O| |X|O| | 'X' Won! |X| |X| |O|X|O| |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | |X| | | |O|X| | |O|O| | |X| | | |O|X| | |O|O| | |X|X| |O|O|X| | |O|O| | |X|X| |O|O|X| |X|O|O| | |X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| | |X| | | | |O| 'X' Won! |O| |X| | |X| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | |X| | | | |O| |O| |X| | |X| | |O| |O| |O|X|X| | |X| | |O| |O| 'O' Won! |O|X|X| |O|X| | |O| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | | | |O|X|O| |O|X| | | | |X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| | | | |O|X| | |X|O| |O| | | | |O|X| | |X|O| |O| |X| | |O|X| | |X|O| |O|O|X| | |O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | |O|X| | |X|X| | |O| | | |O|X| |O|X|X| | |O| | | |O|X| |O|X|X| | |O| | |X|O|X| |O|X|X| |O|O| | |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X|O| | |O|X| | |X| | | |X|O| |O|O|X| | |X| | | |X|O| |O|O|X| |X|X| | | |X|O| |O|O|X| |X|X|O| | |X|O| 'X' Won! |O|O|X| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | |X| |X| | |O| | | |O| | |X| |X| | |O|X| | |O| | |X| |X| | |O|X| |O|O| | |X| |X| |X|O|X| |O|O| | |X| |X| 'O' Won! |X|O|X| |O|O| | |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |O| | | | |X|O| | | | | |O|X| | | |X|O| | | | | |O|X|O| | |X|O| | | |X| |O|X|O| 'O' Won! | |X|O| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| |X| | | |O| |X| |O| |O| |X| | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| |O|X|X| | | |O| | |X|O| |O|X|X| | |O|O| | |X|O| |O|X|X| 'X' Won! |X|O|O| | |X|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X|O| | |X| | |O|O| | |X|X|O| 'O' Won! |O|X| | |O|O| | |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | | |O| | | | |O| |X|X| | | |O|X| | | |O| |X|X|O| | |O|X| | | |O| |X|X|O| | |O|X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | |X|O| |X| | | |O| | | | |X|O| |X| | | |O| |O| | |X|O| |X|X| | |O| |O| 'O' Won! | |X|O| |X|X|O| |O| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X|O|O| | | | | | |O|X| |X|O|O| |X| | | |O|O|X| |X|O|O| |X| | | |O|O|X| |X|O|O| |X|X| | 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| |X|O| | | |X| | |O|O|X| |X|O| | |X|X| | |O|O|X| |X|O| | |X|X| | |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X|O| | | |X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O| | |X|O|O| |O|X| | |X|O|X| |X|O|O| |O|X| | It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| | |O|O| | |X| | | |O|X| | |O|O| |X|X| | 'O' Won! | |O|X| |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | |O|O| |X| |X| | | | | | |O|O| |X|O|X| | | | | | |O|O| |X|O|X| |X| | | | |O|O| 'O' Won! |X|O|X| |X|O| | | |O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | | |O|X| | | |X| |O| | | |O|O|X| | | |X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O|O| | 'X' Won! |O|O|X| | |X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X|O| | | |X| | |X|O| | |X|O| | | |X| | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| | | |O|X|X| | | | | |O| |O| |O|X|X| | | | | |O| |O| |O|X|X| |X| | | 'O' Won! |O|O|O| |O|X|X| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | |X| | | |O|X| | | |O|O| |X| | | |O|X| | | |O|O| |X| | | |O|X|X| | |O|O| |X| |O| |O|X|X| |X|O|O| |X| |O| |O|X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | |O| |X| | |X|O| | |O| | |O|X|X| | |X|O| | |O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| |O|O| | 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | |X|X|O| |O|X| | | | |O| |X|X|O| |O|X| | 'X' Won! | |X|O| |X|X|O| |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| |X| |O| | |X| | | |X|O| 'O' Won! |X| |O| | |X|O| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| |X| | | | | |X|X|O| |O|O|X| | |X| | |X|X|O| |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | |X| | |O|X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | |X|O| 'O' Won! |O|O|X| | |O|X| | |X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X|X| |O| | | |X| |O| | |X|X| |O|O| | 'X' Won! |X| |O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | |O| | |O| | |X| |X| | |X|O| | |O| | |X| |X| |O|X|O| | |O| | |X| |X| 'X' Won! |O|X|O| | |O| | |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |X| |X| |O| 'O' Won! |O|O|X| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X|X| |X|O| | |O| |O| | |X|X| |X|O| | |O|X|O| | |X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | | | | | |O|X|O| | |X| | | | |X| |O|X|O| | |X| | |O| |X| |O|X|O| |X|X| | |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X|X| | |O|O|X| | |O|O| |X|X| | |O|O|X| 'X' Won! |X|O|O| |X|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |O| | |O| | |X| |X| |O|X|O| | |O|O| |X| |X| |O|X|O| 'X' Won! | |O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| |O|X| | | |O|X| | |X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| |O| |X| | | |O| | |X|O| |O| |X| | |X|O| | |X|O| |O| |X| | |X|O| |O|X|O| 'X' Won! |O|X|X| | |X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | |O| | | | |O|X| |O|X| | |O| |X| | |O|X| 'O' Won! |O|X| | |O| |X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X| | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | |O|X| | |O| |O| |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X| | | | |O|X| | |O| | |X| |X| | |O|X| | |O| | |X| |X| | |O|X| | |O|O| |X| |X| | |O|X| |X|O|O| 'O' Won! |X|O|X| | |O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | | | | |O|X| | |O|O|X| | | | | |O|X| | |O|O|X| | |X| | 'O' Won! |O|X| | |O|O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X|O| |O|O|X| | |X|X| | |X|O| |O|O|X| |O|X|X| | |X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| |X| |O| | | |O|X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O|X| | |O|X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X|O| |X| | | 'X' Won! |O|X|X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| |X| | | | | |O|X|O| |X| |X| | | |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X| |X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| |O| | | |O| |X| |X| |X| |O| | |O|O| |X| |X| |X| |O| 'X' Won! |X|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | |O| | | | |O| |X| |X| |X|O| | | | |O| |X| |X| |X|O|O| | | |O| |X| |X| 'X' Won! |X|O|O| | | |O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | |O| 'X' Won! | | |O| |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X| | | |O|O|X| | |X|O| |X| |O| |O|O|X| | |X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| | |X|X| |X| |O| |O| |O| | |X|X| |X|O|O| 'X' Won! |O| |O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| |X|O| | | |O| | |X| |O| |X|O| | 'X' Won! |X|O| | |X| |O| |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X|X| |O|O| | | |O| | | |X|X| |O|O| | |X|O| | | |X|X| 'O' Won! |O|O|O| |X|O| | | |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | |X|O| | |O| |X| | |X| | |X|O|O| |O| |X| | |X|X| |X|O|O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | |X|O| | | | | |X|X|O| 'O' Won! | |X|O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | | | | |X| | |X|O|O| | | | | | |X|O| |X|O|O| | |X| | | |X|O| |X|O|O| | |X| | |O|X|O| |X|O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | |O|X| |X|O| | | | | | | |O|X| |X|O|X| | | | | |O|O|X| |X|O|X| | | | | |O|O|X| |X|O|X| | |X| | |O|O|X| |X|O|X| |O|X| | 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | |X| |O| |X| |O| |O| | | |X| |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| | |O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | 'O' Won! | |O|X| | |O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | | |X|O| | |O|X| | |O| | | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| |O|O|X| | |X|O| | |O|X| 'X' Won! |O|O|X| | |X|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | |O| | |X|X| | |X| |O| | |O| | |X|X| | |X| |O| | |O|O| 'X' Won! |X|X| | |X| |O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | |X|X| | | |O|O| |O|X| | 'O' Won! |X|X|O| | |O|O| |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | | |X| |X|O| | |X|O|O| | | |X| |X|O| | 'O' Won! |X|O|O| | |O|X| |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | |X|X| | |O| | |O| | | |O|X|X| | |O| | |O| | | |O|X|X| | |O|X| |O|O| | |O|X|X| | |O|X| 'X' Won! |O|O|X| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| |O| | | |O| |O| |X| |X| |O|X| | |O| |O| |X| |X| |O|X|O| |O| |O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | | | | | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | |O|X| | 'X' Won! |O|X|O| |X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | | | |O|X|X| |X| |O| | | |O| |O|X|X| |X|X|O| | | |O| |O|X|X| 'O' Won! |X|X|O| | |O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| | |X|O| 'O' Won! |O| | | |X|O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| |O|X| | | | | | | |O|X| |O|X|X| | |O| | | |O|X| |O|X|X| |X|O| | | |O|X| |O|X|X| 'O' Won! |X|O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | | |O| | |O| | 'X' Won! |X|X|X| | | |O| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | |X| |X| | |X|O| |O|O| | |X| |X| | |X|O| |O|O|X| |X| |X| |O|X|O| |O|O|X| |X| |X| 'X' Won! |O|X|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | |X| |X| | | 'O' Won! |O|O|O| | | |X| |X| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O|X| |O| |X| | | | | | |O|X| |O|O|X| | | | | 'X' Won! | |O|X| |O|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O|X| |O| | | | | |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| |O| |X| |X| |O|O|X| 'X' Won! |O| |O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | |X|O| |O|X| | | | | | |O|X|O| |O|X| | | | | | |O|X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | |O| 'X' Won! |O|X|O| |O|X|X| | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | | | | | |O|O|X| |O|X| | |X| | | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | |O|O| |X| |X| |O| |X| | |O|O| 'O' Won! |X| |X| |O| |X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O| |O| | |X|X| 'O' Won! |X|O| | |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X| | | | |X|O| |O|O| | |X| | | | |X|O| |O|O| | |X| | | |X|X|O| |O|O| | |X| |O| |X|X|O| |O|O| | |X|X|O| |X|X|O| 'O' Won! |O|O|O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O| | | | |X|X| | |X|O| |O|O| | | |X|X| | |X|O| |O|O|X| | |X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | | | | 'X' Won! | |O|X| | |X|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | | |O|O| | |X| |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X|O|O| 'X' Won! |X|X|X| |O|O| | |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | | |O| | |X| | |X|X|O| | | |O| | |X| | |X|X|O| | |O|O| | |X| | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | | | | |X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| |O| | |X|X| |X|O|O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X| | |O| |O| |X| |X| |O|X| | |O| |O| |X| |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| | | |X|O|X| |O|O| | |X| | | |X|O|X| |O|O| | |X|X| | |X|O|X| 'O' Won! |O|O|O| |X|X| | |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | |X|O|O| | |O| | |X|X| | |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|O| |X| |X| | |X|O| | |O|O| |X| |X| |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| |X| |O| |X|X| | |O|X|O| |X|O|O| |X|X| | 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| |X|O|X| | | |O| | |O|X| 'O' Won! |X|O|X| | |O|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O| |O| |X| | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O| |X| |O| |O| |X|X|O| |O| |X| |O|X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | |X| |O| |O| |X|O|X| | | |X| |O| |O| |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| | |O|O| | |X| | | | |X|X| |O|O| | |X|O| | | |X|X| |O|O| | 'X' Won! |X|O| | |X|X|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | |O|X| |O| | | |X|X| | | |O|X| |O| | | 'O' Won! |X|X|O| | |O|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| 'O' Won! |O| |X| | |O|X| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | |X|O| | | | | |O|O|X| | |X|O| | | | | 'X' Won! |O|O|X| | |X|O| |X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O| | |X|O|X| | |X|O| 'O' Won! |O|O| | |X|O|X| | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | |O| | |O|X|X| |O|X| | | |O| | |O|X|X| |O|X| | | |O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X|X| |O| | | |X| |O| |O|X|X| |O|X| | |X| |O| |O|X|X| |O|X| | |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| |O|O| | | |O| | | |X|X| 'X' Won! |O|O| | | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | | | |X|O| | 'X' Won! |X| |O| |X| | | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| |O| |O| | |X|X| | | |X| |O| |O| 'O' Won! | |X|X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | | |O|X| |O|X| | | | | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X| | |O| |X| |X|O|X| |O|X| | 'O' Won! |O| |X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | |O|X| |O| | | | | |X| | |O|X| |O| | | | | |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| |O| 'X' Won! |X|X|X| |O|O|X| |O| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| |X| | | | |X| | |O|O|X| |X|O| | | |X| | |O|O|X| |X|O|X| |O|X| | |O|O|X| |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | |X| |O| | | | | | |X|O| |X| |O| | | | | |X|X|O| |X| |O| | | | | |X|X|O| |X|O|O| | | | | |X|X|O| |X|O|O| | |X| | 'O' Won! |X|X|O| |X|O|O| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | | | | 'X' Won! |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| | |O| | |O|O| | |X| |X| |X|O| | |O|O| | |X| |X| |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | |X| | | |O|O|X| |X|O| | |X| | | |O|O|X| |X|O| | |X|X| | 'O' Won! |O|O|X| |X|O| | |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| |O| |X| | |X|O| |O| |X| |O| |X| | |X|O| |O|X|X| 'O' Won! |O| |X| |O|X|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X|X| | | | |O| |X| |O| |X|X| | | |O|O| 'X' Won! |X| |O| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| |O| |X| | | | | |O| 'X' Won! |X| |O| |X| | | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| |X| |O| |X| |O|O| | |X| |X| 'X' Won! |O| |X| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O|X| | |O| | | |X| | | |O|X|O| |O| | | |X| |X| |O|X|O| |O| | | |X| |X| |O|X|O| |O|O| | 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | 'X' Won! | |O|X| | |X| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X|O| | | | |X| |O| | | |X|O| | | |X|X| |O| |O| |X|O| | | |X|X| |O| |O| |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X| | |O|X| | | | |O| 'X' Won! |O|X| | |O|X| | | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | | |O|O| | |X| | |X| | | |X|O|O| | |X| | |X|O| | |X|O|O| 'X' Won! |X|X| | |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| 'O' Won! |O| | | |X|O|X| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | 'O' Won! |O| |X| |O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | |O| | | | |X|O| 'X' Won! |O|X| | |O|X| | | |X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | |X|O|X| | | | | |O|O| | |X|O|X| | |X| | |O|O| | |X|O|X| 'O' Won! | |X| | |O|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | | |O| |X| |X| |X| |O| |O| |O| |X| |X| |X| |O| |O| |O|X|X| 'O' Won! |X| |X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | |O| | | | |X| | |O|X| | |O| | |X| |X| | |O|X| |O|O| | |X| |X| | |O|X| 'X' Won! |O|O|X| |X| |X| | |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| |O| | | | |X| | |O|X|X| |O| | | | |X| | 'O' Won! |O|X|X| |O| | | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | 'X' Won! |O| | | | | |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| | | | | | |X| | |O| |X| | |O| | | |X| | |O| |X| | |O|X| | |X| | |O|O|X| | |O|X| | |X| | |O|O|X| |X|O|X| | |X| | 'O' Won! |O|O|X| |X|O|X| | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| | | |O| | | | |X|X| |O| | | |O|X| | | |X|X| |O| |O| |O|X| | | |X|X| |O| |O| |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | | | |O|O| | 'X' Won! |X|X|X| | | | | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X| |O| | |O| | | |X|O| |X| |O| |X|O| | 'O' Won! | |X|O| |X| |O| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | |O|O| | |X| | | |X|X| | |O|O| | |X| | |O|X|X| | |O|O| | |X|X| |O|X|X| | |O|O| 'O' Won! | |X|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | |X| |O| | | |X|O| | | | |X| |O|O| | |X|O| | | |X|X| |O|O| | |X|O| | |O|X|X| |O|O| | |X|O| | |O|X|X| |O|O| | |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | |X|O| | |X| | | | |O| | |X|O| | |X| | | |O|O| | |X|O| | |X|X| | |O|O| |O|X|O| | |X|X| | |O|O| 'X' Won! |O|X|O| |X|X|X| | |O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | | |X| |O|O| | | |X| | | |O|X| |O|O| | | |X|X| | |O|X| 'O' Won! |O|O|O| | |X|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X| | | |O|X|O| | |X| | |X|O| | |O|X|O| | |X| | |X|O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X| | | |O| | 'X' Won! |O| |X| |O|X| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O| |O| |X| |O|X| | |X| |O| |O| |X| |O|X| | |X| |O| |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |X| | |X|O| |O|X|O| | | |X| | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X|O|X| 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| | | |O| |O| |X|X|O| |X| | | |O| |O| 'O' Won! |X|X|O| |X| | | |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| |X|O| | | | | | |O| |X| |X|O| | |O| | | |O| |X| |X|O| | |O| |X| |O| |X| |X|O|O| |O| |X| |O| |X| 'X' Won! |X|O|O| |O|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | 'X' Won! |X| | | |X| |O| |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | |X|O| | | | |X| |O| | | |X|O| | | |O|X| |O| | | |X|O|X| | |O|X| |O| | | |X|O|X| |O|O|X| |O| | | |X|O|X| |O|O|X| |O|X| | It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| 'O' Won! |X|X|O| |O|X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | | | |X| |O|X| | | |O| | | |O|X| |O|X| | | |O| | 'X' Won! | |O|X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O| |X| | |X|X| 'O' Won! |O| | | |O| |X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | |O|X|X| 'X' Won! | |O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | 'X' Won! |X| |O| |X|O|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| |X|X| | |O| | | | |O|X| |X|X| | |O|O| | | |O|X| |X|X| | |O|O|X| | |O|X| |X|X| | |O|O|X| |O|O|X| 'X' Won! |X|X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | |O| | | | |X|O| |X| | | |O|X| | | |X|O| |X| | | |O|X| | |O|X|O| |X| | | |O|X|X| |O|X|O| |X|O| | |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | |O|X| |O| |X| | | | | 'X' Won! | |O|X| |O| |X| | | |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| |O| |X| | |X| | | |O|O| |O| |X| |X|X| | | |O|O| |O| |X| |X|X|O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | |O|X| |O| |X| | |X| | | |O|X| |O| |X| | |X| | |O|O|X| |O| |X| 'X' Won! | |X|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | | |X| | | |O| |O|X|X| | |O|X| | | |O| |O|X|X| |X|O|X| | | |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | | |O| | |X|O| |O|X| | | | |O| | |X|O| |O|X| | | | |O| |X|X|O| 'O' Won! |O|X|O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | |X|O| | | | |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O| | 'O' Won! |X|O|X| |O|O| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |X|X| | | |X| | |O|O| | |X|X| |O| |X| 'X' Won! | |O|O| |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X|X| | | |X| |O| | | |O|X|X| | |O|X| 'X' Won! |O| |X| |O|X|X| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|X| | | | | 'O' Won! |O|X|X| | |O|X| | | |O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | |X|O| | | |O| |X| | | | |X|O| | |O|O| |X| | | | |X|O| |X|O|O| 'O' Won! |X| |O| | |X|O| |X|O|O| 1.1 Clean Data \u00b6 We will first need to organize the data into a parsable format. Q1 \u00b6 What is the object data and what does it contain? what are the keys of data? what are the keys of each game? # inspect data below by grabbing the first key in data # what are the three different keys within each game? data [ 'game 0' ] {'board': {1: 'O', 2: 'X', 3: 'X', 4: ' ', 5: 'X', 6: ' ', 7: 'O', 8: 'O', 9: 'O'}, 'winner': 'O', 'starting player': 'O'} Q2 \u00b6 Using those keys, iterate through every game in data and append the board, the winner, and the starting player to separate lists. Call these lists: boards, winners, and starters boards = [] winners = [] starters = [] for game in data : boards . append ( data [ game ][ 'board' ]) winners . append ( data [ game ][ 'winner' ]) starters . append ( data [ game ][ 'starting player' ]) Q3 \u00b6 Make a dataframe out of the list boards and call it df . Make a series out of the list winners . Make a series out of the list starters . Make a new column of df called \"Winner\" and set it equal to the pandas Series of the winners. Make a new column of df called \"Starter\" and set it equal to the pandas Series of the starters. # YOUR CODE HERE df = pd . DataFrame ( boards ) df [ \"Winner\" ] = pd . Series ( winners ) df [ \"Starter\" ] = pd . Series ( starters ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 Winner Starter 0 O X X X O O O O O 1 O X O X X X X O O X X 2 O X X O O O O 3 X O X X X O O O O O O 4 X X X O O O X O 1.2 Inferential Analysis \u00b6 We're going to use Bayes Rule or Bayesian Inference to make a probability of winning based on positions of the board. The formula is: $ P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)}$ Where $\\cap$ is the intersection of $A$ and $B$. The example we will use is the following: what is the probability of 'O' being the winner, given that they've played the center piece. $B$ = 'O' played the center piece $A$ = 'O' won the game So what is probability? We will define it in terms of frequencies. So if we are for instance asking what is the probability of player 'O' being in the center piece, it would be defined as: $ P(B) = \\frac{|O_c|} {|O_c| + |X_c| + |empty|}$ Where the pipes, | | , or cardinality represent the count of the indicated observation or set. In this case $O_c$ (O being in the center) and $X_c$ (X being in the center). Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc_Xc_empty 1000 ( df [ 5 ] == 'O' ) . value_counts () False 550 True 450 Name: 5, dtype: int64 # example of assessing the probability of B, O playing the center piece player = 'O' Oc = ( df [ 5 ] == player ) . value_counts () Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc / Oc_Xc_empty False 0.55 True 0.45 Name: 5, dtype: float64 # we can also clean this up and replace the denominator with the whole # observation space (which is just the total number of games, df.shape[0]). # example of assesing probabiliy of A ( df [ 'Winner' ] == 'O' ) . value_counts () / df . shape [ 0 ] False 0.571 True 0.429 Name: Winner, dtype: float64 The $P(B|A) * P(A)$ is the intersection of $B$ and $A$. The intersection is defined as the two events occuring together. Continuing with the example, the probablity of 'O' playing the center piece AND 'O' being the winner is the number of times these observations occured together divided by the whole observation space : # in this view, the total times A and B occured together is 247 player = 'O' df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] 253 # the total observation space is 1000 (1000 games) df . shape [ 0 ] 1000 And so we get: $P(B|A) * P(A) = \\frac{247} {1000} = 0.247 $ In code: df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] / df . shape [ 0 ] 0.253 1.2.1 Behavioral Analysis of the Winner \u00b6 Q4 \u00b6 define the 3 different board piece types and label them middle , side , and corner . Middle should be an int and the other two should be lists. # define the 3 different board piece types middle = 5 side = [ 2 , 4 , 6 , 8 ] corner = [ 1 , 3 , 7 , 9 ] 1.2.1.1 What is the probability of winning after playing the middle piece? \u00b6 Q5 \u00b6 # A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'X' # define the intersection of A AND B, A_B A_B = df . loc [( df [ 'Winner' ] == player ) & ( df [ middle ] == player )] . shape [ 0 ] / \\ df . shape [ 0 ] # define prob B B = ( df [ middle ] == player ) . sum () / \\ df . shape [ 0 ] # return A_B over B (The prob B given A) A_B / B 0.5807962529274004 Q6 \u00b6 # A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'O' # define the intersection of A AND B, A_B A_B = df . loc [( df [ 'Winner' ] == player ) & ( df [ middle ] == player )] . shape [ 0 ] / \\ df . shape [ 0 ] # define prob B B = ( df [ middle ] == player ) . sum () / \\ df . shape [ 0 ] # return A_B over B (The prob B given A) A_B / B 0.5622222222222222 1.2.1.2 What is the probability of winning after playing a side piece? \u00b6 Q7 \u00b6 # A intersect B: O played side and O won / tot games # B: O played side / tot games player = 'O' A_B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.4252136752136752 # A intersect B: X played side and X won / tot games # B: X played side / tot games player = 'X' A_B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.41458106637649617 1.2.1.3 What is the probability of winning after playing a corner piece? \u00b6 Q8 \u00b6 # A intersect B: O played corner and O won / tot games # B: O played corner / tot games player = 'O' A_B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.44235033259423506 Q9 \u00b6 # A intersect B: X played corner and X won / tot games # B: X played corner / tot games player = 'X' A_B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.4375678610206298 Are these results surprising to you? Why? This resource may be illustrative. 1.3 Improving the Analysis \u00b6 In this analysis, we only tracked what moves were made, not the order they were made in. It really limited our assessment! How might we change our recording of the games to track order of moves as well? Do we need to track all the moves or just the first and the winner?","title":"SOLN P1 Statistical Analysis of TicTacToe"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#data-science-foundations-project-part-1-statistical-analysis","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to perform statistical analysis on data generated from our tictactoe program!","title":"Data Science Foundations, Project Part 1: Statistical Analysis"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#10-preparing-environment-and-importing-data","text":"back to top","title":"1.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#101-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"1.0.1 Import Packages"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#102-load-dataset","text":"back to top data = {} for i in range ( 1000 ): game = GameEngine () game . setup_game () board = game . play_game () data [ 'game {} ' . format ( i )] = { 'board' : board . board , 'winner' : board . winner , 'starting player' : board . start_player } | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | |X|X| | | | | |O|O| | |O|X|X| | | | | |O|O| | |O|X|X| | |X| | |O|O| | 'O' Won! |O|X|X| | |X| | |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X|X| | |O| | | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |O| | | | |X|X| |O| | | |O| | | 'O' Won! |O|X|X| |O| | | |O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |O| | |O| | |X| |X| | | |O| | |O| | |X| |X| | | |O| |O|O| | |X| |X| | |X|O| |O|O| | |X|O|X| | |X|O| |O|O| | |X|O|X| |X|X|O| |O|O| | 'O' Won! |X|O|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| |O| |O| 'X' Won! |X|X|X| | | |O| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| |X|O| | | | |O| |O|X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X|O| | |X|O|O| | | | | |X|O|X| |X|O|O| | | |O| |X|O|X| 'X' Won! |X|O|O| | |X|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X|X| |O|O| | |O| | | | |X|X| 'X' Won! |O|O| | |O| | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| | | | |O|X| |X| |O| |X| | | | |O|X| |X| |O| |X| | | |O|O|X| 'X' Won! |X| |O| |X|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X|O|X| | |O| | | |X| | |X|O|X| |O|O| | | |X| | |X|O|X| |O|O| | |X|X| | |X|O|X| 'O' Won! |O|O|O| |X|X| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X| |X| | | | | |X|O|O| |X|O|X| | |X| | |X|O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| |X| |X| | |X|O| |O|O|X| |X| |X| |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| | |O| | |X|X| | |X|O|O| |O|O| | |X|X| | |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | |X| | | |O|X|O| | | |O| |X| | | |O|X|O| |X| |O| |X| | | |O|X|O| 'O' Won! |X| |O| |X| |O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | |X|O| | |X| | |X|O| | |O|X|O| | |X| | |X|O|X| |O|X|O| 'O' Won! | |X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| |O| |X| | | | | |X| |O| |O|X|X| | | | | |X| |O| |O|X|X| |O| | | |X| |O| 'X' Won! |O|X|X| |O|X| | |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X|X| | |X| |O| 'O' Won! | | |O| |X|X|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| |X|O| | | | | | | |X|O| |X|O| | | | |O| | |X|O| |X|O|X| | | |O| | |X|O| |X|O|X| | |O|O| | |X|O| |X|O|X| |X|O|O| 'O' Won! |O|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O| |O| |X|X| | |X|O|O| |O| |O| |X|X| | |X|O|O| |O|X|O| |X|X| | 'O' Won! |X|O|O| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | |X| | |O|X|O| | |O| | | |X| | |O|X|O| | |O| | |X|X| | |O|X|O| | |O| | |X|X|O| |O|X|O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | |O| | |X| |X| 'X' Won! |O| |O| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X|X| | |O| |O| | | | | |X|X| | |O| |O| | | |O| |X|X| | |O| |O| |X| |O| 'O' Won! |X|X|O| |O| |O| |X| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X|O| | | | | | |O|O|X| |X|O|X| | | | | |O|O|X| |X|O|X| |O| | | |O|O|X| |X|O|X| |O|X| | 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | |O| | | |X| | |O|X| |X| |O| | | |X| |O|O|X| |X| |O| | | |X| |O|O|X| |X| |O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | |X| | |O| |O| |X|O| | | |X| | |O| |O| |X|O| | |X|X| | |O| |O| |X|O|O| |X|X| | 'X' Won! |O| |O| |X|O|O| |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | | | | | | |X|X| |O|O| | | | |X| | |X|X| |O|O| | | |O|X| 'X' Won! |X|X|X| |O|O| | | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| | | |O| | | |X| |X|O|O| |X| |O| | |O|X| |X|O|O| |X| |O| 'X' Won! |X|O|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O| |O|X|O| | | | | 'X' Won! | |X|O| |O|X|O| | |X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X|O| | | |O|X| | | |O| |X|O| | | |O|X| | |X|O| |X|O| | |O|O|X| | |X|O| |X|O| | |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| |X|O| | |X|X|O| | |O|O| |X|O| | |X|X|O| | |O|O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X|X| | |O|O| | | | |O| |X|X| | |O|O| | |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | |O|O| | | |X| |X| | | |X|O|O| | | |X| |X| | | |X|O|O| | |O|X| 'X' Won! |X| | | |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O| | 'O' Won! |X|X| | |O|O|O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | |O| |X| |O|X|O| | | | | |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |O|X| 'X' Won! |O| |X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X|X| |X| | | | | |O| |O|X|X| |X| | | | |O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X| |O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X| |O| | |O|X| |O| | | |X| |O| |X|O|X| |O| | | |X|O|O| |X|O|X| |O| |X| |X|O|O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | | |X| | |X| |O| | |O| | |O|X| | |X| |O| | |O| | |O|X|X| |X| |O| | |O| | |O|X|X| |X|O|O| 'X' Won! | |O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | 'O' Won! |X| |X| | | | | |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | | | | |X|O|X| |O| |O| | | | | |X|O|X| |O| |O| | | |X| |X|O|X| |O| |O| | |O|X| |X|O|X| |O| |O| |X|O|X| 'O' Won! |X|O|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X|X| | |O| | |O| |O| | |X|X| 'X' Won! | |O| | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | |O|O| | | |X|X| | |O| | |O|O| | | |X|X| |X|O| | |O|O| | | |X|X| |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| |X| | | | |X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| |X| |O|X|O| 'O' Won! | |X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | |X|O| | | |X| |X| |O| |O|X|O| | |X|X| |X| |O| |O|X|O| | |X|X| |X|O|O| |O|X|O| 'X' Won! |X|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | |O|X| | | |O|X| | |X| | |O|X| | | |O|X| | |X| | |O|X|O| | |O|X| | |X|X| |O|X|O| | |O|X| |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| |O| |O| |X|X|O| |X| |X| |O| |O| 'O' Won! |X|X|O| |X| |X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| |O| | | |X| | | |O| |X|X|O| | | |X| | | |O| |X|X|O| | | |X| |O| |O| |X|X|O| | |X|X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X|X| |X|O| | |O|O| | | |X|X| |X|O| | |O|O| | | |X|X| |X|O|X| 'O' Won! |O|O|O| | |X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| | | |X| |O|O| | | |X|O| | |X|X| |O|O| | | |X|O| 'O' Won! |O|X|X| |O|O| | | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O| | 'X' Won! |X|O|X| | |X|O| |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O| |O| | |X| | | | |X| 'O' Won! |O|O|O| | |X| | | | |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |O| | | |O|X|X| | | | | |O| | | |O|X|X| | | | | |O| |O| |O|X|X| | |X| | |O| |O| 'O' Won! |O|X|X| |O|X| | |O| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X|O| | |X| | |O| | | |O|X|O| |X|X| | |O|O| | |O|X|O| |X|X| | 'X' Won! |O|O|X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X|O|X| | | |X| |O| |O| |X|O|X| | |O|X| |O| |O| |X|O|X| |X|O|X| |O| |O| |X|O|X| 'O' Won! |X|O|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| | | |X|X|O| |O|O|X| |X| |O| |X|X|O| 'X' Won! |O|O|X| |X|X|O| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O|X| | | |X| |O|X|O| | |O|X| 'X' Won! | | |X| |O|X|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | |X|O| | | |X| |O|O| | | |X|O| | | |X| |O|O|X| | |X|O| | | |X| |O|O|X| |O|X|O| | | |X| |O|O|X| |O|X|O| | |X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | |X| | | |O|X|O| |X| | | |X| |O| |O|X|O| |X| | | |X|X|O| |O|X|O| |X| | | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | |O|O|X| | | |X| |X| | | |O|O|X| |O| |X| |X| | | |O|O|X| |O|X|X| |X| | | |O|O|X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | | |O| | | |O| |X|O|X| | |X|O| | | |O| |X|O|X| | |X|O| |O| |O| |X|O|X| |X|X|O| |O| |O| 'O' Won! |X|O|X| |X|X|O| |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O| |O| |O|X|X| | |X| | |O| |O| |O|X|X| | |X|O| |O| |O| 'X' Won! |O|X|X| | |X|O| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | |O|X| |X|O| | | | |X| | |O|X| |X|O| | | | |X| | |O|X| |X|O|O| |X| |X| | |O|X| |X|O|O| 'O' Won! |X|O|X| | |O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | |O| |X| |X| | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| | |O|O| 'X' Won! |X| |O| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | |X|O| | | | |O| |X| | | |X|O| | | | |O| |X| |O| |X|O| | | | |O| |X|X|O| |X|O| | | |O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X| | | 'O' Won! | |X| | |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X| |O| |X| | | |O| |O| |X| |O| |X| | | |O|X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | |O| |X| | |O| | |O|X| | |O| |X| | |O|X| |O|X| | 'O' Won! |O| |X| |O|O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| |O| | | |X| | |O|O| |X|X|O| | | |X| | |O|O| 'O' Won! |X|X|O| | | |X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X| |X| |X|O|O| | |O| | |X| |X| 'X' Won! |X|O|O| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | | |O| | | 'O' Won! |X| |O| |X|O| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | |O| | |O|X|X| | |X| | | |O| | |O|X|X| | |X| | | |O|O| |O|X|X| | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| |O| |X| | |O| | | |O|X| 'X' Won! |O| |X| | |O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | |X|X| | |O| |X| |O| | | |X|X| | |O|O|X| 'X' Won! |O| | | |X|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | | | | |X|O| 'X' Won! | |X|O| | |X| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | |O| |X| |O|X|O| | | | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | |O| | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |O|X| | |O| | | | |X|O| |O|X|X| |O| | | 'O' Won! |O|X|O| |O|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| |X|X| | | |O|O| | | |X| 'O' Won! |X|X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O| |O| |X|O| | |X|X| | |O| |O| |X|O|O| |X|X| | |O| |O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | | |X| | |O|X|O| | | | | |X|X| | |O|X|O| | | | | |X|X| | |O|X|O| | |O| | 'X' Won! |X|X|X| |O|X|O| | |O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | | | |X|X| | 'O' Won! |O|O|O| | | | | |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| | | | | |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| | |X| |O| |O|X| | |X|O| | 'X' Won! |X| |O| |O|X| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O| |X| | | |O| |O|X| | |O| |X| | | |O| |O|X|X| |O| |X| | | |O| 'O' Won! |O|X|X| |O| |X| |O| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| |X| |X| | | | | 'O' Won! |O|O|O| |X| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | |X| | | |X|O|O| |O|X| | |X| | | |X|O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X| |O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | | |X| |O|O| | |X| | | | |X|X| |O|O| | |X| | | | |X|X| |O|O| | |X|O| | 'X' Won! |X|X|X| |O|O| | |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | |X| | 'O' Won! |O|O|O| | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O|O| |X| |O| |X| |X| | |O|O| |X| |O| |X|O|X| | |O|O| 'X' Won! |X| |O| |X|O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | |X|X| | |O| | |O| | | | |X|X| |O|O| | |O| | | | |X|X| |O|O|X| |O|O| | | |X|X| |O|O|X| 'X' Won! |O|O| | |X|X|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | |X| | |O| |O| | |O|X| | |X| | |O| |O| | |O|X| | |X|X| 'O' Won! |O|O|O| | |O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| | |X| | |X|O|O| | | |O| | |X|X| |X|O|O| | | |O| |O|X|X| |X|O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X| |O|X|O| | | |X| | | |X| |O|X|O| | | |X| | |O|X| |O|X|O| | | |X| |X|O|X| |O|X|O| |O| |X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | |O|O| | | | | |X|O|X| | |O|O| | |X| | |X|O|X| 'O' Won! |O|O|O| | |X| | |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| | | |O| | | |X|X|O| |O| |X| |O| | | |X|X|O| |O| |X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| |O| |X| |X|X| | 'X' Won! |O| |O| |O| |X| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | | | | |O|X| | | |O|X| | | |O| |O|X| | | |O|X| |X| |O| |O|X| | | |O|X| |X| |O| |O|X|O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | |O|X| | |O| | |X| | | |X|O|X| | |O| | |X| | | |X|O|X| | |O| | |X| |O| 'X' Won! |X|O|X| |X|O| | |X| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X|O| |X|O| | | |X| | | |X|O| |X|O| | |O|X| | | |X|O| |X|O| | |O|X| | | |X|O| |X|O|X| |O|X| | |O|X|O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | |O| | |X| | |O| |X| | | |O| | |X|X| |O| |X| |O| |O| | |X|X| 'X' Won! |O| |X| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | |X| |O|O| | | |X| | | |O|X| |O|O| | |X|X| | | |O|X| |O|O| | 'O' Won! |X|X|O| | |O|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | |O| |O| | | | |X|X| | | |O| |O|X| | | |X|X| | |O|O| |O|X| | | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |O| | |O| | |X|X|O| | |X|O| | |O| | |X|X|O| |O|X|O| | |O| | |X|X|O| |O|X|O| |X|O| | 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | 'O' Won! |X| | | | | |X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| |O|O| | |O| |X| | | |X| 'X' Won! |O|O|X| |O| |X| | | |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | |O| |X|X|O| | | | | | |X|O| |X|X|O| 'O' Won! | | |O| | |X|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |X| | |X| | |O|X|O| |O| |X| 'X' Won! | |X| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | |O| |X| |O| |X|O| | | | |O| |X|X|O| |X|O| | 'O' Won! | | |O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X|X| | |O| |O| | |O| | |X|X| | |O| |O| | |O|X| |X|X| | |O| |O| |O|O|X| |X|X| | |O|X|O| |O|O|X| |X|X| | 'O' Won! |O|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X| | | | |X|O| |O| |O| |X| |X| | |X|O| |O| |O| |X| |X| |O|X|O| |O| |O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | |X| | | |O|O| |X|X| | |O|X| | 'X' Won! | |O|O| |X|X|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| |X|O| | | | | | | |X|O| |X|O| | | | | | |O|X|O| |X|O|X| | | | | |O|X|O| |X|O|X| |O| | | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | | | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| | | |X|X|O| 'X' Won! |O|X|O| |O|X| | |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | |O| | | |O|O|X| | |X| | |O|X| | |O|O|X| | |X| | |O|X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| |X|X| | 'O' Won! |O|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| |X| | | | | |X|O|O| |X| |X| |O| | | |X|O|O| 'X' Won! |X|X|X| |O| | | |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | |X|X| | | |O| | |O|O| | |X|X| | | |O| | |O|O| | |X|X| | |X|O| | |O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| |O|X|O| It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X| |X| 'O' Won! |O|O|O| |X|X|O| |X| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|O| |X|O| | |O| |X| | |X|O| |X|O|O| |O| |X| | |X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | |O|O| |X| | | |X| |O| | |O|O| |X| |X| |X| |O| | |O|O| |X| |X| |X|O|O| 'X' Won! |X|O|O| |X| |X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | 'X' Won! | | |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X|O| | | | | |X|X|O| | |X|O| | | | | |X|X|O| |O|X|O| | | |X| |X|X|O| |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | |X|X|O| | |O|O| | |X| | |X|X|O| | |O|O| |X|X| | |X|X|O| 'O' Won! |O|O|O| |X|X| | |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X|X| | | | | | |O| |O| |X|X| | | | |O| |O| |O| |X|X| | | |X|O| 'O' Won! |O| |O| |X|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | |O| |O| | | |X| | |X| | |O| |O| | | |X| | |X|O| |O| |O| | |X|X| | |X|O| |O| |O| | |X|X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| | | |O|O|X| |O| | | |X|X| | |O|O|X| |O| | | |X|X|O| |O|O|X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X|X| | |O| | | | |X| |O|X|X| |O|O| | | | |X| |O|X|X| |O|O| | |X| |X| 'O' Won! |O|X|X| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X| | | |X| | | |O|O| |O|X| | |X|X| | | |O|O| |O|X| | 'O' Won! |X|X| | |O|O|O| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | |O| | | |O|X| |X| | | | |O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X| |X| 'O' Won! |O|O| | |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| | | | | |X|O| | |O| |X| |O| | | |X|O| | |O| |X| |O|X| | |X|O| | |O| |X| 'O' Won! |O|X|O| |X|O| | |O| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | |O|X| | |X| |O| | | | | |O|X|X| 'O' Won! |X| |O| | |O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O|X| | |X| | | |O| | |O|O|X| | |X| | |X|O| | |O|O|X| | |X| | |X|O|O| |O|O|X| | |X|X| |X|O|O| |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | |X|X| | | | | |O|O|X| |O|X|X| | | | | |O|O|X| |O|X|X| | |X| | |O|O|X| |O|X|X| | |X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| | | |O| | | |X| |X|O|O| | |X|O| | | |X| |X|O|O| |O|X|O| |X| |X| |X|O|O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| |O| |O| | |X| | |X|O|X| |O| |O| 'O' Won! | |X|O| |X|O|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | |X| | |O| | | |O| |X| | |X|O| |O| | | |O| |X| | |X|O| |O| |X| |O| |X| 'O' Won! | |X|O| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| | | |O| |O| |X| |O| |X| |X| |O| |O| |X| |O| |X|O|X| |O| |O| |X| |O| |X|O|X| |O| |O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | |X| | | | | | |O|X|O| | |X| | | |O| | |O|X|O| | |X| | |X|O| | |O|X|O| 'O' Won! |O|X| | |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| |O|O| | |X| | | |X| |O| |O|O|X| |X|O| | |X| |O| |O|O|X| 'X' Won! |X|O| | |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| |O|O| | | | | | |X| |X| 'X' Won! |O|O| | | | | | |X|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| |X|O| | |O| | | |X|O|X| |X|O| | |O| |X| |X|O|X| |X|O| | 'O' Won! |O| |X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X| | |O| | | |O|O| 'X' Won! |X|X|X| | |O| | | |O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | |X| | | |O|O| |X|X| | | |X|O| | |O|O| |X|X| | |X|X|O| | |O|O| |X|X| | 'O' Won! |X|X|O| |O|O|O| |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | |O| | |X| |X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | |X| | |O|O| | |X|X| | |O|X| | |O|O| | |X|X| | |O|X|X| |O|O| | |X|X|O| |O|X|X| |O|O| | 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | |O| | |X|O| | |X|X| | | |O| | |X|O| | |X|X| | | |O|O| 'X' Won! |X|O| | |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O|X| | | | | | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | 'O' Won! | |X|O| |X|O|O| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | |O|X| | | | |X| | |O|X| |O|X| | | | |X| | |O|X| |O|X| | |O| |X| 'X' Won! | |O|X| |O|X|X| |O| |X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |O| | | | |X|O| | | | | |O|X| | |O|X|O| | | | | |O|X| | |O|X|O| | | |X| |O|X| | 'O' Won! |O|X|O| |O| |X| |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X| | | 'X' Won! |X|O|O| |X| | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | |O| | |O| | | |X|X| | | |O| | |O| |X| |X|X| | |O|O| | |O| |X| |X|X| | |O|O|X| |O| |X| |X|X| | |O|O|X| |O|O|X| 'X' Won! |X|X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | |X| | | | |O| |X|O|O| | |X| | | | |O| |X|O|O| |X|X| | 'O' Won! | | |O| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X| |O| | | |O| | |O|X| |X| |O| | | |O| |X|O|X| |X| |O| | |O|O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | |O|O| | | |X| | |X|X| | |O|O| | | |X| | |X|X|O| |O|O| | | |X| | |X|X|O| |O|O| | |X|X| | 'O' Won! |X|X|O| |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | |O| | |X| | | |O|X|X| | |O| | |X| | | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|X| |X| | | | |X|O| | |O|X| |X|O| | | |X|O| | |O|X| |X|O|X| | |X|O| | |O|X| |X|O|X| |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| 'O' Won! |O| | | |O| |X| |O| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | |X|O|X| | | | | |O|O| | |X|O|X| | | |X| 'O' Won! |O|O| | |X|O|X| | |O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X|O| | | | | | |O|O|X| |X|O| | | |X| | |O|O|X| |X|O| | |O|X| | |O|O|X| |X|O| | |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| | | |X| |X|X|O| |O| |O| | |X|X| |X|X|O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | |X| | | | |X| |O|O| | | |X| | | |O|X| |O|O| | | |X| | |X|O|X| |O|O| | |O|X| | |X|O|X| |O|O| | |O|X|X| |X|O|X| |O|O| | 'O' Won! |O|X|X| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O|X|O| |O| | | |X| |X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | | |O|X| | |X| | |X|O| | | |O|X| |O|X| | |X|O| | | |O|X| |O|X|X| |X|O| | | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | |O| | | |X|X| | |O|O| | |O|X| 'O' Won! | |X|X| |O|O|O| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X|O| |X| |X| |O|X| | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| |X| |O|X| | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X|X| |O|O| | |X|O|X| |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| |X| |X| | | |O| |O| |O| |X| 'X' Won! |X| | | |O|X|O| |O| |X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | |X|X| | | |O| |O| |X| | |X|X| |O| |O| |O| |X| | |X|X| |O|X|O| |O|O|X| | |X|X| |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| |O| |X|O| | |O| | | |X|X|O| |X|O| | |O| | | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| |O| |O| |X| | | |X|O|X| |O| |O| |X| | | |X|O|X| |O| |O| |X|O| | 'X' Won! |X|O|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X|X| | | | | |O|O| | | |X|X| | | |X| |O|O| | | |X|X| | | |X| |O|O| | |O|X|X| | |X|X| |O|O| | |O|X|X| 'O' Won! |O|X|X| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X|O| | |X| | | | |O| |X|X|O| | |X| | | | |O| |X|X|O| |O|X| | |X| |O| |X|X|O| |O|X| | |X|O|O| |X|X|O| |O|X| | 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | |X| |X| | |O| | |O|O| | |X| |X| | |O| | |O|O|X| |X| |X| | |O|O| |O|O|X| |X| |X| 'X' Won! | |O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| | |O| |X| | |O|O| | |X| | |O| |X| |X|O|O| | |X| | |O|O|X| |X|O|O| |X|X| | |O|O|X| |X|O|O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| | |X| | |O| |O| |X|O|X| | |X| | |O| |O| |X|O|X| |X|X| | 'O' Won! |O|O|O| |X|O|X| |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | |O| | | |X|O| | |O|X| | |O|X| | |X|O| | |O|X|O| |O|X| | |X|O| | |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O| | | |X|X| |X| |O| |O|O| | | |X|X| |X|O|O| 'X' Won! |O|O|X| | |X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| | | |X| |X| |O| | |O|X| | |O|X| |X| |O| | |O|X| |X|O|X| |X| |O| 'O' Won! |O|O|X| |X|O|X| |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O|X| |X| | | |O| | | |O|O|X| |X| | | |O| | | |O|O|X| |X| | | |O| |X| |O|O|X| |X|O| | |O| |X| |O|O|X| |X|O| | |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O| | | |X| |O|X|O| | | |O| | | |X| |O|X|O| | | |O| | |X|X| |O|X|O| | |O|O| | |X|X| 'X' Won! |O|X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | |X| | |O| | | |O|X|O| | |X| | 'X' Won! |O|X| | |O|X|O| | |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| | | |X|O|O| 'X' Won! |O|X|X| |O|X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O|O| | |X|X|O| | | |X| |O|O| | 'O' Won! |X|X|O| | | |X| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | 'O' Won! |X| |O| | |O|X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O|X| |X| | | | | |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | |X|X| | |O| |X| |O| |O| |X|X| | |O| |X| 'X' Won! |O| |O| |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O|X| | |X| | |X| | | |O|O|X| | |X|O| |X| | | |O|O|X| |X|X|O| |X|O| | |O|O|X| |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| |X| | | | | |X|O|O| |X| |X| | |O| | |X|O|O| 'X' Won! |X|X|X| | |O| | |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| |X| | |X|O| | | | | |O|O|X| | |X|O| | | | | |O|O|X| |X|X|O| 'O' Won! |O| | | |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X|X| |O|X| | | |O| | |O|X|X| |O|X| | | |O|X| |O|X|X| 'O' Won! |O|X| | |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X| |O| |O| | |X|X| | | |X| |O| |O| | |X|X| | |O|X| |O|X|O| | |X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | |X| | |X|O|X| | | |O| | |X|O| |X|O|X| | | |O| | |X|O| |X|O|X| | |X|O| 'O' Won! |O|X|O| |X|O|X| | |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | |O|X| | |O|X|X| | | | | |O|X| | |O|X|X| | | |O| |O|X| | 'X' Won! |O|X|X| | |X|O| |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X|X| | | |X| | 'O' Won! |O|O|O| |X|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O| | |O| |X| |X| | | | |O|X| |O| |X| |X| | | | |O|X| |O|O|X| |X|X| | | |O|X| |O|O|X| 'O' Won! |X|X|O| | |O|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X|O| | | | | | | |O| |X|X|O| | | | | |X| |O| |X|X|O| |O| | | |X| |O| |X|X|O| |O| | | |X|X|O| 'O' Won! |X|X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | | | | |X|O| | |X|O|O| | | | | |X|O|X| |X|O|O| | | | | |X|O|X| |X|O|O| |O| | | |X|O|X| |X|O|O| |O|X| | It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | | |O| | |O|X| |X|X| | | | |O| | |O|X| |X|X| | | |O|O| 'X' Won! | |O|X| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X| |X| | |O|O| |X|O|O| |X| |X| 'X' Won! | |O|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| |O| | | |X| | | | |X|O| |O| |X| |X| | | | |X|O| |O| |X| |X| | | |O|X|O| |O|X|X| |X| | | |O|X|O| 'O' Won! |O|X|X| |X|O| | |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | |X|O| | | | | |X|O| | | |X|O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | 'X' Won! |X|O|X| |O|X|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X| |X| |O| |O| 'X' Won! |X|O|O| |X|X|X| |O| |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X|O| | | | | |O|X| | | |X|O| | | |X| |O|X| | |O|X|O| | | |X| |O|X| | 'X' Won! |O|X|O| | |X|X| |O|X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O|X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| |O| | | | | | | |X|X|O| |O| | | | |O| | |X|X|O| |O| | | | |O| | |X|X|O| |O|X| | | |O| | |X|X|O| |O|X|O| |X|O| | |X|X|O| |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O|X| | |O| | | | | | |X|O|X| |O|O| | | | | | |X|O|X| |O|O|X| | | | | |X|O|X| |O|O|X| | | |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | |O| | |X|X| |X| |O| | | |O| | |X|X| |X| |O| |O| |O| 'X' Won! |X|X|X| |X| |O| |O| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X|X| | |X|O| | |O|O| | |X|X| | 'X' Won! |X|O| | |O|O| | |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | | | |X| | |O|X| |O| | | | |O|X| 'X' Won! | |O|X| |O| |X| | |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| |X| | | | | |X|O|X| |O| |X| | |O| | 'X' Won! |X|O|X| |O| |X| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | |X| |X| | | |O| |O|O| | |X| |X| |X| |O| 'O' Won! |O|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| | | |X|X| | | |O| | |O|O| | |X|X| | | |O| | |O|O|X| |X|X| | | |O|O| |O|O|X| |X|X| | |X|O|O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | |O| | |X| |O| |X| |X| | |O| | |X| |O| |X| |X| | |O|O| |X| |O| |X| |X| | |O|O| |X|X|O| 'O' Won! |X| |X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X| | | 'O' Won! |X| | | |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| |O| | | |X| | |X| | |O| |O| | | |X| | |X|X| |O| |O| | |O|X| | |X|X| |O| |O| |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | | |X| | |O| |O| |X|X|O| | |X|X| |O| |O| |X|X|O| 'O' Won! | |X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X|X| | 'O' Won! |O|X|O| | |O|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | | |O| | |X|X| |X| |O| | | |O| | |X|X| |X|O|O| | | |O| 'X' Won! |X|X|X| |X|O|O| | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X|O| | |O|X| | | |O| |X|X|O| |O|O|X| | | |O| |X|X|O| |O|O|X| |X| |O| |X|X|O| 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | |O| |X| | |X|O| |X| | | |O| |X| | |X|O| |X| |O| |O| |X| | |X|O| |X| |O| |O|X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | |X|O| |O| | | |X| | | |O|X|O| |O| | | |X| | | |O|X|O| |O|X| | |X| | | |O|X|O| |O|X|O| 'X' Won! |X|X| | |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| | | |O|X| | | | | |X|O| | |O|O|X| | | | | |X|O|X| |O|O|X| | | | | 'O' Won! |X|O|X| |O|O|X| | |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | |X| | | |X|O| |O| |O| | |X| | 'X' Won! | |X|O| |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| | | | |O|O| 'X' Won! |X|X|X| |O| | | | |O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | |O| | | | |X| | |O|X| |X|O| | | | |X| | |O|X| |X|O| | |O| |X| |X|O|X| |X|O| | |O| |X| |X|O|X| |X|O|O| |O| |X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O|X|O| |X| |X| | | |O| |O|X|O| 'X' Won! |X|X|X| | | |O| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | |O|X| | 'X' Won! |X|O|O| |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X|O| | | | |O| |X| | | |X|O| | | | |O| |X|X| | |X|O| | | | |O| |X|X|O| |X|O|X| | | |O| |X|X|O| |X|O|X| |O| |O| |X|X|O| 'X' Won! |X|O|X| |O|X|O| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O|O| | |O|X| | |X| | | |O|O|X| |O|X|O| |X| | | |O|O|X| |O|X|O| |X|X| | |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | |X| |X| | |O| | |O|O|X| |X| |X| | |O| | |O|O|X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| 'O' Won! |O|O|X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X| | |O| |O| | |X| | | |X|X| |O| |O| | |X| | |O|X|X| |O| |O| |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| |O| | | | | |X|O|X| |X| |O| | | | | |X|O|X| |X| |O| | |O| | 'X' Won! |X|O|X| |X| |O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | |X|O|X| | |O| | |O|X|O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X|O| | | |X| | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O|O| | |X| | |X|O|O| |X|O|O| | |X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X|O| | | | | |X|O|X| |O|X|O| | | | | |X|O|X| |O|X|O| | | |O| 'X' Won! |X|O|X| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|O| | |O| | 'X' Won! |X| | | |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | | |X|X|O| | | | | |O| | | |X|X|O| | | |O| |O|X| | |X|X|O| | | |O| |O|X| | |X|X|O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| | | |X|O| | | |X|O| |X| |O| |X|O| | 'X' Won! |X|X|O| |X| |O| |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | |O|O| |O|O|X| 'X' Won! |X|X|X| | |O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | |X|O|O| | |X|O| | | | | |X|O|O| | |X|O| | |X| | |X|O|O| | |X|O| |O|X| | |X|O|O| |X|X|O| |O|X| | |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X|X| | |O|O| | |X| | 'O' Won! | |X|X| |O|O|O| | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | 'O' Won! |O|O|O| | |X| | | |X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X|O| | | |O| 'X' Won! | | |X| |O|X|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | 'X' Won! |X| |O| |X|O|O| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X| |X| |X|O| | |O| |O| |X| |X| |X|O| | |O|X|O| |X| |X| |X|O|O| |O|X|O| 'X' Won! |X|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X|O| | |X| | | |O| |O| |X|O| | |X| |X| |O| |O| |X|O| | |X|O|X| |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | | | |X| | |X|O| |O|X| | |O| |X| | |X|O| |O|X|X| |O| |X| | |X|O| |O|X|X| |O|O|X| 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O| | | | |X| |O|X|X| |O|O| | | | |X| |O|X|X| 'X' Won! |O|O|X| | | |X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| |O| | |X| | | |X| | 'X' Won! |O|X|O| | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | |X| |O|O| | |O|X| | | | |X| |O|O| | |O|X| | |X| |X| |O|O| | |O|X| | |X|O|X| |O|O| | |O|X|X| |X|O|X| |O|O| | 'O' Won! |O|X|X| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| |X| | | |O| |X| | | |O|O|X| | | |O| |X| | | |O|O|X| | | |O| |X|X| | |O|O|X| |O| |O| |X|X| | 'X' Won! |O|O|X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | | |X| |O|O| | | |X| | |X| |X| |O|O| | | |X| | |X|O|X| |O|O| | | |X| | |X|O|X| |O|O| | | |X|X| 'O' Won! |X|O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | |O|O|X| | |X| | | |O| | |O|O|X| | |X|X| | |O|O| |O|O|X| | |X|X| |X|O|O| |O|O|X| | |X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O| | | | |X|X| | |O|X| |O|O| | | |X|X| | |O|X| 'X' Won! |O|O|X| | |X|X| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| | |X|X| | | |O| | | |O|O| |X|X| | | |O| | |X|O|O| |X|X|O| | |O| | |X|O|O| 'X' Won! |X|X|O| |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O| | | |X|O| |X|X| | | |O|O| | |X|O| |X|X| | |X|O|O| | |X|O| |X|X| | 'O' Won! |X|O|O| | |X|O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | |X|O| |O| |O| | | |X| | |X|O| |O|X|O| | | |X| | |X|O| 'O' Won! |O|X|O| | |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| |O| |O| |X| | | |X| |O| |O| |O| |X| | | |X|X|O| 'O' Won! |O| |O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | | | |X| |O|O|X| |X| |O| | | |X| |O|O|X| |X|X|O| | | |X| |O|O|X| |X|X|O| |O| |X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| |O| |X| | |O|O| | | |X| |O|X|X| 'O' Won! | |O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O| |O| |X|X| | 'O' Won! | | | | |O|O|O| |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X| |X| |O|X| | | | |O| |X| |X| |O|X| | | |O|O| 'X' Won! |X| |X| |O|X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | | |O| |X| |X| |X| |O| | |O|O| |X| |X| |X| |O| 'X' Won! |X|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O|O| | | | |X| |X|O| | |O|O|X| |O| |X| |X|O| | |O|O|X| 'X' Won! |O| |X| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |X| | |O| | |X|X|O| | |O|X| 'X' Won! |X|O| | |X|X|O| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| |O| |X| |O| | | | | |X|X|O| |X| |O| | | | | |X|X|O| |X| |O| |O| | | |X|X|O| |X| |O| |O| |X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| | | | |O| | | |X|X| |O| | | |X|O| | | |X|X| |O| | | |X|O|O| | |X|X| |O| |X| |X|O|O| |O|X|X| |O| |X| |X|O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| |X| |O|O| | | | |X| |X| |X| |O|O| | | |O|X| |X| |X| |O|O| | |X|O|X| 'O' Won! |X|O|X| |O|O| | |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X| | | |O|X| |O| | | |O|X|X| | |O|X| 'O' Won! |O| | | |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| |X| | | | | |O|X| | |O|O|X| | | | | |O|X|X| |O|O|X| | | | | |O|X|X| |O|O|X| | |O| | 'X' Won! |O|X|X| |O|O|X| | |O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X|O| | |O| | |O|X|X| | |X|O| | |O| | |O|X|X| |O|X|O| | |O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | | |X| |O| |O| |O| |X| |X| |X| |O| |O| |O| 'O' Won! |X| |X| |X| |O| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | |X| | | | |O|O| |X|O| | |X| | | | |O|O| |X|O|X| |X| | | 'O' Won! | |O|O| |X|O|X| |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| | |O| | |O|O| | 'X' Won! |X|X|X| | |O| | |O|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| 'O' Won! | |X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | |O| |O| |X| |X| | |X| | |O| |O| |X| |X| | |X|O| |O| |O| |X| |X| | |X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| 'X' Won! |X|X|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | |O| | |X| |O| | |O|X| |X|O| | 'O' Won! |X|O|O| | |O|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O| | | | |X| | |O|X|X| 'O' Won! |O| | | |O|X| | |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |O| | | | | |O| | |X|X| |O| | | | | |O| | |X|X| |O| |O| |X| |O| | |X|X| |O| |O| 'O' Won! |X| |O| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | | |X|O| |O|O| | | |X| | | |X|O| |O|O| | | |X|X| |O|X|O| |O|O| | | |X|X| 'X' Won! |O|X|O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | | | | | |X| |O| | |X|O| | | | | |X| |O| | |X|O| | |X| | |X|O|O| | |X|O| | |X| | |X|O|O| | |X|O| |X|X| | |X|O|O| |O|X|O| |X|X| | 'X' Won! |X|O|O| |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | |X| | |X|O| | | |O| |O| |X| | |X|O| | | |O| 'X' Won! |O| |X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|X| |O|O| | |X| | | |O|X|X| |O|O|X| |X| | | |O|X|X| |O|O|X| |X|O| | |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | |X|O| | |O| | | |X|O| | |X|O| | |O| | |X|X|O| | |X|O| 'O' Won! | |O|O| |X|X|O| | |X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | |O| | | | |O| |X| |X| |X|O| | | | |O| |X|O|X| |X|O| | | | |O| |X|O|X| |X|O| | | |X|O| |X|O|X| |X|O| | |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| |O|X| | |X| |O| | | |O| |O|X|X| |X| |O| | |O|O| |O|X|X| |X| |O| |X|O|O| |O|X|X| |X| |O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| |X| |O| | | | |O|O| |X| |X| 'X' Won! |O| | | | |O|O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X|X| | | |O| |X| |O| |O|X|X| | |X|O| |X| |O| |O|X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| |O| | | | | |X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| | | |O|X|X| |X|O|O| |O|X| | |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | | |X| |X|O| | | |O|O| | | |X| |X|O|X| | |O|O| 'O' Won! | |O|X| |X|O|X| | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | | |O|X|O| |X|O| | |X| | | |O|X|O| |X|O| | |X|X| | |O|X|O| |X|O| | |X|X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X|X| | | | | |O|X|O| |O|X|X| 'X' Won! | |X| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X|O| | |O|O| | |X| | |X|X|O| | |O|O| |O|X| | |X|X|O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| | | |X| |X|X|O| |O| |O| | | |X| |X|X|O| |O| |O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |X| | | | | |X|O| | | |O|X| | | | | |X|O|X| | |O|X| | | |O| |X|O|X| | |O|X| | | |O| |X|O|X| |X|O|X| 'O' Won! | |O|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X|O| | | | | |X|O| | | |X|O| |X| | | |X|O| | | |X|O| |X| | | |X|O|O| | |X|O| |X| |X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O|X| | 'O' Won! | | |O| | |O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | 'X' Won! |X| | | |X| |O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| |O| |O|X| | | | |X| |O| |O| |O|X|X| 'O' Won! |O| |X| |O| |O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| |O| |X| |X| |X|O|O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| |O|X| | | |O|X| | |O|X| |O|X| | | |O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| |O| |X| | | |O| | |O|X| |O|X|X| | | |O| | |O|X| |O|X|X| |O| |O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | |O|X| | |X| | | |X|O| | |O|X| | |X| | | |X|O|O| |O|X| | |X|X| | |X|O|O| |O|X| | 'O' Won! |X|X|O| |X|O|O| |O|X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| 'O' Won! |O| |X| | |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X|O| | | | |X| | |O| | |X|O|X| | | |X| |O|O| | |X|O|X| | | |X| |O|O| | |X|O|X| |X| |X| 'O' Won! |O|O|O| |X|O|X| |X| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | | |X| |X|O| | |O| |X| | | |X| |X|O|O| 'X' Won! |O| |X| | |X|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X|X| | | |O|O| |X| | | |X|X|O| | |O|O| 'X' Won! |X| | | |X|X|O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O|X| | | | |X| |O|O|X| |O|X| | |X| |X| |O|O|X| |O|X| | |X|O|X| |O|O|X| |O|X| | 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | |O| | |X|O|X| |X| | | | |O| | |X|O|X| |X| |O| | |O| | |X|O|X| 'X' Won! |X| |O| |X|O| | |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | | | |O|X|X| 'O' Won! |X| |O| | |O| | |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O|O| | | |X|X| |X|O| | |O|O| | | |X|X| |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X|X| | |O| | | |O|O| | |X|X| | |O| | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O|O| | |O|O|X| |X|X| | |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | |X|X| | | | |O| | |O| | |X|X| | | | |O| |O|O| | |X|X| | |X| |O| |O|O| | |X|X|O| |X| |O| |O|O| | |X|X|O| |X| |O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O|X| |O|X|X| | | | | |O|O|X| |O|X|X| | |X| | |O|O|X| |O|X|X| | |X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | |X| | |O| |O| | |X|X| 'O' Won! | |X| | |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| |O|X| | |O|O|X| | | |X| 'X' Won! |O|X|X| |O|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | |X|X| |O| |O| |O| | | | |X|X| |O| |O| |O| | | | |X|X| |O|X|O| |O| |O| | |X|X| |O|X|O| 'X' Won! |O|X|O| | |X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | |O| |X| | |X|O| | |X| | |O| |X| | |X|O| |O|X| | |O| |X| | |X|O| 'X' Won! |O|X| | |O|X|X| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| |O| | | | |X|O| | | |X| |O| | | | |X|O| | |X|X| |O|O| | | |X|O| | |X|X| |O|O|X| | |X|O| | |X|X| |O|O|X| | |X|O| |O|X|X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | | | |O| |X| |X| |O|X|O| | | |O| |X| |X| |O|X|O| |X| |O| |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X| |O| | |O| | |X|X|O| |X| |O| | |O| | |X|X|O| |X| |O| |O|O| | |X|X|O| |X| |O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O| | | | |X| | | |O| |X|O|X| | | |X| | | |O| |X|O|X| | | |X| | |O|O| |X|O|X| | | |X| |X|O|O| 'O' Won! |X|O|X| | |O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | |X| | |O|X| | | | |O| | |X|O| |O|X| | | | |O| | |X|O| |O|X|X| | | |O| |O|X|O| |O|X|X| | | |O| |O|X|O| |O|X|X| |X| |O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| |X| |O| | |X| | | |O|X| |X|O|O| | |X| | | |O|X| |X|O|O| | |X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O| | |X|O|X| | |X| | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X|X| 'O' Won! | |O|O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| | |O|O| |O|X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| |O|O| | |O| | | |X| |X| |O|O| | |O|X| | |X|O|X| |O|O| | |O|X| | |X|O|X| |O|O|X| |O|X| | It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| |O| |X| | | |X|O| | |O| |O| |X|X| | |X|O| | |O| |O| |X|X|O| |X|O|X| |O| |O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | |O| | |O| |X| |X| |X| | |O| | |O| |X| |X| |X| | |O|O| |O|X|X| |X| |X| | |O|O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | |O| | |X| |X| |O|X|O| | |O| | |X|O|X| |O|X|O| | |O| | 'X' Won! |X|O|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | | | | | |O|X| |X| |O| | | |O| | |O|X| |X|X|O| | | |O| | |O|X| |X|X|O| |O| |O| | |O|X| |X|X|O| |O| |O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| |X| | | |O| | |X| | |O| |X| | |O|O| | |X| | |O| |X| | |O|O| | |X|X| 'O' Won! |O| |X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | |X|X| |O| | | | | | | |O|X|X| |O| | | |X| | | |O|X|X| |O| | | |X| | | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | |O|X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|O| |X|O|X| | |X|O| | |O|O| |X|O|X| |X|X|O| 'O' Won! |O|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O|X| |O|X| | 'O' Won! |O|X| | |O|O|X| |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X|X| | |X| | | |O|O| | |X|X| | |X|O| | |O|O| | |X|X| |X|X|O| | |O|O| 'O' Won! | |X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X| | |X| | | |O|O| | |X|X| |O|X| | | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X| | |O| |O| |X| |X| |O|X| | |O| |O| |X| |X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | |X| | |O| | | |O|X| | 'O' Won! |O|X| | |O| | | |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| 'O' Won! |X|X|O| | | |O| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | |X| | | |O| |X| |O| | | |X|O| | 'X' Won! |O| |X| |O|X| | |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |X| |X| |O| |O|O|X| |X| |X| |X|O|O| 'X' Won! |O|O|X| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| |O| |X| | | | |O|O| |X|X|O| 'O' Won! |X| |O| | |O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | |X|X| | | | | |O| |O| | |X|X| 'X' Won! | | | | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | |X|O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| |X| | | |X| | | |O|X|O| |X| | | |X| |O| |O|X|O| |X| | | |X|X|O| 'O' Won! |O|X|O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| | | | | | |X|O| |O| |X| | |X| | | |X|O| |O| |X| | |X| | |O|X|O| |O| |X| | |X|X| |O|X|O| |O| |X| 'O' Won! |O|X|X| |O|X|O| |O| |X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X|O| | | | |X| |O|O| | |X|O|X| 'O' Won! | |O|X| |O|O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| |O| | | |O| |X|X| | |X| |O| | |O|O| |X|X| | |X| |O| 'X' Won! |X|O|O| |X|X| | |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | |O| | | |O|X| | | |X|X| |O| | | |O|X| | | |X|X| |O| |O| |O|X| | | |X|X| |O| |O| |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X|X| | | |O| | | |X| |O|X|X| | |O|O| | | |X| |O|X|X| |X|O|O| | | |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| |X| |X| | | | |O|O| |X|O|X| |X|X| | | |O|O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X|X| |X| | | |O|O| | |O|X|X| |X| | | |O|O| | |O|X|X| |X|X| | |O|O| | |O|X|X| |X|X|O| |O|O| | It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | |X| | | |O| |X| |O| | | |X| | | |O|O|X| |O| | | |X|X| | |O|O|X| |O| | | |X|X| | |O|O|X| |O| |O| 'X' Won! |X|X|X| |O|O|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X|O|X| |O| | | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X| | | |O|O| |X|O|X| |O|X|X| | |O|O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | |X| | | | |O| | |O|X|O| |X| | | | |O| | |O|X|O| |X| | | |X|O| | |O|X|O| |X|O| | |X|O| | |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X|X| | |O| |O| | |X| | |X|X|O| |O| |O| | |X| | |X|X|O| |O| |O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| |X| |O| | |O|X| | | |X| 'O' Won! |X| |O| | |O|X| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O|X|O| | |X| | |X|O| | |O|X|O| | |X|O| |X|O|X| |O|X|O| | |X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | | | | |X|O|X| |O| | | | | |O| |X|O|X| |O| |X| | | |O| |X|O|X| |O|O|X| | | |O| |X|O|X| |O|O|X| |X| |O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| |X| |X| |O| | | | | |O|O|X| |X| |O| | | | | |O|O|X| |X| |O| |X| | | |O|O|X| |X|O|O| |X| | | |O|O|X| |X|O|O| |X| |X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| |O| |O| | | | | |X| 'X' Won! |X| |O| |O|X| | | | |X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| | |X| | | 'X' Won! |X|O| | |X|O| | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | |O| | | | |O|X| |O|X| | |O| |X| | |O|X| |O|X| | |O|O|X| | |O|X| 'X' Won! |O|X|X| |O|O|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| | | |O| | | |X| |X|X|O| | | |O| | |O|X| |X|X|O| | | |O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X| |X| | |O|O| | |X| | |X|O|X| 'X' Won! |X|O|O| | |X| | |X|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O|X| | | | |X| |O|O|X| |O|X| | 'X' Won! | | |X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| | |X| | |O|O| | | |X|X| | |X|O| 'X' Won! |O|O| | |X|X|X| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | |X|O|X| | | | | | |O| | |X|O|X| |O| | | |X|O| | |X|O|X| |O| | | 'O' Won! |X|O|O| |X|O|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| |X|O| | | | | | | |O|X| |X|O|O| | | | | |X|O|X| |X|O|O| 'O' Won! |O| | | |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| |X|X|O| 'O' Won! |O|O|X| | |O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X|O| | | | | | |X|O| |O|X|O| | | |X| | |X|O| |O|X|O| |O| |X| 'X' Won! |X|X|O| |O|X|O| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X|O| | |X|X| |O| | | |O|X|O| | |X|X| |O|O| | |O|X|O| | |X|X| |O|O|X| |O|X|O| 'O' Won! |O|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O|X| | |O| | |X| |O| |X|O|X| | |O| | 'X' Won! |X| |O| |X|O|X| |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| |X| |X| | | |O|O| | |O| |X| |X| |X| |O|O| | |O| |X| 'O' Won! |X| |X| |O|O|O| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| | | | |O| | |X|X|O| |O|X| | | |O| | |X|X|O| |O|X|O| | |O| | |X|X|O| |O|X|O| |X|O| | 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X|X| | |O|O| | | | | | |X|X| |X|O|O| | | | | | |X|X| |X|O|O| | | | | |O|X|X| |X|O|O| |X| | | |O|X|X| |X|O|O| |X| |O| |O|X|X| 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | |X| | | |X|O|O| | |X| | |X| |O| |X|O|O| | |X|X| |X| |O| |X|O|O| | |X|X| |X|O|O| |X|O|O| 'X' Won! |X|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | |X|X| | |O| |O| | | | | |X|X| | |O| |O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X|X| | | |X| | |O| |O| 'O' Won! |X|X| | | |X| | |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | |O| | |O| | | | |X|X| |X|O| | |O| |O| | |X|X| |X|O| | |O|X|O| | |X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O| | 'O' Won! |X|O| | | |O| | |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | 'O' Won! |X| |O| |X| |O| | | |O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | |O| |X| |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| 'X' Won! |X|O| | | |X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X|X| |O|O| | | | |X| |O|X|X| 'X' Won! |O|O|X| | | |X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| 'O' Won! | | |X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | |X|X| | | |O|O| |O|X| | |X|X|O| | |O|O| |O|X|X| |X|X|O| | |O|O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | | | | |O| |X| | |O|X| | | | | 'O' Won! |O| |X| | |O|X| | | |O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | |X| |O| |O| | | | |X| | |X| |O| |O|O| | | |X|X| |X| |O| |O|O| | 'O' Won! | |X|X| |X| |O| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | |X| |O|O|X| 'X' Won! |O| |X| | | |X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | |X| | | | |X| |O| |O| | |X| | | |O|X| |O| |O| |X|X| | | |O|X| 'O' Won! |O|O|O| |X|X| | | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | |X| | | 'O' Won! | |O|X| | |O| | |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | |X| | | | |O|O| |X| | | 'O' Won! |X| | | |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | |O| | | |X| |O|X|X| | | |O| | | |X| |O|X|X| |O| |O| |X| |X| |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|O| | |O| | |O| |X| | |X|O| | |O|X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| | |O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | |O|O| | |X| | |X| |X| | |O|O| 'O' Won! | |X| | |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | |X| | |X| | |O| |O| | | |X| | |X| | |O| |O| | |X|X| | |X| | 'O' Won! |O|O|O| | |X|X| | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | |X| | |X| |X| |O|O| | | |X| | |X|O|X| |O|O| | | |X|X| |X|O|X| |O|O| | 'O' Won! | |X|X| |X|O|X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| |O| | | |X|X| | | |O|X| |O| | | |X|X| | | |O|X| |O|O| | 'X' Won! |X|X|X| | |O|X| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X| | | |X|O| |O| | | | |X| | |X|X|O| |O| | | | |X|O| |X|X|O| |O| | | | |X|O| |X|X|O| |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X|X| | | | | |O|O| | |O|X|X| | | | | |O|O|X| |O|X|X| 'O' Won! |O| | | |O|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | |O|O| | |X|X| | | |X| | |O|O| | |X|X| |O| |X| | |O|O| 'X' Won! |X|X|X| |O| |X| | |O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| | | |X|X|O| | | | | |O| |O| |X|X|O| | | | | |O|X|O| |X|X|O| 'O' Won! | | |O| |O|X|O| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | |X| |X| | | |X|O|O| |O| |X| |X| | | |X|O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X| |O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | |O| | |O| | | | |X|X| | |O|X| |O| | | | |X|X| |O|O|X| |O| | | 'X' Won! | |X|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X|X| | | |X| | 'O' Won! |O|O|O| |X|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| |O|O| | | | |O| |X| |X| |O|O| | |X| |O| |X|O|X| |O|O| | |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| |O| | | |X|X| | |X| |O| |O| | | |X|X|O| |X| |O| |O| | | |X|X|O| |X| |O| |O| |X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | |O|O| | |X| | |X|O| | | |O|O| |X|X| | |X|O| | | |O|O| |X|X|O| |X|O|X| | |O|O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| |O| |O| | | | |X| | |X| |O| |O|X| | |O|X| | |X| |O| |O|X| | |O|X| | |X| |O| |O|X|X| |O|X|O| |X| |O| |O|X|X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O| | |X|O| |O| | | | |X|O| | |X|O| |O| |X| | |X|O| | |X|O| |O|O|X| | |X|O| |X|X|O| |O|O|X| | |X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | | | |O| |O| |X| |X|X| | | | |O| |O| |X| |X|X| | | | |O| |O|O|X| 'X' Won! |X|X| | | |X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| | | |O|O|X| |X| | | |X| | | |O|O|X| |X| |O| |X| | | |O|O|X| |X|X|O| |X| | | |O|O|X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | |X|O| |O| | | | |X| | | |X|O| |O|O| | | |X| | | |X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| 'X' Won! |X|X| | |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | | |O| | | |X| |X|X|O| | | |O| |O| |X| |X|X|O| |X| |O| |O| |X| |X|X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | |O| | | |X|O|X| | | | | |O| | | |X|O|X| |X| | | 'O' Won! |O| | | |X|O|X| |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | | |O| | |X| | |X|O|X| | | |O| 'O' Won! |O|X| | |X|O|X| | | |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | |X| | | | |X| |O| |O| 'O' Won! | |X| | | | |X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | | |O| | |X|O|X| |O|X| | |O|O| | |X|O|X| |O|X| | |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | |O| |O| 'X' Won! |X|X|X| |O| | | |O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| | | |O| | | |O|X|O| |X| | | |O|X| | 'O' Won! |O|X|O| |X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O|X| | | | | |O| |X| | |O|X| | |O| | |O| |X| |X|O|X| | |O| | |O| |X| |X|O|X| |O|O| | |O| |X| 'X' Won! |X|O|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | | | | |O|X| | |O| |X| 'X' Won! |X| | | |O|X| | |O| |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O|O| |X| | | |X| |O| | |O|O| |X| |X| 'O' Won! |X| |O| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X|O| | |O|X| | | | | |X|X|O| | |O|X| | |O| | |X|X|O| | |O|X| | |O|X| |X|X|O| 'O' Won! |O|O|X| | |O|X| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O|O|X| | | | | | |X|X| |O|O|X| | | | | | |X|X| |O|O|X| | |O| | | |X|X| |O|O|X| |X|O| | | |X|X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | |X| |O| | |O|O| 'O' Won! |X|X| | |X| |O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | | | |X| |O|X|O| | |X| | | | |X| |O|X|O| | |X|O| |X| |X| |O|X|O| | |X|O| |X| |X| |O|X|O| |O|X|O| 'X' Won! |X|X|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | | | | | |X|O|O| | |X|X| | | | | |X|O|O| | |X|X| | | |O| |X|O|O| | |X|X| | |X|O| |X|O|O| |O|X|X| | |X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | |X| |X|O| | |O|O| | | | |X| |X|O| | |O|O| | | | |X| |X|O|X| 'O' Won! |O|O| | | |O|X| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| |X|X|O| | |X| | | |O|O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X|X| | | | | | | |O|O| 'O' Won! |X|X| | | | | | |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | | |O| |X| |O| | |X| | |O| |O| |X| |O| | |X| | |O| |O| |X| |O| |X|X| | 'O' Won! |O| |O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X|X| | |O| | |O|O| | | |X|X| | |O| | |O|O|X| | |X|X| | |O|O| |O|O|X| | |X|X| |X|O|O| |O|O|X| | |X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| |X| |X| |O|X| | | |O|O| |X| |X| |O|X|X| | |O|O| |X| |X| |O|X|X| | |O|O| |X|O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O|X| | | | |X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X|X| 'O' Won! |O|X|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| | | |O| |X| | | |X|O|X| | | |O| |X| | | |X|O|X| | |O|O| 'X' Won! |X| | | |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | | |O| |O| | | | |X|X| | |O|O| |O| | | | |X|X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| |O| | |X|X| 'X' Won! |X|O|O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | |X|O| 'O' Won! | | |O| | |X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O| | |X|O| | | | |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O|X| |X|O| | 'O' Won! |O|O|X| |X|O|X| |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | | |X| |O| |O| |O|X|X| | | |X| |O| |O| |O|X|X| | |X|X| 'O' Won! |O| |O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X|X| | |O| | |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| | |O|X| |O|O| | |O|X|X| 'X' Won! | |O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | | |O| |X|O| | |X|X| | | | |O| |X|O| | |X|X|O| | | |O| 'X' Won! |X|O| | |X|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| | | |X|X|O| |O| |X| |O| |O| |X|X|O| |O|X|X| |O| |O| |X|X|O| 'O' Won! |O|X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X|X| |O| | | |X| |O| |O|X|X| |O| | | |X|X|O| |O|X|X| |O| | | |X|X|O| |O|X|X| |O|O| | 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| |O| | | |O| | | |X| |X|X|O| | | |O| | |O|X| |X|X|O| | | |O| |X|O|X| |X|X|O| | |O|O| |X|O|X| |X|X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X|O| |X|O| | | | | | | |X|O| |X|O| | | |X| | | |X|O| |X|O| | | |X|O| | |X|O| |X|O| | |X|X|O| 'O' Won! |O|X|O| |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| |X| | | |O|O| | |O|X|X| |X| | | |O|O| | |O|X|X| |X| |X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|O| |X| | | |O|X|X| | |O|O| |X| |O| |O|X|X| | |O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | |O| | |O|X|O| | |X| | | |O|X| |O|X|O| | |X| | | |O|X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O|X| | |O|X| | | | | | 'O' Won! |O|X| | |O|X| | |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | | | |X|X|O| | | |O| |O| | | |X|X|O| | | |O| |O| | | |X|X|O| |X| |O| |O| | | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| | |O| | | | |X| |X|O|O| | |O| | |X| |X| |X|O|O| |O|O| | |X| |X| |X|O|O| |O|O|X| |X| |X| 'O' Won! |X|O|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| | | |O|O| |X| | | | |X| | | |O|O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| |X|O|O| |X|X| | 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | | | | |X|O| | |O|X| | | |X| | |X|O| | |O|X| | | |X| | |X|O|O| |O|X| | | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X|X| | |O| | | | |O|O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X|O| | |O|O| |X|X| | 'X' Won! |O|X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X|O| |X|O| | | | |X| |O|X|O| |X|O|O| | | |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O|X| | |X|O|O| | |X| | |O|X| | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X|O| |O|X| | 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | |O| |O| | |X|X| | |X| | |O| |O| | |X|X| |O|X| | |O| |O| 'X' Won! | |X|X| |O|X| | |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O|X| |X| | | |X| |O| | |O|X| |X|O| | |X|X|O| | |O|X| |X|O| | |X|X|O| | |O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | |O| |O| |X| | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X|O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |O| 'X' Won! | |O| | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X|X| | |O| | | |O|O| | |X|X| | 'X' Won! |O| | | |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | | | | |O| | | | |O|X| |X| | | |O| | | | |O|X| |X|O| | |O|X| | | |O|X| |X|O| | 'O' Won! |O|X| | | |O|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | |O| | |O|X|X| |X| | | | |O| | |O|X|X| |X| | | |O|O| | |O|X|X| |X|X| | |O|O| | 'O' Won! |O|X|X| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X| | |O| | | | |O|X| |X|X| | |O| | | | |O|X| |X|X| | |O|O| | | |O|X| 'X' Won! |X|X|X| |O|O| | | |O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | |X| | |O|O|X| | | | | | |X|O| |O|O|X| | | | | |X|X|O| |O|O|X| | |O| | |X|X|O| |O|O|X| | |O|X| |X|X|O| |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| |X| |O|O| | | | |X| 'O' Won! |X| |X| |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O|O| 'X' Won! |O|X|X| |O|X| | |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | |O| |X|X| | | |O| | |X| |O| |X|X| | | |O| | |X| |O| |X|X|O| | |O| | 'X' Won! |X| |O| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X|O|X| |O| |X| | | | | |X|O|X| |O| |X| |O| | | |X|O|X| |O|X|X| |O| | | |X|O|X| |O|X|X| |O|O| | |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| |X| |X|O| | | |O| | |X| |X| |X|O| | | |O|O| |X| |X| |X|O|X| | |O|O| 'O' Won! |X| |X| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | | |X|X| | |X|O| | |O|O| | |X|X| |X|X|O| | |O|O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X|O|X| 'X' Won! |X| |O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| 'O' Won! | |O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O|X| | | |O|X| | | |X| |O|X|O| | |O|X| 'X' Won! |X| |X| |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | |O|O| | |X|O| | | |X| | |O|O| | |X|O|X| | |X|O| |O|O| | |X|O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | | |O| |O| | | | |X|X| | | |O| |O|X| | | |X|X| |O| |O| |O|X| | | |X|X| |O| |O| |O|X|X| | |X|X| 'O' Won! |O| |O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| |O|O| | 'X' Won! |O| |X| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | |O| | | |X|O| | |O|X| | |O| | | |X|O| | |O|X|X| 'O' Won! |O| |O| |X|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| |O| | | | | |O| |X|O|X| |O| | | | |X|O| |X|O|X| |O|O| | | |X|O| |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O|X| |O|X| | 'O' Won! |O|X|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | |O| | |X|O| | |X| | | | |O| | |X|O| | |X|X| | | |O|O| |X|O| | |X|X| | | |O|O| |X|O|X| |X|X| | 'O' Won! |O|O|O| |X|O|X| |X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O|X| | | | | | |O| | |X|O|X| |O| | | | |O| | |X|O|X| |O| |X| 'O' Won! | |O| | |X|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X|X|O| 'O' Won! |O|O|O| | | |X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X|O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | |O| | |X|X| | |O|X| | | |O| | |X|X| | |O|X| | |O|O| | 'X' Won! |X|X| | |O|X| | |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X|X| | |O| | |O|O|X| |O|X|X| 'X' Won! | |O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X|X| | | |O|O| | | |X| |X|X| | | |O|O| | |O|X| |X|X| | | |O|O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X|X| | | | | |O|O|X| |O|X|X| | |O| | 'X' Won! |O|O|X| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| |O|X|O| | | | | | |X|O| |O|X|O| |X| | | | |X|O| 'O' Won! |O|X|O| |X| |O| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | |X|X| | |O| |X| |O| | | |X|X|O| |O| |X| |O|X| | |X|X|O| |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | |O| | | |O| |X| |X| |X| |O| | | |O| |X| |X| |X| |O| | | |O| |X|O|X| 'X' Won! |X| |O| |X| |O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O| | | |X| |X| |O| | |O|O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| | |O|O| | |O|X| |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | |X| | |X|O|O| | | |O| | |X| | |X|O|O| | |X|O| | |X| | |X|O|O| | |X|O| |O|X| | |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| |O| |O|X| | | |O| | |X| |O| |O|X|X| |O|O| | |X| |O| |O|X|X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X|X| | | | | 'O' Won! |O|O|X| |O|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| 'X' Won! | |O|O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| 'X' Won! |X|X|X| | |O| | | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | |O| | | |X| |O| |X|X| | |O| | | |X| |O| |X|X|O| |O| | | 'X' Won! |X| |O| |X|X|O| |O| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X|O| |X|O| | 'X' Won! |X|O|X| |O|X|O| |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| |X| |O| | | |X|X| | |O| |X| |O|O| | |X|X| | |O| |X| |O|O|X| |X|X|O| |O| |X| |O|O|X| 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X| | | | |O| | |O| | |X|X| | | |X|O| | |O| | |X|X| | | |X|O| |O|O| | 'X' Won! |X|X|X| | |X|O| |O|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | |O| | |X| |X| |O|X|O| | |O| | |X| |X| |O|X|O| |O|O| | 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| | | |O| |X| |O| |X| |X| | | |O|O|X| |O| |X| |X| | | 'X' Won! |O|O|X| |O|X|X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | |X| | |O| | | |O|X|O| | |X| | |O| | | |O|X|O| | |X|X| |O|O| | |O|X|O| | |X|X| 'X' Won! |O|O| | |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| | | |X| 'X' Won! |X|O|O| |X|X|O| | | |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| |O| |O| 'X' Won! |X| |O| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| 'O' Won! |O|X|X| |O| | | |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O|O| | |X| | | |X|O| | |O|O| | |X|X| | |X|O| | |O|O| | |X|X| | |X|O|O| 'X' Won! |O|O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X|X| | | |O|O| | |X|O| |X|X| | | |O|O| |X|X|O| |X|X| | 'O' Won! | |O|O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X|O| | | | | |X| |O| | |X|O| | | |X| |X| |O| |O|X|O| | | |X| |X| |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X| |X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X| |O| | |O|O| | |X|X| |X| |O| | |O|O| |O|X|X| |X| |O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| |O| |X| |X|X| | |O| |O| 'X' Won! |O| |X| |X|X|X| |O| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X|O| | |O| |X| | | | | |X|O| | |O|X|X| | | | | |X|O|O| |O|X|X| | | | | 'X' Won! |X|O|O| |O|X|X| | | |X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| |X| | | |X|O| | | |O|O| |X| | | |X|O|X| | |O|O| |X| | | 'O' Won! |X|O|X| | |O|O| |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O|O| |X| | | |X|O|X| | |O|O| |X| | | 'O' Won! |X|O|X| | |O|O| |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| | | |X|O|X| | | |X| |O|O| | |X|O|X| | | |X| |O|O| | |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | |X|O| |O| | | | |X| | | |X|O| |O|O| | | |X|X| | |X|O| |O|O| | | |X|X| |O|X|O| |O|O| | | |X|X| |O|X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X|O| | |O|O| | |X|X| | |X|O| | |O|O| | 'O' Won! |X|X|O| |X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| |X| | | |X| | | |O|O|X| |X| | | |X|O| | 'X' Won! |O|O|X| |X|X| | |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | |X| |X| | |O|O| | | | | |X| |X| | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | |O|O| | |X|O| | |X|X| | |O|O| | |X|O| | 'O' Won! |X|X| | |O|O|O| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| |O| | | | | |X|O| | |X|X|O| | | | | |X|O| | |X|X|O| |O| | | |X|O| | |X|X|O| |O| |X| |X|O| | |X|X|O| |O|O|X| |X|O| | It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O|X|X| | | | | |O| | | 'O' Won! |O|X|X| |O| | | |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| |O| | | |X|X| | |X| |O| |O| | | |X|X| | |X| |O| |O|O| | |X|X| | |X|X|O| |O|O| | 'O' Won! |X|X| | |X|X|O| |O|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | | | |O|O|X| | | |X| | | | | 'X' Won! |O|O|X| | | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | | |X| | | |O| |X|O| | | | |X| |O| |O| |X|O|X| | | |X| |O| |O| |X|O|X| |O| |X| |O| |O| |X|O|X| |O| |X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X|O| | |O| | |X| |O| |X|X|O| | |O| | |X|O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | |O| | | | |X| |O| |X| | |O| | | |X|X| |O| |X| | |O| | |O|X|X| 'X' Won! |O| |X| | |O|X| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X|X| | |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X|O| |X| | | 'X' Won! |O| |X| | |X|O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | 'O' Won! |O| |X| |O|X| | |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | | |O| | | |O| |O|X|X| |X| |O| | | |O| |O|X|X| |X| |O| | |O|O| |O|X|X| |X|X|O| | |O|O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| |X| | | | | |O| | |O|X| |X| | | | |O|O| | |O|X| |X| | | |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | | |X|X| | |O|X| | |O|O| | |X|X| | |O|X| | |O|O| | |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| 'X' Won! |X|O| | |O|X| | | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X|O| | |O| | | |X| |O| |X|O| | |O|X| | |X| |O| |X|O| | |O|X|O| |X| |O| |X|O| | |O|X|O| |X|X|O| 'O' Won! |X|O|O| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| |O| | | |X| |O| |X| |X| |O| | | |X| |O|O|X| |X| |O| |X| |X| |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X|X| | |X|O| | |O|O| | |X|X| 'X' Won! | |X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | |X| | |X| | |O|O| | |O| |X| | |X|X| |O|O| | |O| |X| 'O' Won! |O|X|X| |O|O| | |O| |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X|X| | |O| |O| 'O' Won! | |X| | |X|X| | |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X| | |O|X| | | | |O| 'X' Won! |O|X| | |O|X| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O|O| | | |X|X| | |X|O| |O|O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| |O|O| | 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X| | |O| |O| | | | | 'X' Won! |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| |O| |O|O|X| | | |X| |X| |O| |O|O|X| | | |X| |X|O|O| |O|O|X| |X| |X| |X|O|O| 'O' Won! |O|O|X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |X| |O|O|X| | |O| | |X| |X| 'X' Won! |O|O|X| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | | | |O|X| | | |X|O| | | | | |O|X| | |X|X|O| | | |O| |O|X| | |X|X|O| 'X' Won! | |X|O| |O|X| | |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X|X| | |X| |O| |O| |O| |X|X| | 'X' Won! |X| |O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X| | |X| | | | |O|O| | |X| | |X|X| | | |O|O| 'O' Won! | |X| | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | |O| |X| |X| | | |O|O| | |O|X|X| 'O' Won! |X| |O| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | |O|O| | | | |O| 'X' Won! |X|X|X| |O|O| | | | |O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | |O|X| | | |X| 'X' Won! | |O|X| | |O|X| | | |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| | | | |O| | | |X|X| |O|X| | | |O| | | |X|X| |O|X| | |O|O| | | |X|X| |O|X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| | | | | |O|X| | |O| |X| | | | | |O|X| | |O|X|X| | | | | |O|X| | |O|X|X| | | |O| |O|X|X| |O|X|X| | | |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| |O| |O| |O| |X| | | |X| |O| |O| |O| |X| | |X|X| 'O' Won! |O|O|O| |O| |X| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| |O| | | | | |X| |O| |X| 'X' Won! |O| |X| | | |X| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | |X| | |O| |O| |X|O| | | |X| | |O| |O| |X|O| | | |X|X| |O| |O| |X|O|O| | |X|X| |O| |O| 'X' Won! |X|O|O| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | | |X| | | |O| 'O' Won! |O|X| | | |O|X| | | |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| | | |O|X| | |X|X| |O|O| | | |O|X| 'X' Won! |X|X|X| |O|O| | | |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | |O|X| | |X| |O| 'X' Won! |O|X| | |O|X| | |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| | | | | |X|X| | |O|O|X| | |O| | |X|X| | |O|O|X| |X|O| | |X|X| | 'O' Won! |O|O|X| |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X| | | |O|O| | |X| |O| |X| | | |O|O|X| |X| |O| |X|O| | |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X| | | |X|O| |O| | | |O|X| | | |X|O| |O| |X| |O|X| | 'O' Won! | |X|O| |O|O|X| |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O|O| | |O| |X| | | |X| 'X' Won! |O|O|X| |O| |X| | | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | 'X' Won! |O|O| | |O|X| | |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | |X|X| | | |O| |O| |X| | |X|X| | |O|O| |O| |X| | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | |O| | |X| | | |O|X| | |X|O| | |X| | | |O|X|O| |X|O| | |X|X| | |O|X|O| |X|O| | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O|X|X| | | | | | |O|O| |O|X|X| | | | | |X|O|O| |O|X|X| | | | | |X|O|O| |O|X|X| | |O| | 'X' Won! |X|O|O| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| |X| | | |X| 'O' Won! |O|O|O| |X| |X| | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| |X|X|O| | |O| | | | |X| |X|X|O| | |O|O| 'X' Won! | | |X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O|X|X| | | |X| |O| | | |O|X|X| | | |X| |O| |O| |O|X|X| | | |X| |O|X|O| 'O' Won! |O|X|X| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | |O| |X| | | | | |O|X| | |O| |X| | | | | |O|X|X| 'O' Won! |O| |X| |O| | | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | | |X| | |X|X| 'O' Won! |O|O|O| | | |X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| | |X|O| | |X| | |X|O|O| |O|X|O| | |X| | |X|O|O| |O|X|O| | |X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | 'O' Won! |X|O| | | |O| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O|X| |X| | | |O| | | |O|O|X| |X| | | |O|X| | |O|O|X| |X| | | |O|X| | |O|O|X| |X|O| | |O|X|X| |O|O|X| |X|O| | 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| | | |X|O| | | |O|X| |X| |O| |X|O| | 'X' Won! |X|O|X| |X| |O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | |X| | |X| |O| |O| |O| | |X| | |X|X|O| |O| |O| | |X| | |X|X|O| |O| |O| |O|X| | |X|X|O| |O| |O| |O|X|X| 'O' Won! |X|X|O| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| |O| | | | | |X| |X|O|O| |O|X| | |O| |X| |X|O|O| |O|X| | |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| |X| |O| |O| | | |X| |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | | |X| |O|O|X| |O|X|O| |X| |X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |O| | |O| | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O| | | |X|X| |X| |O| |O|O| | | |X|X| |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | | | |O| |O|O|X| | |X| | | |X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| |X|X| | 'O' Won! |O|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | | | |X| |O|O| | |X| | | |O| |X| |O|O| | |X| |X| |O| |X| 'O' Won! |O|O|O| |X| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | |O| | | |O| | |X| |X| | |O| | | |O|X| |X| |X| 'O' Won! | |O| | | |O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X|O| |X| | | | | |O| |O|X|O| |X| | | | |X|O| |O|X|O| |X| | | |O|X|O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | |O|O| | |O|X|X| |X|X| | 'O' Won! |O|O|O| |O|X|X| |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | | | |O|X|O| | | |X| | |X| | |O|X|O| | | |X| | |X|O| |O|X|O| | | |X| |X|X|O| |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X|O|X| | |O| | |O| | | |X|O|X| |X|O| | |O| | | 'O' Won! |X|O|X| |X|O| | |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| |X| | | | |X| | |O|O|X| |X|O| | | |X| | |O|O|X| |X|O|X| |O|X| | |O|O|X| |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| |X| |X| |O| | | | | |O|O|X| 'X' Won! |X| |O| | |X| | |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |O| |X| |O| |O|O|X| |X|X|O| |X| |O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | |O| |O| | | | |X|X| 'X' Won! | | |O| |O| | | |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | |X| | | |X| |O| | |O|O| |X| | | |X| |O| | |O|O| |X|X| | 'O' Won! |X| |O| | |O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X|O| | |X| | |X| |O| |O|X|O| | |X| | |X|O|O| |O|X|O| |X|X| | |X|O|O| |O|X|O| 'O' Won! |X|X|O| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X|X| | | |O|O| 'O' Won! | | | | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X| |O| |X|O| | | |O| | 'X' Won! |X| |O| |X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | | |X| |X|O| | | |O| | |O| |X| |X|O| | | |O| | |O|X|X| |X|O| | | |O| | |O|X|X| |X|O| | |O|O| | |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |X| | |O| | | |O| |X| | |X| | |O| |X| |O| |X| | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O| |X| |X|X| | |O|O|X| |O| |X| |X|X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| |X| | | |O| |X|X|O| |O| |X| 'X' Won! |X| |O| |X|X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| |O|X| | | |O|X| |X|X|O| |O|X| | | |O|X| |X|X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | |O| |O| |O|X|X| | | | | |O|X|O| |O|X|X| | | | | 'O' Won! |O|X|O| |O|X|X| |O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | |O|X| | | | |O| | |X| | |O|X| | | | |O| | |X|X| |O|X| | | | |O| |O|X|X| |O|X|X| | | |O| |O|X|X| 'O' Won! |O|X|X| |O| |O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | |O| | | | |X| |X| |O| |X|O| | | | |X| |X| |O| |X|O| | |O| |X| |X| |O| |X|O|X| |O| |X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | |O|X| | |X| | |O|O|X| | |O|X| |O|X| | |O|O|X| | |O|X| 'X' Won! |O|X|X| |O|O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| |O|X| | | | |O| | |O|X| |O|X| | | |X|O| |O|O|X| |O|X| | | |X|O| |O|O|X| |O|X|X| | |X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| | |X|X| 'O' Won! | |X| | |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| | |O| | |O| |X| |X|O|X| |X|O| | |O| |X| |X|O|X| 'O' Won! |X|O| | |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O| |X| | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O| |X| |X|X| | |O|O|X| |O| |X| |X|X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| |O| |X| | | | |O| | |X|X|O| |X|O| | | |O| | |X|X|O| 'X' Won! |X|O| | |X|O| | |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | |O| |X|O| | |X|X| | | | |O| |X|O| | |X|X|O| | | |O| 'X' Won! |X|O| | |X|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X|X| |O| | | |O|O| | | |X|X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O|O| | 'X' Won! |O|O|X| |X|X|X| |O|O| | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | 'O' Won! |X| |O| | |O|X| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | | |X|O| 'O' Won! |O| |X| |X|O| | | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| |X| |X| |O| |O| | | |O| |X| |X| |O| |O| |X| |O| |X| |X| 'O' Won! |O|O|O| |X| |O| |X| |X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| | | |O|O| 'X' Won! |X|X|X| | |O| | | |O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X|X| | |O| |O| 'O' Won! | | | | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O|X| |O|X| | |O|X|O| | |O|X| |O|X| | |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X|X| | | |O|O| |X| |O| |X|X| | | |O|O| |X|X|O| |X|X| | | |O|O| 'O' Won! |X|X|O| |X|X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | |X|O|X| | | | | | |O| | |X|O|X| | | | | | |O|X| |X|O|X| | | | | |O|O|X| |X|O|X| | |X| | |O|O|X| |X|O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O|X| | | | | | | |X| |O|O|X| |O| | | 'X' Won! | | |X| |O|O|X| |O| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O|O| | | |X| | | | |X| |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| 'X' Won! | |O|X| |O|O| | |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| |X| |X| |O| 'O' Won! |O| |X| |O|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | |O| |X| | | |O|X|O| | | |O| |X| |X| |O|X|O| | |O|O| |X| |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | | |X| | |X| |O| |O|X| | |O|X| | 'X' Won! |X|X|O| |O|X| | |O|X| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | 'X' Won! | |O|X| | |X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X| |O| | |O| | | |X|O| |X|X|O| | |O| | |O|X|O| |X|X|O| | |O| | |O|X|O| |X|X|O| |X|O| | 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O|X| |X| | | |X|O|O| | |O|X| |X| | | 'X' Won! |X|O|O| |X|O|X| |X| | | | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | |O|X| |O|X| | | | |X| | |O|X| |O|X| | |O| |X| | |O|X| |O|X| | |O| |X| |X|O|X| |O|X| | |O|O|X| |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | |X| |O|X| | | |O|O| | | |X| |O|X| | | |O|O| | | |X| |O|X|X| 'O' Won! |O|O|O| | | |X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | | |X| 'O' Won! |X|X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | |O| |O|X| | | | |X| |X| |O| |O|X| | | | |X| |X| |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X|O| | | |O| | |X| |O| |X|O|X| | |O| | |X| |O| |X|O|X| | |O|O| |X| |O| 'X' Won! |X|O|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O| | |X|X| | |X| |O| | |O| | |X|X|O| |X|X|O| | |O| | |X|X|O| 'O' Won! |X|X|O| | |O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X|O| |X|O| | |X|O| | |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | |O| | |O|X|X| |X|O| | | |O| | |O|X|X| |X|O| | | |O|O| |O|X|X| |X|O|X| | |O|O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | | | |O| | | |X|X|O| | | | | |O|X| | |X|X|O| | | | | |O|X|O| 'X' Won! |X|X|O| | |X| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X|X| | |X| | | |O|O| |O|X|X| | |X|X| | |O|O| |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | |X| |O| | |O| | | |X| | |X| |O| | |O| | | |X|O| |X|X|O| | |O| | | |X|O| |X|X|O| |O|O| | | |X|O| |X|X|O| |O|O| | |X|X|O| 'O' Won! |X|X|O| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| |X| | | |X| |O| | | |O| 'O' Won! |X| |O| |X| |O| | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | |X|O| | | | | |X| | | | |X|O| | | |O| |X| |X| | |X|O| | | |O| |X| |X| | |X|O| |O| |O| |X| |X| |X|X|O| |O| |O| |X|O|X| |X|X|O| |O| |O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | |X| | | |O|X|O| |X|O| | |X| | | |O|X|O| |X|O| | |X| |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'X' Won! |X|O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| | | |O|O| | | |O|X| |X| |X| |O|O| | |O|O|X| |X| |X| |O|O| | 'X' Won! |O|O|X| |X| |X| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| |O|O| | | | |X| |X|O|X| |O|O| | | |X|X| |X|O|X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | |O| | |O| | | |X| |X| | |O|X| |O| | | |X| |X| | |O|X| |O|O| | 'X' Won! |X| |X| | |O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | |O| | |X|O| |O|X| | | | |O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | |O| |O| |O|X|X| | | | | |O| |O| |O|X|X| |X| | | |O| |O| |O|X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X|X|O| |O| | | | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O| |X| |O| |O| |X|X|O| |O|X|X| |O| |O| |X|X|O| 'O' Won! |O|X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | |O| | | | |X| |X|O|O| |X|O| | | | |X| |X|O|O| |X|O| | |O| |X| |X|O|O| |X|O|X| |O| |X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | |O|X| | |X| | |O| | | |X|O|X| | |X| | |O| | | |X|O|X| | |X| | |O| |O| |X|O|X| | |X| | |O|X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | | |X| | | |O| |X|O| | | | |X| | |O|O| |X|O| | |X| |X| | |O|O| 'O' Won! |X|O| | |X| |X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O|O| |X| | | |O|X| | |X|O|O| |X| |X| |O|X| | 'O' Won! |X|O|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| | |O|X| |X|X|O| | |O|O| | |O|X| |X|X|O| | |O|O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O|O| | | | |X| |X| |X| |O|O| | | | |X| |X|O|X| |O|O| | | | |X| |X|O|X| |O|O| | |X| |X| 'O' Won! |X|O|X| |O|O| | |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | |X|O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| |X|O| | | |O|X| 'O' Won! | |X|O| |X|O| | |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| |O|O| | |X| | | |O| |X| |O|O| | |X|X| | |O|O|X| |O|O| | |X|X| | |O|O|X| |O|O|X| |X|X| | 'O' Won! |O|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| |O| | |X|X| |O|O| | |X| |O| | |X|X| |O|O| | |X|O|O| | |X|X| |O|O|X| |X|O|O| 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | | |O|O|X| |X| |X| |O| | | |O|O|X| |X| |X| |O|O| | |O|O|X| 'X' Won! |X| |X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | |X| | |O|X| | | |O|O| | |X| | |O|X| | | |O|O| | |X|X| |O|X| | | |O|O| | |X|X| |O|X|O| 'X' Won! | |O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O|O| | | |X| | |X| |X| |O|O| | | |X| | |X|O|X| |O|O| | |X|X| | |X|O|X| |O|O| | 'O' Won! |X|X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | |X| | |X| | | |O| |O| | |X| | |X| |O| |O| |O| | |X| | |X|X|O| 'O' Won! |O|O|O| | |X| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| | | |O| | | |X| |O|X|X| | | |O| | | |X| |O|X|X| | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | | |O| |X| | | |O|X|O| | |X|O| |X| | | |O|X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| |O| |X| | |O|O| | |X|X| |O| |X| | |O|O| | |X|X| |O|O|X| | |O|O| 'X' Won! |X|X|X| |O|O|X| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O|X| | |X| |X| |O| |O| |O|X|O| |X| |X| |O| |O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X|X| |X|O| | |O| | | |O|X|X| |X|O| | |O| |X| |O|X|X| |X|O| | |O|O|X| |O|X|X| 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | 'O' Won! | | |X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O| | | |X|O|X| | |O| | |O| | | |X|O|X| | |O| | |O|X| | |X|O|X| | |O| | |O|X|O| |X|O|X| |X|O| | |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O|X|X| | |X| | 'O' Won! |O| | | |O|X|X| |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| | 'X' Won! |X| | | |X|O| | |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| | |O| | |X| | | |X|O|X| | |O| | 'O' Won! |X|O| | |X|O|X| | |O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | |O| |X| |O| | |O|X| |X| |O| |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | |O| |X| | |O| | |O|X| | |O| |X| |X|O| | |O|X| | |O| |X| |X|O|O| |O|X| | |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O|O| | | |O|X| | |X|X| |O|O| | | |O|X| | |X|X| |O|O| | |O|O|X| 'X' Won! | |X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| |X| |O| |X| | | |O| |O| |X| |O| |X| | | |O| |O|X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | | | |X| |O| | |X|O| |X| | | |X| |O| |O|X|O| |X| | | |X| |O| |O|X|O| |X|X| | |X| |O| 'O' Won! |O|X|O| |X|X|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X| |O| | |O| | |X| |O| |X|X|O| | |O| | |X| |O| |X|X|O| |O|O| | 'X' Won! |X| |O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | |X|O| | |X| | | | |O| |O|X|O| | |X| | | | |O| |O|X|O| |X|X| | | | |O| |O|X|O| |X|X| | |O| |O| 'X' Won! |O|X|O| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O|X| | | | | | |X|O|O| |O|X| | | | | | |X|O|O| |O|X|X| | | | | |X|O|O| |O|X|X| | | |O| |X|O|O| |O|X|X| | |X|O| It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | |O| | |O| | |X|X| | | |X|O| | |O| | |X|X|O| | |X|O| |X|O| | |X|X|O| | |X|O| 'O' Won! |X|O|O| |X|X|O| | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| |X|O|X| |X| |X| |O| |O| |X|O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| | | |X|X| | | |O| |O|O| | 'X' Won! |X|X|X| | | |O| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X|O| | | |X| | | |O| | 'X' Won! |X|O| | | |X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | | | | | |X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | |X| |O|X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| |O| |X| |O| | | | |X|X| |O| |X| |O| | | | |X|X| |O|O|X| |O| | | 'X' Won! | |X|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | |O|X| | | | |X| 'X' Won! |X|O| | |O|X| | | | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| |O| | |X| | |O|O|X| |X| |O| | |X|O| |O|O|X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | |X| |O|O| | |X| | | 'O' Won! | | |X| |O|O|O| |X| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| | | | | |O|X| | |X| |O| |X| | | |O|X| | |X| |O| |X| |O| |O|X| | |X| |O| |X|X|O| 'O' Won! |O|X|O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | |O|O|X| | |X| | | |O|X| |O|O|X| | |X| | | |O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| |O|X| | It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| 'O' Won! |O|O|O| | | | | |X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| |O| | | |X| |X|O|O| |X| |O| 'O' Won! |O| |X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| |X| | | | |O|X| | | |O| |X|O| | | |O|X| | | |O| |X|O| | | |O|X| | |X|O| |X|O| | | |O|X| |O|X|O| |X|O|X| | |O|X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| |O| |O| | | | | | |X|X| |O| |O| | | |X| | |X|X| |O| |O| | | |X| |O|X|X| |O| |O| |X| |X| |O|X|X| 'O' Won! |O|O|O| |X| |X| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | |X|O| | | | |X| | |O| | |X|O|O| | | |X| | |O|X| |X|O|O| | | |X| |O|O|X| |X|O|O| | | |X| |O|O|X| |X|O|O| | |X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | | |X|X| | |O| | |O| |O| | |X|X| | |O| | |O| |O| | |X|X| |X|O| | |O| |O| | |X|X| |X|O|O| 'X' Won! |O| |O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | | |O|X| |X| | | | | |O| | |O|X| |X| |X| | | |O| | |O|X| |X|O|X| | | |O| | |O|X| |X|O|X| | |X|O| 'O' Won! |O|O|X| |X|O|X| | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | | | | |O|O| 'X' Won! |X|X|X| | | | | | |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | | |X| |X| | | |O|O|X| |O| |X| |X| | | 'X' Won! |O|O|X| |O| |X| |X| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| | |O|X| | |X| | | |O|O| |X|O|X| | |X| | | |O|O| |X|O|X| | |X|O| | |O|O| 'X' Won! |X|O|X| | |X|O| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O|O| | | | | |X| |X| |X|O|O| | | | | |X| |X| |X|O|O| | | |O| 'X' Won! |X|X|X| |X|O|O| | | |O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| |X| |X| |X| |O| |O| |O|O|X| 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | | |X| | | |O|X| |O| |O| | |X| | | |O|X| |O|X|O| | |X| | | |O|X| |O|X|O| | |X|O| | |O|X| |O|X|O| |X|X|O| | |O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| |X| |O| | | |X| |O| |X| |X| |O| |O| |X| |O| 'X' Won! |X| |X| |O|X|O| |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | |O|O| | |X| |X| |O| | | |O|O| | |X| |X| |O| |X| |O|O| | |X| |X| |O|O|X| 'X' Won! |O|O| | |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O| | 'O' Won! |X|O|X| |O|O| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| |X|O| | | | |O| |O|X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | |X| |X| |O| |X| 'O' Won! |O|O|O| |X| |X| |O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X|X| 'O' Won! |O| | | |O| |X| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O|X|X| | | |O| | | | | |O|X|X| | | |O| | | |O| |O|X|X| |X| |O| | | |O| |O|X|X| |X| |O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| |O| | | | |X| | | | |O| |O| | | | |X| | |X| |O| |O| | | | |X|O| |X| |O| 'X' Won! |O| |X| | |X|O| |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| |X|O| | | | |O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O| | |X|O|O| 'O' Won! |O|X|X| |X|O| | |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | | | | |O|X|X| |O|O| | | | |X| |O|X|X| |O|O| | | |O|X| |O|X|X| 'X' Won! |O|O|X| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X|O| |O| |X| |X| |O| | |X|O| |O| |X| |X| |O| |O|X|O| |O|X|X| |X| |O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | |X|X| | | | | |O| |O| | |X|X| | | |O| |O| |O| | |X|X| |X| |O| |O| |O| | |X|X| |X|O|O| 'X' Won! |O| |O| |X|X|X| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | |O| | |X| |X| |O|X| | | |O| | |X| |X| |O|X| | | |O|O| |X| |X| |O|X|X| | |O|O| 'O' Won! |X| |X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X|O| | | |X| |O|X| | | |X|O| | | |X| |O|X| | | |X|O| |O| |X| |O|X|X| | |X|O| |O| |X| |O|X|X| | |X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| |O| |X| |X| | | |O| |O| |O|X|X| |X|O| | |O| |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| |O|O| | | |O| | | |X|X| |O|O| | |X|O| | | |X|X| |O|O| | |X|O| | |O|X|X| |O|O| | |X|O|X| |O|X|X| 'O' Won! |O|O|O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | |O| | | |X| | |X| |O| |X|O| | | |X| | |X| |O| |X|O| | | |X| | |X|O|O| 'X' Won! |X|O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X|X|O| 'O' Won! | | |O| | | |O| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|X| | |O| | |O| |X| |O|X|X| | |O| | |O|X|X| |O|X|X| | |O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | |O| | | | |X|X| | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| | |O|O| |O|X|X| | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X|O| | |X|O| 'X' Won! | |X| | |O|X|O| | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X| | | |X| | |O|O|X| |O|X| | | |X|O| 'X' Won! |O|O|X| |O|X| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O|O|X| | |X| | |X|O| | |O|O|X| | |X| | |X|O|O| |O|O|X| |X|X| | |X|O|O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X| |X| | |O| | |O|O| | |X| |X| | |O|X| |O|O| | |X| |X| 'O' Won! | |O|X| |O|O|O| |X| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| |X| |O| | |O|X| |O| |X| |X| |O| | |O|X| |O| |X| |X|X|O| | |O|X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| |O| |O| | |X|O| | | |X| |O|X|O| |O|X|O| | | |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O|X|X| | | | | |O| | | |O|X|X| | | | | |O| |O| |O|X|X| |X| | | |O| |O| 'O' Won! |O|X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X|O| | |O|O| |X| | | |X|X|O| | |O|O| 'O' Won! |X| |O| |X|X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O|O| |X| | | | |X| | |X|O|O| |X| |O| | |X| | |X|O|O| 'X' Won! |X| |O| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| | |O|X| | | | | |X|X|O| |O|O|X| | | |X| |X|X|O| |O|O|X| | |O|X| |X|X|O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O| | |O|X|X| 'O' Won! |O|X|O| | |O| | |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O| |O| |X| |X| 'X' Won! |O| | | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | |X| |X| |O|O| | | | |O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O| | |O|X|O| |X| |X| |O|O| | |O|X|O| |X| |X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | |X| | | | |O| | |X|O| |O|X| | | | |O| 'X' Won! | |X|O| |O|X| | | |X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | |O|X|X| |X|O|X| |O|O| | |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O|O| |X| | | |O| | | |X|O|O| |X|X| | |O| | | |X|O|O| |X|X| | |O|O| | 'X' Won! |X|O|O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | |O|X|O| | | |X| | | | | |O|X|O| | | |X| | | |X| |O|X|O| |O| |X| | | |X| |O|X|O| |O| |X| |X| |X| |O|X|O| |O| |X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O| |X| |O|X| | | |X|O| |O| |X| |O|X| | | |X|O| |O| |X| |O|X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | | |X|O|O| |O|X|X| | | | | |X|O|O| |O|X|X| |X| | | |X|O|O| 'O' Won! |O|X|X| |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | | |O|O| | | | | |X|X| | |X|O|O| | | | | |X|X| | |X|O|O| | | |O| |X|X| | 'X' Won! |X|O|O| |X| |O| |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | |O| | |X| | |X|O| | | | |O| |X|X| | |X|O| | | |O|O| |X|X| | |X|O| | 'X' Won! | |O|O| |X|X|X| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| | | |O|O| |X| |X| |O|X| | | |O|O| |X| |X| |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | |X| | |O|X|O| | | | | |O|X| | |O|X|O| | | | | 'X' Won! |O|X| | |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | |O| | |X|X| |X| |O| | | |O| | |X|X| |X| |O| |O| |O| | |X|X| 'X' Won! |X| |O| |O|X|O| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| |X| | | | |X| | | | |O| |X|O| | | |X| | | | |O| |X|O|X| | |X| | | | |O| |X|O|X| | |X|O| | | |O| 'X' Won! |X|O|X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | |O|X| | |X| |O| | | | | |O|X|X| |X| |O| | | | | |O|X|X| |X| |O| | |O| | |O|X|X| |X| |O| |X|O| | |O|X|X| |X| |O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | |X| | | | |O|O| |X| | | |X| |O| | |O|O| |X|X| | |X| |O| | |O|O| 'O' Won! |X|X| | |X| |O| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | |O|O|X| | |X| | | | |X| |O|O|X| | |X| | | |O|X| |O|O|X| | |X| | | |O|X| |O|O|X| |X|X| | | |O|X| |O|O|X| |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X|O| | |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| 'O' Won! |O| | | |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | |X| | |X| |O| |O| |O| | |X| | |X| |O| |O| |O| | |X|X| |X| |O| |O| |O| |O|X|X| |X| |O| |O|X|O| |O|X|X| |X| |O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | |O| | | | |X|X| |X|O| | |O| | | | |X|X| |X|O| | |O| |O| | |X|X| |X|O|X| |O| |O| | |X|X| |X|O|X| |O| |O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O|X| | |O|O| | | | |X| |O|X| | |O|O|X| | | |X| |O|X| | |O|O|X| | |O|X| |O|X| | 'X' Won! |O|O|X| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X| | |X| | | | |X|O| |O|X| | |X| |O| | |X|O| 'X' Won! |O|X| | |X|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| |O| |X| | | | | |X|X|O| |O| |X| | |O| | |X|X|O| |O| |X| | |O|X| |X|X|O| |O| |X| |O|O|X| |X|X|O| |O| |X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | |O| | |X|X| | | |O| | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O|X| |X|X| | 'O' Won! |O|O| | |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X|O|O| |X| | | | | |X| |X|O|O| |X| | | | |O|X| |X|O|O| |X| | | | |O|X| |X|O|O| |X|X| | | |O|X| |X|O|O| |X|X|O| 'X' Won! |X|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X| | | | |X| |O|X| | |O|X|O| | | |X| 'X' Won! |O|X| | |O|X|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O| | | | |X|X| |X|O|O| |O| | | |O|X|X| |X|O|O| |O|X| | |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | |X|O| |X|O| | | | | | |O|X|O| |X|O| | | | | | |O|X|O| |X|O|X| | | | | |O|X|O| |X|O|X| | |O| | |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| | |X| | |O|X| | | |O|X| | |X| | |O|X| | |O|O|X| 'X' Won! |X|X| | |O|X| | |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | | | | | |X|X| 'O' Won! |O|O|O| | | | | | |X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| |X| | |O| | |O|X| | |O| |X| | |O| | |O|X| | |O|X|X| 'O' Won! |O|O| | |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | |O|X| | |X| | |O| | | |X|O|X| | |X| | |O| | | |X|O|X| | |X|O| |O| | | |X|O|X| | |X|O| |O|X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X| |O| |O| | | | |O|X| |X|X|O| |O| | | | |O|X| |X|X|O| |O| |O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | | | |X| | |O|X|O| 'X' Won! |O|X| | | |X| | |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| |O| |X|O| | | |O|X| |X| |O| |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| 'O' Won! | |O| | | |O|X| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | | | |O| | | |X|X|O| | | |X| |O| | | |X|X|O| |O| |X| |O| | | |X|X|O| |O|X|X| |O| | | |X|X|O| |O|X|X| |O|O| | 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| |X| | |O|X| |O| | | |X| |X| |O|O|X| 'X' Won! |O| | | |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |X| |O| |O| |X| | | | | |X| |O| |O| |X| |X| | | |X| |O| |O| |X| |X| | |O|X| |O|X|O| |X| |X| | |O|X| |O|X|O| |X|O|X| | |O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | | |X|X| |X| |O| |O|O| | | |X|X| 'X' Won! |X| |O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| |O| |O|X| | | | |X| |O|X|O| 'O' Won! |O|X| | | |O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | | | | |X| |X| |O|O| | | |X| | |X| |X| |O|O| | |O|X| | |X| |X| |O|O| | |O|X|X| |X|O|X| |O|O| | |O|X|X| 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| |O|X| | | |O|O| | | |X| |O|X| | | |O|O| | |X|X| 'O' Won! |O|X| | |O|O|O| | |X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| |X| |X| |X| | |O|O| |O| |X| 'X' Won! |X|X|X| | |O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | |O| |X| | | | |O|X| | |X|O| |X| | | | |O|X| |O|X|O| |X| | | | |O|X| |O|X|O| |X|X| | | |O|X| |O|X|O| |X|X| | |O|O|X| 'X' Won! |O|X|O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | | | |X|O|O| |X|O| | 'X' Won! |X| | | |X|O|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | |X|O| | |O| | |O| |X| | |X|O| | |O| | |O| |X| |X|X|O| |O|O| | |O| |X| |X|X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | 'X' Won! | | | | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | | | | | |X|X| |O|O| | |O| | | 'X' Won! |X|X|X| |O|O| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | | | |X| |O| |O| |X|X| | |O| |X| |O| |O| |X|X| | 'X' Won! |O| |X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| | | |X| |X|O|O| |X|X|O| 'O' Won! |O| |X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | |X| | |X| |O| | |O| | | |X| | |X|O|O| | |O| | | |X| | |X|O|O| |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| |X|O| | 'O' Won! |O|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| |O| | |O|X| |X| |X| |O| |O| |O|O|X| |X| |X| |O| |O| 'X' Won! |O|O|X| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| | |X|O|X| |X| | | |O|O| | 'O' Won! |X|O|X| |X| | | |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X|O| | | |X| | |O|O| | |X|O| | |X|X| | 'O' Won! |O|O| | |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O|O| |O| | | |X| |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| |O| |X| |X| |X|O|O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | | | |X| | |O|X|O| |O| | | | |X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O|O| | |X|X| | 'X' Won! |O|X|O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | |X| |O|O| | |X|X| | 'O' Won! | | |X| |O|O|O| |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| |X| | |O| | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | |X| |X| |O|O|X| |O|O| | |X| |X| 'X' Won! |O|O|X| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | | |O| |X| |O|X|X| | | | | |O| |X| |O|X|X| | |O| | |O| |X| 'X' Won! |O|X|X| | |O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X|X| | | | |O| |O| | | |X|X| | |X| |O| |O| | | |X|X| | |X| |O| |O| |O| |X|X| | |X|X|O| |O| |O| |X|X| | 'O' Won! |X|X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O|X| | |X| |O| | |X| | |O|X| | |X|O|O| | |X| | |O|X| | |X|O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | |X| |O| | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| | |X| | | |X|O| |X|O|O| | |X| | | |X|O| |X|O|O| | |X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O| | |X| |O| | |X|O| |X|O| | |X| |O| 'O' Won! |O|X|O| |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X|O| | 'X' Won! |X| |O| |X| |O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X| | | | |X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X| |X| |O|X|X| |O| |O| 'O' Won! |X| |X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| |O| |X|O| | | | | | |X|X|O| |X|O| | | | | | |X|X|O| |X|O|O| 'X' Won! |X| | | |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| |O| | | |O|X|X| | | |X| |O| | | |O|X|X| | | |X| |O|O| | |O|X|X| | |X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | 'X' Won! | |O| | |X|X|X| | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O|X| |O| | | | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O|O|X| | |X| | |O|O|X| 'X' Won! |O|O|X| | |X|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| | |O| | |X|O|X| | | |O| | |O|X| |X|O|X| |O| |O| | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X|O| | | | | |X|O|O| |X|X|O| | |O| | |X|O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| | | |X| 'O' Won! |O|O|O| | | |X| | | |X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X|O|X| | |O| | | |X| | |X|O|X| | |O|O| | |X| | |X|O|X| 'X' Won! |X|O|O| | |X| | |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | |O| |X| |X| |X|O|O| | | |O| |X| |X| |X|O|O| |O| |O| 'X' Won! |X|X|X| |X|O|O| |O| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O| | | | |X| | | |O| |X|O| | | | |X| | |X|O| |X|O| | |O| |X| | |X|O| |X|O|X| |O| |X| | |X|O| |X|O|X| |O| |X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| |X|X| | | |O|X| | |O|O| |X|X| | 'X' Won! | |O|X| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O| |X| | | |O| | |X| | |O|O|X| | | |O| | |X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| |X|X|O| | |X|O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | | | | |O|X| | | |O|X| | | | | |O|X|O| | |O|X| 'X' Won! |X| | | |O|X|O| | |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | |X|O| | |O|X|O| | | | | |X|O| | |O|X|O| | | |X| |X|O| | |O|X|O| | |O|X| |X|O| | |O|X|O| |X|O|X| |X|O| | 'O' Won! |O|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | | |X| |X| | | |O|O| | |O| |X| |X| | | |O|O| | |O| |X| |X|X| | |O|O| | |O| |X| |X|X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | |O|O|X| 'X' Won! |X| | | | |X| | |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | |O| |O| | | |X| | |X| | |O| |O| | |X|X| | |X| | 'O' Won! |O|O|O| | |X|X| | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X|O| | |O| | | |X| | |X|X|O| | |O| | | |X| | 'O' Won! |X|X|O| | |O| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | |O| | | | |X| | |O|X|X| |O| | | | |X| | |O|X|X| |O| |O| | |X|X| |O|X|X| |O| |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| |X|O| | | |O|O| |O|X|X| |X|O| | |X|O|O| |O|X|X| |X|O| | It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O|X| | |O|X| | |X|O| | |O|X| 'X' Won! |X|O|X| | |X|O| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X|X| | |O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | |O|X| |O| |O| |X| |X| | |O|X| |O| |O| |X|O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | |X| |X| | | |O| |X|O| | |X| |X| |O| |O| |X|O| | 'X' Won! |X| |X| |O|X|O| |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | |X| | | |O|X| | |O|O| | |X| | | |O|X| | |O|O| | |X|X| |O|O|X| | |O|O| | |X|X| |O|O|X| |X|O|O| | |X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| | |X| | | | |O| 'X' Won! |O| |X| | |X| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | |X| | | | |O| |O| |X| | |X| | |O| |O| |O|X|X| | |X| | |O| |O| 'O' Won! |O|X|X| |O|X| | |O| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | | | |O|X|O| |O|X| | | | |X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| | | | |O|X| | |X|O| |O| | | | |O|X| | |X|O| |O| |X| | |O|X| | |X|O| |O|O|X| | |O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | |O|X| | |X|X| | |O| | | |O|X| |O|X|X| | |O| | | |O|X| |O|X|X| | |O| | |X|O|X| |O|X|X| |O|O| | |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X|O| | |O|X| | |X| | | |X|O| |O|O|X| | |X| | | |X|O| |O|O|X| |X|X| | | |X|O| |O|O|X| |X|X|O| | |X|O| 'X' Won! |O|O|X| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | |X| |X| | |O| | | |O| | |X| |X| | |O|X| | |O| | |X| |X| | |O|X| |O|O| | |X| |X| |X|O|X| |O|O| | |X| |X| 'O' Won! |X|O|X| |O|O| | |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |O| | | | |X|O| | | | | |O|X| | | |X|O| | | | | |O|X|O| | |X|O| | | |X| |O|X|O| 'O' Won! | |X|O| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| |X| | | |O| |X| |O| |O| |X| | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| |O|X|X| | | |O| | |X|O| |O|X|X| | |O|O| | |X|O| |O|X|X| 'X' Won! |X|O|O| | |X|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X|O| | |X| | |O|O| | |X|X|O| 'O' Won! |O|X| | |O|O| | |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | | |O| | | | |O| |X|X| | | |O|X| | | |O| |X|X|O| | |O|X| | | |O| |X|X|O| | |O|X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | |X|O| |X| | | |O| | | | |X|O| |X| | | |O| |O| | |X|O| |X|X| | |O| |O| 'O' Won! | |X|O| |X|X|O| |O| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X|O|O| | | | | | |O|X| |X|O|O| |X| | | |O|O|X| |X|O|O| |X| | | |O|O|X| |X|O|O| |X|X| | 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| |X|O| | | |X| | |O|O|X| |X|O| | |X|X| | |O|O|X| |X|O| | |X|X| | |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X|O| | | |X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O| | |X|O|O| |O|X| | |X|O|X| |X|O|O| |O|X| | It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| | |O|O| | |X| | | |O|X| | |O|O| |X|X| | 'O' Won! | |O|X| |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | |O|O| |X| |X| | | | | | |O|O| |X|O|X| | | | | | |O|O| |X|O|X| |X| | | | |O|O| 'O' Won! |X|O|X| |X|O| | | |O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | | |O|X| | | |X| |O| | | |O|O|X| | | |X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O|O| | 'X' Won! |O|O|X| | |X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X|O| | | |X| | |X|O| | |X|O| | | |X| | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| | | |O|X|X| | | | | |O| |O| |O|X|X| | | | | |O| |O| |O|X|X| |X| | | 'O' Won! |O|O|O| |O|X|X| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | |X| | | |O|X| | | |O|O| |X| | | |O|X| | | |O|O| |X| | | |O|X|X| | |O|O| |X| |O| |O|X|X| |X|O|O| |X| |O| |O|X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | |O| |X| | |X|O| | |O| | |O|X|X| | |X|O| | |O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| |O|O| | 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | |X|X|O| |O|X| | | | |O| |X|X|O| |O|X| | 'X' Won! | |X|O| |X|X|O| |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| |X| |O| | |X| | | |X|O| 'O' Won! |X| |O| | |X|O| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| |X| | | | | |X|X|O| |O|O|X| | |X| | |X|X|O| |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | |X| | |O|X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | |X|O| 'O' Won! |O|O|X| | |O|X| | |X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X|X| |O| | | |X| |O| | |X|X| |O|O| | 'X' Won! |X| |O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | |O| | |O| | |X| |X| | |X|O| | |O| | |X| |X| |O|X|O| | |O| | |X| |X| 'X' Won! |O|X|O| | |O| | |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |X| |X| |O| 'O' Won! |O|O|X| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X|X| |X|O| | |O| |O| | |X|X| |X|O| | |O|X|O| | |X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | | | | | |O|X|O| | |X| | | | |X| |O|X|O| | |X| | |O| |X| |O|X|O| |X|X| | |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X|X| | |O|O|X| | |O|O| |X|X| | |O|O|X| 'X' Won! |X|O|O| |X|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |O| | |O| | |X| |X| |O|X|O| | |O|O| |X| |X| |O|X|O| 'X' Won! | |O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| |O|X| | | |O|X| | |X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| |O| |X| | | |O| | |X|O| |O| |X| | |X|O| | |X|O| |O| |X| | |X|O| |O|X|O| 'X' Won! |O|X|X| | |X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | |O| | | | |O|X| |O|X| | |O| |X| | |O|X| 'O' Won! |O|X| | |O| |X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X| | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | |O|X| | |O| |O| |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X| | | | |O|X| | |O| | |X| |X| | |O|X| | |O| | |X| |X| | |O|X| | |O|O| |X| |X| | |O|X| |X|O|O| 'O' Won! |X|O|X| | |O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | | | | |O|X| | |O|O|X| | | | | |O|X| | |O|O|X| | |X| | 'O' Won! |O|X| | |O|O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X|O| |O|O|X| | |X|X| | |X|O| |O|O|X| |O|X|X| | |X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| |X| |O| | | |O|X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O|X| | |O|X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X|O| |X| | | 'X' Won! |O|X|X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| |X| | | | | |O|X|O| |X| |X| | | |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X| |X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| |O| | | |O| |X| |X| |X| |O| | |O|O| |X| |X| |X| |O| 'X' Won! |X|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | |O| | | | |O| |X| |X| |X|O| | | | |O| |X| |X| |X|O|O| | | |O| |X| |X| 'X' Won! |X|O|O| | | |O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | |O| 'X' Won! | | |O| |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X| | | |O|O|X| | |X|O| |X| |O| |O|O|X| | |X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| | |X|X| |X| |O| |O| |O| | |X|X| |X|O|O| 'X' Won! |O| |O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| |X|O| | | |O| | |X| |O| |X|O| | 'X' Won! |X|O| | |X| |O| |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X|X| |O|O| | | |O| | | |X|X| |O|O| | |X|O| | | |X|X| 'O' Won! |O|O|O| |X|O| | | |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | |X|O| | |O| |X| | |X| | |X|O|O| |O| |X| | |X|X| |X|O|O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | |X|O| | | | | |X|X|O| 'O' Won! | |X|O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | | | | |X| | |X|O|O| | | | | | |X|O| |X|O|O| | |X| | | |X|O| |X|O|O| | |X| | |O|X|O| |X|O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | |O|X| |X|O| | | | | | | |O|X| |X|O|X| | | | | |O|O|X| |X|O|X| | | | | |O|O|X| |X|O|X| | |X| | |O|O|X| |X|O|X| |O|X| | 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | |X| |O| |X| |O| |O| | | |X| |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| | |O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | 'O' Won! | |O|X| | |O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | | |X|O| | |O|X| | |O| | | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| |O|O|X| | |X|O| | |O|X| 'X' Won! |O|O|X| | |X|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | |O| | |X|X| | |X| |O| | |O| | |X|X| | |X| |O| | |O|O| 'X' Won! |X|X| | |X| |O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | |X|X| | | |O|O| |O|X| | 'O' Won! |X|X|O| | |O|O| |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | | |X| |X|O| | |X|O|O| | | |X| |X|O| | 'O' Won! |X|O|O| | |O|X| |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | |X|X| | |O| | |O| | | |O|X|X| | |O| | |O| | | |O|X|X| | |O|X| |O|O| | |O|X|X| | |O|X| 'X' Won! |O|O|X| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| |O| | | |O| |O| |X| |X| |O|X| | |O| |O| |X| |X| |O|X|O| |O| |O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | | | | | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | |O|X| | 'X' Won! |O|X|O| |X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | | | |O|X|X| |X| |O| | | |O| |O|X|X| |X|X|O| | | |O| |O|X|X| 'O' Won! |X|X|O| | |O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| | |X|O| 'O' Won! |O| | | |X|O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| |O|X| | | | | | | |O|X| |O|X|X| | |O| | | |O|X| |O|X|X| |X|O| | | |O|X| |O|X|X| 'O' Won! |X|O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | | |O| | |O| | 'X' Won! |X|X|X| | | |O| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | |X| |X| | |X|O| |O|O| | |X| |X| | |X|O| |O|O|X| |X| |X| |O|X|O| |O|O|X| |X| |X| 'X' Won! |O|X|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | |X| |X| | | 'O' Won! |O|O|O| | | |X| |X| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O|X| |O| |X| | | | | | |O|X| |O|O|X| | | | | 'X' Won! | |O|X| |O|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O|X| |O| | | | | |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| |O| |X| |X| |O|O|X| 'X' Won! |O| |O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | |X|O| |O|X| | | | | | |O|X|O| |O|X| | | | | | |O|X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | |O| 'X' Won! |O|X|O| |O|X|X| | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | | | | | |O|O|X| |O|X| | |X| | | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | |O|O| |X| |X| |O| |X| | |O|O| 'O' Won! |X| |X| |O| |X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O| |O| | |X|X| 'O' Won! |X|O| | |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X| | | | |X|O| |O|O| | |X| | | | |X|O| |O|O| | |X| | | |X|X|O| |O|O| | |X| |O| |X|X|O| |O|O| | |X|X|O| |X|X|O| 'O' Won! |O|O|O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O| | | | |X|X| | |X|O| |O|O| | | |X|X| | |X|O| |O|O|X| | |X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | | | | 'X' Won! | |O|X| | |X|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | | |O|O| | |X| |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X|O|O| 'X' Won! |X|X|X| |O|O| | |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | | |O| | |X| | |X|X|O| | | |O| | |X| | |X|X|O| | |O|O| | |X| | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | | | | |X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| |O| | |X|X| |X|O|O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X| | |O| |O| |X| |X| |O|X| | |O| |O| |X| |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| | | |X|O|X| |O|O| | |X| | | |X|O|X| |O|O| | |X|X| | |X|O|X| 'O' Won! |O|O|O| |X|X| | |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | |X|O|O| | |O| | |X|X| | |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|O| |X| |X| | |X|O| | |O|O| |X| |X| |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| |X| |O| |X|X| | |O|X|O| |X|O|O| |X|X| | 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| |X|O|X| | | |O| | |O|X| 'O' Won! |X|O|X| | |O|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O| |O| |X| | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O| |X| |O| |O| |X|X|O| |O| |X| |O|X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | |X| |O| |O| |X|O|X| | | |X| |O| |O| |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| | |O|O| | |X| | | | |X|X| |O|O| | |X|O| | | |X|X| |O|O| | 'X' Won! |X|O| | |X|X|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | |O|X| |O| | | |X|X| | | |O|X| |O| | | 'O' Won! |X|X|O| | |O|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| 'O' Won! |O| |X| | |O|X| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | |X|O| | | | | |O|O|X| | |X|O| | | | | 'X' Won! |O|O|X| | |X|O| |X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O| | |X|O|X| | |X|O| 'O' Won! |O|O| | |X|O|X| | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | |O| | |O|X|X| |O|X| | | |O| | |O|X|X| |O|X| | | |O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X|X| |O| | | |X| |O| |O|X|X| |O|X| | |X| |O| |O|X|X| |O|X| | |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| |O|O| | | |O| | | |X|X| 'X' Won! |O|O| | | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | | | |X|O| | 'X' Won! |X| |O| |X| | | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| |O| |O| | |X|X| | | |X| |O| |O| 'O' Won! | |X|X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | | |O|X| |O|X| | | | | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X| | |O| |X| |X|O|X| |O|X| | 'O' Won! |O| |X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | |O|X| |O| | | | | |X| | |O|X| |O| | | | | |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| |O| 'X' Won! |X|X|X| |O|O|X| |O| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| |X| | | | |X| | |O|O|X| |X|O| | | |X| | |O|O|X| |X|O|X| |O|X| | |O|O|X| |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | |X| |O| | | | | | |X|O| |X| |O| | | | | |X|X|O| |X| |O| | | | | |X|X|O| |X|O|O| | | | | |X|X|O| |X|O|O| | |X| | 'O' Won! |X|X|O| |X|O|O| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | | | | 'X' Won! |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| | |O| | |O|O| | |X| |X| |X|O| | |O|O| | |X| |X| |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | |X| | | |O|O|X| |X|O| | |X| | | |O|O|X| |X|O| | |X|X| | 'O' Won! |O|O|X| |X|O| | |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| |O| |X| | |X|O| |O| |X| |O| |X| | |X|O| |O|X|X| 'O' Won! |O| |X| |O|X|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X|X| | | | |O| |X| |O| |X|X| | | |O|O| 'X' Won! |X| |O| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| |O| |X| | | | | |O| 'X' Won! |X| |O| |X| | | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| |X| |O| |X| |O|O| | |X| |X| 'X' Won! |O| |X| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O|X| | |O| | | |X| | | |O|X|O| |O| | | |X| |X| |O|X|O| |O| | | |X| |X| |O|X|O| |O|O| | 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | 'X' Won! | |O|X| | |X| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X|O| | | | |X| |O| | | |X|O| | | |X|X| |O| |O| |X|O| | | |X|X| |O| |O| |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X| | |O|X| | | | |O| 'X' Won! |O|X| | |O|X| | | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | | |O|O| | |X| | |X| | | |X|O|O| | |X| | |X|O| | |X|O|O| 'X' Won! |X|X| | |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| 'O' Won! |O| | | |X|O|X| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | 'O' Won! |O| |X| |O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | |O| | | | |X|O| 'X' Won! |O|X| | |O|X| | | |X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | |X|O|X| | | | | |O|O| | |X|O|X| | |X| | |O|O| | |X|O|X| 'O' Won! | |X| | |O|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | | |O| |X| |X| |X| |O| |O| |O| |X| |X| |X| |O| |O| |O|X|X| 'O' Won! |X| |X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | |O| | | | |X| | |O|X| | |O| | |X| |X| | |O|X| |O|O| | |X| |X| | |O|X| 'X' Won! |O|O|X| |X| |X| | |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| |O| | | | |X| | |O|X|X| |O| | | | |X| | 'O' Won! |O|X|X| |O| | | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | 'X' Won! |O| | | | | |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| | | | | | |X| | |O| |X| | |O| | | |X| | |O| |X| | |O|X| | |X| | |O|O|X| | |O|X| | |X| | |O|O|X| |X|O|X| | |X| | 'O' Won! |O|O|X| |X|O|X| | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| | | |O| | | | |X|X| |O| | | |O|X| | | |X|X| |O| |O| |O|X| | | |X|X| |O| |O| |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | | | |O|O| | 'X' Won! |X|X|X| | | | | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X| |O| | |O| | | |X|O| |X| |O| |X|O| | 'O' Won! | |X|O| |X| |O| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | |O|O| | |X| | | |X|X| | |O|O| | |X| | |O|X|X| | |O|O| | |X|X| |O|X|X| | |O|O| 'O' Won! | |X|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | |X| |O| | | |X|O| | | | |X| |O|O| | |X|O| | | |X|X| |O|O| | |X|O| | |O|X|X| |O|O| | |X|O| | |O|X|X| |O|O| | |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | |X|O| | |X| | | | |O| | |X|O| | |X| | | |O|O| | |X|O| | |X|X| | |O|O| |O|X|O| | |X|X| | |O|O| 'X' Won! |O|X|O| |X|X|X| | |O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | | |X| |O|O| | | |X| | | |O|X| |O|O| | | |X|X| | |O|X| 'O' Won! |O|O|O| | |X|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X| | | |O|X|O| | |X| | |X|O| | |O|X|O| | |X| | |X|O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X| | | |O| | 'X' Won! |O| |X| |O|X| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O| |O| |X| |O|X| | |X| |O| |O| |X| |O|X| | |X| |O| |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |X| | |X|O| |O|X|O| | | |X| | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X|O|X| 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| | | |O| |O| |X|X|O| |X| | | |O| |O| 'O' Won! |X|X|O| |X| | | |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| |X|O| | | | | | |O| |X| |X|O| | |O| | | |O| |X| |X|O| | |O| |X| |O| |X| |X|O|O| |O| |X| |O| |X| 'X' Won! |X|O|O| |O|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | 'X' Won! |X| | | |X| |O| |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | |X|O| | | | |X| |O| | | |X|O| | | |O|X| |O| | | |X|O|X| | |O|X| |O| | | |X|O|X| |O|O|X| |O| | | |X|O|X| |O|O|X| |O|X| | It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| 'O' Won! |X|X|O| |O|X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | | | |X| |O|X| | | |O| | | |O|X| |O|X| | | |O| | 'X' Won! | |O|X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O| |X| | |X|X| 'O' Won! |O| | | |O| |X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | |O|X|X| 'X' Won! | |O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | 'X' Won! |X| |O| |X|O|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| |X|X| | |O| | | | |O|X| |X|X| | |O|O| | | |O|X| |X|X| | |O|O|X| | |O|X| |X|X| | |O|O|X| |O|O|X| 'X' Won! |X|X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | |O| | | | |X|O| |X| | | |O|X| | | |X|O| |X| | | |O|X| | |O|X|O| |X| | | |O|X|X| |O|X|O| |X|O| | |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | |O|X| |O| |X| | | | | 'X' Won! | |O|X| |O| |X| | | |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| |O| |X| | |X| | | |O|O| |O| |X| |X|X| | | |O|O| |O| |X| |X|X|O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | |O|X| |O| |X| | |X| | | |O|X| |O| |X| | |X| | |O|O|X| |O| |X| 'X' Won! | |X|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | | |X| | | |O| |O|X|X| | |O|X| | | |O| |O|X|X| |X|O|X| | | |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | | |O| | |X|O| |O|X| | | | |O| | |X|O| |O|X| | | | |O| |X|X|O| 'O' Won! |O|X|O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | |X|O| | | | |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O| | 'O' Won! |X|O|X| |O|O| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |X|X| | | |X| | |O|O| | |X|X| |O| |X| 'X' Won! | |O|O| |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X|X| | | |X| |O| | | |O|X|X| | |O|X| 'X' Won! |O| |X| |O|X|X| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|X| | | | | 'O' Won! |O|X|X| | |O|X| | | |O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | |X|O| | | |O| |X| | | | |X|O| | |O|O| |X| | | | |X|O| |X|O|O| 'O' Won! |X| |O| | |X|O| |X|O|O|","title":"1.0.2 Load Dataset"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#11-clean-data","text":"We will first need to organize the data into a parsable format.","title":"1.1 Clean Data"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q1","text":"What is the object data and what does it contain? what are the keys of data? what are the keys of each game? # inspect data below by grabbing the first key in data # what are the three different keys within each game? data [ 'game 0' ] {'board': {1: 'O', 2: 'X', 3: 'X', 4: ' ', 5: 'X', 6: ' ', 7: 'O', 8: 'O', 9: 'O'}, 'winner': 'O', 'starting player': 'O'}","title":"Q1"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q2","text":"Using those keys, iterate through every game in data and append the board, the winner, and the starting player to separate lists. Call these lists: boards, winners, and starters boards = [] winners = [] starters = [] for game in data : boards . append ( data [ game ][ 'board' ]) winners . append ( data [ game ][ 'winner' ]) starters . append ( data [ game ][ 'starting player' ])","title":"Q2"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q3","text":"Make a dataframe out of the list boards and call it df . Make a series out of the list winners . Make a series out of the list starters . Make a new column of df called \"Winner\" and set it equal to the pandas Series of the winners. Make a new column of df called \"Starter\" and set it equal to the pandas Series of the starters. # YOUR CODE HERE df = pd . DataFrame ( boards ) df [ \"Winner\" ] = pd . Series ( winners ) df [ \"Starter\" ] = pd . Series ( starters ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 Winner Starter 0 O X X X O O O O O 1 O X O X X X X O O X X 2 O X X O O O O 3 X O X X X O O O O O O 4 X X X O O O X O","title":"Q3"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#12-inferential-analysis","text":"We're going to use Bayes Rule or Bayesian Inference to make a probability of winning based on positions of the board. The formula is: $ P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)}$ Where $\\cap$ is the intersection of $A$ and $B$. The example we will use is the following: what is the probability of 'O' being the winner, given that they've played the center piece. $B$ = 'O' played the center piece $A$ = 'O' won the game So what is probability? We will define it in terms of frequencies. So if we are for instance asking what is the probability of player 'O' being in the center piece, it would be defined as: $ P(B) = \\frac{|O_c|} {|O_c| + |X_c| + |empty|}$ Where the pipes, | | , or cardinality represent the count of the indicated observation or set. In this case $O_c$ (O being in the center) and $X_c$ (X being in the center). Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc_Xc_empty 1000 ( df [ 5 ] == 'O' ) . value_counts () False 550 True 450 Name: 5, dtype: int64 # example of assessing the probability of B, O playing the center piece player = 'O' Oc = ( df [ 5 ] == player ) . value_counts () Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc / Oc_Xc_empty False 0.55 True 0.45 Name: 5, dtype: float64 # we can also clean this up and replace the denominator with the whole # observation space (which is just the total number of games, df.shape[0]). # example of assesing probabiliy of A ( df [ 'Winner' ] == 'O' ) . value_counts () / df . shape [ 0 ] False 0.571 True 0.429 Name: Winner, dtype: float64 The $P(B|A) * P(A)$ is the intersection of $B$ and $A$. The intersection is defined as the two events occuring together. Continuing with the example, the probablity of 'O' playing the center piece AND 'O' being the winner is the number of times these observations occured together divided by the whole observation space : # in this view, the total times A and B occured together is 247 player = 'O' df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] 253 # the total observation space is 1000 (1000 games) df . shape [ 0 ] 1000 And so we get: $P(B|A) * P(A) = \\frac{247} {1000} = 0.247 $ In code: df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] / df . shape [ 0 ] 0.253","title":"1.2 Inferential Analysis"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#121-behavioral-analysis-of-the-winner","text":"","title":"1.2.1 Behavioral Analysis of the Winner"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q4","text":"define the 3 different board piece types and label them middle , side , and corner . Middle should be an int and the other two should be lists. # define the 3 different board piece types middle = 5 side = [ 2 , 4 , 6 , 8 ] corner = [ 1 , 3 , 7 , 9 ]","title":"Q4"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#1211-what-is-the-probability-of-winning-after-playing-the-middle-piece","text":"","title":"1.2.1.1 What is the probability of winning after playing the middle piece?"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q5","text":"# A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'X' # define the intersection of A AND B, A_B A_B = df . loc [( df [ 'Winner' ] == player ) & ( df [ middle ] == player )] . shape [ 0 ] / \\ df . shape [ 0 ] # define prob B B = ( df [ middle ] == player ) . sum () / \\ df . shape [ 0 ] # return A_B over B (The prob B given A) A_B / B 0.5807962529274004","title":"Q5"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q6","text":"# A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'O' # define the intersection of A AND B, A_B A_B = df . loc [( df [ 'Winner' ] == player ) & ( df [ middle ] == player )] . shape [ 0 ] / \\ df . shape [ 0 ] # define prob B B = ( df [ middle ] == player ) . sum () / \\ df . shape [ 0 ] # return A_B over B (The prob B given A) A_B / B 0.5622222222222222","title":"Q6"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#1212-what-is-the-probability-of-winning-after-playing-a-side-piece","text":"","title":"1.2.1.2 What is the probability of winning after playing a side piece?"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q7","text":"# A intersect B: O played side and O won / tot games # B: O played side / tot games player = 'O' A_B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.4252136752136752 # A intersect B: X played side and X won / tot games # B: X played side / tot games player = 'X' A_B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.41458106637649617","title":"Q7"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#1213-what-is-the-probability-of-winning-after-playing-a-corner-piece","text":"","title":"1.2.1.3 What is the probability of winning after playing a corner piece?"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q8","text":"# A intersect B: O played corner and O won / tot games # B: O played corner / tot games player = 'O' A_B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.44235033259423506","title":"Q8"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q9","text":"# A intersect B: X played corner and X won / tot games # B: X played corner / tot games player = 'X' A_B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.4375678610206298 Are these results surprising to you? Why? This resource may be illustrative.","title":"Q9"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#13-improving-the-analysis","text":"In this analysis, we only tracked what moves were made, not the order they were made in. It really limited our assessment! How might we change our recording of the games to track order of moves as well? Do we need to track all the moves or just the first and the winner?","title":"1.3 Improving the Analysis"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/","text":"Data Science Foundations Project Part 2: Heuristical Agents \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com We makin' some wack AI today 2.0 Preparing Environment and Importing Data \u00b6 back to top 2.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 2.0.2 Load Dataset \u00b6 back to top 2.1 AI Heuristics \u00b6 Develop a better AI based on your analyses of game play so far. Q1 \u00b6 We need to decide what our heuristic strategy will be and orient around the tools/methods we have available to deliver that strategy. # we will define some variables to help us define the types of positions middle = 5 side = [ 2 , 4 , 6 , 8 ] corner = [ 1 , 3 , 7 , 9 ] # recall that our board is a dictionary tictactoe = TicTacToe () tictactoe . board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} # and we have a win_patterns object to help us with the algorithm tictactoe . win_patterns [[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 4, 7], [2, 5, 8], [3, 6, 9], [1, 5, 9], [7, 5, 3]] for example, if we want to check if the middle piece is available, and play it if it is. How do we do that? # set some key variables player = 'X' opponent = 'O' avail_moves = [ i for i in tictactoe . board . keys () if tictactoe . board [ i ] == ' ' ] # a variable that will keep track if we've found a move we like or not move_found = False # <- some other moves we might want to make would go here -> # # and now for our middle piece play if move_found == False : # if no other move has been found yet if middle in avail_moves : # if middle is available move_found = True # then change our move_found status move = middle # update our move Note: in the following when I say return a move I mean when we wrap this up in a function we will want the return to be for a move. For now I just mean that the result of your code in Q3 is to take the variable name move and set it equal to the tic-tac-toe board piece the AI will play Our standard approach will be to always return a move by the agent . Whether the agent is heruistical or from some other ML framework we always want to return a move Q2 \u00b6 Write down your algorithm steps in markdown. i.e. play winning move block opponent's winning move play corner play center play edge Q3 \u00b6 Begin to codify your algorithm from Q3. Make sure that no matter what, you return a move # some starting variables for you self = TicTacToe () # this is useful cheat for when we actually put this in as a method player_label = 'X' opponent = 'O' avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] # temp board will allow us to play hypothetical moves and see where they get us # in case you need it temp_board = self . board . copy () # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True 2.2 Wrapping our Agent \u00b6 Now that we've created a conditional tree for our AI to make a decision, we need to integrate this within the gaming framework we've made so far. How should we do this? Let's define this thought pattern or tree as an agent. Recall our play_game function within GameEngine def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break ######################################################################## ##################### WE WANT TO CHANGE THESE LINES #################### ######################################################################## else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 2.2.1 Redefining the Random Agent \u00b6 In particular, we want to change lines 30-37 to take our gaming agent in as a parameter to make decisions. Let's try this. In setup_game we want to have the option to set the AI type/level. In play_game we want to make a call to that AI to make the move. For instance, our random AI will go from: while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break to: def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ########## our fresh off the assembly line tictactoe playing robot ########### ############################################################################## def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : pass self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Let's test that our random ai works now in this format random . seed ( 12 ) game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| <__main__.GameEngine at 0x7eff0f31b040> Let's try it with a user player: random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2) 1 select AI level (1, 2) 1 who will go first? (X, (AI), or O (Player)) X | | | | | | | | | | | | | | | | | | | | | |X| | O, what's your move? 1 |O| | | | | | | | |X| | |O| | | | |X| | | |X| | O, what's your move? 3 |O| |O| | |X| | | |X| | |O| |O| | |X| | | |X|X| O, what's your move? 2 'O' Won! |O|O|O| | |X| | | |X|X| <__main__.GameEngine at 0x7eff0f2e36a0> Q4 \u00b6 Now let's fold in our specialized AI agent. Add your code under the heurstic_ai function. Note that the player_label is passed as an input parameter now class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ################### YOUR BADASS HEURISTIC AGENT GOES HERE #################### ############################################################################## def heuristic_ai ( self , player_label ): # SOME HELPER VARIABLES IF YOU NEED THEM opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () ################## YOUR CODE GOES HERE, RETURN THAT MOVE! ################## # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True ############################################################################ return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Q5 \u00b6 And we'll test that it works! random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2) 1 select AI level (1, 2) 2 who will go first? (X, (AI), or O (Player)) X | | | | | | | | | | | | | | | | | | | | | | |X| O, what's your move? 3 | | |O| | | | | | | |X| | | |O| | | | | |X| |X| O, what's your move? 2 | |O|O| | | | | |X| |X| 'X' Won! | |O|O| | | | | |X|X|X| <__main__.GameEngine at 0x7eff4d3304f0> Q6 \u00b6 Test the autorun feature! game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X|O| | |O| |O| | |X|X| |X|O| | |O| |O| |O|X|X| |X|O| | |O| |O| |O|X|X| |X|O|X| 'O' Won! |O|O|O| |O|X|X| |X|O|X| <__main__.GameEngine at 0x7eff0f2bbdc0>","title":"SOLN P2 Heuristical TicTacToe Agents"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#data-science-foundations-project-part-2-heuristical-agents","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com We makin' some wack AI today","title":"Data Science Foundations  Project Part 2: Heuristical Agents"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#20-preparing-environment-and-importing-data","text":"back to top","title":"2.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#201-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"2.0.1 Import Packages"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#202-load-dataset","text":"back to top","title":"2.0.2 Load Dataset"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#21-ai-heuristics","text":"Develop a better AI based on your analyses of game play so far.","title":"2.1 AI Heuristics"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#q1","text":"We need to decide what our heuristic strategy will be and orient around the tools/methods we have available to deliver that strategy. # we will define some variables to help us define the types of positions middle = 5 side = [ 2 , 4 , 6 , 8 ] corner = [ 1 , 3 , 7 , 9 ] # recall that our board is a dictionary tictactoe = TicTacToe () tictactoe . board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} # and we have a win_patterns object to help us with the algorithm tictactoe . win_patterns [[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 4, 7], [2, 5, 8], [3, 6, 9], [1, 5, 9], [7, 5, 3]] for example, if we want to check if the middle piece is available, and play it if it is. How do we do that? # set some key variables player = 'X' opponent = 'O' avail_moves = [ i for i in tictactoe . board . keys () if tictactoe . board [ i ] == ' ' ] # a variable that will keep track if we've found a move we like or not move_found = False # <- some other moves we might want to make would go here -> # # and now for our middle piece play if move_found == False : # if no other move has been found yet if middle in avail_moves : # if middle is available move_found = True # then change our move_found status move = middle # update our move Note: in the following when I say return a move I mean when we wrap this up in a function we will want the return to be for a move. For now I just mean that the result of your code in Q3 is to take the variable name move and set it equal to the tic-tac-toe board piece the AI will play Our standard approach will be to always return a move by the agent . Whether the agent is heruistical or from some other ML framework we always want to return a move","title":"Q1"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#q2","text":"Write down your algorithm steps in markdown. i.e. play winning move block opponent's winning move play corner play center play edge","title":"Q2"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#q3","text":"Begin to codify your algorithm from Q3. Make sure that no matter what, you return a move # some starting variables for you self = TicTacToe () # this is useful cheat for when we actually put this in as a method player_label = 'X' opponent = 'O' avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] # temp board will allow us to play hypothetical moves and see where they get us # in case you need it temp_board = self . board . copy () # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True","title":"Q3"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#22-wrapping-our-agent","text":"Now that we've created a conditional tree for our AI to make a decision, we need to integrate this within the gaming framework we've made so far. How should we do this? Let's define this thought pattern or tree as an agent. Recall our play_game function within GameEngine def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break ######################################################################## ##################### WE WANT TO CHANGE THESE LINES #################### ######################################################################## else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"2.2 Wrapping our Agent"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#221-redefining-the-random-agent","text":"In particular, we want to change lines 30-37 to take our gaming agent in as a parameter to make decisions. Let's try this. In setup_game we want to have the option to set the AI type/level. In play_game we want to make a call to that AI to make the move. For instance, our random AI will go from: while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break to: def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ########## our fresh off the assembly line tictactoe playing robot ########### ############################################################################## def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : pass self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Let's test that our random ai works now in this format random . seed ( 12 ) game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| <__main__.GameEngine at 0x7eff0f31b040> Let's try it with a user player: random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2) 1 select AI level (1, 2) 1 who will go first? (X, (AI), or O (Player)) X | | | | | | | | | | | | | | | | | | | | | |X| | O, what's your move? 1 |O| | | | | | | | |X| | |O| | | | |X| | | |X| | O, what's your move? 3 |O| |O| | |X| | | |X| | |O| |O| | |X| | | |X|X| O, what's your move? 2 'O' Won! |O|O|O| | |X| | | |X|X| <__main__.GameEngine at 0x7eff0f2e36a0>","title":"2.2.1 Redefining the Random Agent"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#q4","text":"Now let's fold in our specialized AI agent. Add your code under the heurstic_ai function. Note that the player_label is passed as an input parameter now class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ################### YOUR BADASS HEURISTIC AGENT GOES HERE #################### ############################################################################## def heuristic_ai ( self , player_label ): # SOME HELPER VARIABLES IF YOU NEED THEM opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () ################## YOUR CODE GOES HERE, RETURN THAT MOVE! ################## # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True ############################################################################ return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"Q4"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#q5","text":"And we'll test that it works! random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2) 1 select AI level (1, 2) 2 who will go first? (X, (AI), or O (Player)) X | | | | | | | | | | | | | | | | | | | | | | |X| O, what's your move? 3 | | |O| | | | | | | |X| | | |O| | | | | |X| |X| O, what's your move? 2 | |O|O| | | | | |X| |X| 'X' Won! | |O|O| | | | | |X|X|X| <__main__.GameEngine at 0x7eff4d3304f0>","title":"Q5"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#q6","text":"Test the autorun feature! game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X|O| | |O| |O| | |X|X| |X|O| | |O| |O| |O|X|X| |X|O| | |O| |O| |O|X|X| |X|O|X| 'O' Won! |O|O|O| |O|X|X| |X|O|X| <__main__.GameEngine at 0x7eff0f2bbdc0>","title":"Q6"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/","text":"Data Science Foundations Project Part 3: 1-Step Look Ahead \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we're working on a more advanced AI structure: 1-step lookahead. 3.0 Preparing Environment and Importing Data \u00b6 back to top 3.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] the_final_move = None # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True the_final_move = move break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## self . ai_level = 2 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 3.0.2 Load Dataset \u00b6 back to top 3.1 Rethinking gameplay \u00b6 To implement the broader strategies used in game theory and machine learning, we need to rebroadcast our approach to creating our AI agent. In the heurstical agent model, we thought in terms of checking for specific move types, defined by what kind of advantage they give us during game play, i.e. see if a winning move is available, a blocking move, if a corner place is free, etc. Rather than thinking with this look and check mindset that is centered around specific strategies and our own prior knowledge about the game (we know that a center piece is statistically likely to give us a higher chance of winning) we will evaluate every available move to the AI, and rate them quantitatively. switching from ordinal to interval Notice the datatype change when we move from giving simple preferences of moves to actual scores of moves. Catalog this in your mind for future reference when considering datatypes! 3.1.1 One-Step Look Ahead \u00b6 For now, when we rate our boards, we will only look 1-step ahead in gameplay. Hence the name we give this AI strategy, 1-step lookahead The beginning portion of our code will look about the same as the heuristic AI model. Recall: def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() but now, instead of searching progressively through our preferred move-types (winning, middle, etc.) . We are going to give every available move (1, 3, 7, etc.) a score. Our score regimen will look like the following: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move Q1 Rewrite avail_moves \u00b6 define avail_moves as a dictionary of available moves with scores for each move as empty strings. We will update this dictionary with numerical scores in the next step # we're going to steal the parameter names to # prototype our new function self = TicTacToe () player_label = 'X' opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] # instead of a list, we want avail_moves to now be a dictionary that will # contain the move and its score avail_moves = { i : ' ' for i in self . board if self . board [ i ] == ' ' } temp_board = self . board . copy () avail_moves {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} Q2 Score each move in avail_moves \u00b6 Now let's fold this into our new one_step_ai function. Remember: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## avail_moves = { i : ' ' for i in self . board if self . board [ i ] == ' ' } for move in avail_moves . keys (): avail_moves [ move ] = 1 temp_board = self . board . copy () # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score avail_moves [ move ] = 100 temp_board [ move ] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## for move in avail_moves . keys (): temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: avail_moves [ move ] = 10 temp_board [ move ] = ' ' ############################################################################## ################### All remaining moves receive a score of 1 ############################################################################## return avail_moves Q3 Test one_step_ai \u00b6 That's great, but how do we check that our code will work when a winning move is available, or a losing move is just around the corner? let's create a unit test for these! # just defining a new game self = TicTacToe () player_label = 'X' # seeding the board with some X's self . board [ 1 ] = 'X' self . board [ 2 ] = 'X' self . board {1: 'X', 2: 'X', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} Now test the winning move. Your code should return 100 at move 3 and 1 everywhere else one_step_ai ( self , player_label ) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} We can test the losing move by reversing the players player_label = 'O' one_step_ai ( self , player_label ) {3: 10, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} great! Let's keep these shorthand codes in mind when we go to write actual unit tests with the one_step_ai function embedded in the GameEngine module. We're not done yet, recall that our other ai agents returned the actual selected move, not a dictionary of the moves with scores. We need to create a move from this dictionary and return it. Here's what the general procedure will look like: Grab the maximum score (after assigning scores to all of avail_moves) Select all moves that have this maximum score Return a random selection of the moves with the max score and then in code format: avail_moves = one_step_ai ( self , player_label ) # 1. grab the maximum score max_score = max ( avail_moves . values ()) # 2. select all moves that have this maximum score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # 3. return a random selection of the moves with the max score move = random . choice ( valid ) move 3 3.2 Putting it all together \u00b6 Q4 Finish one_step_ai to return a move \u00b6 Let's see if we can rewrite our game engine to take new AI models in as a passable parameter. This way our base module will be much cleaner, and allow us to continue to write new functions for the base engine as long as they pass along the same variables. # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( board , win_patterns , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] temp_board = board . copy () # define avail_moves ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## avail_moves = { i : ' ' for i in board if board [ i ] == ' ' } for move in avail_moves . keys (): avail_moves [ move ] = 1 temp_board = board . copy () # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score avail_moves [ move ] = 100 temp_board [ move ] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## for move in avail_moves . keys (): temp_board [ move ] = opponent for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: avail_moves [ move ] = 10 temp_board [ move ] = ' ' # 1. grab the maximum score max_score = max ( avail_moves . values ()) # 2. select all moves that have this maximum score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # 3. return a random selection of the moves with the max score move = random . choice ( valid ) return move 3.2.1 Allow GameEngine to take an ai agent as a passable parameter \u00b6 Let's rewrite our GameEngine to take an ai agent as a passable parameter under user_ai . The default value will be None Additional user_ai criteria will be that user_ai receives board , win_patterns and player_label and returns move . class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' , user_ai = None ): super () . __init__ () self . setup = setup self . user_ai = user_ai def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self . user_ai == None : level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) else : self . ai_level = 3 if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## if self . user_ai == None : self . ai_level = 2 else : self . ai_level = 3 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) ########## # Our user-defined AI agent ########## elif self . ai_level == 3 : move = self . user_ai ( self . board , self . win_patterns , player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Test the auto and user functions game = GameEngine ( setup = 'user' , user_ai = one_step_ai ) game . setup_game () How many Players? (type 0, 1, or 2) 1 who will go first? (X, (AI), or O (Player)) X game . play_game () | | | | | | | | | | | | | | | | |X| | | | | | | O, what's your move? 3 | | |O| |X| | | | | | | | |X|O| |X| | | | | | | O, what's your move? 6 | |X|O| |X| |O| | | | | | |X|O| |X| |O| | | |X| O, what's your move? 1 |O|X|O| |X| |O| | | |X| |O|X|O| |X| |O| | |X|X| O, what's your move? 5 |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| <__main__.GameEngine at 0x7f4614708850> 3.3 Write Unit Tests for the New Code \u00b6 There are many tests we could write here def test_user_ai (): random . seed ( 42 ) game = GameEngine ( setup = 'auto' , user_ai = one_step_ai ) game . setup_game () outcome = game . play_game () assert outcome . winner == 'X' , 'X should have won!' test_user_ai () | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| | |X|O| |O| | | 'X' Won! |X|O|X| | |X|O| |O| |X|","title":"SOLN P3 1 Step Look Ahead Agents"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#data-science-foundations-project-part-3-1-step-look-ahead","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we're working on a more advanced AI structure: 1-step lookahead.","title":"Data Science Foundations  Project Part 3: 1-Step Look Ahead"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#30-preparing-environment-and-importing-data","text":"back to top","title":"3.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#301-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] the_final_move = None # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True the_final_move = move break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## self . ai_level = 2 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"3.0.1 Import Packages"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#302-load-dataset","text":"back to top","title":"3.0.2 Load Dataset"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#31-rethinking-gameplay","text":"To implement the broader strategies used in game theory and machine learning, we need to rebroadcast our approach to creating our AI agent. In the heurstical agent model, we thought in terms of checking for specific move types, defined by what kind of advantage they give us during game play, i.e. see if a winning move is available, a blocking move, if a corner place is free, etc. Rather than thinking with this look and check mindset that is centered around specific strategies and our own prior knowledge about the game (we know that a center piece is statistically likely to give us a higher chance of winning) we will evaluate every available move to the AI, and rate them quantitatively. switching from ordinal to interval Notice the datatype change when we move from giving simple preferences of moves to actual scores of moves. Catalog this in your mind for future reference when considering datatypes!","title":"3.1 Rethinking gameplay"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#311-one-step-look-ahead","text":"For now, when we rate our boards, we will only look 1-step ahead in gameplay. Hence the name we give this AI strategy, 1-step lookahead The beginning portion of our code will look about the same as the heuristic AI model. Recall: def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() but now, instead of searching progressively through our preferred move-types (winning, middle, etc.) . We are going to give every available move (1, 3, 7, etc.) a score. Our score regimen will look like the following: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move","title":"3.1.1 One-Step Look Ahead"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#q1-rewrite-avail_moves","text":"define avail_moves as a dictionary of available moves with scores for each move as empty strings. We will update this dictionary with numerical scores in the next step # we're going to steal the parameter names to # prototype our new function self = TicTacToe () player_label = 'X' opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] # instead of a list, we want avail_moves to now be a dictionary that will # contain the move and its score avail_moves = { i : ' ' for i in self . board if self . board [ i ] == ' ' } temp_board = self . board . copy () avail_moves {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '}","title":"Q1 Rewrite avail_moves"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#q2-score-each-move-in-avail_moves","text":"Now let's fold this into our new one_step_ai function. Remember: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## avail_moves = { i : ' ' for i in self . board if self . board [ i ] == ' ' } for move in avail_moves . keys (): avail_moves [ move ] = 1 temp_board = self . board . copy () # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score avail_moves [ move ] = 100 temp_board [ move ] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## for move in avail_moves . keys (): temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: avail_moves [ move ] = 10 temp_board [ move ] = ' ' ############################################################################## ################### All remaining moves receive a score of 1 ############################################################################## return avail_moves","title":"Q2 Score each move in avail_moves"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#q3-test-one_step_ai","text":"That's great, but how do we check that our code will work when a winning move is available, or a losing move is just around the corner? let's create a unit test for these! # just defining a new game self = TicTacToe () player_label = 'X' # seeding the board with some X's self . board [ 1 ] = 'X' self . board [ 2 ] = 'X' self . board {1: 'X', 2: 'X', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} Now test the winning move. Your code should return 100 at move 3 and 1 everywhere else one_step_ai ( self , player_label ) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} We can test the losing move by reversing the players player_label = 'O' one_step_ai ( self , player_label ) {3: 10, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} great! Let's keep these shorthand codes in mind when we go to write actual unit tests with the one_step_ai function embedded in the GameEngine module. We're not done yet, recall that our other ai agents returned the actual selected move, not a dictionary of the moves with scores. We need to create a move from this dictionary and return it. Here's what the general procedure will look like: Grab the maximum score (after assigning scores to all of avail_moves) Select all moves that have this maximum score Return a random selection of the moves with the max score and then in code format: avail_moves = one_step_ai ( self , player_label ) # 1. grab the maximum score max_score = max ( avail_moves . values ()) # 2. select all moves that have this maximum score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # 3. return a random selection of the moves with the max score move = random . choice ( valid ) move 3","title":"Q3 Test one_step_ai"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#32-putting-it-all-together","text":"","title":"3.2 Putting it all together"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#q4-finish-one_step_ai-to-return-a-move","text":"Let's see if we can rewrite our game engine to take new AI models in as a passable parameter. This way our base module will be much cleaner, and allow us to continue to write new functions for the base engine as long as they pass along the same variables. # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( board , win_patterns , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] temp_board = board . copy () # define avail_moves ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## avail_moves = { i : ' ' for i in board if board [ i ] == ' ' } for move in avail_moves . keys (): avail_moves [ move ] = 1 temp_board = board . copy () # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score avail_moves [ move ] = 100 temp_board [ move ] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## for move in avail_moves . keys (): temp_board [ move ] = opponent for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: avail_moves [ move ] = 10 temp_board [ move ] = ' ' # 1. grab the maximum score max_score = max ( avail_moves . values ()) # 2. select all moves that have this maximum score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # 3. return a random selection of the moves with the max score move = random . choice ( valid ) return move","title":"Q4 Finish one_step_ai to return a move"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#321-allow-gameengine-to-take-an-ai-agent-as-a-passable-parameter","text":"Let's rewrite our GameEngine to take an ai agent as a passable parameter under user_ai . The default value will be None Additional user_ai criteria will be that user_ai receives board , win_patterns and player_label and returns move . class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' , user_ai = None ): super () . __init__ () self . setup = setup self . user_ai = user_ai def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self . user_ai == None : level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) else : self . ai_level = 3 if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## if self . user_ai == None : self . ai_level = 2 else : self . ai_level = 3 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) ########## # Our user-defined AI agent ########## elif self . ai_level == 3 : move = self . user_ai ( self . board , self . win_patterns , player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Test the auto and user functions game = GameEngine ( setup = 'user' , user_ai = one_step_ai ) game . setup_game () How many Players? (type 0, 1, or 2) 1 who will go first? (X, (AI), or O (Player)) X game . play_game () | | | | | | | | | | | | | | | | |X| | | | | | | O, what's your move? 3 | | |O| |X| | | | | | | | |X|O| |X| | | | | | | O, what's your move? 6 | |X|O| |X| |O| | | | | | |X|O| |X| |O| | | |X| O, what's your move? 1 |O|X|O| |X| |O| | | |X| |O|X|O| |X| |O| | |X|X| O, what's your move? 5 |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| <__main__.GameEngine at 0x7f4614708850>","title":"3.2.1 Allow GameEngine to take an ai agent as a passable parameter"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#33-write-unit-tests-for-the-new-code","text":"There are many tests we could write here def test_user_ai (): random . seed ( 42 ) game = GameEngine ( setup = 'auto' , user_ai = one_step_ai ) game . setup_game () outcome = game . play_game () assert outcome . winner == 'X' , 'X should have won!' test_user_ai () | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| | |X|O| |O| | | 'X' Won! |X|O|X| | |X|O| |O| |X|","title":"3.3 Write Unit Tests for the New Code"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/","text":"Data Science Foundations Session 1: Regression and Analysis \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at fitting data to a curve using regression . We will also look at using regression to make predictions for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error. 1.0 Preparing Environment and Importing Data \u00b6 back to top 1.0.1 Import Packages \u00b6 back to top # Import pandas, pyplot, ipywidgets import pandas as pd from matplotlib import pyplot as plt from ipywidgets import interact # Import Scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score # for enrichment topics import seaborn as sns import numpy as np 1.0.2 Load Dataset \u00b6 back to top For our discussion on regression and descriptive statistics today we will use a well known dataset of different wines and their quality ratings df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) df . shape (6497, 13) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 1.1 What is regression? \u00b6 It is the process of finding a relationship between dependent and independent variables to find trends in data. This abstract definition means that you have one variable (the dependent variable) which depends on one or more variables (the independent variables). One of the reasons for which we want to regress data is to understand whether there is a trend between two variables. Housing Prices Example We can imagine this scenario with housing prices. Envision a mixed dataset of continuous and discrete independent variables. Some features could be continuous, floating point values like location ranking and housing condition. Others could be descrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a regression model. 1.2 Linear regression fitting with scikit-learn \u00b6 \ud83c\udfcb\ufe0f Exercise 1: rudimentary EDA \u00b6 What does the data look like? Recall how to visualize data in a pandas dataframe for every column calculate the * skew: `df.skew()` * kurtosis: `df.kurtosis()` * pearsons correlation with the dependent variable: `df.corr()` * number of missing entries `df.isnull()` and organize this into a new dataframe note: pearsons is just one type of correlation, another available to us spearman which differs from pearsons in that it depends on ranked values rather than their direct quantities, you can read more here df . isnull () . sum () type 0 fixed acidity 10 volatile acidity 8 citric acid 3 residual sugar 2 chlorides 2 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 9 sulphates 4 alcohol 0 quality 0 dtype: int64 # Cell for Exercise 1 # part A # using df.<method> define the following four variables with the results from # skew(), kurtosis(), corr() (while selecting for quality), and isnull() # for isnull() you'll notice the return is a dataframe of booleans. we would # like to simply know the number of null values for each column. change the # return of isnull() using the sum() method along the columns skew = df . skew () kurt = df . kurtosis () pear = df . corr ()[ 'quality' ] null = df . isnull () . sum ( axis = 0 ) # part B # on line 13, put these results in a list using square brackets and call # pd.DataFrame on the list to make your new DataFrame! store it under the # variable name dff dff = pd . DataFrame ([ skew , kurt , pear , null ]) # part C # take the transpose of this DataFrame using dff.T. reassign dff to this copy dff = dff . T # part D # set the column names to 'skew', 'kurtosis', 'pearsons _quality', and # 'null count' using dff.columns dff . columns = [ 'skew' , 'kurtosis' , 'pearsons _quality' , 'null count' ] # Now return dff to the output to view your hand work dff # uncomment this line /tmp/ipykernel_1422/4028752270.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. skew = df.skew() /tmp/ipykernel_1422/4028752270.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. kurt = df.kurtosis() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count fixed acidity 1.722805 5.057727 -0.077031 10.0 volatile acidity 1.495512 2.827081 -0.265953 8.0 citric acid 0.473032 2.401582 0.085706 3.0 residual sugar 1.435000 4.358134 -0.036825 2.0 chlorides 5.399849 50.894874 -0.200886 2.0 free sulfur dioxide 1.220066 7.906238 0.055463 0.0 total sulfur dioxide -0.001177 -0.371664 -0.041385 0.0 density 0.503602 6.606067 -0.305858 0.0 pH 0.386966 0.370068 0.019366 9.0 sulphates 1.798467 8.659892 0.038729 4.0 alcohol 0.565718 -0.531687 0.444319 0.0 quality 0.189623 0.232322 1.000000 0.0 type NaN NaN NaN 0.0 I have gone ahead and repeated this exercise with the red vs white wine types: red = df . loc [ df [ 'type' ] == 'red' ] wht = df . loc [ df [ 'type' ] == 'white' ] def get_summary ( df ): skew = df . skew () kurt = df . kurtosis () pear = df . corr ()[ 'quality' ] null = df . isnull () . sum () med = df . median () men = df . mean () dff = pd . DataFrame ([ skew , kurt , pear , null , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'pearsons _quality' , 'null count' , 'median' , 'mean' ] return dff dffr = get_summary ( red ) dffw = get_summary ( wht ) desc = pd . concat ([ dffr , dffw ], keys = [ 'red' , 'white' ]) /tmp/ipykernel_1422/2387423026.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. skew = df.skew() /tmp/ipykernel_1422/2387423026.py:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. kurt = df.kurtosis() /tmp/ipykernel_1422/2387423026.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. med = df.median() /tmp/ipykernel_1422/2387423026.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. men = df.mean() desc .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count median mean red fixed acidity 0.982192 1.132624 0.123834 2.0 7.90000 8.322104 volatile acidity 0.672862 1.226846 -0.390858 1.0 0.52000 0.527738 citric acid 0.317891 -0.788476 0.226917 1.0 0.26000 0.271145 residual sugar 4.540655 28.617595 0.013732 0.0 2.20000 2.538806 chlorides 5.680347 41.715787 -0.128907 0.0 0.07900 0.087467 free sulfur dioxide 1.250567 2.023562 -0.050656 0.0 14.00000 15.874922 total sulfur dioxide 1.515531 3.809824 -0.185100 0.0 38.00000 46.467792 density 0.071288 0.934079 -0.174919 0.0 0.99675 0.996747 pH 0.194803 0.814690 -0.057094 2.0 3.31000 3.310864 sulphates 2.429115 11.712632 0.251685 2.0 0.62000 0.658078 alcohol 0.860829 0.200029 0.476166 0.0 10.20000 10.422983 quality 0.217802 0.296708 1.000000 0.0 6.00000 5.636023 type NaN NaN NaN 0.0 NaN NaN white fixed acidity 0.647981 2.176560 -0.114032 8.0 6.80000 6.855532 volatile acidity 1.578595 5.095526 -0.194976 7.0 0.26000 0.278252 citric acid 1.284217 6.182036 -0.009194 2.0 0.32000 0.334250 residual sugar 1.076601 3.469536 -0.097373 2.0 5.20000 6.393250 chlorides 5.023412 37.560847 -0.210181 2.0 0.04300 0.045778 free sulfur dioxide 1.406745 11.466342 0.008158 0.0 34.00000 35.308085 total sulfur dioxide 0.390710 0.571853 -0.174737 0.0 134.00000 138.360657 density 0.977773 9.793807 -0.307123 0.0 0.99374 0.994027 pH 0.458402 0.532552 0.098858 7.0 3.18000 3.188203 sulphates 0.977361 1.589847 0.053690 2.0 0.47000 0.489835 alcohol 0.487342 -0.698425 0.435575 0.0 10.40000 10.514267 quality 0.155796 0.216526 1.000000 0.0 6.00000 5.877909 type NaN NaN NaN 0.0 NaN NaN def my_fig ( metric = desc . columns ): fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) pd . DataFrame ( desc [ metric ]) . unstack ()[ metric ] . T . plot ( kind = 'barh' , ax = ax ) interact ( my_fig ) interactive(children=(Dropdown(description='metric', options=('skew', 'kurtosis', 'pearsons _quality', 'null c\u2026 <function __main__.my_fig(metric=Index(['skew', 'kurtosis', 'pearsons _quality', 'null count', 'median', 'mean'], dtype='object'))> \ud83d\ude4b Question 1: Discussion Around EDA Plot \u00b6 What do we think of this plot? metric = mean , the cholrides values metric = kurtosis , residual sugar metric = pearsons _quality , magnitudes and directions How to improve the plot, what other plots would we like to see? For instance, what if we were really curious about the high kurtosis for chlorides content? What more would we like to glean about the distribution of chloride content? # we can use df.describe() to take a look at the quantile values and min/max df [ 'chlorides' ] . describe () count 6495.000000 mean 0.056042 std 0.035036 min 0.009000 25% 0.038000 50% 0.047000 75% 0.065000 max 0.611000 Name: chlorides, dtype: float64 # and see how these values appear in a KDE fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) df [ 'chlorides' ] . plot ( kind = 'kde' , ax = ax ) ax . set_xlim ( 0 , .61 ) (0.0, 0.61) # lastly we may want to look at the raw values themselves. We can sort them # too view outliers df [ 'chlorides' ] . sort_values ( ascending = False )[: 50 ] 5156 0.611 5049 0.610 5004 0.467 4979 0.464 5590 0.422 6268 0.415 6270 0.415 5652 0.415 6217 0.414 5949 0.414 5349 0.413 6158 0.403 4981 0.401 5628 0.387 6063 0.369 4915 0.368 5067 0.360 5179 0.358 484 0.346 5189 0.343 4917 0.341 5124 0.337 4940 0.332 1217 0.301 687 0.290 4473 0.271 5079 0.270 6272 0.267 5138 0.263 1865 0.255 5466 0.250 1034 0.244 5674 0.243 5675 0.241 683 0.240 1638 0.239 5045 0.236 6456 0.235 6468 0.230 5465 0.226 5464 0.226 5564 0.222 2186 0.217 5996 0.216 6333 0.214 5206 0.214 6332 0.214 5205 0.213 4497 0.212 1835 0.211 Name: chlorides, dtype: float64 1.2.2 Visualizing the data set - motivating regression analysis \u00b6 In order to demonstrate simple linear regression with this dataset we will look at two particular features: fixed acidity and density . We can create a scatter plot of fixed acidity vs density for the red wine in the dataset using df.plot() and see that there appears to be a general trend between the two features: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) df . loc [ df [ 'type' ] == 'red' ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> Now the question is: How do we quantify this trend? 1.2.3 Estimating the regression coefficients \u00b6 It looks like density increases with fixed acidity following a line, maybe something like y(x)= m \\cdot x + b \\;\\;\\;\\;\\;\\;\\;\\; \\sf{eq. 1} with \\( y=\\sf density \\), \\(x=\\sf fixed \\space acidity\\), and \\(m\\) the slope and \\(b\\) the intercept. To solve the problem, we need to find the values of \\(b\\) and \\(m\\) in equation 1 to best fit the data. This is called linear regression . In linear regression our goal is to minimize the error between computed values of positions \\(y^{\\sf calc}(x_i)\\equiv y^{\\sf calc}_i\\) and known values \\(y^{\\sf exact}(x_i)\\equiv y^{\\sf exact}_i\\), i.e. find \\(b\\) and \\(m\\) which lead to lowest value of \\epsilon (m,b) =SS_{\\sf res}=\\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - m\\cdot x_i - b \\right)^2\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sf{eq. 2} Otherwise known as the residual sum of squares To find out more see e.g. https://en.wikipedia.org/wiki/Simple_linear_regression \ud83d\ude4b Question 2: linear regression loss function \u00b6 Do we always want m and b to be large positive numbers so as to minimize eq. 2? Luckily scikit-learn contains many functions related to regression including linear regression . The function we will use is called LinearRegression() . # Create linear regression object model = linear_model.LinearRegression() # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red['density'].values.reshape(-1, 1) y = red['fixed acidity'].values.reshape(-1, 1) # Create linear regression object model = linear_model . LinearRegression () # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red [ 'density' ] . values . reshape ( - 1 , 1 ) y = red [ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) print(red['density'].values.shape, red['fixed acidity'].values.shape) print(x.shape, y.shape) print ( red [ 'density' ] . values . shape , red [ 'fixed acidity' ] . values . shape ) print ( x . shape , y . shape ) (1599,) (1599,) (1599, 1) (1599, 1) # Fit to the data model.fit(x, y) # Extract the values of interest m = model.coef_[0][0] b = model.intercept_[0] # Print the slope m and intercept b print('Scikit learn - Slope: ', m , 'Intercept: ', b ) What happens when we try to fit the data as is? # Fit to the data # model.fit(x, y) \ud83c\udfcb\ufe0f Exercise 2: drop Null Values (and practice pandas operations) \u00b6 Let's look back at our dataset description dataframe above, what do we notice, what contains null values? There are several strategies for dealing with null values. For now let's take the simplest case, and drop rows in our dataframe that contain null # Cell for Exercise 2 # For this templated exercise you are going to complete everything in one line # of code, but we are going to break it up into steps. So for each part (A, B, # etc.) paste your answer from the previous part to begin (your opertaions will # read from left to right) # step A # select the 'density' and 'fixed acidity' columns of red. make sure the return # is a dataframe df [[ 'density' , 'fixed acidity' ]] # step B # now use the dropna() method on axis 0 (the rows) to drop any null values df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 ) # step B # select column 'density' df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] # step C # select the values df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values # step D # reshape the result with an empty second dimension using .reshape() and store # the result under variable x x = df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values . reshape ( - 1 , 1 ) # repeat the same process with 'fixed acidity' and variable y y = df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) Now that we have our x and y arrays we can fit using ScikitLearn x = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values . reshape ( - 1 , 1 ) y = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) \ud83d\ude4b Question 3: why do we drop null values across both columns? \u00b6 Notice in the above cell how we selected both density and fixed acidity before calling dropna ? Why did we do that? Why didn't we just select density in the x variable case and fixed acidity in the y variable case? # Fit to the data model . fit ( x , y ) # Extract the values of interest m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # Print the slope m and intercept b print ( 'Scikit learn - Slope: ' , m , 'Intercept: ' , b ) Scikit learn - Slope: 616.01314280661 Intercept: -605.6880086750523 \ud83c\udfcb\ufe0f Exercise 3: calculating y_pred \u00b6 Estimate the values of \\(y\\) by using your fitted parameters. Hint: Use your model.coef_ and model.intercept_ parameters to estimate y_pred following equation 1 # define y_pred in terms of m, x, and b y_pred = m * x + b # uncomment the following lines! fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x , y_pred , ls = '' , marker = '*' ) ax . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f6781becca0>] We can also return predictions directly with the model object using the predict() method note: it is great to get in the habit of utilizing model outputs this way, as the API will be similar across all scikit-learn models (and sometimes models in other libraries as well!) # Another way to get this is using the model.predict function y_pred = model . predict ( x ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x , y_pred , ls = '' , marker = '*' ) ax . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f6781b5c790>] 1.3 Error and topics of model fitting (assessing model accuracy) \u00b6 1.3.1 Measuring the quality of fit \u00b6 1.3.1.1 Mean Squared Error \u00b6 The plot in Section 1.2.3 looks good, but numerically what is our error? What is the mean value of $\\epsilon$, i.e. the Mean Squared Error (MSE) ? {\\sf MSE}=\\epsilon_{\\sf ave} = \\frac{\\sum_{i=1}^{N_{\\sf times}}\\left(y^{\\sf exact}_i - m\\cdot t_i - b \\right)^2}{N_{\\sf times}}\\;\\;\\;\\;\\;\\sf eq. 3 # The mean squared error print('Mean squared error: %.2f' % mean_squared_error(y, y_pred)) # The mean squared error print ( 'Mean squared error: %.2f ' % mean_squared_error ( y , y_pred )) Mean squared error: 1.68 1.3.1.2 R-square \u00b6 Another way to measure error is the regression score, \\(R^2\\). \\(R^2\\) is generally defined as the ratio of the total sum of squares \\(SS_{\\sf tot}\\) to the residual sum of squares \\(SS_{\\sf res}\\): SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\; \\sf eq. 4 SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2\\;\\;\\;\\;\\; \\sf eq. 5 R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}} \\;\\;\\;\\;\\;\\; \\sf eq. 6 In eq. 4, \\(\\bar{y}=\\sum_i y^{\\sf exact}_i/N\\) is the average value of y for \\(N\\) points. The best value of \\(R^2\\) is 1 but it can also take a negative value if the error is large. See all the different regression metrics here . \ud83d\ude4b Question 4: lets understand \\(R^2\\) \u00b6 Do we need a large value of \\(SS_{\\sf tot}\\) to minimize \\(R^2\\) - is this something which we have the power to control? # Print the coefficient of determination - 1 is perfect prediction print('Coefficient of determination: %.2f' % r2_score(y, y_pred)) # Print the coefficient of determination - 1 is perfect prediction print ( 'Coefficient of determination: %.2f ' % r2_score ( y , y_pred )) Coefficient of determination: 0.45 1.3.2 Corollaries with classification models \u00b6 For classification tasks, we typically assess accuracy vs MSE or R-square, since we are dealing with categorical rather than numerical predictions. What is accuracy? It is defined as the ratio of True assignments to all assignments. For a binary positive/negative classification task this can be written as the following: Acc = \\frac{T_p + T_n}{F_p + F_n + T_p + T_n} Where \\(T\\) is True, \\(F\\) is false, \\(p\\) is positive, \\(n\\) is negative Just as a quick example, we can perform this type of task on our wine dataset by predicting whether a given data entry is for red or white wine: logdf = df . copy () . dropna ( axis = 0 ) y_train = logdf . pop ( 'type' ) . values . reshape ( - 1 , 1 ) x_train = logdf . dropna ( axis = 0 ) . values # train a logistic regression model on the training set from sklearn.linear_model import LogisticRegression # instantiate model logreg = LogisticRegression () # fit model logreg . fit ( x_train , y_train ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( LogisticRegression() # make class predictions for the testing set y_pred_class = logreg . predict ( x_train ) # calculate accuracy from sklearn import metrics print ( metrics . accuracy_score ( y_train , y_pred_class )) 0.9797307751818041 1.3.3 Beyond a single input feature \u00b6 ( also: quick appreciative beat for folding in domain area expertise into our models and features ) The acidity of the wine (the dependent variable v) could depend on: potassium from the soil (increases alkalinity) unripe grapes (increases acidity) grapes grown in colder climates or reduced sunshine create less sugar (increases acidity) preprocessing such as adding tartaric acid to the grape juice before fermentation (increases acidity) malolactic fermentation (reduces acidity) + others So in our lab today we will look at folding in additional variables in our dataset into the model 1.4 Multivariate regression \u00b6 Let's now turn our attention to wine quality. The value we aim to predict or evaluate is the quality of each wine in our dataset. This is our dependent variable. We will look at how this is related to the 12 other independent variables, also known as input features . We're going to do this with only the red wine data red . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 4898 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 4899 red 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5 4900 red 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5 4901 red 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6 4902 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 1.4.1 Linear regression with all input fields \u00b6 For this example, notice we have a categorical data variable in the 'type' column. We will ignore this for now, and only work with our red wines. In the future we will discuss how to deal with categorical variable such as this in a mathematical representation. # this is a list of all our features or independent variables features = list ( red . columns [ 1 :]) # we're going to remove our target or dependent variable, density from this # list features . remove ( 'density' ) # now we define X and y according to these lists of names X = red . dropna ( axis = 0 )[ features ] . values y = red . dropna ( axis = 0 )[ 'density' ] . values # we will talk about scaling/centering our data at a later time X = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) red . isnull () . sum ( axis = 0 ) # we are getting rid of some nasty nulls! type 0 fixed acidity 2 volatile acidity 1 citric acid 1 residual sugar 0 chlorides 0 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 2 sulphates 2 alcohol 0 quality 0 dtype: int64 # Create linear regression object - note that we are using all the input features model = linear_model.LinearRegression() model.fit(X, y) y_calc = model.predict(X) # Create linear regression object - note that we are using all the input features model = linear_model . LinearRegression () model . fit ( X , y ) y_calc = model . predict ( X ) Let's see what the coefficients look like ... print(\"Fit coefficients: \\n\", model.coef_, \"\\nNumber of coefficients:\", len(model.coef_)) print ( \"Fit coefficients: \\n \" , model . coef_ , \" \\n Number of coefficients:\" , len ( model . coef_ )) Fit coefficients: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] Number of coefficients: 11 We have 11 !!! That's because we are regressing respect to all 11 independent variables !!! So now, y_{\\sf calc}= m_1x_1 +\\, m_2x_2 \\,+ \\,m_3x_3 \\,+\\,... \\,+ \\,b =\\sum_{i=1}^{13}m_i x_i + b\\;\\;\\;\\;\\; \\sf eq. 7 print(\"We have 13 slopes / weights:\\n\\n\", model.coef_) print(\"\\nAnd one intercept: \", model.intercept_) print ( \"We have 11 slopes / weights: \\n\\n \" , model . coef_ ) print ( \" \\n And one intercept: \" , model . intercept_ ) We have 11 slopes / weights: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] And one intercept: 0.9967517451349656 # This size should match the number of columns in X if len(X[0]) == len(model.coef_): print(\"All good! The number of coefficients matches the number of input features.\") else: print(\"Hmm .. something strange is going on.\") # This size should match the number of columns in X if len ( X [ 0 ]) == len ( model . coef_ ): print ( \"All good! The number of coefficients matches the number of input features.\" ) else : print ( \"Hmm .. something strange is going on.\" ) All good! The number of coefficients matches the number of input features. \ud83c\udfcb\ufe0f Exercise 4: evaluate the error \u00b6 Let's evaluate the error by computing the MSE and \\(R^2\\) metrics (see eq. 3 and 6). # The mean squared error # part A # calculate the MSE using mean_squared_error() # mse = # part B # calculate the R square using r2_score() # r2 = print('Mean squared error: {:.2f}'.format(mse) print('Coefficient of determination: {:.2f}'.format(r2) # The mean squared error # part A # calculate the MSE using mean_squared_error() mse = mean_squared_error ( y , model . predict ( X )) # part B # calculate the R square using r2_score() r2 = r2_score ( y , model . predict ( X )) print ( 'Mean squared error: {:.2e} ' . format ( mse )) print ( 'Coefficient of determination: {:.2f} ' . format ( r2 )) Mean squared error: 5.62e-07 Coefficient of determination: 0.84 \ud83c\udfcb\ufe0f Exercise 5: make a plot of y actual vs y predicted \u00b6 We can also look at how well the computed values match the true values graphically by generating a scatterplot. # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style # plt.plot() plt.title(\"Linear regression - computed values on entire data set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style plt . plot ( y , model . predict ( X ), ls = '' , marker = '.' ) plt . title ( \"Linear regression - computed values on entire data set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () \ud83c\udf52 1.4.2 Enrichment : Splitting into train and test sets \u00b6 note: more of this topic is covered in Model Selection and Validation To see whether we can predict, we will carry out our regression only on a part, 80%, of the full data set. This part is called the training data. We will then test the trained model to predict the rest of the data, 20% - the test data. The function which fits won't see the test data until it has to predict it. We will motivate the use of train/test sets more explicitly in Model Selection and Validation We start by splitting out data using scikit-learn's train_test_split() function: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) ``` ```python X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) Now we check the size of y_train and y_test , the sum should be the size of y! If this works then we move on and carry out regression but we only use the training data! if len(y_test)+len(y_train) == len(y): print('All good, ready to to go and regress!\\n') # Carry out linear regression print('Running linear regression algorithm on the training set\\n') model = linear_model.LinearRegression() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_pred_test = model.predict(X_test) if len ( y_test ) + len ( y_train ) == len ( y ): print ( 'All good, ready to to go and regress! \\n ' ) # Carry out linear regression print ( 'Running linear regression algorithm on the training set \\n ' ) model = linear_model . LinearRegression () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_pred_test = model . predict ( X_test ) All good, ready to to go and regress! Running linear regression algorithm on the training set Fit coefficients and intercept: [ 1.62385613e-03 1.10578142e-04 7.75216492e-07 5.87755741e-04 7.65190323e-05 -1.03490059e-04 8.87357873e-05 7.79083342e-04 2.23534769e-04 -9.99858829e-04 5.85256438e-06] 0.9967531628434799 Now we can plot our predicted values to see how accurate we are in predicting. We will generate a scatterplot and computing the MSE and \\(R^2\\) metrics of error. sns.scatterplot(x=y_pred_test, y=y_test, color=\"mediumvioletred\", s=50) plt.title(\"Linear regression - predict test set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred_test)) sns . scatterplot ( x = y_pred_test , y = y_test , color = \"mediumvioletred\" , s = 50 ) plt . title ( \"Linear regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2e ' % mean_squared_error ( y_test , y_pred_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_pred_test )) Mean squared error: 5.45e-07 Coefficient of determination: 0.87 1.4.2.1 Other data considerations \u00b6 Do we need all the independent variables? Topics of interential statistics covered in a couple sessions Can we output integer quality scores? Topics of non-binary classification tasks covered in week 4 \ud83c\udf52 1.4.3 Enrichment : Other regression algorithms \u00b6 There are many other regression algorithms. The two we want to highlight here are Ridge, LASSO, and Elastic Net. They differ by an added term to the loss function. Let's review. Eq. 2 expanded to multivariate form yields: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 for Ridge regression, we add a regularization term known as L2 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}\\beta_{j}^2 for LASSO (Least Absolute Shrinkage and Selection Operator) we add L1 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}|\\beta_{j}| The key difference here is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. Elastic Net is a combination of these two regularization methods. model = linear_model.Ridge() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_calc_test = model.predict(X_test) model = linear_model . Ridge () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_calc_test = model . predict ( X_test ) Fit coefficients and intercept: [ 1.61930554e-03 1.11227142e-04 2.64709094e-06 5.87271456e-04 7.58510569e-05 -1.02851782e-04 8.76686650e-05 7.75641517e-04 2.23315063e-04 -9.98653815e-04 5.26839010e-06] 0.9967531358810221 sns.scatterplot(x=y_calc_test, y=y_test, color=\"lightseagreen\", s=50) plt.title(\"Ridge regression - predict test set\",fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_calc_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_calc_test)) sns . scatterplot ( x = y_calc_test , y = y_test , color = \"lightseagreen\" , s = 50 ) plt . title ( \"Ridge regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2e ' % mean_squared_error ( y_test , y_calc_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_calc_test )) Mean squared error: 5.45e-07 Coefficient of determination: 0.87 \ud83c\udfcb\ufe0f Exercise 6: Tune Hyperparameter for Ridge Regression \u00b6 Use the docstring to peak into the hyperparameters for Ridge Regression. What is the optimal value of lambda? Plot the \\(\\beta\\) values vs \\(\\lambda\\) from the results of your analysis # cell for exercise 3 out_lambdas = [] out_coefs = [] out_scores = [] for i in range ( 10 ): lambdas = [] coefs = [] scores = [] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) for lamb in range ( 1 , int ( 5e3 ), 20 ): model = linear_model . Ridge ( alpha = lamb ) model . fit ( X_train , y_train ) lambdas . append ( lamb ) coefs . append ( model . coef_ ) scores . append ( r2_score ( y_test , model . predict ( X_test ))) # print('MSE: %.4f' % mean_squared_error(y_test, model.predict(X_test))) # print('R2: %.4f' % r2_score(y_test, model.predict(X_test))) out_lambdas . append ( lambdas ) out_coefs . append ( coefs ) out_scores . append ( scores ) coef_means = np . array ( out_coefs ) . mean ( axis = 0 ) coef_stds = np . array ( out_coefs ) . std ( axis = 0 ) results_means = pd . DataFrame ( coef_means , columns = features ) results_stds = pd . DataFrame ( coef_stds , columns = features ) results_means [ 'lambda' ] = [ i for i in lambdas ] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) for feat in features : ax . errorbar ([ i for i in lambdas ], results_means [ feat ], yerr = results_stds [ feat ], label = feat ) # results.plot('lambda', 'scores', ax=ax[1]) ax . legend () <matplotlib.legend.Legend at 0x7f6777ffbe20> results = pd . DataFrame ( coefs , columns = features ) results [ 'lambda' ] = [ i for i in lambdas ] results [ 'scores' ] = scores fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) for feat in features : results . plot ( 'lambda' , feat , ax = ax [ 0 ]) results . plot ( 'lambda' , 'scores' , ax = ax [ 1 ]) <AxesSubplot:xlabel='lambda'> \ud83c\udf52 1.5 Enrichment : Additional Regression Exercises \u00b6 Problem 1) Number and choice of input features \u00b6 Load the red wine dataset and evaluate how the linear regression predictions changes as you change the number and choice of input features . The total number of columns in X is 11 and each column represent a specific input feature. Estimate the MSE print(X_train.shape) print ( X_train . shape ) (1274, 11) If you want to use the first 5 features you could proceed as following: X_train_five = X_train[:,0:5] X_test_five = X_test[:,0:5] X_train_five = X_train [:, 0 : 5 ] X_test_five = X_test [:, 0 : 5 ] Check that the new variables have the shape your expect print(X_train_five.shape) print(X_test_five.shape) print ( X_train_five . shape ) print ( X_test_five . shape ) (1274, 5) (319, 5) Now you can use these to train your linear regression model and repeat for different numbers or sets of input features! Note that you do not need to change the output feature! It's size is independent from the number of input features, yet recall that its length is the same as the number of values per input feature. Questions to think about while you work on this problem - How many input feature variables does one need? Is there a maximum or minimum number? - Could one input feature variable be better than the rest? - What if values are missing for one of the input feature variables - is it still worth using it? - Can you use L1 or L2 to determine these optimum features more quickly? Problem 2) Type of regression algorithm \u00b6 Try using other types of linear regression methods on the wine dataset: the LASSO model and the Elastic net model which are described by the sklearn.linear_model.ElasticNet() sklearn.linear_model.Lasso() scikit-learn functions. For more detail see ElasticNet and Lasso . Questions to think about while you work on this problem - How does the error change with each model? - Which model seems to perform best? - How can you optimize the hyperparameter, \\(\\lambda\\) - Does one model do better than the other at determining which input features are more important? - How about non linear regression / what if the data does not follow a line? from sklearn.linear_model import ElasticNet from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.linear_model import LinearRegression for model in [ ElasticNet , Lasso , Ridge , LinearRegression ]: model = model () model . fit ( X_train , y_train ) print ( str ( model )) print ( 'Mean squared error: %.ef' % mean_squared_error ( y_test , model . predict ( X_test ))) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , model . predict ( X_test ))) print () ElasticNet() Mean squared error: 4e-06f Coefficient of determination: -0.01 Lasso() Mean squared error: 4e-06f Coefficient of determination: -0.01 Ridge() Mean squared error: 6e-07f Coefficient of determination: 0.85 LinearRegression() Mean squared error: 6e-07f Coefficient of determination: 0.85 References \u00b6 Linear Regression To find out more see simple linear regression scikit-learn Scikit-learn Linear regression in scikit-learn Metrics of error The Boston dataset Pearson correlation To find out more see pearson Irreducible error, bias and variance Great Coursera videos here and here","title":"SOLN S1 Regression and Analysis"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#data-science-foundations-session-1-regression-and-analysis","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at fitting data to a curve using regression . We will also look at using regression to make predictions for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error.","title":"Data Science Foundations Session 1: Regression and Analysis"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#10-preparing-environment-and-importing-data","text":"back to top","title":"1.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#101-import-packages","text":"back to top # Import pandas, pyplot, ipywidgets import pandas as pd from matplotlib import pyplot as plt from ipywidgets import interact # Import Scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score # for enrichment topics import seaborn as sns import numpy as np","title":"1.0.1 Import Packages"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#102-load-dataset","text":"back to top For our discussion on regression and descriptive statistics today we will use a well known dataset of different wines and their quality ratings df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) df . shape (6497, 13) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6","title":"1.0.2 Load Dataset"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#11-what-is-regression","text":"It is the process of finding a relationship between dependent and independent variables to find trends in data. This abstract definition means that you have one variable (the dependent variable) which depends on one or more variables (the independent variables). One of the reasons for which we want to regress data is to understand whether there is a trend between two variables. Housing Prices Example We can imagine this scenario with housing prices. Envision a mixed dataset of continuous and discrete independent variables. Some features could be continuous, floating point values like location ranking and housing condition. Others could be descrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a regression model.","title":"1.1 What is regression?"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#12-linear-regression-fitting-with-scikit-learn","text":"","title":"1.2  Linear regression fitting with scikit-learn"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#exercise-1-rudimentary-eda","text":"What does the data look like? Recall how to visualize data in a pandas dataframe for every column calculate the * skew: `df.skew()` * kurtosis: `df.kurtosis()` * pearsons correlation with the dependent variable: `df.corr()` * number of missing entries `df.isnull()` and organize this into a new dataframe note: pearsons is just one type of correlation, another available to us spearman which differs from pearsons in that it depends on ranked values rather than their direct quantities, you can read more here df . isnull () . sum () type 0 fixed acidity 10 volatile acidity 8 citric acid 3 residual sugar 2 chlorides 2 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 9 sulphates 4 alcohol 0 quality 0 dtype: int64 # Cell for Exercise 1 # part A # using df.<method> define the following four variables with the results from # skew(), kurtosis(), corr() (while selecting for quality), and isnull() # for isnull() you'll notice the return is a dataframe of booleans. we would # like to simply know the number of null values for each column. change the # return of isnull() using the sum() method along the columns skew = df . skew () kurt = df . kurtosis () pear = df . corr ()[ 'quality' ] null = df . isnull () . sum ( axis = 0 ) # part B # on line 13, put these results in a list using square brackets and call # pd.DataFrame on the list to make your new DataFrame! store it under the # variable name dff dff = pd . DataFrame ([ skew , kurt , pear , null ]) # part C # take the transpose of this DataFrame using dff.T. reassign dff to this copy dff = dff . T # part D # set the column names to 'skew', 'kurtosis', 'pearsons _quality', and # 'null count' using dff.columns dff . columns = [ 'skew' , 'kurtosis' , 'pearsons _quality' , 'null count' ] # Now return dff to the output to view your hand work dff # uncomment this line /tmp/ipykernel_1422/4028752270.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. skew = df.skew() /tmp/ipykernel_1422/4028752270.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. kurt = df.kurtosis() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count fixed acidity 1.722805 5.057727 -0.077031 10.0 volatile acidity 1.495512 2.827081 -0.265953 8.0 citric acid 0.473032 2.401582 0.085706 3.0 residual sugar 1.435000 4.358134 -0.036825 2.0 chlorides 5.399849 50.894874 -0.200886 2.0 free sulfur dioxide 1.220066 7.906238 0.055463 0.0 total sulfur dioxide -0.001177 -0.371664 -0.041385 0.0 density 0.503602 6.606067 -0.305858 0.0 pH 0.386966 0.370068 0.019366 9.0 sulphates 1.798467 8.659892 0.038729 4.0 alcohol 0.565718 -0.531687 0.444319 0.0 quality 0.189623 0.232322 1.000000 0.0 type NaN NaN NaN 0.0 I have gone ahead and repeated this exercise with the red vs white wine types: red = df . loc [ df [ 'type' ] == 'red' ] wht = df . loc [ df [ 'type' ] == 'white' ] def get_summary ( df ): skew = df . skew () kurt = df . kurtosis () pear = df . corr ()[ 'quality' ] null = df . isnull () . sum () med = df . median () men = df . mean () dff = pd . DataFrame ([ skew , kurt , pear , null , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'pearsons _quality' , 'null count' , 'median' , 'mean' ] return dff dffr = get_summary ( red ) dffw = get_summary ( wht ) desc = pd . concat ([ dffr , dffw ], keys = [ 'red' , 'white' ]) /tmp/ipykernel_1422/2387423026.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. skew = df.skew() /tmp/ipykernel_1422/2387423026.py:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. kurt = df.kurtosis() /tmp/ipykernel_1422/2387423026.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. med = df.median() /tmp/ipykernel_1422/2387423026.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. men = df.mean() desc .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count median mean red fixed acidity 0.982192 1.132624 0.123834 2.0 7.90000 8.322104 volatile acidity 0.672862 1.226846 -0.390858 1.0 0.52000 0.527738 citric acid 0.317891 -0.788476 0.226917 1.0 0.26000 0.271145 residual sugar 4.540655 28.617595 0.013732 0.0 2.20000 2.538806 chlorides 5.680347 41.715787 -0.128907 0.0 0.07900 0.087467 free sulfur dioxide 1.250567 2.023562 -0.050656 0.0 14.00000 15.874922 total sulfur dioxide 1.515531 3.809824 -0.185100 0.0 38.00000 46.467792 density 0.071288 0.934079 -0.174919 0.0 0.99675 0.996747 pH 0.194803 0.814690 -0.057094 2.0 3.31000 3.310864 sulphates 2.429115 11.712632 0.251685 2.0 0.62000 0.658078 alcohol 0.860829 0.200029 0.476166 0.0 10.20000 10.422983 quality 0.217802 0.296708 1.000000 0.0 6.00000 5.636023 type NaN NaN NaN 0.0 NaN NaN white fixed acidity 0.647981 2.176560 -0.114032 8.0 6.80000 6.855532 volatile acidity 1.578595 5.095526 -0.194976 7.0 0.26000 0.278252 citric acid 1.284217 6.182036 -0.009194 2.0 0.32000 0.334250 residual sugar 1.076601 3.469536 -0.097373 2.0 5.20000 6.393250 chlorides 5.023412 37.560847 -0.210181 2.0 0.04300 0.045778 free sulfur dioxide 1.406745 11.466342 0.008158 0.0 34.00000 35.308085 total sulfur dioxide 0.390710 0.571853 -0.174737 0.0 134.00000 138.360657 density 0.977773 9.793807 -0.307123 0.0 0.99374 0.994027 pH 0.458402 0.532552 0.098858 7.0 3.18000 3.188203 sulphates 0.977361 1.589847 0.053690 2.0 0.47000 0.489835 alcohol 0.487342 -0.698425 0.435575 0.0 10.40000 10.514267 quality 0.155796 0.216526 1.000000 0.0 6.00000 5.877909 type NaN NaN NaN 0.0 NaN NaN def my_fig ( metric = desc . columns ): fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) pd . DataFrame ( desc [ metric ]) . unstack ()[ metric ] . T . plot ( kind = 'barh' , ax = ax ) interact ( my_fig ) interactive(children=(Dropdown(description='metric', options=('skew', 'kurtosis', 'pearsons _quality', 'null c\u2026 <function __main__.my_fig(metric=Index(['skew', 'kurtosis', 'pearsons _quality', 'null count', 'median', 'mean'], dtype='object'))>","title":"\ud83c\udfcb\ufe0f Exercise 1: rudimentary EDA"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#question-1-discussion-around-eda-plot","text":"What do we think of this plot? metric = mean , the cholrides values metric = kurtosis , residual sugar metric = pearsons _quality , magnitudes and directions How to improve the plot, what other plots would we like to see? For instance, what if we were really curious about the high kurtosis for chlorides content? What more would we like to glean about the distribution of chloride content? # we can use df.describe() to take a look at the quantile values and min/max df [ 'chlorides' ] . describe () count 6495.000000 mean 0.056042 std 0.035036 min 0.009000 25% 0.038000 50% 0.047000 75% 0.065000 max 0.611000 Name: chlorides, dtype: float64 # and see how these values appear in a KDE fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) df [ 'chlorides' ] . plot ( kind = 'kde' , ax = ax ) ax . set_xlim ( 0 , .61 ) (0.0, 0.61) # lastly we may want to look at the raw values themselves. We can sort them # too view outliers df [ 'chlorides' ] . sort_values ( ascending = False )[: 50 ] 5156 0.611 5049 0.610 5004 0.467 4979 0.464 5590 0.422 6268 0.415 6270 0.415 5652 0.415 6217 0.414 5949 0.414 5349 0.413 6158 0.403 4981 0.401 5628 0.387 6063 0.369 4915 0.368 5067 0.360 5179 0.358 484 0.346 5189 0.343 4917 0.341 5124 0.337 4940 0.332 1217 0.301 687 0.290 4473 0.271 5079 0.270 6272 0.267 5138 0.263 1865 0.255 5466 0.250 1034 0.244 5674 0.243 5675 0.241 683 0.240 1638 0.239 5045 0.236 6456 0.235 6468 0.230 5465 0.226 5464 0.226 5564 0.222 2186 0.217 5996 0.216 6333 0.214 5206 0.214 6332 0.214 5205 0.213 4497 0.212 1835 0.211 Name: chlorides, dtype: float64","title":"\ud83d\ude4b Question 1: Discussion Around EDA Plot"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#122-visualizing-the-data-set-motivating-regression-analysis","text":"In order to demonstrate simple linear regression with this dataset we will look at two particular features: fixed acidity and density . We can create a scatter plot of fixed acidity vs density for the red wine in the dataset using df.plot() and see that there appears to be a general trend between the two features: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) df . loc [ df [ 'type' ] == 'red' ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> Now the question is: How do we quantify this trend?","title":"1.2.2 Visualizing the data set - motivating regression analysis"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#123-estimating-the-regression-coefficients","text":"It looks like density increases with fixed acidity following a line, maybe something like y(x)= m \\cdot x + b \\;\\;\\;\\;\\;\\;\\;\\; \\sf{eq. 1} with \\( y=\\sf density \\), \\(x=\\sf fixed \\space acidity\\), and \\(m\\) the slope and \\(b\\) the intercept. To solve the problem, we need to find the values of \\(b\\) and \\(m\\) in equation 1 to best fit the data. This is called linear regression . In linear regression our goal is to minimize the error between computed values of positions \\(y^{\\sf calc}(x_i)\\equiv y^{\\sf calc}_i\\) and known values \\(y^{\\sf exact}(x_i)\\equiv y^{\\sf exact}_i\\), i.e. find \\(b\\) and \\(m\\) which lead to lowest value of \\epsilon (m,b) =SS_{\\sf res}=\\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - m\\cdot x_i - b \\right)^2\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sf{eq. 2} Otherwise known as the residual sum of squares To find out more see e.g. https://en.wikipedia.org/wiki/Simple_linear_regression","title":"1.2.3 Estimating the regression coefficients"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#question-2-linear-regression-loss-function","text":"Do we always want m and b to be large positive numbers so as to minimize eq. 2? Luckily scikit-learn contains many functions related to regression including linear regression . The function we will use is called LinearRegression() . # Create linear regression object model = linear_model.LinearRegression() # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red['density'].values.reshape(-1, 1) y = red['fixed acidity'].values.reshape(-1, 1) # Create linear regression object model = linear_model . LinearRegression () # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red [ 'density' ] . values . reshape ( - 1 , 1 ) y = red [ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) print(red['density'].values.shape, red['fixed acidity'].values.shape) print(x.shape, y.shape) print ( red [ 'density' ] . values . shape , red [ 'fixed acidity' ] . values . shape ) print ( x . shape , y . shape ) (1599,) (1599,) (1599, 1) (1599, 1) # Fit to the data model.fit(x, y) # Extract the values of interest m = model.coef_[0][0] b = model.intercept_[0] # Print the slope m and intercept b print('Scikit learn - Slope: ', m , 'Intercept: ', b ) What happens when we try to fit the data as is? # Fit to the data # model.fit(x, y)","title":"\ud83d\ude4b Question 2: linear regression loss function"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#exercise-2-drop-null-values-and-practice-pandas-operations","text":"Let's look back at our dataset description dataframe above, what do we notice, what contains null values? There are several strategies for dealing with null values. For now let's take the simplest case, and drop rows in our dataframe that contain null # Cell for Exercise 2 # For this templated exercise you are going to complete everything in one line # of code, but we are going to break it up into steps. So for each part (A, B, # etc.) paste your answer from the previous part to begin (your opertaions will # read from left to right) # step A # select the 'density' and 'fixed acidity' columns of red. make sure the return # is a dataframe df [[ 'density' , 'fixed acidity' ]] # step B # now use the dropna() method on axis 0 (the rows) to drop any null values df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 ) # step B # select column 'density' df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] # step C # select the values df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values # step D # reshape the result with an empty second dimension using .reshape() and store # the result under variable x x = df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values . reshape ( - 1 , 1 ) # repeat the same process with 'fixed acidity' and variable y y = df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) Now that we have our x and y arrays we can fit using ScikitLearn x = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values . reshape ( - 1 , 1 ) y = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'fixed acidity' ] . values . reshape ( - 1 , 1 )","title":"\ud83c\udfcb\ufe0f Exercise 2: drop Null Values (and practice pandas operations)"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#question-3-why-do-we-drop-null-values-across-both-columns","text":"Notice in the above cell how we selected both density and fixed acidity before calling dropna ? Why did we do that? Why didn't we just select density in the x variable case and fixed acidity in the y variable case? # Fit to the data model . fit ( x , y ) # Extract the values of interest m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # Print the slope m and intercept b print ( 'Scikit learn - Slope: ' , m , 'Intercept: ' , b ) Scikit learn - Slope: 616.01314280661 Intercept: -605.6880086750523","title":"\ud83d\ude4b Question 3: why do we drop null values across both columns?"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#exercise-3-calculating-y_pred","text":"Estimate the values of \\(y\\) by using your fitted parameters. Hint: Use your model.coef_ and model.intercept_ parameters to estimate y_pred following equation 1 # define y_pred in terms of m, x, and b y_pred = m * x + b # uncomment the following lines! fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x , y_pred , ls = '' , marker = '*' ) ax . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f6781becca0>] We can also return predictions directly with the model object using the predict() method note: it is great to get in the habit of utilizing model outputs this way, as the API will be similar across all scikit-learn models (and sometimes models in other libraries as well!) # Another way to get this is using the model.predict function y_pred = model . predict ( x ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x , y_pred , ls = '' , marker = '*' ) ax . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f6781b5c790>]","title":"\ud83c\udfcb\ufe0f Exercise 3: calculating y_pred"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#13-error-and-topics-of-model-fitting-assessing-model-accuracy","text":"","title":"1.3 Error and topics of model fitting (assessing model accuracy)"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#131-measuring-the-quality-of-fit","text":"","title":"1.3.1 Measuring the quality of fit"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#1311-mean-squared-error","text":"The plot in Section 1.2.3 looks good, but numerically what is our error? What is the mean value of $\\epsilon$, i.e. the Mean Squared Error (MSE) ? {\\sf MSE}=\\epsilon_{\\sf ave} = \\frac{\\sum_{i=1}^{N_{\\sf times}}\\left(y^{\\sf exact}_i - m\\cdot t_i - b \\right)^2}{N_{\\sf times}}\\;\\;\\;\\;\\;\\sf eq. 3 # The mean squared error print('Mean squared error: %.2f' % mean_squared_error(y, y_pred)) # The mean squared error print ( 'Mean squared error: %.2f ' % mean_squared_error ( y , y_pred )) Mean squared error: 1.68","title":"1.3.1.1 Mean Squared Error"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#1312-r-square","text":"Another way to measure error is the regression score, \\(R^2\\). \\(R^2\\) is generally defined as the ratio of the total sum of squares \\(SS_{\\sf tot}\\) to the residual sum of squares \\(SS_{\\sf res}\\): SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\; \\sf eq. 4 SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2\\;\\;\\;\\;\\; \\sf eq. 5 R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}} \\;\\;\\;\\;\\;\\; \\sf eq. 6 In eq. 4, \\(\\bar{y}=\\sum_i y^{\\sf exact}_i/N\\) is the average value of y for \\(N\\) points. The best value of \\(R^2\\) is 1 but it can also take a negative value if the error is large. See all the different regression metrics here .","title":"1.3.1.2 R-square"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#question-4-lets-understand-r2","text":"Do we need a large value of \\(SS_{\\sf tot}\\) to minimize \\(R^2\\) - is this something which we have the power to control? # Print the coefficient of determination - 1 is perfect prediction print('Coefficient of determination: %.2f' % r2_score(y, y_pred)) # Print the coefficient of determination - 1 is perfect prediction print ( 'Coefficient of determination: %.2f ' % r2_score ( y , y_pred )) Coefficient of determination: 0.45","title":"\ud83d\ude4b Question 4: lets understand \\(R^2\\)"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#132-corollaries-with-classification-models","text":"For classification tasks, we typically assess accuracy vs MSE or R-square, since we are dealing with categorical rather than numerical predictions. What is accuracy? It is defined as the ratio of True assignments to all assignments. For a binary positive/negative classification task this can be written as the following: Acc = \\frac{T_p + T_n}{F_p + F_n + T_p + T_n} Where \\(T\\) is True, \\(F\\) is false, \\(p\\) is positive, \\(n\\) is negative Just as a quick example, we can perform this type of task on our wine dataset by predicting whether a given data entry is for red or white wine: logdf = df . copy () . dropna ( axis = 0 ) y_train = logdf . pop ( 'type' ) . values . reshape ( - 1 , 1 ) x_train = logdf . dropna ( axis = 0 ) . values # train a logistic regression model on the training set from sklearn.linear_model import LogisticRegression # instantiate model logreg = LogisticRegression () # fit model logreg . fit ( x_train , y_train ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( LogisticRegression() # make class predictions for the testing set y_pred_class = logreg . predict ( x_train ) # calculate accuracy from sklearn import metrics print ( metrics . accuracy_score ( y_train , y_pred_class )) 0.9797307751818041","title":"1.3.2 Corollaries with classification models"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#133-beyond-a-single-input-feature","text":"( also: quick appreciative beat for folding in domain area expertise into our models and features ) The acidity of the wine (the dependent variable v) could depend on: potassium from the soil (increases alkalinity) unripe grapes (increases acidity) grapes grown in colder climates or reduced sunshine create less sugar (increases acidity) preprocessing such as adding tartaric acid to the grape juice before fermentation (increases acidity) malolactic fermentation (reduces acidity) + others So in our lab today we will look at folding in additional variables in our dataset into the model","title":"1.3.3 Beyond a single input feature"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#14-multivariate-regression","text":"Let's now turn our attention to wine quality. The value we aim to predict or evaluate is the quality of each wine in our dataset. This is our dependent variable. We will look at how this is related to the 12 other independent variables, also known as input features . We're going to do this with only the red wine data red . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 4898 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 4899 red 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5 4900 red 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5 4901 red 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6 4902 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5","title":"1.4 Multivariate regression"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#141-linear-regression-with-all-input-fields","text":"For this example, notice we have a categorical data variable in the 'type' column. We will ignore this for now, and only work with our red wines. In the future we will discuss how to deal with categorical variable such as this in a mathematical representation. # this is a list of all our features or independent variables features = list ( red . columns [ 1 :]) # we're going to remove our target or dependent variable, density from this # list features . remove ( 'density' ) # now we define X and y according to these lists of names X = red . dropna ( axis = 0 )[ features ] . values y = red . dropna ( axis = 0 )[ 'density' ] . values # we will talk about scaling/centering our data at a later time X = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) red . isnull () . sum ( axis = 0 ) # we are getting rid of some nasty nulls! type 0 fixed acidity 2 volatile acidity 1 citric acid 1 residual sugar 0 chlorides 0 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 2 sulphates 2 alcohol 0 quality 0 dtype: int64 # Create linear regression object - note that we are using all the input features model = linear_model.LinearRegression() model.fit(X, y) y_calc = model.predict(X) # Create linear regression object - note that we are using all the input features model = linear_model . LinearRegression () model . fit ( X , y ) y_calc = model . predict ( X ) Let's see what the coefficients look like ... print(\"Fit coefficients: \\n\", model.coef_, \"\\nNumber of coefficients:\", len(model.coef_)) print ( \"Fit coefficients: \\n \" , model . coef_ , \" \\n Number of coefficients:\" , len ( model . coef_ )) Fit coefficients: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] Number of coefficients: 11 We have 11 !!! That's because we are regressing respect to all 11 independent variables !!! So now, y_{\\sf calc}= m_1x_1 +\\, m_2x_2 \\,+ \\,m_3x_3 \\,+\\,... \\,+ \\,b =\\sum_{i=1}^{13}m_i x_i + b\\;\\;\\;\\;\\; \\sf eq. 7 print(\"We have 13 slopes / weights:\\n\\n\", model.coef_) print(\"\\nAnd one intercept: \", model.intercept_) print ( \"We have 11 slopes / weights: \\n\\n \" , model . coef_ ) print ( \" \\n And one intercept: \" , model . intercept_ ) We have 11 slopes / weights: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] And one intercept: 0.9967517451349656 # This size should match the number of columns in X if len(X[0]) == len(model.coef_): print(\"All good! The number of coefficients matches the number of input features.\") else: print(\"Hmm .. something strange is going on.\") # This size should match the number of columns in X if len ( X [ 0 ]) == len ( model . coef_ ): print ( \"All good! The number of coefficients matches the number of input features.\" ) else : print ( \"Hmm .. something strange is going on.\" ) All good! The number of coefficients matches the number of input features.","title":"1.4.1 Linear regression with all input fields"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#exercise-4-evaluate-the-error","text":"Let's evaluate the error by computing the MSE and \\(R^2\\) metrics (see eq. 3 and 6). # The mean squared error # part A # calculate the MSE using mean_squared_error() # mse = # part B # calculate the R square using r2_score() # r2 = print('Mean squared error: {:.2f}'.format(mse) print('Coefficient of determination: {:.2f}'.format(r2) # The mean squared error # part A # calculate the MSE using mean_squared_error() mse = mean_squared_error ( y , model . predict ( X )) # part B # calculate the R square using r2_score() r2 = r2_score ( y , model . predict ( X )) print ( 'Mean squared error: {:.2e} ' . format ( mse )) print ( 'Coefficient of determination: {:.2f} ' . format ( r2 )) Mean squared error: 5.62e-07 Coefficient of determination: 0.84","title":"\ud83c\udfcb\ufe0f Exercise 4: evaluate the error"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#exercise-5-make-a-plot-of-y-actual-vs-y-predicted","text":"We can also look at how well the computed values match the true values graphically by generating a scatterplot. # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style # plt.plot() plt.title(\"Linear regression - computed values on entire data set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style plt . plot ( y , model . predict ( X ), ls = '' , marker = '.' ) plt . title ( \"Linear regression - computed values on entire data set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show ()","title":"\ud83c\udfcb\ufe0f Exercise 5: make a plot of y actual vs y predicted"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#142-enrichment-splitting-into-train-and-test-sets","text":"note: more of this topic is covered in Model Selection and Validation To see whether we can predict, we will carry out our regression only on a part, 80%, of the full data set. This part is called the training data. We will then test the trained model to predict the rest of the data, 20% - the test data. The function which fits won't see the test data until it has to predict it. We will motivate the use of train/test sets more explicitly in Model Selection and Validation We start by splitting out data using scikit-learn's train_test_split() function: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) ``` ```python X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) Now we check the size of y_train and y_test , the sum should be the size of y! If this works then we move on and carry out regression but we only use the training data! if len(y_test)+len(y_train) == len(y): print('All good, ready to to go and regress!\\n') # Carry out linear regression print('Running linear regression algorithm on the training set\\n') model = linear_model.LinearRegression() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_pred_test = model.predict(X_test) if len ( y_test ) + len ( y_train ) == len ( y ): print ( 'All good, ready to to go and regress! \\n ' ) # Carry out linear regression print ( 'Running linear regression algorithm on the training set \\n ' ) model = linear_model . LinearRegression () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_pred_test = model . predict ( X_test ) All good, ready to to go and regress! Running linear regression algorithm on the training set Fit coefficients and intercept: [ 1.62385613e-03 1.10578142e-04 7.75216492e-07 5.87755741e-04 7.65190323e-05 -1.03490059e-04 8.87357873e-05 7.79083342e-04 2.23534769e-04 -9.99858829e-04 5.85256438e-06] 0.9967531628434799 Now we can plot our predicted values to see how accurate we are in predicting. We will generate a scatterplot and computing the MSE and \\(R^2\\) metrics of error. sns.scatterplot(x=y_pred_test, y=y_test, color=\"mediumvioletred\", s=50) plt.title(\"Linear regression - predict test set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred_test)) sns . scatterplot ( x = y_pred_test , y = y_test , color = \"mediumvioletred\" , s = 50 ) plt . title ( \"Linear regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2e ' % mean_squared_error ( y_test , y_pred_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_pred_test )) Mean squared error: 5.45e-07 Coefficient of determination: 0.87","title":"\ud83c\udf52 1.4.2 Enrichment: Splitting into train and test sets"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#1421-other-data-considerations","text":"Do we need all the independent variables? Topics of interential statistics covered in a couple sessions Can we output integer quality scores? Topics of non-binary classification tasks covered in week 4","title":"1.4.2.1 Other data considerations"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#143-enrichment-other-regression-algorithms","text":"There are many other regression algorithms. The two we want to highlight here are Ridge, LASSO, and Elastic Net. They differ by an added term to the loss function. Let's review. Eq. 2 expanded to multivariate form yields: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 for Ridge regression, we add a regularization term known as L2 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}\\beta_{j}^2 for LASSO (Least Absolute Shrinkage and Selection Operator) we add L1 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}|\\beta_{j}| The key difference here is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. Elastic Net is a combination of these two regularization methods. model = linear_model.Ridge() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_calc_test = model.predict(X_test) model = linear_model . Ridge () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_calc_test = model . predict ( X_test ) Fit coefficients and intercept: [ 1.61930554e-03 1.11227142e-04 2.64709094e-06 5.87271456e-04 7.58510569e-05 -1.02851782e-04 8.76686650e-05 7.75641517e-04 2.23315063e-04 -9.98653815e-04 5.26839010e-06] 0.9967531358810221 sns.scatterplot(x=y_calc_test, y=y_test, color=\"lightseagreen\", s=50) plt.title(\"Ridge regression - predict test set\",fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_calc_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_calc_test)) sns . scatterplot ( x = y_calc_test , y = y_test , color = \"lightseagreen\" , s = 50 ) plt . title ( \"Ridge regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2e ' % mean_squared_error ( y_test , y_calc_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_calc_test )) Mean squared error: 5.45e-07 Coefficient of determination: 0.87","title":"\ud83c\udf52 1.4.3 Enrichment: Other regression algorithms"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#exercise-6-tune-hyperparameter-for-ridge-regression","text":"Use the docstring to peak into the hyperparameters for Ridge Regression. What is the optimal value of lambda? Plot the \\(\\beta\\) values vs \\(\\lambda\\) from the results of your analysis # cell for exercise 3 out_lambdas = [] out_coefs = [] out_scores = [] for i in range ( 10 ): lambdas = [] coefs = [] scores = [] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) for lamb in range ( 1 , int ( 5e3 ), 20 ): model = linear_model . Ridge ( alpha = lamb ) model . fit ( X_train , y_train ) lambdas . append ( lamb ) coefs . append ( model . coef_ ) scores . append ( r2_score ( y_test , model . predict ( X_test ))) # print('MSE: %.4f' % mean_squared_error(y_test, model.predict(X_test))) # print('R2: %.4f' % r2_score(y_test, model.predict(X_test))) out_lambdas . append ( lambdas ) out_coefs . append ( coefs ) out_scores . append ( scores ) coef_means = np . array ( out_coefs ) . mean ( axis = 0 ) coef_stds = np . array ( out_coefs ) . std ( axis = 0 ) results_means = pd . DataFrame ( coef_means , columns = features ) results_stds = pd . DataFrame ( coef_stds , columns = features ) results_means [ 'lambda' ] = [ i for i in lambdas ] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) for feat in features : ax . errorbar ([ i for i in lambdas ], results_means [ feat ], yerr = results_stds [ feat ], label = feat ) # results.plot('lambda', 'scores', ax=ax[1]) ax . legend () <matplotlib.legend.Legend at 0x7f6777ffbe20> results = pd . DataFrame ( coefs , columns = features ) results [ 'lambda' ] = [ i for i in lambdas ] results [ 'scores' ] = scores fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) for feat in features : results . plot ( 'lambda' , feat , ax = ax [ 0 ]) results . plot ( 'lambda' , 'scores' , ax = ax [ 1 ]) <AxesSubplot:xlabel='lambda'>","title":"\ud83c\udfcb\ufe0f Exercise 6: Tune Hyperparameter for Ridge Regression"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#15-enrichment-additional-regression-exercises","text":"","title":"\ud83c\udf52 1.5 Enrichment: Additional Regression Exercises"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#problem-1-number-and-choice-of-input-features","text":"Load the red wine dataset and evaluate how the linear regression predictions changes as you change the number and choice of input features . The total number of columns in X is 11 and each column represent a specific input feature. Estimate the MSE print(X_train.shape) print ( X_train . shape ) (1274, 11) If you want to use the first 5 features you could proceed as following: X_train_five = X_train[:,0:5] X_test_five = X_test[:,0:5] X_train_five = X_train [:, 0 : 5 ] X_test_five = X_test [:, 0 : 5 ] Check that the new variables have the shape your expect print(X_train_five.shape) print(X_test_five.shape) print ( X_train_five . shape ) print ( X_test_five . shape ) (1274, 5) (319, 5) Now you can use these to train your linear regression model and repeat for different numbers or sets of input features! Note that you do not need to change the output feature! It's size is independent from the number of input features, yet recall that its length is the same as the number of values per input feature. Questions to think about while you work on this problem - How many input feature variables does one need? Is there a maximum or minimum number? - Could one input feature variable be better than the rest? - What if values are missing for one of the input feature variables - is it still worth using it? - Can you use L1 or L2 to determine these optimum features more quickly?","title":"Problem 1) Number and choice of input features"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#problem-2-type-of-regression-algorithm","text":"Try using other types of linear regression methods on the wine dataset: the LASSO model and the Elastic net model which are described by the sklearn.linear_model.ElasticNet() sklearn.linear_model.Lasso() scikit-learn functions. For more detail see ElasticNet and Lasso . Questions to think about while you work on this problem - How does the error change with each model? - Which model seems to perform best? - How can you optimize the hyperparameter, \\(\\lambda\\) - Does one model do better than the other at determining which input features are more important? - How about non linear regression / what if the data does not follow a line? from sklearn.linear_model import ElasticNet from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.linear_model import LinearRegression for model in [ ElasticNet , Lasso , Ridge , LinearRegression ]: model = model () model . fit ( X_train , y_train ) print ( str ( model )) print ( 'Mean squared error: %.ef' % mean_squared_error ( y_test , model . predict ( X_test ))) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , model . predict ( X_test ))) print () ElasticNet() Mean squared error: 4e-06f Coefficient of determination: -0.01 Lasso() Mean squared error: 4e-06f Coefficient of determination: -0.01 Ridge() Mean squared error: 6e-07f Coefficient of determination: 0.85 LinearRegression() Mean squared error: 6e-07f Coefficient of determination: 0.85","title":"Problem 2) Type of regression algorithm"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#references","text":"Linear Regression To find out more see simple linear regression scikit-learn Scikit-learn Linear regression in scikit-learn Metrics of error The Boston dataset Pearson correlation To find out more see pearson Irreducible error, bias and variance Great Coursera videos here and here","title":"References"},{"location":"solutions/SOLN_S2_Inferential_Statistics/","text":"Data Science Foundations, Session 2: Inferential Statistics \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at the utility of EDA combined with inferential statistics. 2.0 Preparing Environment and Importing Data \u00b6 back to top 2.0.1 Import Packages \u00b6 back to top # The modules we've seen before import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns # our stats modules import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy 2.0.2 Load Dataset \u00b6 back to top For this session, we will use dummy datasets from sklearn. df = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... ... ... ... ... ... 1663 Tiramisu Chocolate Outer Doughnut Pear Amethyst Fickelgruber 12/2020 38128.802589 0.420111 1664 Tiramisu Chocolate Outer Doughnut Pear Burgundy Zebrabar 12/2020 108.642857 0.248659 1665 Tiramisu Chocolate Outer Doughnut Pear Teal Zebrabar 12/2020 3517.933333 0.378501 1666 Tiramisu Chocolate Outer Doughnut Rock and Rye Amethyst Slugworth 12/2020 10146.898432 0.213149 1667 Tiramisu Chocolate Outer Doughnut Rock and Rye Burgundy Zebrabar 12/2020 1271.904762 0.431813 1668 rows \u00d7 9 columns descriptors = df . columns [: - 2 ] for col in df . columns [: - 2 ]: print ( col ) print ( df [ col ] . unique ()) print () Base Cake ['Butter' 'Cheese' 'Chiffon' 'Pound' 'Sponge' 'Tiramisu'] Truffle Type ['Candy Outer' 'Chocolate Outer' 'Jelly Filled'] Primary Flavor ['Butter Pecan' 'Ginger Lime' 'Margarita' 'Pear' 'Pink Lemonade' 'Raspberry Ginger Ale' 'Sassafras' 'Spice' 'Wild Cherry Cream' 'Cream Soda' 'Horchata' 'Kettle Corn' 'Lemon Bar' 'Orange Pineapple\\tP' 'Plum' 'Orange' 'Butter Toffee' 'Lemon' 'Acai Berry' 'Apricot' 'Birch Beer' 'Cherry Cream Spice' 'Creme de Menthe' 'Fruit Punch' 'Ginger Ale' 'Grand Mariner' 'Orange Brandy' 'Pecan' 'Toasted Coconut' 'Watermelon' 'Wintergreen' 'Vanilla' 'Bavarian Cream' 'Black Licorice' 'Caramel Cream' 'Cheesecake' 'Cherry Cola' 'Coffee' 'Irish Cream' 'Lemon Custard' 'Mango' 'Sour' 'Amaretto' 'Blueberry' 'Butter Milk' 'Chocolate Mint' 'Coconut' 'Dill Pickle' 'Gingersnap' 'Chocolate' 'Doughnut'] Secondary Flavor ['Toffee' 'Banana' 'Rum' 'Tutti Frutti' 'Vanilla' 'Mixed Berry' 'Whipped Cream' 'Apricot' 'Passion Fruit' 'Peppermint' 'Dill Pickle' 'Black Cherry' 'Wild Cherry Cream' 'Papaya' 'Mango' 'Cucumber' 'Egg Nog' 'Pear' 'Rock and Rye' 'Tangerine' 'Apple' 'Black Currant' 'Kiwi' 'Lemon' 'Hazelnut' 'Butter Rum' 'Fuzzy Navel' 'Mojito' 'Ginger Beer'] Color Group ['Taupe' 'Amethyst' 'Burgundy' 'White' 'Black' 'Opal' 'Citrine' 'Rose' 'Slate' 'Teal' 'Tiffany' 'Olive'] Customer ['Slugworth' 'Perk-a-Cola' 'Fickelgruber' 'Zebrabar' \"Dandy's Candies\"] Date ['1/2020' '2/2020' '3/2020' '4/2020' '5/2020' '6/2020' '7/2020' '8/2020' '9/2020' '10/2020' '11/2020' '12/2020'] 2.1 Many Flavors of Statistical Tests \u00b6 https://luminousmen.com/post/descriptive-and-inferential-statistics Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (\u201cinferences\u201d) from that data. With inferential statistics, you take data from samples and make generalizations about a population statshowto Moods Median Test Kruskal-Wallis Test (Another comparison of Medians test) T-Test Analysis of Variance (ANOVA) One Way ANOVA Two Way ANOVA MANOVA Factorial ANOVA When do I use each of these? We will talk about this as we proceed through the examples. This page from minitab has good rules of thumb on the subject. 2.1.1 What is Mood's Median? \u00b6 You can use Chi-Square to test for a goodness of fit (whether a sample of data represents a distribution) or whether two variables are related (using a contingency table, which we will create below!) A special case of Pearon's Chi-Squared Test: We create a table that counts the observations above and below the global median for two different groups. We then perform a chi-squared test of significance on this contingency table Null hypothesis: the Medians are all equal The chi-square test statistic: x^2 = \\sum{\\frac{(O-E)^2}{E}} Where \\(O\\) is the observed frequency and \\(E\\) is the expected frequency. Let's take an example , say we have three shifts with the following production rates: np . random . seed ( 7 ) shift_one = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_two = [ round ( i ) for i in np . random . normal ( 21 , 3 , 10 )] print ( shift_one ) print ( shift_two ) [21, 15, 16, 17, 14, 16, 16, 11, 19, 18] [19, 20, 23, 20, 20, 17, 23, 21, 22, 16] stat , p , m , table = scipy . stats . median_test ( shift_one , shift_two , correction = False ) what is median_test returning? print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 7.20 p-value of the test: 0.007 the grand median: 18.5 Let's evaluate that test statistic ourselves by taking a look at the contingency table: table array([[2, 8], [8, 2]]) This is easier to make sense of if we order the shift times shift_one . sort () shift_one [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] When we look at shift one, we see that 8 values are at or below the grand median. shift_two . sort () shift_two [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] For shift two, only two are at or below the grand median. Since the sample sizes are the same, the expected value for both groups is the same, 5 above and 5 below the grand median. The chi-square is then: X^2 = \\frac{(2-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(2-5)^2}{5} ( 3 - 5 ) ** 2 / 5 + ( 7 - 5 ) ** 2 / 5 + ( 7 - 5 ) ** 2 / 5 + ( 3 - 5 ) ** 2 / 5 3.2 Our p-value, or the probability of observing the null-hypothsis, is under 0.05. We can conclude that these shift performances were drawn under seperate distributions. For comparison, let's do this analysis again with shifts of equal performances np . random . seed ( 3 ) shift_three = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_four = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] stat , p , m , table = scipy . stats . median_test ( shift_three , shift_four , correction = False ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000 the grand median: 15.5 and the shift raw values: shift_three . sort () shift_four . sort () print ( shift_three ) print ( shift_four ) [10, 14, 15, 15, 15, 16, 16, 16, 17, 21] [11, 12, 13, 14, 15, 16, 19, 19, 19, 21] table array([[5, 5], [5, 5]]) 2.1.2 When to Use Mood's? \u00b6 Mood's Median Test is highly flexible but has the following assumptions: Considers only one categorical factor Response variable is continuous (our shift rates) Data does not need to be normally distributed But the distributions are similarly shaped Sample sizes can be unequal and small (less than 20 observations) Other considerations: Not as powerful as Kruskal-Wallis Test but still useful for small sample sizes or when there are outliers \ud83c\udfcb\ufe0f Exercise 1: Use Mood's Median Test \u00b6 Part A Perform moods median test on Base Cake (Categorical Variable) and EBITDA/KG (Continuous Variable) in Truffle data \u00b6 We're also going to get some practice with pandas groupby. # what is returned by this groupby? gp = df . groupby ( 'Base Cake' ) How do we find out? We could iterate through it: # seems to be a tuple of some sort for i in gp : print ( i ) break ('Butter', Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns]) # the first object appears to be the group print ( i [ 0 ]) # the second object appears to be the df belonging to that group print ( i [ 1 ]) Butter Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns] going back to our diagram from our earlier pandas session. It looks like whenever we split in the groupby method, we create separate dataframes as well as their group label: Ok, so we know gp is separate dataframes. How do we turn the 'EBITDA/KG' column of each dataframe into arrays to then pass to median_test ? # complete this for loop for i , j in gp : print ( i ) print ( j [ 'EBITDA/KG' ] . values ) break # turn 'EBITDA/KG' of j into an array using the .values attribute # print this to the screen Butter [ 5.00423594e-01 2.20395451e-01 1.71013869e-01 2.33024872e-01 4.80689371e-01 1.64934546e-01 2.03213256e-01 1.78681400e-01 1.25050726e-01 2.17021951e-01 7.95955185e-02 3.25042287e-01 2.17551215e-01 2.48152299e-01 -1.20503094e-02 1.47190567e-01 3.84488948e-01 2.05438764e-01 1.32190256e-01 3.23019144e-01 -9.73361477e-03 1.98397692e-01 1.67067902e-01 -2.60063690e-02 1.30365325e-01 2.36337749e-01 -9.70556780e-02 1.59051819e-01 -8.76572259e-02 -3.32199843e-02 -5.05704451e-02 -5.56458806e-02 -8.86273564e-02 4.32267857e-02 -1.88615579e-01 4.24939227e-01 9.35136847e-02 -3.43605950e-02 1.63823520e-01 2.78522916e-01 1.29207730e-01 1.79194495e-01 1.37419569e-01 1.31372653e-01 2.53275225e-01 2.26761431e-01 1.10173466e-01 1.99338787e-01 -2.01250197e-01 1.16567591e-01 1.32324984e-01 4.02912418e-01 9.35051765e-02 1.65865814e-01 2.12269112e-01 2.53461571e-01 1.89055713e-01 1.20416365e-01 3.95276612e-02 2.93121770e-01 1.40947082e-01 -1.21555832e-01 1.56455622e-01 -1.29776953e-02 -6.17934014e-02 -8.19904808e-02 -3.14711557e-02 -8.03820228e-02 1.63839981e-01 8.34406336e-02 1.49369698e-01 1.05990633e-01 1.27399979e-01 2.26634255e-01 -2.20801929e-03 -6.92044284e-02 1.74048414e-01 1.30933438e-01 1.27620323e-01 2.78652749e-01 2.14772018e-01 1.40864278e-01 1.23745138e-01 1.66586809e-01 2.91940995e-01 2.49925584e-01 8.65447719e-02 3.80907774e-01 2.70851719e-01 3.32946265e-01 9.00795862e-03 2.00960974e-01 2.72623570e-01 3.35902190e-01 1.27337723e-01 2.36618545e-01 -6.82774785e-02 3.13166906e-01 2.15752651e-01 9.29694447e-02 3.60809152e-02 2.32488112e-01 3.38200308e-02 1.70916188e-01 2.81620452e-01 -1.61981289e-01 -4.14570666e-02 1.13465970e-02 2.28733252e-01 9.87516565e-02 3.52732668e-02 6.32598661e-02 2.10300526e-01 1.98761726e-01 1.38832882e-01 2.95465366e-01 2.68022024e-01 3.22389724e-01 4.04867623e-01 2.38086167e-01 1.12586985e-01 1.94010438e-01 1.96757297e-01 1.65215620e-01 1.22730941e-02 1.14415249e-01 3.26252563e-01 1.89080695e-01 -5.11830382e-02 2.41661008e-01 2.00063672e-01 3.07633312e-01 4.20740234e-01 1.34764192e-01 -4.75993730e-02 1.52973888e-02 1.87709908e-01 7.20193743e-02 3.48745346e-02 2.77659158e-01 2.73466257e-01 1.32419725e-01 2.85933859e-02 3.99622870e-02 -7.46829380e-02 9.03915641e-02 -9.61708181e-02 7.16896946e-02 1.08714611e-01 1.18536709e-01 8.52229628e-02 4.13523715e-01 7.71194281e-01 1.73738798e-01 3.05406909e-01 1.53831064e-01 2.06911408e-01 1.13075512e-01 1.29416734e-01 1.60275533e-01 2.29962628e-01 2.50895646e-01 1.73060658e-01 2.01020670e-01 3.16227457e-01 1.57652647e-01 5.47188384e-02 2.61436808e-01 1.46570523e-01 1.58977569e-01 2.11215119e-01 1.40679855e-01 -8.00696326e-02 1.59842103e-01 2.00211820e-01 9.92221921e-02 -1.91516176e-02 -5.02510162e-02 -9.15402427e-02 4.28019215e-02 1.06537078e-01 -3.24195486e-01 1.79861627e-02 -1.29900711e-01 -1.18627679e-01 -1.26903307e-01 -1.12941251e-01 2.81344485e-01 -5.75519167e-02 1.62155727e-02 2.14084866e-01 2.05315240e-01 1.27598359e-01 1.89025252e-01 3.96820478e-01 1.20290515e-01 3.32130996e-01 1.37858897e-01 9.78393589e-02 3.51731323e-01 1.10782088e-01 2.27390210e-01 3.89559348e-01 1.74184808e-01 3.08568571e-01 1.71747215e-01 2.33275587e-01 2.56728635e-01 3.02423314e-01 2.74374851e-01 3.27629705e-02 5.61005655e-02 1.68330538e-01 1.12578506e-01 1.08314409e-02 1.33944964e-01 -2.12285231e-01 -1.21224032e-01 1.07819533e-01 3.17613330e-02 2.84300351e-01 -1.58586907e-01 1.36753020e-01 1.26197635e-01 7.40448636e-02 2.35065994e-01 -6.15319415e-02 -7.51966701e-02 4.13427726e-01 1.60539980e-01 1.09901498e-01 1.74329568e-01 1.48135527e-01 1.85728609e-01 2.85476612e-01 2.24898461e-01 1.33343564e-01 1.80618963e-01 2.03080820e-02 2.16728570e-01 1.86566493e-01 1.25929822e-01 1.79317565e-01 3.88162321e-01 2.03009067e-01 2.64872648e-01 4.95978731e-01 1.52347749e-01 -7.23596372e-02 1.29552280e-01 6.16496157e-02 1.05956924e-01 -2.71699836e-01 -5.64473565e-03 -2.50275527e-02 1.29269950e-01 -1.87247727e-01 -3.49347255e-01 -1.93280406e-01 7.87217542e-02 2.21951811e-01 7.10999656e-02 3.49382049e-02 1.48398799e-01 5.65517753e-02 1.05690961e-01 2.55476023e-01 1.28401889e-01 1.33289903e-01 1.14201836e-01 1.43169893e-01 5.69591438e-01 1.54755202e-01 1.55028578e-01 1.64827975e-01 4.67083700e-01 3.31029661e-02 1.62382617e-01 1.54156022e-01 6.55873722e-01 -5.31208735e-02 2.37122763e-01 2.71368392e-01 4.69144223e-01 1.62923984e-01 1.22718216e-01 1.68055251e-01 1.35999904e-01 2.04736813e-01 1.27146904e-01 -1.12549423e-01 3.24840692e-03 7.10375441e-02 7.90146006e-03 5.79775663e-02 -1.57867224e-01 1.33194074e-01 1.11364361e-01 1.95665062e-01 5.57144416e-02 -6.22623725e-02 2.59366443e-01 1.96512306e-02 -2.47699823e-02 3.37429602e-01 1.84628626e-01 2.42417229e-01 1.88852778e-01 2.10930109e-01 2.10416004e-01 2.81527817e-01 5.45666352e-01 1.85856370e-01 4.88939364e-01 1.29308220e-01 1.30534366e-01 4.31600221e-01 1.42478827e-01 1.11633119e-01 1.45026679e-01 2.79724659e-01 3.33422150e-01 4.92846588e-01 1.88026032e-01 4.35734950e-01 1.29765005e-01 1.36498013e-01 1.27056277e-01 2.39063615e-01 -1.49002763e-01 2.00230923e-02 1.23378339e-01 6.12350194e-02 -1.57952580e-01 5.93742728e-02 -6.88460761e-03 7.48854198e-02 6.45607765e-02 8.47908994e-03 2.15403273e-01 6.38359483e-02 -6.30232436e-04 4.09513551e-01 3.59478228e-01 1.15102395e-01 1.56907967e-01 1.60361237e-01 3.16259692e-01 4.37763243e-01 1.82457530e-01 3.12791208e-01 1.59771151e-01 -6.63636501e-02 3.37363422e-01 2.58858115e-01 1.81217734e-01 3.73234115e-02 1.44936318e-01 3.16879135e-01 4.73967251e-01 2.43696316e-01 2.73749525e-01 2.46270449e-02 2.27465471e-01 1.71915626e-01 6.96528119e-02 1.51926333e-01 1.91790172e-01 -1.70457889e-01 1.94258861e-02 1.05929285e-01 2.46869777e-01 -6.42981449e-03 1.22480623e-01 1.27650832e-01 1.23734951e-01 2.01582021e-01 7.66321281e-02 1.25943788e-01 -5.22321249e-02 2.95908687e-01 3.44925520e-01 1.07812252e-01 1.15365733e-01 2.13185926e-01 1.29626595e-01 4.15526961e-01 1.23294607e-01 1.45059294e-01 1.81411556e-01 1.06561684e-01 1.20626826e-01 2.19538968e-01 3.16034720e-01 9.72365601e-02 1.83261409e-01 1.47228661e-01 1.57946602e-01 3.83712037e-01 1.36031656e-01 3.75214905e-02 1.97768668e-02 3.06073435e-02 -1.01445936e-01 1.41457346e-01 4.89799924e-02 1.35908206e-01 2.95765484e-02 1.34596792e-01 -2.45031560e-01 9.09800159e-02 -2.80465423e-02 4.60956009e-03 4.76391647e-02 9.71343281e-02 6.71838252e-02 -1.45994631e-02 -5.39188915e-02 2.79919933e-01 2.31919186e-01 1.12801182e-01 1.13704532e-01 4.26356671e-01 1.90428244e-01 1.10496872e-01 3.31699294e-01 1.36443699e-01 1.97119264e-01 -5.57694684e-03 1.11270325e-01 4.61516648e-01 2.68630982e-01 1.00774945e-01 1.41438672e-01 3.97197924e-01 1.92009640e-01 1.34873803e-01 2.20134800e-01 1.11572142e-01 2.04669213e-02 2.21970350e-01 -1.13088611e-01 2.39645009e-01 2.70424952e-01 2.65250470e-01 7.79145265e-02 4.09394578e-03 -2.78502700e-01 -1.88647588e-02 -8.11508107e-02 2.05797599e-01 1.58278762e-01 -1.59274599e-01 4.31328198e-01 1.05097241e-01 1.85069899e-01] After you've completed the previous step, turn this into a list comprehension and pass the result to a variable called margins # complete the code below margins = [ j [ 'EBITDA/KG' ] . values for i , j in gp ] Remember the list unpacking we did for the tic tac toe project? We're going to do the same thing here. Unpack the margins list for median_test and run the cell below! # complete the following line stat , p , m , table = scipy . stats . median_test ( * margins , correction = False ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.2e} \" . format ( p )) print ( \"the grand median: {:.2e} \" . format ( m )) The pearsons chi-square test statistic: 448.81 p-value of the test: 8.85e-95 the grand median: 2.16e-01 Part B View the distributions of the data using matplotlib and seaborn \u00b6 What a fantastic statistical result we found! Can we affirm our result with some visualizations? I hope so! Create a boxplot below using pandas. In your call to df.boxplot() the by parameter should be set to Base Cake and the column parameter should be set to EBITDA/KG # YOUR BOXPLOT HERE fig , ax = plt . subplots ( figsize = ( 10 , 5 )) df . boxplot ( by = 'Base Cake' , column = 'EBITDA/KG' , ax = ax ) <AxesSubplot:title={'center':'EBITDA/KG'}, xlabel='Base Cake'> For comparison, I've shown the boxplot below using seaborn! fig , ax = plt . subplots ( figsize = ( 10 , 7 )) ax = sns . boxplot ( x = 'Base Cake' , y = 'EBITDA/KG' , data = df , color = '#A0cbe8' ) Part C Perform Moods Median on all the other groups \u00b6 # Recall the other descriptors we have descriptors Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') for desc in descriptors : # YOUR CODE FORM MARGINS BELOW gp = df . groupby ( desc ) margins = [ j [ 'EBITDA/KG' ] . values for i , j in gp ] # UNPACK MARGINS INTO MEDIAN_TEST stat , p , m , table = scipy . stats . median_test ( * margins , correction = False ) print ( desc ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:e} \" . format ( p )) print ( \"the grand median: {} \" . format ( m ), end = ' \\n\\n ' ) Base Cake The pearsons chi-square test statistic: 448.81 p-value of the test: 8.851450e-95 the grand median: 0.21604872880760184 Truffle Type The pearsons chi-square test statistic: 22.86 p-value of the test: 1.088396e-05 the grand median: 0.21604872880760184 Primary Flavor The pearsons chi-square test statistic: 638.99 p-value of the test: 3.918933e-103 the grand median: 0.21604872880760184 Secondary Flavor The pearsons chi-square test statistic: 323.13 p-value of the test: 6.083210e-52 the grand median: 0.21604872880760184 Color Group The pearsons chi-square test statistic: 175.18 p-value of the test: 1.011412e-31 the grand median: 0.21604872880760184 Customer The pearsons chi-square test statistic: 5.66 p-value of the test: 2.257760e-01 the grand median: 0.21604872880760184 Date The pearsons chi-square test statistic: 5.27 p-value of the test: 9.175929e-01 the grand median: 0.21604872880760184 Part D Many boxplots \u00b6 And finally, we will confirm these visually. Complete the Boxplot for each group: for desc in descriptors : fig , ax = plt . subplots ( figsize = ( 10 , 5 )) sns . boxplot ( x = desc , y = 'EBITDA/KG' , data = df , color = '#A0cbe8' , ax = ax ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 9 ( ) missing from current font. fig.canvas.print_figure(bytes_io, **kw) 2.1.3 What is a T-test? \u00b6 There are 1-sample and 2-sample T-tests (note: we would use a 1-sample T-test just to determine if the sample mean is equal to a hypothesized population mean) Within 2-sample T-tests we have independent and dependent T-tests (uncorrelated or correlated samples) For independent, two-sample T-tests: Equal variance (or pooled) T-test scipy.stats.ttest_ind(equal_var=True) Unequal variance T-test scipy.stats.ttest_ind(equal_var=False) also called Welch's T-test For dependent T-tests: * Paired (or correlated) T-test * scipy.stats.ttest_rel A full discussion on T-tests is outside the scope of this session, but we can refer to wikipedia for more information, including formulas on how each statistic is computed: * student's T-test 2.1.3.1 Demonstration of T-tests \u00b6 back to top We'll assume our shifts are of equal variance and proceed with the appropriate independent two-sample T-test... print ( shift_one ) print ( shift_two ) [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] To calculate the T-test, we follow a slightly different statistical formula: $T=\\frac{\\mu_1 - \\mu_2}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$ where $\\mu$ are the means of the two groups, $n$ are the sample sizes and $s$ is the pooled standard deviation, also known as the cummulative variance (depending on if you square it or not): $s= \\sqrt{\\frac{(n_1-1)\\sigma_1^2 + (n_2-1)\\sigma_2^2}{n_1 + n_2 - 2}}$ where $\\sigma$ are the standard deviations. What you'll notice here is we are combining the two variances, we can only do this if we assume the variances are somewhat equal, this is known as the equal variances t-test. mean_shift_one = np . mean ( shift_one ) mean_shift_two = np . mean ( shift_two ) print ( mean_shift_one , mean_shift_two ) 16.3 20.1 com_var = (( np . sum ([( i - mean_shift_one ) ** 2 for i in shift_one ]) + np . sum ([( i - mean_shift_two ) ** 2 for i in shift_two ])) / ( len ( shift_one ) + len ( shift_two ) - 2 )) print ( com_var ) 6.5 T = ( np . abs ( mean_shift_one - mean_shift_two ) / ( np . sqrt ( com_var / len ( shift_one ) + com_var / len ( shift_two )))) T 3.3328204733667115 We see that this hand-computed result matches that of the scipy module: scipy . stats . ttest_ind ( shift_two , shift_one , equal_var = True ) Ttest_indResult(statistic=3.3328204733667115, pvalue=0.0037029158660758575) 2.1.4 What are F-statistics and the F-test? \u00b6 The F-statistic is simply a ratio of two variances, or the ratio of mean squares mean squares is the estimate of population variance that accounts for the degrees of freedom to compute that estimate. We will explore this in the context of ANOVA 2.1.4.1 What is Analysis of Variance? \u00b6 ANOVA uses the F-test to determine whether the variability between group means is larger than the variability within the groups. If that statistic is large enough, you can conclude that the means of the groups are not equal. The caveat is that ANOVA tells us whether there is a difference in means but it does not tell us where the difference is. To find where the difference is between the groups, we have to conduct post-hoc tests. There are two main types: * One-way (one factor) and * Two-way (two factor) where factor is an independent variable Ind A Ind B Dep X H 10 X I 12 Y I 11 Y H 20 ANOVA Hypotheses \u00b6 Null hypothesis : group means are equal Alternative hypothesis : at least one group mean is different from the other groups ANOVA Assumptions \u00b6 Residuals (experimental error) are normally distributed (test with Shapiro-Wilk) Homogeneity of variances (variances are equal between groups) (test with Bartlett's) Observations are sampled independently from each other Note: ANOVA assumptions can be checked using test statistics (e.g. Shapiro-Wilk, Bartlett\u2019s, Levene\u2019s test) and the visual approaches such as residual plots (e.g. QQ-plots) and histograms. Steps for ANOVA \u00b6 Check sample sizes: equal observations must be in each group Calculate Sum of Square between groups and within groups (\\(SS_B, SS_E\\)) Calculate Mean Square between groups and within groups (\\(MS_B, MS_E\\)) Calculate F value (\\(MS_B/MS_E\\)) This might be easier to see in a table: Source of Variation degree of freedom (Df) Sum of squares (SS) Mean square (MS) F value Between Groups Df_b = P-1 SS_B MS_B = SS_B / Df_B MS_B / MS_E Within Groups Df_E = P(N-1) SS_E MS_E = SS_E / Df_E total Df_T = PN-1 SS_T Where: SS_B = \\sum_{i}^{P}{(\\bar{y}_i-\\bar{y})^2} SS_E = \\sum_{ik}^{PN}{(\\bar{y}_{ik}-\\bar{y}_i)^2} SS_T = SS_B + SS_E Let's go back to our shift data to take an example: shifts = pd . DataFrame ([ shift_one , shift_two , shift_three , shift_four ]) . T shifts . columns = [ 'A' , 'B' , 'C' , 'D' ] shifts . boxplot () <AxesSubplot:> 2.1.4.2 SNS Boxplot \u00b6 this is another great way to view boxplot data. Notice how sns also shows us the raw data alongside the box and whiskers using a swarmplot . shift_melt = pd . melt ( shifts . reset_index (), id_vars = [ 'index' ], value_vars = [ 'A' , 'B' , 'C' , 'D' ]) shift_melt . columns = [ 'index' , 'shift' , 'rate' ] ax = sns . boxplot ( x = 'shift' , y = 'rate' , data = shift_melt , color = '#A0cbe8' ) ax = sns . swarmplot ( x = \"shift\" , y = \"rate\" , data = shift_melt , color = '#79706e' ) Anyway back to ANOVA... fvalue , pvalue = stats . f_oneway ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( fvalue , pvalue ) 5.599173553719008 0.0029473487978665873 We can get this in the format of the table we saw above: # get ANOVA table import statsmodels.api as sm from statsmodels.formula.api import ols # Ordinary Least Squares (OLS) model model = ols ( 'rate ~ C(shift)' , data = shift_melt ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(shift) 135.5 3.0 5.599174 0.002947 Residual 290.4 36.0 NaN NaN The Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9750654697418213 0.5121709108352661 We can use Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. w , pvalue = stats . bartlett ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( w , pvalue ) 1.3763632854696672 0.711084540821183 2.1.4.3 ANOVA Interpretation \u00b6 The p value form ANOVA analysis is significant ( p < 0.05) and we can conclude there are significant difference between the shifts. But we do not know which shift(s) are different. For this we need to perform a post hoc test. There are a multitude of these that are beyond the scope of this discussion ( Tukey-kramer is one such test) 2.1.5 Putting it all together \u00b6 In summary, there are many statistical tests at our disposal when performing inferential statistical analysis. In times like these, a simple decision tree can be extraordinarily useful! source: scribbr \ud83c\udf52 2.2 Enrichment: Evaluate statistical significance of product margin: a snake in the garden \u00b6 2.2.1 Mood's Median on product descriptors \u00b6 The first issue we run into with moods is... what? We can only perform moods on two groups at a time. How can we get around this? Let's take a look at the category with the fewest descriptors. If we remember, this was the Truffle Types. df . columns Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date', 'KG', 'EBITDA/KG'], dtype='object') df [ 'Truffle Type' ] . unique () array(['Candy Outer', 'Chocolate Outer', 'Jelly Filled'], dtype=object) col = 'Truffle Type' moodsdf = pd . DataFrame () for truff in df [ col ] . unique (): # for each group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) print ( \" {} : N= {} \" . format ( truff , size )) print ( \"Welch's T-Test for Unequal Variances\" ) print ( scipy . stats . ttest_ind ( group , pop , equal_var = False )) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue print () moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ col , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] Candy Outer: N=288 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=-2.7615297773427474, pvalue=0.0059110489226580736) Chocolate Outer: N=1356 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=4.409449025092911, pvalue=1.1932685612874952e-05) Jelly Filled: N=24 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=-8.414252306793504, pvalue=7.929912531660087e-09) \ud83d\ude4b\u200d\u2640\ufe0f Question 1: Moods Results on Truffle Type \u00b6 What do we notice about the resultant table? p-values Most are quite small (really low probability of achieving these table results under a single distribution) group sizes: our Jelly Filled group is relatively small moodsdf . sort_values ( 'p_value' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Truffle Type pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Jelly Filled 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 0 Chocolate Outer 6.627496 0.010042 0.216049 0.262601 0.225562 1356 0.000012 [[699, 135], [657, 177]] 0 Candy Outer 1.515066 0.218368 0.216049 0.230075 0.204264 288 0.005911 [[134, 700], [154, 680]] We can go ahead and repeat this analysis for all of our product categories: df . columns [: 5 ] Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group'], dtype='object') moodsdf = pd . DataFrame () for col in df . columns [: 5 ]: for truff in df [ col ] . unique (): group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] print ( moodsdf . shape ) (101, 10) moodsdf = moodsdf . loc [( moodsdf [ 'welch p' ] < 0.005 ) & ( moodsdf [ 'p_value' ] < 0.005 )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( moodsdf . shape ) (51, 10) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Secondary Flavor Papaya 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 1 Primary Flavor Orange Pineapple\\tP 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 2 Primary Flavor Cherry Cream Spice 10.156401 0.001438 0.216049 0.018702 0.009701 12 0.000001 [[0, 834], [12, 822]] 3 Secondary Flavor Cucumber 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 4 Truffle Type Jelly Filled 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 5 Primary Flavor Orange 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 6 Primary Flavor Toasted Coconut 15.261253 0.000094 0.216049 0.037002 0.028392 24 0.0 [[2, 832], [22, 812]] 7 Secondary Flavor Apricot 15.261253 0.000094 0.216049 0.060312 0.037422 24 0.0 [[2, 832], [22, 812]] 8 Primary Flavor Kettle Corn 29.062065 0.0 0.216049 0.055452 0.045891 60 0.0 [[9, 825], [51, 783]] 9 Primary Flavor Acai Berry 18.643248 0.000016 0.216049 0.036505 0.049466 24 0.0 [[1, 833], [23, 811]] 10 Primary Flavor Pink Lemonade 10.156401 0.001438 0.216049 0.039862 0.056349 12 0.000011 [[0, 834], [12, 822]] 11 Secondary Flavor Black Cherry 58.900366 0.0 0.216049 0.055975 0.062898 96 0.0 [[11, 823], [85, 749]] 12 Primary Flavor Watermelon 15.261253 0.000094 0.216049 0.04405 0.067896 24 0.0 [[2, 832], [22, 812]] 13 Primary Flavor Plum 34.851608 0.0 0.216049 0.084963 0.079993 72 0.0 [[11, 823], [61, 773]] 14 Secondary Flavor Dill Pickle 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 15 Primary Flavor Horchata 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 16 Primary Flavor Lemon Custard 12.217457 0.000473 0.216049 0.079389 0.087969 24 0.000006 [[3, 831], [21, 813]] 17 Primary Flavor Fruit Punch 10.156401 0.001438 0.216049 0.078935 0.090326 12 0.000076 [[0, 834], [12, 822]] 18 Base Cake Chiffon 117.046226 0.0 0.216049 0.127851 0.125775 288 0.0 [[60, 774], [228, 606]] 19 Base Cake Butter 134.36727 0.0 0.216049 0.142082 0.139756 456 0.0 [[122, 712], [334, 500]] 20 Secondary Flavor Banana 10.805348 0.001012 0.216049 0.163442 0.15537 60 0.0 [[17, 817], [43, 791]] 21 Primary Flavor Cream Soda 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 22 Secondary Flavor Peppermint 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 23 Primary Flavor Grand Mariner 10.581767 0.001142 0.216049 0.197463 0.165529 72 0.000829 [[22, 812], [50, 784]] 24 Color Group Amethyst 20.488275 0.000006 0.216049 0.195681 0.167321 300 0.0 [[114, 720], [186, 648]] 25 Color Group Burgundy 10.999677 0.000911 0.216049 0.193048 0.171465 120 0.000406 [[42, 792], [78, 756]] 26 Color Group White 35.76526 0.0 0.216049 0.19 0.177264 432 0.0 [[162, 672], [270, 564]] 27 Color Group Opal 11.587164 0.000664 0.216049 0.317878 0.259304 324 0.0 [[190, 644], [134, 700]] 28 Secondary Flavor Apple 27.283292 0.0 0.216049 0.326167 0.293876 36 0.001176 [[34, 800], [2, 832]] 29 Secondary Flavor Tangerine 32.626389 0.0 0.216049 0.342314 0.319273 48 0.000113 [[44, 790], [4, 830]] 30 Secondary Flavor Black Currant 34.778391 0.0 0.216049 0.357916 0.332449 36 0.0 [[36, 798], [0, 834]] 31 Secondary Flavor Pear 16.614303 0.000046 0.216049 0.373034 0.33831 60 0.000031 [[46, 788], [14, 820]] 32 Primary Flavor Vanilla 34.778391 0.0 0.216049 0.378053 0.341626 36 0.000001 [[36, 798], [0, 834]] 33 Color Group Citrine 10.156401 0.001438 0.216049 0.390728 0.342512 12 0.001925 [[12, 822], [0, 834]] 34 Color Group Teal 13.539679 0.000234 0.216049 0.323955 0.3446 96 0.00121 [[66, 768], [30, 804]] 35 Base Cake Tiramisu 52.360619 0.0 0.216049 0.388267 0.362102 144 0.0 [[114, 720], [30, 804]] 36 Primary Flavor Doughnut 74.935256 0.0 0.216049 0.439721 0.379361 108 0.0 [[98, 736], [10, 824]] 37 Secondary Flavor Ginger Beer 22.363443 0.000002 0.216049 0.444895 0.382283 24 0.000481 [[24, 810], [0, 834]] 38 Color Group Rose 18.643248 0.000016 0.216049 0.42301 0.407061 24 0.000062 [[23, 811], [1, 833]] 39 Base Cake Cheese 66.804744 0.0 0.216049 0.450934 0.435638 84 0.0 [[79, 755], [5, 829]] 40 Primary Flavor Butter Toffee 60.181468 0.0 0.216049 0.50366 0.456343 60 0.0 [[60, 774], [0, 834]] 41 Color Group Slate 10.156401 0.001438 0.216049 0.540214 0.483138 12 0.000017 [[12, 822], [0, 834]] 42 Primary Flavor Gingersnap 22.363443 0.000002 0.216049 0.643218 0.623627 24 0.0 [[24, 810], [0, 834]] 43 Primary Flavor Dill Pickle 22.363443 0.000002 0.216049 0.642239 0.655779 24 0.0 [[24, 810], [0, 834]] 44 Color Group Olive 44.967537 0.0 0.216049 0.637627 0.670186 60 0.0 [[56, 778], [4, 830]] 45 Primary Flavor Butter Milk 10.156401 0.001438 0.216049 0.699284 0.688601 12 0.0 [[12, 822], [0, 834]] 46 Base Cake Sponge 127.156266 0.0 0.216049 0.698996 0.699355 120 0.0 [[120, 714], [0, 834]] 47 Primary Flavor Chocolate Mint 10.156401 0.001438 0.216049 0.685546 0.699666 12 0.0 [[12, 822], [0, 834]] 48 Primary Flavor Coconut 10.156401 0.001438 0.216049 0.732777 0.717641 12 0.0 [[12, 822], [0, 834]] 49 Primary Flavor Blueberry 22.363443 0.000002 0.216049 0.759643 0.72536 24 0.0 [[24, 810], [0, 834]] 50 Primary Flavor Amaretto 10.156401 0.001438 0.216049 0.782156 0.764845 12 0.0 [[12, 822], [0, 834]] \ud83c\udf52\ud83c\udf52 2.2.2 Enrichment : Broad Analysis of Categories: ANOVA \u00b6 Recall our \"melted\" shift data. It will be useful to think of getting our Truffle data in this format: shift_melt . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index shift rate 0 0 A 11 1 1 A 14 2 2 A 15 3 3 A 16 4 4 A 16 df . columns = df . columns . str . replace ( ' ' , '_' ) df . columns = df . columns . str . replace ( '/' , '_' ) # get ANOVA table # Ordinary Least Squares (OLS) model model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Recall the Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9576056599617004 1.2598073820281984e-21 And the Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. gb = df . groupby ( 'Truffle_Type' )[ 'EBITDA_KG' ] gb <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fb6940a7370> w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( w , pvalue ) 109.93252546442552 1.344173733366234e-24 Wow it looks like our data is not drawn from a normal distribution! Let's check this for other categories... We can wrap these in a for loop: for col in df . columns [: 5 ]: print ( col ) model = ols ( 'EBITDA_KG ~ C( {} )' . format ( col ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) w , pvalue = stats . shapiro ( model . resid ) print ( \"Shapiro: \" , w , pvalue ) gb = df . groupby ( col )[ 'EBITDA_KG' ] w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( \"Bartlett: \" , w , pvalue ) print () Base_Cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Base_Cake) 39.918103 5.0 314.869955 1.889884e-237 Residual 42.140500 1662.0 NaN NaN Shapiro: 0.9634131193161011 4.1681337029688696e-20 Bartlett: 69.83288886114286 1.1102218566048873e-13 Truffle_Type .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Shapiro: 0.9576056599617004 1.2598073820281984e-21 Bartlett: 109.93252546442552 1.344173733366234e-24 Primary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Primary_Flavor) 50.270639 50.0 51.143649 1.153434e-292 Residual 31.787964 1617.0 NaN NaN Shapiro: 0.948470413684845 9.90281706784179e-24 Bartlett: 210.15130419114982 1.5872504991225816e-21 Secondary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Secondary_Flavor) 15.088382 28.0 13.188089 1.929302e-54 Residual 66.970220 1639.0 NaN NaN Shapiro: 0.9548103213310242 2.649492974953278e-22 Bartlett: 420.6274502894812 1.2373007035089195e-71 Color_Group .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Color_Group) 16.079685 11.0 36.689347 6.544980e-71 Residual 65.978918 1656.0 NaN NaN Shapiro: 0.969061017036438 1.8926407335144587e-18 Bartlett: 136.55525281340468 8.164787784033709e-24 \ud83c\udf52\ud83c\udf52 2.2.3 Enrichment : Visual Analysis of Residuals: QQ-Plots \u00b6 This can be distressing and is often why we want visual methods to see what is going on with our data! model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () #create instance of influence influence = model . get_influence () #obtain standardized residuals standardized_residuals = influence . resid_studentized_internal # res.anova_std_residuals are standardized residuals obtained from ANOVA (check above) sm . qqplot ( standardized_residuals , line = '45' ) plt . xlabel ( \"Theoretical Quantiles\" ) plt . ylabel ( \"Standardized Residuals\" ) plt . show () # histogram plt . hist ( model . resid , bins = 'auto' , histtype = 'bar' , ec = 'k' ) plt . xlabel ( \"Residuals\" ) plt . ylabel ( 'Frequency' ) plt . show () We see that a lot of our data is swayed by extremely high and low values, so what can we conclude? You need the right test statistic for the right job, in this case, we are littered with unequal variance in our groupings so we use the moods median and welch (unequal variance t-test) to make conclusions about our data References \u00b6 Renesh Bedre ANOVA Minitab ANOVA Analytics Vidhya ANOVA Renesh Bedre Hypothesis Testing Real Statistics Turkey-kramer Mutual Information","title":"SOLN S2 Inferential Statistics"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#data-science-foundations-session-2-inferential-statistics","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at the utility of EDA combined with inferential statistics.","title":"Data Science Foundations, Session 2: Inferential Statistics"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#20-preparing-environment-and-importing-data","text":"back to top","title":"2.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#201-import-packages","text":"back to top # The modules we've seen before import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns # our stats modules import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy","title":"2.0.1 Import Packages"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#202-load-dataset","text":"back to top For this session, we will use dummy datasets from sklearn. df = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... ... ... ... ... ... 1663 Tiramisu Chocolate Outer Doughnut Pear Amethyst Fickelgruber 12/2020 38128.802589 0.420111 1664 Tiramisu Chocolate Outer Doughnut Pear Burgundy Zebrabar 12/2020 108.642857 0.248659 1665 Tiramisu Chocolate Outer Doughnut Pear Teal Zebrabar 12/2020 3517.933333 0.378501 1666 Tiramisu Chocolate Outer Doughnut Rock and Rye Amethyst Slugworth 12/2020 10146.898432 0.213149 1667 Tiramisu Chocolate Outer Doughnut Rock and Rye Burgundy Zebrabar 12/2020 1271.904762 0.431813 1668 rows \u00d7 9 columns descriptors = df . columns [: - 2 ] for col in df . columns [: - 2 ]: print ( col ) print ( df [ col ] . unique ()) print () Base Cake ['Butter' 'Cheese' 'Chiffon' 'Pound' 'Sponge' 'Tiramisu'] Truffle Type ['Candy Outer' 'Chocolate Outer' 'Jelly Filled'] Primary Flavor ['Butter Pecan' 'Ginger Lime' 'Margarita' 'Pear' 'Pink Lemonade' 'Raspberry Ginger Ale' 'Sassafras' 'Spice' 'Wild Cherry Cream' 'Cream Soda' 'Horchata' 'Kettle Corn' 'Lemon Bar' 'Orange Pineapple\\tP' 'Plum' 'Orange' 'Butter Toffee' 'Lemon' 'Acai Berry' 'Apricot' 'Birch Beer' 'Cherry Cream Spice' 'Creme de Menthe' 'Fruit Punch' 'Ginger Ale' 'Grand Mariner' 'Orange Brandy' 'Pecan' 'Toasted Coconut' 'Watermelon' 'Wintergreen' 'Vanilla' 'Bavarian Cream' 'Black Licorice' 'Caramel Cream' 'Cheesecake' 'Cherry Cola' 'Coffee' 'Irish Cream' 'Lemon Custard' 'Mango' 'Sour' 'Amaretto' 'Blueberry' 'Butter Milk' 'Chocolate Mint' 'Coconut' 'Dill Pickle' 'Gingersnap' 'Chocolate' 'Doughnut'] Secondary Flavor ['Toffee' 'Banana' 'Rum' 'Tutti Frutti' 'Vanilla' 'Mixed Berry' 'Whipped Cream' 'Apricot' 'Passion Fruit' 'Peppermint' 'Dill Pickle' 'Black Cherry' 'Wild Cherry Cream' 'Papaya' 'Mango' 'Cucumber' 'Egg Nog' 'Pear' 'Rock and Rye' 'Tangerine' 'Apple' 'Black Currant' 'Kiwi' 'Lemon' 'Hazelnut' 'Butter Rum' 'Fuzzy Navel' 'Mojito' 'Ginger Beer'] Color Group ['Taupe' 'Amethyst' 'Burgundy' 'White' 'Black' 'Opal' 'Citrine' 'Rose' 'Slate' 'Teal' 'Tiffany' 'Olive'] Customer ['Slugworth' 'Perk-a-Cola' 'Fickelgruber' 'Zebrabar' \"Dandy's Candies\"] Date ['1/2020' '2/2020' '3/2020' '4/2020' '5/2020' '6/2020' '7/2020' '8/2020' '9/2020' '10/2020' '11/2020' '12/2020']","title":"2.0.2 Load Dataset"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#21-many-flavors-of-statistical-tests","text":"https://luminousmen.com/post/descriptive-and-inferential-statistics Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (\u201cinferences\u201d) from that data. With inferential statistics, you take data from samples and make generalizations about a population statshowto Moods Median Test Kruskal-Wallis Test (Another comparison of Medians test) T-Test Analysis of Variance (ANOVA) One Way ANOVA Two Way ANOVA MANOVA Factorial ANOVA When do I use each of these? We will talk about this as we proceed through the examples. This page from minitab has good rules of thumb on the subject.","title":"2.1 Many Flavors of Statistical Tests"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#211-what-is-moods-median","text":"You can use Chi-Square to test for a goodness of fit (whether a sample of data represents a distribution) or whether two variables are related (using a contingency table, which we will create below!) A special case of Pearon's Chi-Squared Test: We create a table that counts the observations above and below the global median for two different groups. We then perform a chi-squared test of significance on this contingency table Null hypothesis: the Medians are all equal The chi-square test statistic: x^2 = \\sum{\\frac{(O-E)^2}{E}} Where \\(O\\) is the observed frequency and \\(E\\) is the expected frequency. Let's take an example , say we have three shifts with the following production rates: np . random . seed ( 7 ) shift_one = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_two = [ round ( i ) for i in np . random . normal ( 21 , 3 , 10 )] print ( shift_one ) print ( shift_two ) [21, 15, 16, 17, 14, 16, 16, 11, 19, 18] [19, 20, 23, 20, 20, 17, 23, 21, 22, 16] stat , p , m , table = scipy . stats . median_test ( shift_one , shift_two , correction = False ) what is median_test returning? print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 7.20 p-value of the test: 0.007 the grand median: 18.5 Let's evaluate that test statistic ourselves by taking a look at the contingency table: table array([[2, 8], [8, 2]]) This is easier to make sense of if we order the shift times shift_one . sort () shift_one [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] When we look at shift one, we see that 8 values are at or below the grand median. shift_two . sort () shift_two [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] For shift two, only two are at or below the grand median. Since the sample sizes are the same, the expected value for both groups is the same, 5 above and 5 below the grand median. The chi-square is then: X^2 = \\frac{(2-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(2-5)^2}{5} ( 3 - 5 ) ** 2 / 5 + ( 7 - 5 ) ** 2 / 5 + ( 7 - 5 ) ** 2 / 5 + ( 3 - 5 ) ** 2 / 5 3.2 Our p-value, or the probability of observing the null-hypothsis, is under 0.05. We can conclude that these shift performances were drawn under seperate distributions. For comparison, let's do this analysis again with shifts of equal performances np . random . seed ( 3 ) shift_three = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_four = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] stat , p , m , table = scipy . stats . median_test ( shift_three , shift_four , correction = False ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000 the grand median: 15.5 and the shift raw values: shift_three . sort () shift_four . sort () print ( shift_three ) print ( shift_four ) [10, 14, 15, 15, 15, 16, 16, 16, 17, 21] [11, 12, 13, 14, 15, 16, 19, 19, 19, 21] table array([[5, 5], [5, 5]])","title":"2.1.1 What is Mood's Median?"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#212-when-to-use-moods","text":"Mood's Median Test is highly flexible but has the following assumptions: Considers only one categorical factor Response variable is continuous (our shift rates) Data does not need to be normally distributed But the distributions are similarly shaped Sample sizes can be unequal and small (less than 20 observations) Other considerations: Not as powerful as Kruskal-Wallis Test but still useful for small sample sizes or when there are outliers","title":"2.1.2 When to Use Mood's?"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#exercise-1-use-moods-median-test","text":"","title":"\ud83c\udfcb\ufe0f Exercise 1: Use Mood's Median Test"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#part-a-perform-moods-median-test-on-base-cake-categorical-variable-and-ebitdakg-continuous-variable-in-truffle-data","text":"We're also going to get some practice with pandas groupby. # what is returned by this groupby? gp = df . groupby ( 'Base Cake' ) How do we find out? We could iterate through it: # seems to be a tuple of some sort for i in gp : print ( i ) break ('Butter', Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns]) # the first object appears to be the group print ( i [ 0 ]) # the second object appears to be the df belonging to that group print ( i [ 1 ]) Butter Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns] going back to our diagram from our earlier pandas session. It looks like whenever we split in the groupby method, we create separate dataframes as well as their group label: Ok, so we know gp is separate dataframes. How do we turn the 'EBITDA/KG' column of each dataframe into arrays to then pass to median_test ? # complete this for loop for i , j in gp : print ( i ) print ( j [ 'EBITDA/KG' ] . values ) break # turn 'EBITDA/KG' of j into an array using the .values attribute # print this to the screen Butter [ 5.00423594e-01 2.20395451e-01 1.71013869e-01 2.33024872e-01 4.80689371e-01 1.64934546e-01 2.03213256e-01 1.78681400e-01 1.25050726e-01 2.17021951e-01 7.95955185e-02 3.25042287e-01 2.17551215e-01 2.48152299e-01 -1.20503094e-02 1.47190567e-01 3.84488948e-01 2.05438764e-01 1.32190256e-01 3.23019144e-01 -9.73361477e-03 1.98397692e-01 1.67067902e-01 -2.60063690e-02 1.30365325e-01 2.36337749e-01 -9.70556780e-02 1.59051819e-01 -8.76572259e-02 -3.32199843e-02 -5.05704451e-02 -5.56458806e-02 -8.86273564e-02 4.32267857e-02 -1.88615579e-01 4.24939227e-01 9.35136847e-02 -3.43605950e-02 1.63823520e-01 2.78522916e-01 1.29207730e-01 1.79194495e-01 1.37419569e-01 1.31372653e-01 2.53275225e-01 2.26761431e-01 1.10173466e-01 1.99338787e-01 -2.01250197e-01 1.16567591e-01 1.32324984e-01 4.02912418e-01 9.35051765e-02 1.65865814e-01 2.12269112e-01 2.53461571e-01 1.89055713e-01 1.20416365e-01 3.95276612e-02 2.93121770e-01 1.40947082e-01 -1.21555832e-01 1.56455622e-01 -1.29776953e-02 -6.17934014e-02 -8.19904808e-02 -3.14711557e-02 -8.03820228e-02 1.63839981e-01 8.34406336e-02 1.49369698e-01 1.05990633e-01 1.27399979e-01 2.26634255e-01 -2.20801929e-03 -6.92044284e-02 1.74048414e-01 1.30933438e-01 1.27620323e-01 2.78652749e-01 2.14772018e-01 1.40864278e-01 1.23745138e-01 1.66586809e-01 2.91940995e-01 2.49925584e-01 8.65447719e-02 3.80907774e-01 2.70851719e-01 3.32946265e-01 9.00795862e-03 2.00960974e-01 2.72623570e-01 3.35902190e-01 1.27337723e-01 2.36618545e-01 -6.82774785e-02 3.13166906e-01 2.15752651e-01 9.29694447e-02 3.60809152e-02 2.32488112e-01 3.38200308e-02 1.70916188e-01 2.81620452e-01 -1.61981289e-01 -4.14570666e-02 1.13465970e-02 2.28733252e-01 9.87516565e-02 3.52732668e-02 6.32598661e-02 2.10300526e-01 1.98761726e-01 1.38832882e-01 2.95465366e-01 2.68022024e-01 3.22389724e-01 4.04867623e-01 2.38086167e-01 1.12586985e-01 1.94010438e-01 1.96757297e-01 1.65215620e-01 1.22730941e-02 1.14415249e-01 3.26252563e-01 1.89080695e-01 -5.11830382e-02 2.41661008e-01 2.00063672e-01 3.07633312e-01 4.20740234e-01 1.34764192e-01 -4.75993730e-02 1.52973888e-02 1.87709908e-01 7.20193743e-02 3.48745346e-02 2.77659158e-01 2.73466257e-01 1.32419725e-01 2.85933859e-02 3.99622870e-02 -7.46829380e-02 9.03915641e-02 -9.61708181e-02 7.16896946e-02 1.08714611e-01 1.18536709e-01 8.52229628e-02 4.13523715e-01 7.71194281e-01 1.73738798e-01 3.05406909e-01 1.53831064e-01 2.06911408e-01 1.13075512e-01 1.29416734e-01 1.60275533e-01 2.29962628e-01 2.50895646e-01 1.73060658e-01 2.01020670e-01 3.16227457e-01 1.57652647e-01 5.47188384e-02 2.61436808e-01 1.46570523e-01 1.58977569e-01 2.11215119e-01 1.40679855e-01 -8.00696326e-02 1.59842103e-01 2.00211820e-01 9.92221921e-02 -1.91516176e-02 -5.02510162e-02 -9.15402427e-02 4.28019215e-02 1.06537078e-01 -3.24195486e-01 1.79861627e-02 -1.29900711e-01 -1.18627679e-01 -1.26903307e-01 -1.12941251e-01 2.81344485e-01 -5.75519167e-02 1.62155727e-02 2.14084866e-01 2.05315240e-01 1.27598359e-01 1.89025252e-01 3.96820478e-01 1.20290515e-01 3.32130996e-01 1.37858897e-01 9.78393589e-02 3.51731323e-01 1.10782088e-01 2.27390210e-01 3.89559348e-01 1.74184808e-01 3.08568571e-01 1.71747215e-01 2.33275587e-01 2.56728635e-01 3.02423314e-01 2.74374851e-01 3.27629705e-02 5.61005655e-02 1.68330538e-01 1.12578506e-01 1.08314409e-02 1.33944964e-01 -2.12285231e-01 -1.21224032e-01 1.07819533e-01 3.17613330e-02 2.84300351e-01 -1.58586907e-01 1.36753020e-01 1.26197635e-01 7.40448636e-02 2.35065994e-01 -6.15319415e-02 -7.51966701e-02 4.13427726e-01 1.60539980e-01 1.09901498e-01 1.74329568e-01 1.48135527e-01 1.85728609e-01 2.85476612e-01 2.24898461e-01 1.33343564e-01 1.80618963e-01 2.03080820e-02 2.16728570e-01 1.86566493e-01 1.25929822e-01 1.79317565e-01 3.88162321e-01 2.03009067e-01 2.64872648e-01 4.95978731e-01 1.52347749e-01 -7.23596372e-02 1.29552280e-01 6.16496157e-02 1.05956924e-01 -2.71699836e-01 -5.64473565e-03 -2.50275527e-02 1.29269950e-01 -1.87247727e-01 -3.49347255e-01 -1.93280406e-01 7.87217542e-02 2.21951811e-01 7.10999656e-02 3.49382049e-02 1.48398799e-01 5.65517753e-02 1.05690961e-01 2.55476023e-01 1.28401889e-01 1.33289903e-01 1.14201836e-01 1.43169893e-01 5.69591438e-01 1.54755202e-01 1.55028578e-01 1.64827975e-01 4.67083700e-01 3.31029661e-02 1.62382617e-01 1.54156022e-01 6.55873722e-01 -5.31208735e-02 2.37122763e-01 2.71368392e-01 4.69144223e-01 1.62923984e-01 1.22718216e-01 1.68055251e-01 1.35999904e-01 2.04736813e-01 1.27146904e-01 -1.12549423e-01 3.24840692e-03 7.10375441e-02 7.90146006e-03 5.79775663e-02 -1.57867224e-01 1.33194074e-01 1.11364361e-01 1.95665062e-01 5.57144416e-02 -6.22623725e-02 2.59366443e-01 1.96512306e-02 -2.47699823e-02 3.37429602e-01 1.84628626e-01 2.42417229e-01 1.88852778e-01 2.10930109e-01 2.10416004e-01 2.81527817e-01 5.45666352e-01 1.85856370e-01 4.88939364e-01 1.29308220e-01 1.30534366e-01 4.31600221e-01 1.42478827e-01 1.11633119e-01 1.45026679e-01 2.79724659e-01 3.33422150e-01 4.92846588e-01 1.88026032e-01 4.35734950e-01 1.29765005e-01 1.36498013e-01 1.27056277e-01 2.39063615e-01 -1.49002763e-01 2.00230923e-02 1.23378339e-01 6.12350194e-02 -1.57952580e-01 5.93742728e-02 -6.88460761e-03 7.48854198e-02 6.45607765e-02 8.47908994e-03 2.15403273e-01 6.38359483e-02 -6.30232436e-04 4.09513551e-01 3.59478228e-01 1.15102395e-01 1.56907967e-01 1.60361237e-01 3.16259692e-01 4.37763243e-01 1.82457530e-01 3.12791208e-01 1.59771151e-01 -6.63636501e-02 3.37363422e-01 2.58858115e-01 1.81217734e-01 3.73234115e-02 1.44936318e-01 3.16879135e-01 4.73967251e-01 2.43696316e-01 2.73749525e-01 2.46270449e-02 2.27465471e-01 1.71915626e-01 6.96528119e-02 1.51926333e-01 1.91790172e-01 -1.70457889e-01 1.94258861e-02 1.05929285e-01 2.46869777e-01 -6.42981449e-03 1.22480623e-01 1.27650832e-01 1.23734951e-01 2.01582021e-01 7.66321281e-02 1.25943788e-01 -5.22321249e-02 2.95908687e-01 3.44925520e-01 1.07812252e-01 1.15365733e-01 2.13185926e-01 1.29626595e-01 4.15526961e-01 1.23294607e-01 1.45059294e-01 1.81411556e-01 1.06561684e-01 1.20626826e-01 2.19538968e-01 3.16034720e-01 9.72365601e-02 1.83261409e-01 1.47228661e-01 1.57946602e-01 3.83712037e-01 1.36031656e-01 3.75214905e-02 1.97768668e-02 3.06073435e-02 -1.01445936e-01 1.41457346e-01 4.89799924e-02 1.35908206e-01 2.95765484e-02 1.34596792e-01 -2.45031560e-01 9.09800159e-02 -2.80465423e-02 4.60956009e-03 4.76391647e-02 9.71343281e-02 6.71838252e-02 -1.45994631e-02 -5.39188915e-02 2.79919933e-01 2.31919186e-01 1.12801182e-01 1.13704532e-01 4.26356671e-01 1.90428244e-01 1.10496872e-01 3.31699294e-01 1.36443699e-01 1.97119264e-01 -5.57694684e-03 1.11270325e-01 4.61516648e-01 2.68630982e-01 1.00774945e-01 1.41438672e-01 3.97197924e-01 1.92009640e-01 1.34873803e-01 2.20134800e-01 1.11572142e-01 2.04669213e-02 2.21970350e-01 -1.13088611e-01 2.39645009e-01 2.70424952e-01 2.65250470e-01 7.79145265e-02 4.09394578e-03 -2.78502700e-01 -1.88647588e-02 -8.11508107e-02 2.05797599e-01 1.58278762e-01 -1.59274599e-01 4.31328198e-01 1.05097241e-01 1.85069899e-01] After you've completed the previous step, turn this into a list comprehension and pass the result to a variable called margins # complete the code below margins = [ j [ 'EBITDA/KG' ] . values for i , j in gp ] Remember the list unpacking we did for the tic tac toe project? We're going to do the same thing here. Unpack the margins list for median_test and run the cell below! # complete the following line stat , p , m , table = scipy . stats . median_test ( * margins , correction = False ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.2e} \" . format ( p )) print ( \"the grand median: {:.2e} \" . format ( m )) The pearsons chi-square test statistic: 448.81 p-value of the test: 8.85e-95 the grand median: 2.16e-01","title":"Part A Perform moods median test on Base Cake (Categorical Variable) and EBITDA/KG (Continuous Variable) in Truffle data"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#part-b-view-the-distributions-of-the-data-using-matplotlib-and-seaborn","text":"What a fantastic statistical result we found! Can we affirm our result with some visualizations? I hope so! Create a boxplot below using pandas. In your call to df.boxplot() the by parameter should be set to Base Cake and the column parameter should be set to EBITDA/KG # YOUR BOXPLOT HERE fig , ax = plt . subplots ( figsize = ( 10 , 5 )) df . boxplot ( by = 'Base Cake' , column = 'EBITDA/KG' , ax = ax ) <AxesSubplot:title={'center':'EBITDA/KG'}, xlabel='Base Cake'> For comparison, I've shown the boxplot below using seaborn! fig , ax = plt . subplots ( figsize = ( 10 , 7 )) ax = sns . boxplot ( x = 'Base Cake' , y = 'EBITDA/KG' , data = df , color = '#A0cbe8' )","title":"Part B View the distributions of the data using matplotlib and seaborn"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#part-c-perform-moods-median-on-all-the-other-groups","text":"# Recall the other descriptors we have descriptors Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') for desc in descriptors : # YOUR CODE FORM MARGINS BELOW gp = df . groupby ( desc ) margins = [ j [ 'EBITDA/KG' ] . values for i , j in gp ] # UNPACK MARGINS INTO MEDIAN_TEST stat , p , m , table = scipy . stats . median_test ( * margins , correction = False ) print ( desc ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:e} \" . format ( p )) print ( \"the grand median: {} \" . format ( m ), end = ' \\n\\n ' ) Base Cake The pearsons chi-square test statistic: 448.81 p-value of the test: 8.851450e-95 the grand median: 0.21604872880760184 Truffle Type The pearsons chi-square test statistic: 22.86 p-value of the test: 1.088396e-05 the grand median: 0.21604872880760184 Primary Flavor The pearsons chi-square test statistic: 638.99 p-value of the test: 3.918933e-103 the grand median: 0.21604872880760184 Secondary Flavor The pearsons chi-square test statistic: 323.13 p-value of the test: 6.083210e-52 the grand median: 0.21604872880760184 Color Group The pearsons chi-square test statistic: 175.18 p-value of the test: 1.011412e-31 the grand median: 0.21604872880760184 Customer The pearsons chi-square test statistic: 5.66 p-value of the test: 2.257760e-01 the grand median: 0.21604872880760184 Date The pearsons chi-square test statistic: 5.27 p-value of the test: 9.175929e-01 the grand median: 0.21604872880760184","title":"Part C Perform Moods Median on all the other groups"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#part-d-many-boxplots","text":"And finally, we will confirm these visually. Complete the Boxplot for each group: for desc in descriptors : fig , ax = plt . subplots ( figsize = ( 10 , 5 )) sns . boxplot ( x = desc , y = 'EBITDA/KG' , data = df , color = '#A0cbe8' , ax = ax ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 9 ( ) missing from current font. fig.canvas.print_figure(bytes_io, **kw)","title":"Part D Many boxplots"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#213-what-is-a-t-test","text":"There are 1-sample and 2-sample T-tests (note: we would use a 1-sample T-test just to determine if the sample mean is equal to a hypothesized population mean) Within 2-sample T-tests we have independent and dependent T-tests (uncorrelated or correlated samples) For independent, two-sample T-tests: Equal variance (or pooled) T-test scipy.stats.ttest_ind(equal_var=True) Unequal variance T-test scipy.stats.ttest_ind(equal_var=False) also called Welch's T-test For dependent T-tests: * Paired (or correlated) T-test * scipy.stats.ttest_rel A full discussion on T-tests is outside the scope of this session, but we can refer to wikipedia for more information, including formulas on how each statistic is computed: * student's T-test","title":"2.1.3 What is a T-test?"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#2131-demonstration-of-t-tests","text":"back to top We'll assume our shifts are of equal variance and proceed with the appropriate independent two-sample T-test... print ( shift_one ) print ( shift_two ) [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] To calculate the T-test, we follow a slightly different statistical formula: $T=\\frac{\\mu_1 - \\mu_2}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$ where $\\mu$ are the means of the two groups, $n$ are the sample sizes and $s$ is the pooled standard deviation, also known as the cummulative variance (depending on if you square it or not): $s= \\sqrt{\\frac{(n_1-1)\\sigma_1^2 + (n_2-1)\\sigma_2^2}{n_1 + n_2 - 2}}$ where $\\sigma$ are the standard deviations. What you'll notice here is we are combining the two variances, we can only do this if we assume the variances are somewhat equal, this is known as the equal variances t-test. mean_shift_one = np . mean ( shift_one ) mean_shift_two = np . mean ( shift_two ) print ( mean_shift_one , mean_shift_two ) 16.3 20.1 com_var = (( np . sum ([( i - mean_shift_one ) ** 2 for i in shift_one ]) + np . sum ([( i - mean_shift_two ) ** 2 for i in shift_two ])) / ( len ( shift_one ) + len ( shift_two ) - 2 )) print ( com_var ) 6.5 T = ( np . abs ( mean_shift_one - mean_shift_two ) / ( np . sqrt ( com_var / len ( shift_one ) + com_var / len ( shift_two )))) T 3.3328204733667115 We see that this hand-computed result matches that of the scipy module: scipy . stats . ttest_ind ( shift_two , shift_one , equal_var = True ) Ttest_indResult(statistic=3.3328204733667115, pvalue=0.0037029158660758575)","title":"2.1.3.1 Demonstration of T-tests"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#214-what-are-f-statistics-and-the-f-test","text":"The F-statistic is simply a ratio of two variances, or the ratio of mean squares mean squares is the estimate of population variance that accounts for the degrees of freedom to compute that estimate. We will explore this in the context of ANOVA","title":"2.1.4 What are F-statistics and the F-test?"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#2141-what-is-analysis-of-variance","text":"ANOVA uses the F-test to determine whether the variability between group means is larger than the variability within the groups. If that statistic is large enough, you can conclude that the means of the groups are not equal. The caveat is that ANOVA tells us whether there is a difference in means but it does not tell us where the difference is. To find where the difference is between the groups, we have to conduct post-hoc tests. There are two main types: * One-way (one factor) and * Two-way (two factor) where factor is an independent variable Ind A Ind B Dep X H 10 X I 12 Y I 11 Y H 20","title":"2.1.4.1 What is Analysis of Variance?"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#anova-hypotheses","text":"Null hypothesis : group means are equal Alternative hypothesis : at least one group mean is different from the other groups","title":"ANOVA Hypotheses"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#anova-assumptions","text":"Residuals (experimental error) are normally distributed (test with Shapiro-Wilk) Homogeneity of variances (variances are equal between groups) (test with Bartlett's) Observations are sampled independently from each other Note: ANOVA assumptions can be checked using test statistics (e.g. Shapiro-Wilk, Bartlett\u2019s, Levene\u2019s test) and the visual approaches such as residual plots (e.g. QQ-plots) and histograms.","title":"ANOVA Assumptions"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#steps-for-anova","text":"Check sample sizes: equal observations must be in each group Calculate Sum of Square between groups and within groups (\\(SS_B, SS_E\\)) Calculate Mean Square between groups and within groups (\\(MS_B, MS_E\\)) Calculate F value (\\(MS_B/MS_E\\)) This might be easier to see in a table: Source of Variation degree of freedom (Df) Sum of squares (SS) Mean square (MS) F value Between Groups Df_b = P-1 SS_B MS_B = SS_B / Df_B MS_B / MS_E Within Groups Df_E = P(N-1) SS_E MS_E = SS_E / Df_E total Df_T = PN-1 SS_T Where: SS_B = \\sum_{i}^{P}{(\\bar{y}_i-\\bar{y})^2} SS_E = \\sum_{ik}^{PN}{(\\bar{y}_{ik}-\\bar{y}_i)^2} SS_T = SS_B + SS_E Let's go back to our shift data to take an example: shifts = pd . DataFrame ([ shift_one , shift_two , shift_three , shift_four ]) . T shifts . columns = [ 'A' , 'B' , 'C' , 'D' ] shifts . boxplot () <AxesSubplot:>","title":"Steps for ANOVA"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#2142-sns-boxplot","text":"this is another great way to view boxplot data. Notice how sns also shows us the raw data alongside the box and whiskers using a swarmplot . shift_melt = pd . melt ( shifts . reset_index (), id_vars = [ 'index' ], value_vars = [ 'A' , 'B' , 'C' , 'D' ]) shift_melt . columns = [ 'index' , 'shift' , 'rate' ] ax = sns . boxplot ( x = 'shift' , y = 'rate' , data = shift_melt , color = '#A0cbe8' ) ax = sns . swarmplot ( x = \"shift\" , y = \"rate\" , data = shift_melt , color = '#79706e' ) Anyway back to ANOVA... fvalue , pvalue = stats . f_oneway ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( fvalue , pvalue ) 5.599173553719008 0.0029473487978665873 We can get this in the format of the table we saw above: # get ANOVA table import statsmodels.api as sm from statsmodels.formula.api import ols # Ordinary Least Squares (OLS) model model = ols ( 'rate ~ C(shift)' , data = shift_melt ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(shift) 135.5 3.0 5.599174 0.002947 Residual 290.4 36.0 NaN NaN The Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9750654697418213 0.5121709108352661 We can use Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. w , pvalue = stats . bartlett ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( w , pvalue ) 1.3763632854696672 0.711084540821183","title":"2.1.4.2 SNS Boxplot"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#2143-anova-interpretation","text":"The p value form ANOVA analysis is significant ( p < 0.05) and we can conclude there are significant difference between the shifts. But we do not know which shift(s) are different. For this we need to perform a post hoc test. There are a multitude of these that are beyond the scope of this discussion ( Tukey-kramer is one such test)","title":"2.1.4.3 ANOVA Interpretation"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#215-putting-it-all-together","text":"In summary, there are many statistical tests at our disposal when performing inferential statistical analysis. In times like these, a simple decision tree can be extraordinarily useful! source: scribbr","title":"2.1.5 Putting it all together"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#22-enrichment-evaluate-statistical-significance-of-product-margin-a-snake-in-the-garden","text":"","title":"\ud83c\udf52 2.2 Enrichment: Evaluate statistical significance of product margin: a snake in the garden"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#221-moods-median-on-product-descriptors","text":"The first issue we run into with moods is... what? We can only perform moods on two groups at a time. How can we get around this? Let's take a look at the category with the fewest descriptors. If we remember, this was the Truffle Types. df . columns Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date', 'KG', 'EBITDA/KG'], dtype='object') df [ 'Truffle Type' ] . unique () array(['Candy Outer', 'Chocolate Outer', 'Jelly Filled'], dtype=object) col = 'Truffle Type' moodsdf = pd . DataFrame () for truff in df [ col ] . unique (): # for each group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) print ( \" {} : N= {} \" . format ( truff , size )) print ( \"Welch's T-Test for Unequal Variances\" ) print ( scipy . stats . ttest_ind ( group , pop , equal_var = False )) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue print () moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ col , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] Candy Outer: N=288 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=-2.7615297773427474, pvalue=0.0059110489226580736) Chocolate Outer: N=1356 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=4.409449025092911, pvalue=1.1932685612874952e-05) Jelly Filled: N=24 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=-8.414252306793504, pvalue=7.929912531660087e-09)","title":"2.2.1 Mood's Median on product descriptors"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#question-1-moods-results-on-truffle-type","text":"What do we notice about the resultant table? p-values Most are quite small (really low probability of achieving these table results under a single distribution) group sizes: our Jelly Filled group is relatively small moodsdf . sort_values ( 'p_value' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Truffle Type pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Jelly Filled 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 0 Chocolate Outer 6.627496 0.010042 0.216049 0.262601 0.225562 1356 0.000012 [[699, 135], [657, 177]] 0 Candy Outer 1.515066 0.218368 0.216049 0.230075 0.204264 288 0.005911 [[134, 700], [154, 680]] We can go ahead and repeat this analysis for all of our product categories: df . columns [: 5 ] Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group'], dtype='object') moodsdf = pd . DataFrame () for col in df . columns [: 5 ]: for truff in df [ col ] . unique (): group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] print ( moodsdf . shape ) (101, 10) moodsdf = moodsdf . loc [( moodsdf [ 'welch p' ] < 0.005 ) & ( moodsdf [ 'p_value' ] < 0.005 )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( moodsdf . shape ) (51, 10) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Secondary Flavor Papaya 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 1 Primary Flavor Orange Pineapple\\tP 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 2 Primary Flavor Cherry Cream Spice 10.156401 0.001438 0.216049 0.018702 0.009701 12 0.000001 [[0, 834], [12, 822]] 3 Secondary Flavor Cucumber 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 4 Truffle Type Jelly Filled 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 5 Primary Flavor Orange 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 6 Primary Flavor Toasted Coconut 15.261253 0.000094 0.216049 0.037002 0.028392 24 0.0 [[2, 832], [22, 812]] 7 Secondary Flavor Apricot 15.261253 0.000094 0.216049 0.060312 0.037422 24 0.0 [[2, 832], [22, 812]] 8 Primary Flavor Kettle Corn 29.062065 0.0 0.216049 0.055452 0.045891 60 0.0 [[9, 825], [51, 783]] 9 Primary Flavor Acai Berry 18.643248 0.000016 0.216049 0.036505 0.049466 24 0.0 [[1, 833], [23, 811]] 10 Primary Flavor Pink Lemonade 10.156401 0.001438 0.216049 0.039862 0.056349 12 0.000011 [[0, 834], [12, 822]] 11 Secondary Flavor Black Cherry 58.900366 0.0 0.216049 0.055975 0.062898 96 0.0 [[11, 823], [85, 749]] 12 Primary Flavor Watermelon 15.261253 0.000094 0.216049 0.04405 0.067896 24 0.0 [[2, 832], [22, 812]] 13 Primary Flavor Plum 34.851608 0.0 0.216049 0.084963 0.079993 72 0.0 [[11, 823], [61, 773]] 14 Secondary Flavor Dill Pickle 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 15 Primary Flavor Horchata 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 16 Primary Flavor Lemon Custard 12.217457 0.000473 0.216049 0.079389 0.087969 24 0.000006 [[3, 831], [21, 813]] 17 Primary Flavor Fruit Punch 10.156401 0.001438 0.216049 0.078935 0.090326 12 0.000076 [[0, 834], [12, 822]] 18 Base Cake Chiffon 117.046226 0.0 0.216049 0.127851 0.125775 288 0.0 [[60, 774], [228, 606]] 19 Base Cake Butter 134.36727 0.0 0.216049 0.142082 0.139756 456 0.0 [[122, 712], [334, 500]] 20 Secondary Flavor Banana 10.805348 0.001012 0.216049 0.163442 0.15537 60 0.0 [[17, 817], [43, 791]] 21 Primary Flavor Cream Soda 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 22 Secondary Flavor Peppermint 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 23 Primary Flavor Grand Mariner 10.581767 0.001142 0.216049 0.197463 0.165529 72 0.000829 [[22, 812], [50, 784]] 24 Color Group Amethyst 20.488275 0.000006 0.216049 0.195681 0.167321 300 0.0 [[114, 720], [186, 648]] 25 Color Group Burgundy 10.999677 0.000911 0.216049 0.193048 0.171465 120 0.000406 [[42, 792], [78, 756]] 26 Color Group White 35.76526 0.0 0.216049 0.19 0.177264 432 0.0 [[162, 672], [270, 564]] 27 Color Group Opal 11.587164 0.000664 0.216049 0.317878 0.259304 324 0.0 [[190, 644], [134, 700]] 28 Secondary Flavor Apple 27.283292 0.0 0.216049 0.326167 0.293876 36 0.001176 [[34, 800], [2, 832]] 29 Secondary Flavor Tangerine 32.626389 0.0 0.216049 0.342314 0.319273 48 0.000113 [[44, 790], [4, 830]] 30 Secondary Flavor Black Currant 34.778391 0.0 0.216049 0.357916 0.332449 36 0.0 [[36, 798], [0, 834]] 31 Secondary Flavor Pear 16.614303 0.000046 0.216049 0.373034 0.33831 60 0.000031 [[46, 788], [14, 820]] 32 Primary Flavor Vanilla 34.778391 0.0 0.216049 0.378053 0.341626 36 0.000001 [[36, 798], [0, 834]] 33 Color Group Citrine 10.156401 0.001438 0.216049 0.390728 0.342512 12 0.001925 [[12, 822], [0, 834]] 34 Color Group Teal 13.539679 0.000234 0.216049 0.323955 0.3446 96 0.00121 [[66, 768], [30, 804]] 35 Base Cake Tiramisu 52.360619 0.0 0.216049 0.388267 0.362102 144 0.0 [[114, 720], [30, 804]] 36 Primary Flavor Doughnut 74.935256 0.0 0.216049 0.439721 0.379361 108 0.0 [[98, 736], [10, 824]] 37 Secondary Flavor Ginger Beer 22.363443 0.000002 0.216049 0.444895 0.382283 24 0.000481 [[24, 810], [0, 834]] 38 Color Group Rose 18.643248 0.000016 0.216049 0.42301 0.407061 24 0.000062 [[23, 811], [1, 833]] 39 Base Cake Cheese 66.804744 0.0 0.216049 0.450934 0.435638 84 0.0 [[79, 755], [5, 829]] 40 Primary Flavor Butter Toffee 60.181468 0.0 0.216049 0.50366 0.456343 60 0.0 [[60, 774], [0, 834]] 41 Color Group Slate 10.156401 0.001438 0.216049 0.540214 0.483138 12 0.000017 [[12, 822], [0, 834]] 42 Primary Flavor Gingersnap 22.363443 0.000002 0.216049 0.643218 0.623627 24 0.0 [[24, 810], [0, 834]] 43 Primary Flavor Dill Pickle 22.363443 0.000002 0.216049 0.642239 0.655779 24 0.0 [[24, 810], [0, 834]] 44 Color Group Olive 44.967537 0.0 0.216049 0.637627 0.670186 60 0.0 [[56, 778], [4, 830]] 45 Primary Flavor Butter Milk 10.156401 0.001438 0.216049 0.699284 0.688601 12 0.0 [[12, 822], [0, 834]] 46 Base Cake Sponge 127.156266 0.0 0.216049 0.698996 0.699355 120 0.0 [[120, 714], [0, 834]] 47 Primary Flavor Chocolate Mint 10.156401 0.001438 0.216049 0.685546 0.699666 12 0.0 [[12, 822], [0, 834]] 48 Primary Flavor Coconut 10.156401 0.001438 0.216049 0.732777 0.717641 12 0.0 [[12, 822], [0, 834]] 49 Primary Flavor Blueberry 22.363443 0.000002 0.216049 0.759643 0.72536 24 0.0 [[24, 810], [0, 834]] 50 Primary Flavor Amaretto 10.156401 0.001438 0.216049 0.782156 0.764845 12 0.0 [[12, 822], [0, 834]]","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: Moods Results on Truffle Type"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#222-enrichment-broad-analysis-of-categories-anova","text":"Recall our \"melted\" shift data. It will be useful to think of getting our Truffle data in this format: shift_melt . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index shift rate 0 0 A 11 1 1 A 14 2 2 A 15 3 3 A 16 4 4 A 16 df . columns = df . columns . str . replace ( ' ' , '_' ) df . columns = df . columns . str . replace ( '/' , '_' ) # get ANOVA table # Ordinary Least Squares (OLS) model model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Recall the Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9576056599617004 1.2598073820281984e-21 And the Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. gb = df . groupby ( 'Truffle_Type' )[ 'EBITDA_KG' ] gb <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fb6940a7370> w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( w , pvalue ) 109.93252546442552 1.344173733366234e-24 Wow it looks like our data is not drawn from a normal distribution! Let's check this for other categories... We can wrap these in a for loop: for col in df . columns [: 5 ]: print ( col ) model = ols ( 'EBITDA_KG ~ C( {} )' . format ( col ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) w , pvalue = stats . shapiro ( model . resid ) print ( \"Shapiro: \" , w , pvalue ) gb = df . groupby ( col )[ 'EBITDA_KG' ] w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( \"Bartlett: \" , w , pvalue ) print () Base_Cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Base_Cake) 39.918103 5.0 314.869955 1.889884e-237 Residual 42.140500 1662.0 NaN NaN Shapiro: 0.9634131193161011 4.1681337029688696e-20 Bartlett: 69.83288886114286 1.1102218566048873e-13 Truffle_Type .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Shapiro: 0.9576056599617004 1.2598073820281984e-21 Bartlett: 109.93252546442552 1.344173733366234e-24 Primary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Primary_Flavor) 50.270639 50.0 51.143649 1.153434e-292 Residual 31.787964 1617.0 NaN NaN Shapiro: 0.948470413684845 9.90281706784179e-24 Bartlett: 210.15130419114982 1.5872504991225816e-21 Secondary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Secondary_Flavor) 15.088382 28.0 13.188089 1.929302e-54 Residual 66.970220 1639.0 NaN NaN Shapiro: 0.9548103213310242 2.649492974953278e-22 Bartlett: 420.6274502894812 1.2373007035089195e-71 Color_Group .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Color_Group) 16.079685 11.0 36.689347 6.544980e-71 Residual 65.978918 1656.0 NaN NaN Shapiro: 0.969061017036438 1.8926407335144587e-18 Bartlett: 136.55525281340468 8.164787784033709e-24","title":"\ud83c\udf52\ud83c\udf52 2.2.2 Enrichment: Broad Analysis of Categories: ANOVA"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#223-enrichment-visual-analysis-of-residuals-qq-plots","text":"This can be distressing and is often why we want visual methods to see what is going on with our data! model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () #create instance of influence influence = model . get_influence () #obtain standardized residuals standardized_residuals = influence . resid_studentized_internal # res.anova_std_residuals are standardized residuals obtained from ANOVA (check above) sm . qqplot ( standardized_residuals , line = '45' ) plt . xlabel ( \"Theoretical Quantiles\" ) plt . ylabel ( \"Standardized Residuals\" ) plt . show () # histogram plt . hist ( model . resid , bins = 'auto' , histtype = 'bar' , ec = 'k' ) plt . xlabel ( \"Residuals\" ) plt . ylabel ( 'Frequency' ) plt . show () We see that a lot of our data is swayed by extremely high and low values, so what can we conclude? You need the right test statistic for the right job, in this case, we are littered with unequal variance in our groupings so we use the moods median and welch (unequal variance t-test) to make conclusions about our data","title":"\ud83c\udf52\ud83c\udf52 2.2.3 Enrichment: Visual Analysis of Residuals: QQ-Plots"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#references","text":"Renesh Bedre ANOVA Minitab ANOVA Analytics Vidhya ANOVA Renesh Bedre Hypothesis Testing Real Statistics Turkey-kramer Mutual Information","title":"References"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/","text":"Data Science Foundations, Session 3: Model Selection and Validation \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com At the end of session 1, we saw the basic recipe for creating a supervised machine learning model: Environment setup and importing data Rudimentary exploratory data analysis Feature engineering (these were created for us) Choosing and training a model: choose model choose hyperparameters fit using (training) data predict using (validation) data In session 1, I chose our model and hyperparameters preemptively. How did I do that? In the real world, you won't necessarily have the best intution about how to make these choices. In today's session, we will algorithmize the way we approach choosing and training a model Note: I will import libraries at the beginning of this notebook, as is good practice, but will reimport them as they are used to remind ourselves where each method came from! 3.0 Preparing Environment and Importing Data \u00b6 back to top 3.0.1 Import Packages \u00b6 back to top # Pandas library for the pandas dataframes import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import seaborn as sns import random import scipy.stats as stats from patsy import dmatrices from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import LabelEncoder , StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn import metrics from sklearn.metrics import r2_score , mean_squared_error from sklearn.datasets import load_iris 3.0.2 Load Dataset \u00b6 back to top In session 1 we cursorily discussed why we may need strategies for validating our model. Here we'll discuss it more in depth. I'm going to take a simple example. In the following, I have a dataset that contains some data about flowers. It's a very famous dataset used in the ML world iris = load_iris () X = iris . data y = iris . target print ( X . shape ) print ( y . shape ) (150, 4) (150,) let's go ahead and load our wine dataset as well... wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) wine . dropna ( inplace = True ) wine [ 'quality_label' ] = wine [ 'quality' ] . apply ( lambda x : 'low' if x <= 5 else 'med' if x <= 7 else 'high' ) class_tp = { 'red' : 0 , 'white' : 1 } y_tp = wine [ 'type' ] . map ( class_tp ) wine [ 'type_encoding' ] = y_tp class_ql = { 'low' : 0 , 'med' : 1 , 'high' : 2 } y_ql = wine [ 'quality_label' ] . map ( class_ql ) wine [ 'quality_encoding' ] = y_ql wine . drop ([ 'type' , 'quality_label' , 'quality' ], axis = 1 , inplace = True ) wine . columns = wine . columns . str . replace ( ' ' , '_' ) wine . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide density pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1 3.1 Model Validation \u00b6 back to top doing it the wrong way While we're here, I'm going to introduce a VERY SIMPLE supervised learning method called K-Nearest Neighbors. 3.1.0 K-Nearest Neighbors \u00b6 back to top K-Nearest Neighbors is perhaps the simplest algorithm of them all. It is essentially a lookup table: We select the hyperparameter K, and when assigning a new value a data label, assign it according to, the majority label in the vicinity of the new datapoint. The vicinity being determined by K, the number of nearest neighbors we are going to assess. knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X , y ) KNeighborsRegressor(n_neighbors=1) knn . score ( X , y ) 1.0 Wow! we achieved a model with a perfect score! But is this really how we would expect the model to perform on data it had never seen before? Probably not. How do we actually check the performance of our model? 3.1.1 Holdout Sets \u00b6 back to top The way we account for unseen data, in practice, is to leave a portion of the dataset out for testing. This way, we can estimate how our model will perform on entirely new data it may come across in application. X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.6 , random_state = 42 ) knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X_train , y_train ) print ( knn . score ( X_test , y_test )) 0.9753593429158111 We see that we get a more reasonable value for our performance! 3.1.2 Data Leakage and Cross-Validation \u00b6 back to top An even more rigorous method to leaving out a single test set, is to perform cross validation. Imagine a situation where we are trying to estimate the best value of K in our KNN algorithm. If we continually train our model with new values of K on our training set, and test with our testing set, \"knowledge\" of our test set values with leak into our model, as we choose the best value for K based on how it performs on our test set (even though we did not train on this test set). We call this phenomenon data leakage . CV or Cross Validation overcomes this by only evaluating our parameters with our training set. [image src](https://scikit-learn.org/stable/modules/cross_validation.html) In this scheme, we don't evaluate our model on the test set until the very end. Rather, we estimate our hyperparameter performances by slicing the training set into cross folds [image src](https://scikit-learn.org/stable/modules/cross_validation.html) from sklearn.model_selection import cross_val_score scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) scores array([0.91666667, 0.81725888, 0.85714286, 1. , 0.91 ]) print ( \" %0.2f accuracy with a standard deviation of %0.3f \" % ( scores . mean (), scores . std ())) 0.90 accuracy with a standard deviation of 0.062 More information on the cross_val_score method in sklearn can be found here An additional topic on cross validation is the extreme leave-one-out validation, you can read more about that here Taking these building blocks, if we wanted to for example optimize the number of nearest neighbors without incurring data leakage. We might take the following approach: for k in range ( 1 , 10 ): knn = KNeighborsRegressor ( n_neighbors = k ) scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) print ( \"k= %0.0f , %0.3f accuracy with a standard deviation of %0.3f \" % ( k , scores . mean (), scores . std ())) k=1, 0.900 accuracy with a standard deviation of 0.062 k=2, 0.925 accuracy with a standard deviation of 0.071 k=3, 0.916 accuracy with a standard deviation of 0.077 k=4, 0.932 accuracy with a standard deviation of 0.046 k=5, 0.928 accuracy with a standard deviation of 0.045 k=6, 0.924 accuracy with a standard deviation of 0.047 k=7, 0.929 accuracy with a standard deviation of 0.041 k=8, 0.924 accuracy with a standard deviation of 0.047 k=9, 0.923 accuracy with a standard deviation of 0.042 k = 4 # select best hyperparameters knn = KNeighborsRegressor ( n_neighbors = k ) knn . fit ( X_train , y_train ) # final train print ( knn . score ( X_test , y_test )) 0.9845995893223819 3.1.3 Bias-Variance Tradeoff \u00b6 back to top This next concept will be most easily understood if we go ahead an make up some data ourselves, I'm going to do that now. # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) Text(0, 0.5, 'Y') Let's fit to just a portion of this data random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] plt . plot ( X_train , y_train , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f9c34de5820>] Now let's take two extreme scenarios: fitting to these datapoints a linear line and a high order polynomial. Keeping in mind the larger dataset (the population) as well as the (irreducible) error we introduced in our data generating function will really illustrate our point! # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) # plot linear model ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) # plot polynomial model ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has high bias because we are forcing the functional form without much consideration to the underlying data \u2014 we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has low variance with respect to the underlying data. On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has low bias because we don't introduce many constraints on the final form of the model. it is high variance because depending on the underlying training data, the final outcome of the model can change quite drastically! In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further: what happens when we retrain these models on different samples of the data population and let's use this to better understand what we mean by bias and variance what happens when we tie this back in with the irreducible error we introduced to the data generator? and let's use this to better understand irreducible error # we're going to perform this task again, but this time for 5 rounds, training # on different samples of the population data random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) for samples in range ( 5 ): X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) As we can see, depending on what data we train our model on, the high bias model changes relatively slightly, while the high variance model changes a whole awful lot! The high variance model is prone to something we call overfitting . It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the population : # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance models are particuarly prone to the phenomenon of over fitting and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this. x = np . arange ( 20 , 100 ) y = [ func ( t , err = 0 ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f9c3424b970>] random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') This time, our high variance model really gets it ! And this is because the data we trained on actually is a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the bias variance tradeoff \ud83c\udfcb\ufe0f Exercise 1: Quantitatively Define Performance \u00b6 Up until now, we've explored this idea of bias variance tradeoff from a qualitative standpoint. As an exercise, continue with this idea, this time calculating the mean squared error (MSE) and R-square between the model and UNSEEN (non-training data) population data. error metrics r2_score mean_squared_error Do this for a 9 th order polynomial and repeat for population data with low, med, and high degrees of error and small and large training fractions (for 6 total comparisons). Complete the chart below. error training fraction MSE R2 0.1 .2 5.20e-03 0.97 0.1 .8 3.24e-03 0.98 0.4 .2 8.32e-02 0.65 0.4 .8 5.18e-02 0.80 0.8 .2 3.33e-01 0.08 0.8 .8 2.07e-01 0.52 # Code Cell for Exercise 1 from sklearn.preprocessing import PolynomialFeatures random . seed ( 42 ) # function to generate data def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) ################################################################################ ########## CHANGE ERR TO CHANGE THE AMOUNT OF NOISE IN YOUR DATA ############### ################################################################################ err = .8 # change the error (.1 - 0.9) y_actual = [ func ( t , err ) for t in x ] ################################################################################ ### SAMPLE THE DATA FOR TRAINING ################################################################################ train_size = 0.8 # change the training size x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = train_size , random_state = 42 ) # solving our training data with a 9-degree polynomial coefs = np . polyfit ( x_train , y_train , 9 ) # generate y data with 9-degree polynomial model and X_seq y_pred = np . polyval ( coefs , x_test ) ################################################################################ ### CALCULATE MSE AND R2 ################################################################################ mse = mean_squared_error ( y_test , y_pred ) r2 = r2_score ( y_test , y_pred ) # look at results print ( \"irreducible error: {} \" . format ( err )) print ( \"training fraction: {} \" . format ( train_size )) print ( \"mean square error: {:.2e} \" . format ( mse )) print ( \"r2: {:.2f} \" . format ( r2 )) irreducible error: 0.8 training fraction: 0.8 mean square error: 2.07e-01 r2: 0.52 3.1.4 Learning Curves \u00b6 back to top To move from qualitative to quantitative understanding of bias-variance tradeoff we need to introduce some metric for model performance. A good one to use here is R-square, a measure of the degree to which predictions match actual values. We can import a tool from sklearn to calculate this for us. from sklearn.metrics import r2_score 3.1.4.1 Considering Model Complexity \u00b6 back to top In a learning curve, we will typically plot the training and testing scores together, to give a sense of when we have either too much bias or too much variance in our model. I'm going to go ahead and recreate the original data distribution we introduced in 1.1.3 # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f9c34c91d30>] Now let's itteratively introduce more complexity into our model random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score = r2_score ( np . polyval ( coefs , X_train ), y_train ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score , ls = '' , marker = '.' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f9c34c57f40> As we see from both plots, the score on the training data increases with added model complexity. Giving us the expected perfect fit when the order is the same as the number of data points! This is part I of our learning curve. Part II consists of plotting the training data score with the testing data score. Something else I'm going to do, is define the training portion of the data as a fraction of the overall dataset . This is a typical practice in machine learning. To keep the comparisons the same as up until now, I will keep this training fraction low at .2 random . seed ( 42 ) # defining my training fraction training_frac = .2 # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) train_indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in train_indices ] test_indices = [ i for i in range ( len ( x )) if i not in train_indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) ax [ 0 ] . plot ( X_test , y_test , ls = '' , marker = '.' , color = 'grey' , alpha = 0.5 ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score1 )) ax [ 1 ] . plot ( complexity , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend ([ 'Train $R^2$' , 'Test $R^2$' ]) ax [ 0 ] . legend () <matplotlib.legend.Legend at 0x7f9c2c7c2580> As we can see, The 2 nd order polynomial achieves the greatest best test set data \\(R^2\\), while the highest order polynomial achieves the best training set data \\(R^2\\). This learning curve is explanative of what we see generally, namely a divergence after some degree of complexity between training and test set performances. In this case, we would resolve to choose the 2 nd order polynomial as the best model for our data. img src 3.1.4.2 Considering Training Set Size \u00b6 back to top The last piece of the puzzle we require, to fully cover learning curves, is the effect of training data size on the model. Let's explore. random . seed ( 42 ) # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .9 , 50 ): # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax . plot ( training_frac , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax . plot ( training_frac , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax . set_title ( \"9th-order Polynomial Score with Increasing Training Set Size\" ) ax . legend ([ 'Train' , 'Test' ]) ax . set_xlabel ( 'Training Fraction' ) ax . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$') What we see here is a trend that happens generally, as our amount of training data increases, our models handle more complexity. This is because the more data we have, the more we are able to dicipher noise from signal. Our model will ignore noise as it does not generalize well across the other datapoints. Instead, it will will fit to relationships that appear across the entire training dataset. img src \ud83c\udfcb\ufe0f Exercise 2: Visualization \u00b6 Starting with the code below, make a side-by-side plot of a 3 rd degree polynomial and a 9 th degree polynomial. On the x axis slowly increase the training set size, on the y axis plot the scores for the training and test sets. # Code Cell for Exercise 2 random . seed ( 42 ) # create the figure and axes fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .95 , 50 ): # create test and training data x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = training_frac , random_state = 42 ) # solving our training data with a n-degree polynomial coefs1 = np . polyfit ( x_train , y_train , 9 ) coefs2 = np . polyfit ( x_train , y_train , 3 ) # recording the scores for the training and test sets score1_train = r2_score ( np . polyval ( coefs1 , x_train ), y_train ) score1_test = r2_score ( np . polyval ( coefs1 , x_test ), y_test ) score2_train = r2_score ( np . polyval ( coefs2 , x_train ), y_train ) score2_test = r2_score ( np . polyval ( coefs2 , x_test ), y_test ) ax [ 0 ] . plot ( training_frac , score1_train , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax [ 0 ] . plot ( training_frac , score1_test , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax [ 1 ] . plot ( training_frac , score2_train , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax [ 1 ] . plot ( training_frac , score2_test , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax [ 0 ] . set_title ( \"9th-order Polynomial Score\" ) ax [ 1 ] . set_title ( \"3rd-order Polynomial Score\" ) ax [ 0 ] . legend ([ 'Train' , 'Test' ]) ax [ 0 ] . set_xlabel ( 'Training Fraction' ) ax [ 1 ] . set_xlabel ( 'Training Fraction' ) ax [ 0 ] . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$') \ud83d\ude4b\u200d\u2640\ufe0f Question 1: In what regions of the plots are we overfitting? \u00b6 Where in these plots is overfitting occuring? Why is it different for each polynomial? 3.2 Model Validation in Practice \u00b6 back to top We will now turn our attention to practical implementation. In practice, there are a wide number of variables (called hyperparameters) to consider when choosing a model. Scikit learn has a useful method called Grid Search that will iterate through every possible combination of a range of hyperparameter settings you provide as input. Before we get started with grid search, we'll need to switch over from our numpy polynomial fit method to one in sklearn. Here, the caveat is our actual model will solve for the coefficients infront of the polynomials. We will engineer the polynomial features ourselves. This is an example of feature engineering which we will revisit in depth in the next session. from sklearn.preprocessing import PolynomialFeatures , StandardScaler from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) 3.2.1 Grid Search \u00b6 back to top from sklearn.model_selection import GridSearchCV param_grid = { 'polynomialfeatures__degree' : np . arange ( 10 ), 'linearregression__fit_intercept' : [ True , False ]} grid = GridSearchCV ( PolynomialRegression (), param_grid , cv = 7 ) # create test and training data random . seed ( 42 ) X_train = random . sample ( list ( x ), int ( int ( len ( x )) * .8 )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] grid . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) GridSearchCV(cv=7, estimator=Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()), ('linearregression', LinearRegression())]), param_grid={'linearregression__fit_intercept': [True, False], 'polynomialfeatures__degree': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}) grid . best_params_ {'linearregression__fit_intercept': True, 'polynomialfeatures__degree': 3} to grab the best model from the CV/search outcome. we use grid.best_estimator model = grid . best_estimator_ # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 5 )) ax . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax . plot ( X_train , y_train , ls = '' , marker = '.' ) ax . set_ylim ( min ( y ), max ( y )) ax . set_title ( \"Best Grid Search CV Model\" ) Text(0.5, 1.0, 'Best Grid Search CV Model') \ud83c\udfcb\ufe0f Exercise 3: Grid Search \u00b6 There are two objectives for this exercise: Complete the object param_grid by reading the parameters below in StandardScaler . We would like to include in our grid search whether or not to scale our input features so that they are centered at 0 ( with_mean ) and have unit variance ( with_std ) After you have completed the grid search, create a side-by-side plot of actual versus predicted values for training data (left plot) and testing data (right plot) Include in your plot title the R2 for testing and R2 for training StandardScaler () . get_params () . keys () dict_keys(['copy', 'with_mean', 'with_std']) df = wine . copy () y = df . pop ( 'density' ) X = df display ( X . head ()) display ( y . head ()) print ( X . shape ) print ( y . shape ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) def NormalizedRegression ( ** kwargs ): return make_pipeline ( StandardScaler (), LinearRegression ( ** kwargs )) ###################################################################################################### ####################### YOUR EXTRA PARAMETERS GO IN THE DICTIONARY BELOW ############################# ###################################################################################################### param_grid = { 'linearregression__fit_intercept' : [ True , False ], 'standardscaler__with_mean' : [ True , False ], 'standardscaler__with_std' : [ True , False ]} grid = GridSearchCV ( NormalizedRegression (), param_grid , cv = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 0 1.0010 1 0.9940 2 0.9951 3 0.9956 4 0.9956 Name: density, dtype: float64 (6463, 12) (6463,) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) {'linearregression__fit_intercept': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True} model = grid . best_estimator_ ###################################################################################################### ##################################### ADJUST THE PLOT LOGIC BELOW #################################### ###################################################################################################### fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.962') References \u00b6 back to top Model Validation \u00b6 cross_val_score leave-one-out","title":"SOLN S3 Model Selection and Validation"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#data-science-foundations-session-3-model-selection-and-validation","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com At the end of session 1, we saw the basic recipe for creating a supervised machine learning model: Environment setup and importing data Rudimentary exploratory data analysis Feature engineering (these were created for us) Choosing and training a model: choose model choose hyperparameters fit using (training) data predict using (validation) data In session 1, I chose our model and hyperparameters preemptively. How did I do that? In the real world, you won't necessarily have the best intution about how to make these choices. In today's session, we will algorithmize the way we approach choosing and training a model Note: I will import libraries at the beginning of this notebook, as is good practice, but will reimport them as they are used to remind ourselves where each method came from!","title":"Data Science Foundations, Session 3: Model Selection and Validation"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#30-preparing-environment-and-importing-data","text":"back to top","title":"3.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#301-import-packages","text":"back to top # Pandas library for the pandas dataframes import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import seaborn as sns import random import scipy.stats as stats from patsy import dmatrices from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import LabelEncoder , StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn import metrics from sklearn.metrics import r2_score , mean_squared_error from sklearn.datasets import load_iris","title":"3.0.1 Import Packages"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#302-load-dataset","text":"back to top In session 1 we cursorily discussed why we may need strategies for validating our model. Here we'll discuss it more in depth. I'm going to take a simple example. In the following, I have a dataset that contains some data about flowers. It's a very famous dataset used in the ML world iris = load_iris () X = iris . data y = iris . target print ( X . shape ) print ( y . shape ) (150, 4) (150,) let's go ahead and load our wine dataset as well... wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) wine . dropna ( inplace = True ) wine [ 'quality_label' ] = wine [ 'quality' ] . apply ( lambda x : 'low' if x <= 5 else 'med' if x <= 7 else 'high' ) class_tp = { 'red' : 0 , 'white' : 1 } y_tp = wine [ 'type' ] . map ( class_tp ) wine [ 'type_encoding' ] = y_tp class_ql = { 'low' : 0 , 'med' : 1 , 'high' : 2 } y_ql = wine [ 'quality_label' ] . map ( class_ql ) wine [ 'quality_encoding' ] = y_ql wine . drop ([ 'type' , 'quality_label' , 'quality' ], axis = 1 , inplace = True ) wine . columns = wine . columns . str . replace ( ' ' , '_' ) wine . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide density pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1","title":"3.0.2 Load Dataset"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#31-model-validation","text":"back to top doing it the wrong way While we're here, I'm going to introduce a VERY SIMPLE supervised learning method called K-Nearest Neighbors.","title":"3.1 Model Validation"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#310-k-nearest-neighbors","text":"back to top K-Nearest Neighbors is perhaps the simplest algorithm of them all. It is essentially a lookup table: We select the hyperparameter K, and when assigning a new value a data label, assign it according to, the majority label in the vicinity of the new datapoint. The vicinity being determined by K, the number of nearest neighbors we are going to assess. knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X , y ) KNeighborsRegressor(n_neighbors=1) knn . score ( X , y ) 1.0 Wow! we achieved a model with a perfect score! But is this really how we would expect the model to perform on data it had never seen before? Probably not. How do we actually check the performance of our model?","title":"3.1.0 K-Nearest Neighbors"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#311-holdout-sets","text":"back to top The way we account for unseen data, in practice, is to leave a portion of the dataset out for testing. This way, we can estimate how our model will perform on entirely new data it may come across in application. X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.6 , random_state = 42 ) knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X_train , y_train ) print ( knn . score ( X_test , y_test )) 0.9753593429158111 We see that we get a more reasonable value for our performance!","title":"3.1.1 Holdout Sets"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#312-data-leakage-and-cross-validation","text":"back to top An even more rigorous method to leaving out a single test set, is to perform cross validation. Imagine a situation where we are trying to estimate the best value of K in our KNN algorithm. If we continually train our model with new values of K on our training set, and test with our testing set, \"knowledge\" of our test set values with leak into our model, as we choose the best value for K based on how it performs on our test set (even though we did not train on this test set). We call this phenomenon data leakage . CV or Cross Validation overcomes this by only evaluating our parameters with our training set. [image src](https://scikit-learn.org/stable/modules/cross_validation.html) In this scheme, we don't evaluate our model on the test set until the very end. Rather, we estimate our hyperparameter performances by slicing the training set into cross folds [image src](https://scikit-learn.org/stable/modules/cross_validation.html) from sklearn.model_selection import cross_val_score scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) scores array([0.91666667, 0.81725888, 0.85714286, 1. , 0.91 ]) print ( \" %0.2f accuracy with a standard deviation of %0.3f \" % ( scores . mean (), scores . std ())) 0.90 accuracy with a standard deviation of 0.062 More information on the cross_val_score method in sklearn can be found here An additional topic on cross validation is the extreme leave-one-out validation, you can read more about that here Taking these building blocks, if we wanted to for example optimize the number of nearest neighbors without incurring data leakage. We might take the following approach: for k in range ( 1 , 10 ): knn = KNeighborsRegressor ( n_neighbors = k ) scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) print ( \"k= %0.0f , %0.3f accuracy with a standard deviation of %0.3f \" % ( k , scores . mean (), scores . std ())) k=1, 0.900 accuracy with a standard deviation of 0.062 k=2, 0.925 accuracy with a standard deviation of 0.071 k=3, 0.916 accuracy with a standard deviation of 0.077 k=4, 0.932 accuracy with a standard deviation of 0.046 k=5, 0.928 accuracy with a standard deviation of 0.045 k=6, 0.924 accuracy with a standard deviation of 0.047 k=7, 0.929 accuracy with a standard deviation of 0.041 k=8, 0.924 accuracy with a standard deviation of 0.047 k=9, 0.923 accuracy with a standard deviation of 0.042 k = 4 # select best hyperparameters knn = KNeighborsRegressor ( n_neighbors = k ) knn . fit ( X_train , y_train ) # final train print ( knn . score ( X_test , y_test )) 0.9845995893223819","title":"3.1.2 Data Leakage and Cross-Validation"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#313-bias-variance-tradeoff","text":"back to top This next concept will be most easily understood if we go ahead an make up some data ourselves, I'm going to do that now. # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) Text(0, 0.5, 'Y') Let's fit to just a portion of this data random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] plt . plot ( X_train , y_train , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f9c34de5820>] Now let's take two extreme scenarios: fitting to these datapoints a linear line and a high order polynomial. Keeping in mind the larger dataset (the population) as well as the (irreducible) error we introduced in our data generating function will really illustrate our point! # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) # plot linear model ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) # plot polynomial model ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has high bias because we are forcing the functional form without much consideration to the underlying data \u2014 we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has low variance with respect to the underlying data. On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has low bias because we don't introduce many constraints on the final form of the model. it is high variance because depending on the underlying training data, the final outcome of the model can change quite drastically! In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further: what happens when we retrain these models on different samples of the data population and let's use this to better understand what we mean by bias and variance what happens when we tie this back in with the irreducible error we introduced to the data generator? and let's use this to better understand irreducible error # we're going to perform this task again, but this time for 5 rounds, training # on different samples of the population data random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) for samples in range ( 5 ): X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) As we can see, depending on what data we train our model on, the high bias model changes relatively slightly, while the high variance model changes a whole awful lot! The high variance model is prone to something we call overfitting . It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the population : # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance models are particuarly prone to the phenomenon of over fitting and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this. x = np . arange ( 20 , 100 ) y = [ func ( t , err = 0 ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f9c3424b970>] random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') This time, our high variance model really gets it ! And this is because the data we trained on actually is a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the bias variance tradeoff","title":"3.1.3 Bias-Variance Tradeoff"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#exercise-1-quantitatively-define-performance","text":"Up until now, we've explored this idea of bias variance tradeoff from a qualitative standpoint. As an exercise, continue with this idea, this time calculating the mean squared error (MSE) and R-square between the model and UNSEEN (non-training data) population data. error metrics r2_score mean_squared_error Do this for a 9 th order polynomial and repeat for population data with low, med, and high degrees of error and small and large training fractions (for 6 total comparisons). Complete the chart below. error training fraction MSE R2 0.1 .2 5.20e-03 0.97 0.1 .8 3.24e-03 0.98 0.4 .2 8.32e-02 0.65 0.4 .8 5.18e-02 0.80 0.8 .2 3.33e-01 0.08 0.8 .8 2.07e-01 0.52 # Code Cell for Exercise 1 from sklearn.preprocessing import PolynomialFeatures random . seed ( 42 ) # function to generate data def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) ################################################################################ ########## CHANGE ERR TO CHANGE THE AMOUNT OF NOISE IN YOUR DATA ############### ################################################################################ err = .8 # change the error (.1 - 0.9) y_actual = [ func ( t , err ) for t in x ] ################################################################################ ### SAMPLE THE DATA FOR TRAINING ################################################################################ train_size = 0.8 # change the training size x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = train_size , random_state = 42 ) # solving our training data with a 9-degree polynomial coefs = np . polyfit ( x_train , y_train , 9 ) # generate y data with 9-degree polynomial model and X_seq y_pred = np . polyval ( coefs , x_test ) ################################################################################ ### CALCULATE MSE AND R2 ################################################################################ mse = mean_squared_error ( y_test , y_pred ) r2 = r2_score ( y_test , y_pred ) # look at results print ( \"irreducible error: {} \" . format ( err )) print ( \"training fraction: {} \" . format ( train_size )) print ( \"mean square error: {:.2e} \" . format ( mse )) print ( \"r2: {:.2f} \" . format ( r2 )) irreducible error: 0.8 training fraction: 0.8 mean square error: 2.07e-01 r2: 0.52","title":"\ud83c\udfcb\ufe0f Exercise 1: Quantitatively Define Performance"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#314-learning-curves","text":"back to top To move from qualitative to quantitative understanding of bias-variance tradeoff we need to introduce some metric for model performance. A good one to use here is R-square, a measure of the degree to which predictions match actual values. We can import a tool from sklearn to calculate this for us. from sklearn.metrics import r2_score","title":"3.1.4 Learning Curves"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#3141-considering-model-complexity","text":"back to top In a learning curve, we will typically plot the training and testing scores together, to give a sense of when we have either too much bias or too much variance in our model. I'm going to go ahead and recreate the original data distribution we introduced in 1.1.3 # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f9c34c91d30>] Now let's itteratively introduce more complexity into our model random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score = r2_score ( np . polyval ( coefs , X_train ), y_train ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score , ls = '' , marker = '.' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f9c34c57f40> As we see from both plots, the score on the training data increases with added model complexity. Giving us the expected perfect fit when the order is the same as the number of data points! This is part I of our learning curve. Part II consists of plotting the training data score with the testing data score. Something else I'm going to do, is define the training portion of the data as a fraction of the overall dataset . This is a typical practice in machine learning. To keep the comparisons the same as up until now, I will keep this training fraction low at .2 random . seed ( 42 ) # defining my training fraction training_frac = .2 # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) train_indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in train_indices ] test_indices = [ i for i in range ( len ( x )) if i not in train_indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) ax [ 0 ] . plot ( X_test , y_test , ls = '' , marker = '.' , color = 'grey' , alpha = 0.5 ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score1 )) ax [ 1 ] . plot ( complexity , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend ([ 'Train $R^2$' , 'Test $R^2$' ]) ax [ 0 ] . legend () <matplotlib.legend.Legend at 0x7f9c2c7c2580> As we can see, The 2 nd order polynomial achieves the greatest best test set data \\(R^2\\), while the highest order polynomial achieves the best training set data \\(R^2\\). This learning curve is explanative of what we see generally, namely a divergence after some degree of complexity between training and test set performances. In this case, we would resolve to choose the 2 nd order polynomial as the best model for our data. img src","title":"3.1.4.1 Considering Model Complexity"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#3142-considering-training-set-size","text":"back to top The last piece of the puzzle we require, to fully cover learning curves, is the effect of training data size on the model. Let's explore. random . seed ( 42 ) # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .9 , 50 ): # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax . plot ( training_frac , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax . plot ( training_frac , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax . set_title ( \"9th-order Polynomial Score with Increasing Training Set Size\" ) ax . legend ([ 'Train' , 'Test' ]) ax . set_xlabel ( 'Training Fraction' ) ax . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$') What we see here is a trend that happens generally, as our amount of training data increases, our models handle more complexity. This is because the more data we have, the more we are able to dicipher noise from signal. Our model will ignore noise as it does not generalize well across the other datapoints. Instead, it will will fit to relationships that appear across the entire training dataset. img src","title":"3.1.4.2 Considering Training Set Size"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#exercise-2-visualization","text":"Starting with the code below, make a side-by-side plot of a 3 rd degree polynomial and a 9 th degree polynomial. On the x axis slowly increase the training set size, on the y axis plot the scores for the training and test sets. # Code Cell for Exercise 2 random . seed ( 42 ) # create the figure and axes fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .95 , 50 ): # create test and training data x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = training_frac , random_state = 42 ) # solving our training data with a n-degree polynomial coefs1 = np . polyfit ( x_train , y_train , 9 ) coefs2 = np . polyfit ( x_train , y_train , 3 ) # recording the scores for the training and test sets score1_train = r2_score ( np . polyval ( coefs1 , x_train ), y_train ) score1_test = r2_score ( np . polyval ( coefs1 , x_test ), y_test ) score2_train = r2_score ( np . polyval ( coefs2 , x_train ), y_train ) score2_test = r2_score ( np . polyval ( coefs2 , x_test ), y_test ) ax [ 0 ] . plot ( training_frac , score1_train , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax [ 0 ] . plot ( training_frac , score1_test , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax [ 1 ] . plot ( training_frac , score2_train , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax [ 1 ] . plot ( training_frac , score2_test , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax [ 0 ] . set_title ( \"9th-order Polynomial Score\" ) ax [ 1 ] . set_title ( \"3rd-order Polynomial Score\" ) ax [ 0 ] . legend ([ 'Train' , 'Test' ]) ax [ 0 ] . set_xlabel ( 'Training Fraction' ) ax [ 1 ] . set_xlabel ( 'Training Fraction' ) ax [ 0 ] . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$')","title":"\ud83c\udfcb\ufe0f Exercise 2: Visualization"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#question-1-in-what-regions-of-the-plots-are-we-overfitting","text":"Where in these plots is overfitting occuring? Why is it different for each polynomial?","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: In what regions of the plots are we overfitting?"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#32-model-validation-in-practice","text":"back to top We will now turn our attention to practical implementation. In practice, there are a wide number of variables (called hyperparameters) to consider when choosing a model. Scikit learn has a useful method called Grid Search that will iterate through every possible combination of a range of hyperparameter settings you provide as input. Before we get started with grid search, we'll need to switch over from our numpy polynomial fit method to one in sklearn. Here, the caveat is our actual model will solve for the coefficients infront of the polynomials. We will engineer the polynomial features ourselves. This is an example of feature engineering which we will revisit in depth in the next session. from sklearn.preprocessing import PolynomialFeatures , StandardScaler from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs ))","title":"3.2 Model Validation in Practice"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#321-grid-search","text":"back to top from sklearn.model_selection import GridSearchCV param_grid = { 'polynomialfeatures__degree' : np . arange ( 10 ), 'linearregression__fit_intercept' : [ True , False ]} grid = GridSearchCV ( PolynomialRegression (), param_grid , cv = 7 ) # create test and training data random . seed ( 42 ) X_train = random . sample ( list ( x ), int ( int ( len ( x )) * .8 )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] grid . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) GridSearchCV(cv=7, estimator=Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()), ('linearregression', LinearRegression())]), param_grid={'linearregression__fit_intercept': [True, False], 'polynomialfeatures__degree': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}) grid . best_params_ {'linearregression__fit_intercept': True, 'polynomialfeatures__degree': 3} to grab the best model from the CV/search outcome. we use grid.best_estimator model = grid . best_estimator_ # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 5 )) ax . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax . plot ( X_train , y_train , ls = '' , marker = '.' ) ax . set_ylim ( min ( y ), max ( y )) ax . set_title ( \"Best Grid Search CV Model\" ) Text(0.5, 1.0, 'Best Grid Search CV Model')","title":"3.2.1 Grid Search"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#exercise-3-grid-search","text":"There are two objectives for this exercise: Complete the object param_grid by reading the parameters below in StandardScaler . We would like to include in our grid search whether or not to scale our input features so that they are centered at 0 ( with_mean ) and have unit variance ( with_std ) After you have completed the grid search, create a side-by-side plot of actual versus predicted values for training data (left plot) and testing data (right plot) Include in your plot title the R2 for testing and R2 for training StandardScaler () . get_params () . keys () dict_keys(['copy', 'with_mean', 'with_std']) df = wine . copy () y = df . pop ( 'density' ) X = df display ( X . head ()) display ( y . head ()) print ( X . shape ) print ( y . shape ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) def NormalizedRegression ( ** kwargs ): return make_pipeline ( StandardScaler (), LinearRegression ( ** kwargs )) ###################################################################################################### ####################### YOUR EXTRA PARAMETERS GO IN THE DICTIONARY BELOW ############################# ###################################################################################################### param_grid = { 'linearregression__fit_intercept' : [ True , False ], 'standardscaler__with_mean' : [ True , False ], 'standardscaler__with_std' : [ True , False ]} grid = GridSearchCV ( NormalizedRegression (), param_grid , cv = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 0 1.0010 1 0.9940 2 0.9951 3 0.9956 4 0.9956 Name: density, dtype: float64 (6463, 12) (6463,) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) {'linearregression__fit_intercept': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True} model = grid . best_estimator_ ###################################################################################################### ##################################### ADJUST THE PLOT LOGIC BELOW #################################### ###################################################################################################### fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.962')","title":"\ud83c\udfcb\ufe0f Exercise 3: Grid Search"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#references","text":"back to top","title":"References"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#model-validation","text":"cross_val_score leave-one-out","title":"Model Validation"},{"location":"solutions/SOLN_S4_Feature_Engineering/","text":"Data Science Foundations, Session 4: Feature Engineering \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we talked about model pipelines and conveniently began with a suitable set of input data. In the real world, this is hardly ever the case! What is constant is this: at the end of the day, our models need numbers. Not only this, but a suitable set of numbers. What does that mean? The answer to that question is the subject of our session today. 4.0 Preparing Environment and Importing Data \u00b6 back to top 4.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats as stats from scipy.stats import gamma from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor import seaborn as sns ; sns . set () from sklearn.datasets import load_iris from sklearn.metrics import mean_squared_error , r2_score 4.0.2 Load Dataset \u00b6 back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) orders = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_orders.csv' ) time_cols = [ i for i in orders . columns if '/' in i ] margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 dfcat = margin . columns [: - 2 ] dfcat Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') orders . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer 1/2020 2/2020 3/2020 4/2020 5/2020 6/2020 7/2020 8/2020 9/2020 10/2020 11/2020 12/2020 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 4.1 Categorical Features \u00b6 back to top At the end of the day, our algorithms operate on numerical values. How do you get from a series of string values to numerical values? margin [ 'Customer' ] . unique () array(['Slugworth', 'Perk-a-Cola', 'Fickelgruber', 'Zebrabar', \"Dandy's Candies\"], dtype=object) A naive way to do it would be to assign a number to every entry 'Slugworth' = 1 'Perk-a-Cola' = 2 'Dandy's Candies' = 3 but we would inadvertently end up with some weird mathematical relationships between these variables, e.g. Dandy's Candies - Perk-a-Cola = Slugworth (3 - 2 = 1). A work around for this is to think multi-dimensionally we express our categorical values as vectors in a hyperspace where they cannot be expressed in terms of one another, i.e. they are orthogonal 'Slugworth' = [1,0,0] 'Perk-a-Cola' = [0,1,0] 'Dandy's Candies' = [0,0,1] such a scheme, in machine learning vernacular, is termed one-hot encoding. 4.1.1 One-Hot Encoding \u00b6 back to top sklearn has a couple useful libraries for one-hot encoding. let's start with the OneHotEncoder class in its preprocessing library from sklearn.preprocessing import OneHotEncoder # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ 'Customer' ] . values . reshape ( - 1 , 1 ) # fit our encoder to this data enc . fit ( X_cat ) OneHotEncoder() After fitting our encoder, we can then use this object to create our training array. # as a reference here's our original data display ( X_cat [: 10 ]) print ( X_cat . shape , end = ' \\n\\n ' ) onehotlabels = enc . transform ( X_cat ) . toarray () print ( onehotlabels . shape , end = ' \\n\\n ' ) # And here is our new data onehotlabels [: 10 ] array([['Slugworth'], ['Slugworth'], ['Perk-a-Cola'], ['Fickelgruber'], ['Fickelgruber'], ['Fickelgruber'], ['Slugworth'], ['Zebrabar'], ['Slugworth'], ['Zebrabar']], dtype=object) (1668, 1) (1668, 5) array([[0., 0., 0., 1., 0.], [0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]]) We have our customer information one-hot encoded, we need to do this for all our variables and concatenate them with our regular numerical variables in our original dataframe. # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ dfcat ] . values # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [ \"KG\" ] print ( X_num . shape ) X = np . concatenate (( onehotlabels , X_num . values . reshape ( - 1 , 1 )), axis = 1 ) X . shape (1668,) (1668, 119) And now we grab our EBITDA (margin) data for prediction y = margin [ \"EBITDA/KG\" ] \ud83c\udfcb\ufe0f Exercise 1: Create a simple linear model \u00b6 Using the X and y sets, use train_test_split and LinearRegression to make a baseline model based on what we've learned so far. Assess your model performance visually by plottying y_test vs y_test_pred # Cell for Exercise 1 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 ) model = LinearRegression () model . fit ( X_train , y_train ) y_test_pred = model . predict ( X_test ) fig , ax = plt . subplots ( figsize = ( 5 , 5 )) ax . plot ( y_test , y_test_pred , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7fa4b01f90d0>] \ud83d\ude4b Question 1: \u00b6 How can we assess the relative feature importance of the features in our model? We could be tempted to inspect the coefficients ( linear.coef_ ) of our model to evaluate the relative feature importance, but in order to do this our features need to be scaled (so that the relative coefficient sizes are meaningful). What other issues might there be (think categorical vs continuous variables). 4.2 Derived Features \u00b6 back to top Can we recall an example of where we've seen this previously? That's right earlier on in our session on model selection and validation we derived some polynomial features to create our polynomial model using the linear regression class in sklearn. We actually see this a lot in engineering, where we will describe log relationships or some other transformation of the original variable. Actually let me see if I can find an example in my handy BSL... concentration profiles in continous stirred tank vs plug flow reactors. Notice the y-axis is log scale. Thanks Bird, Stewart, Lightfoot! Can we think of other examples where we would like to derive features from our input data? 4.2.1 Creating Polynomials \u00b6 back to top Let's revisit our example from the previous session, right before we introduced Grid Search in sklearn # from Model Selection and Validation, 1.2.1 from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) in the above, we use sklearn's convenient tool, make_pipeline to join together the preprocessing tool PolynomialFeatures and the basic model LinearRegression . Let's take a look at what PolynomialFeatures does to some simple data x = np . arange ( 1 , 11 ) y = x ** 3 print ( x ) print ( y ) [ 1 2 3 4 5 6 7 8 9 10] [ 1 8 27 64 125 216 343 512 729 1000] features = PolynomialFeatures ( degree = 3 ) X2 = features . fit_transform ( x . reshape ( - 1 , 1 )) we see our new feature set contains our original features, plus new features up to the nth-degree polynomial we set when creating the features object from PolynomialFeatures print ( X2 ) [[ 1. 1. 1. 1.] [ 1. 2. 4. 8.] [ 1. 3. 9. 27.] [ 1. 4. 16. 64.] [ 1. 5. 25. 125.] [ 1. 6. 36. 216.] [ 1. 7. 49. 343.] [ 1. 8. 64. 512.] [ 1. 9. 81. 729.] [ 1. 10. 100. 1000.]] model = LinearRegression () . fit ( X2 , y ) yhat = model . predict ( X2 ) plt . scatter ( x , y ) plt . plot ( x , yhat ); 4.2.2 Dealing with Time Series \u00b6 back to top Often, we will be dealing with time series data, whether its data generated by machinery, reactors, or sales and customers. In the following we discuss some simple practices for dealing with time series data. \ud83c\udf52 4.2.2.1 Enrichment : Fast Fourier Transform \u00b6 back to top Sometimes we'll want to create a more sophisticated transformation of our input data. As engineers, this can often have to do with some empirical knowledge we understand about our process. When working with equipment and machinery, we will often want to convert a signal from the time to frequency domain. Let's cover how we can do that with numpy! [img src]( https://www.nti-audio.com/en/support/know-how/fast-fourier-transform-fft#:~:text=The%20%22Fast%20Fourier%20Transform%22%20(,frequency%20information%20about%20the%20signal .) What I've drawn here in the following is called a square-wave signal t = np . linspace ( 0 , 1 , 501 ) # FFT should be given an integer number of cycles so we leave out last sample t = t [: - 1 ] f = 5 # linear frequency in Hz w = f * 2 * np . pi # radial frequency h = 4 # height of square wave amp = 4 * h / np . pi s = amp * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) # here is the call to numpy FFT F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ], d = t [ 1 ]) # reorder frequency spectrum and frequency bins with 0 Hz at the center F = np . fft . fftshift ( F ) freq = np . fft . fftshift ( freq ) # scale frequency spectrum to correct amplitude F = F / t . size # amplitudes amps = [ max ( np . sin ( w * t )), max ( np . sin ( w * t * 3 ) / 3 ), max ( np . sin ( w * t * 5 ) / 5 )] fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) ax [ 0 ] . set_xlim ( 0 , 1 ) ax [ 0 ] . set_xlabel ( 'Time (s)' ) # tells us about the amplitude of the component at the # corresponding frequency. Multiplied by two because the # signal power is split between (-) and (+) frequency branches # of FFT, but we're only visualizing the (+) branch magnitude = 2 * np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , 30 ) ax [ 1 ] . set_xlabel ( 'Frequency (Hz)' ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain') 4.2.2.2 Rolling Windows \u00b6 back to top to see an example of this dataset in action visit this link One powerful technique for dealing with time series data, is to create a rolling window of features based on the historical data. The proper window size can usually be determined by trial and error, or constraints around access to the data itself. In the above gif, we have a window size of 7. What that means is for whatever time step units we are in (that could be minutes, days, months, etc.) we will have 7 of them included in a single instance or observation. This instance or observation is then interpreted by our model and used to assess the target value, typically the quantity in the very next time step after the window (the green bar in the gif). Let's take an example with the orders data tidy_orders = orders . melt ( id_vars = orders . columns [: 6 ], var_name = 'Date' , value_name = 'KG' ) display ( tidy_orders . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 In the next exercise, we are going to attempt to predict an order amount, based on the previous order history. We will scrub all categorical labels and only use historical amounts to inform our models. In effect the data that the model will see will look like the following: fig , ax = plt . subplots ( 3 , 2 , figsize = ( 10 , 20 )) indices = np . argwhere ( ax ) color_dict = { 0 : 'tab:blue' , 1 : 'tab:green' , 2 : 'tab:orange' , 3 : 'tab:red' , 4 : 'tab:pink' , 5 : 'tab:brown' } for index , customer in enumerate ( tidy_orders . Customer . unique ()): orders . loc [ orders . Customer == customer ] . iloc [:, 6 :] . reset_index () . T . plot ( c = color_dict [ index ], legend = False , ax = ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]]) ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]] . set_title ( customer ) What we may notice is that there is very little noise or drift in our order history, but there is certainly some periodicity. The question is can we use a linear model to predict the next order amount based on these history orders? \ud83c\udfcb\ufe0f Exercise 2: Optimize Rolling Window Size for Customer Forecasts \u00b6 For this exercise, you will use the process_data function below to help you optimize the window size for predicting the order quantity in any given month. You will train a LinearRegression model. create a model using a window size of 3 and predict the order quantity for the month immediately following the window create a model for window sizes 1-11 and report the \\(R^2\\) for each model def process_data ( Xy , time_cols = 12 , window = 3 , remove_null = False ): \"\"\" This function splits your time series data into the proper windows Parameters ---------- Xy: array The input data. If there are non-time series columns, assumes they are on the left and time columns are on the right. time_cols: int The number of time columns, default 12 window: int The time window size, default 3 Returns ------- X_: array The independent variables, includes time and non-time series columns with the new window y_: array The dependent variable, selected from the time columns at the end of the window labels: The time series labels, can be used in subsequent plot \"\"\" # separate the non-time series columns X_cat = Xy [:,: - time_cols ] # select the columns to apply the sweeping window X = Xy [:, - time_cols :] X_ = [] y = [] for i in range ( X . shape [ 1 ] - window ): # after attaching the current window to the non-time series # columns, add it to a growing list X_ . append ( np . concatenate (( X_cat , X [:, i : i + window ]), axis = 1 )) # add the next time delta after the window to the list of y # values y . append ( X [:, i + window ]) # X_ is 3D: [number of replicates from sweeping window, # length of input data, # size of new feature with categories and time] # we want to reshape X_ so that the replicates due to the sweeping window is # a part of the same dimension as the instances of the input data X_ = np . array ( X_ ) . reshape ( X . shape [ 0 ] * np . array ( X_ ) . shape [ 0 ], window + X_cat . shape [ 1 ]) y = np . array ( y ) . reshape ( X . shape [ 0 ] * np . array ( y ) . shape [ 0 ],) if remove_null : # remove training data where the target is 0 (may be unfair advantage) X_ = X_ [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] y = y [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] # create labels that show the previous month values used to train the model labels = [] for row in X_ : labels . append ( \"X: {} \" . format ( np . array2string ( row [ - window :] . astype ( float ) . round ()))) return X_ , y , labels # Code Cell for Exercise 2 kg_month_data = orders . values [:, 6 :] # use kg_month_data and the function process_data to create your X, y arrays # then use train_test_split to create train and test portions # USE y_test and y_pred for your actual and true test data # change only window parameter in process_data() print ( \"window R2\" ) for window in range ( 1 , 12 ): ###################### ### YOUR CODE HERE ### ###################### X , y , labels = process_data ( kg_month_data , time_cols = 12 , window = 3 ) features = PolynomialFeatures ( degree = 4 ) X2 = features . fit_transform ( X ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.6 ) model = LinearRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( \" {} , \\t {:.2f} \" . format ( window , r2_score ( y_test , y_pred ))) window R2 1, 0.97 2, 0.88 3, 0.80 4, 0.96 5, 0.90 6, 0.86 7, 0.61 8, 0.89 9, 0.76 10, 0.74 11, 0.87 #### RUN AFTER EXERCISE 2.2.2.2.1 #### fig = px . scatter ( x = y_test , y = y_pred , labels = { \"y\" : \"Prediction\" , \"x\" : \"Actual\" }) fig . update_layout ( autosize = False , width = 800 , height = 500 , title = 'R2: {:.3f} ' . format ( r2_score ( y_test , y_pred )) ) require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"239a6e0b-8482-49c8-9619-ebde564b91cd\")) { Plotly.newPlot( \"239a6e0b-8482-49c8-9619-ebde564b91cd\", [{\"hovertemplate\":\"Actual=%{x}<br>Prediction=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4528.380503144654,64.01384083044982,15617.489114658929,4528.380503144654,5698.392857142857,124348.89322916669,21939.111111111117,5712.33671988389,64983.42138364781,1483.5,93302.72435897436,1236.25,773.5849056603774,518.4642857142858,3798.928571428572,13671.296296296296,188.7692307692308,4057.832278481013,101149.25,14626.074074074077,12274.009146341465,141456.76315789475,4793.829296424452,74.78571428571429,10861.245674740485,86.24595469255664,19129.33333333333,39307.04113924051,74.78571428571429,46247.0,188.7692307692308,85.32258064516128,90677.41228070176,9306.611111111111,220.55016181229772,466477.57812500006,1158.125,220.55016181229772,1901.1290322580644,19129.33333333333,2637.8993710691825,1868.867924528302,3798.928571428572,2252.261437908497,5222.5,181.5552699228792,5038.387096774193,120.98615916955016,10146.898432174508,85.32258064516128,286.6666666666667,18613.222222222223,818.1891025641025,2499.201741654572,2010.9477124183009,6984.389273356401,42704.25889967639,11715.487421383648,39307.04113924051,440.859375,428.0,96.59546925566345,102.64285714285714,3569.0359477124184,13671.296296296296,773.5849056603774,2010.9477124183009,42704.25889967639,40608.41346153846,83347.75862068967,10796.31640058055,74.78571428571429,80801.72807017544,2540.0,5222.5,131.55526992287918,299024.0885416667,3882.680981595092,895.9082278481012,1180.5379746835442,15617.489114658929,85.32258064516128,6185.59509202454,16749.13157894737,47142.92857142857,3786.966463414634,936.280487804878,15244.632812500002,3272.80163599182,1271.904761904762,47142.92857142857,356.6666666666667,10861.245674740485,108.64285714285714,21438.1875,16918.132716049382,7892.142857142857,1901.1290322580644,2647.5,5038.387096774193,1677.5078125000002,39307.04113924051,51795.97953216374,916.3717948717948,41871.69230769231,12870.766853932584,1271.904761904762,181.5552699228792,211540.7575757576,3798.928571428572,2054.328561690525,3578.592162554427,2854.854368932039,51795.97953216374,11791.8,149.57142857142858,103.3653846153846,119015.43674698794,2499.201741654572,19286.94968553459,2185.3410740203194,19569.69230769231,12317.397660818711,6965.361445783132,86.24595469255664,3071.4691011235955,13867.125,50574.625,14626.074074074077,1005.4545454545454,16918.132716049382,2543.106741573034,10245.87593728698,8608.709677419354,150.3654970760234,3517.9333333333334,14626.074074074077,3177.0,2499.201741654572,345.6428571428572,10736.622807017544,13799.769319492503,12691.209677419354,11.175616835994196,2010.9477124183009,10736.622807017544,71138.51966292135,8104.40251572327,62.168396770472896,211540.7575757576,16569.261006289307,445.54838709677415,16918.132716049382,42.484177215189874,150.3654970760234,11.175616835994196,299024.0885416667,579.0625,10146.898432174508,181.5552699228792,494.57142857142856,411.1303344867359,2355.267295597484,494.57142857142856,79091.23475609756,10146.898432174508,1005.4545454545454,5038.387096774193,191.11111111111111,9304.668674698794,19569.69230769231,8250.491573033707,466477.57812500006,3048.0,1630.1966292134832,64.01384083044982,3569.0359477124184,1901.1290322580644,3272.80163599182,9772.200520833334,2185.3410740203194,28694.0,314.3203883495146,8104.40251572327,18557.572327044025,2231.547169811321,1432.2712418300653,773.5849056603774,40735.10802469136,314.3203883495146,471.91011235955057,42.484177215189874,9194.277108433736,8173.714285714285,837.8787878787879,1604.1437908496732,22.50980392156863,14.663461538461538,663176.1973875181,2854.854368932039,428.0,43.64516129032258,1526.2857142857142,1432.2712418300653,5456.578050443081,22331.935185185182,150.3654970760234,4057.832278481013,1868.867924528302,1630.1966292134832,4723.49129172714,5038.387096774193,19364.70552147239,117.22222222222224,15110.75,4057.832278481013,83347.75862068967,7892.142857142857,19129.33333333333,1968.8904494382025,207.39062500000003,62.168396770472896,3578.592162554427,1677.5078125000002,2499.201741654572,1254.4444444444443,1236.25,3578.592162554427,1992.4528301886792,14626.074074074077,1236.25,6965.361445783132,36.37096774193548,40735.10802469136,20.098039215686278,1432.2712418300653,538.8571428571429,101149.25,93302.72435897436,181.5552699228792,28694.0,2054.328561690525,5872.384615384615,31412.04644412192,5872.384615384615,854.952380952381,31412.04644412192,1253403.0130624091,63142.24137931035,20.098039215686278,3569.0359477124184,16569.261006289307,19286.94968553459,19286.94968553459,837.8787878787879,3665.809768637532,108.64285714285714,56.07911392405064,13671.296296296296,371.2903225806451,4057.832278481013,47142.92857142857,21438.1875,54.833333333333336,26081.56401384083,2540.0,115.76923076923076,16.423076923076923,8608.709677419354,4463.780120481927,8250.491573033707,15110.75,8173.714285714285,157100.37650602407,104499.0512820513,9076.930817610064,2611.25,428.0,14.663461538461538,2694.0,3569.0359477124184,86.24595469255664,678.7183544303797,494.57142857142856,712.4603174603175,663176.1973875181,10245.87593728698,5071.786163522012,1868.867924528302,26081.56401384083,3403.344867358708,4793.829296424452,64983.42138364781,3272.80163599182,5712.33671988389,9194.277108433736,608.4770114942529,42.484177215189874,466477.57812500006,3695.444059976932,3517.9333333333334,345.6428571428572,579.0625,7892.142857142857,5038.387096774193,45601.61516853933,1992.4528301886792,2647.5,5872.384615384615,6965.361445783132,64.01384083044982,45601.61516853933,23123.5,1992.4528301886792,2540.0,9060.337370242214,14.663461538461538,3882.680981595092,36.37096774193548,193984.2734375,2231.547169811321,108.64285714285714,329.7142857142857,117.49826989619376,773.5849056603774,36.37096774193548,8608.709677419354,371.2903225806451,45481.42307692308,10245.87593728698,63142.24137931035,678.7183544303797,11555.9375,4528.380503144654,2499.201741654572,10736.622807017544,5222.5,1901.1290322580644,93302.72435897436,1702.1929824561405,114208.8534107402,343.1394601542417,10245.87593728698,8173.714285714285,140637.14285714287,132.94270833333334,579.0625,663176.1973875181,1180.5379746835442,1236.25,26081.56401384083,854.952380952381,2010.9477124183009,30698.85714285714,11.175616835994196,90677.41228070176,38128.80258899677,663176.1973875181,71138.51966292135,1236.25,31412.04644412192,50574.625,83347.75862068967,12274.009146341465,90677.41228070176,808.2857142857143,11.175616835994196,11555.9375,86.24595469255664,41871.69230769231,19129.33333333333,329.7142857142857,1236.25,2637.8993710691825,579.7777777777778,30698.85714285714,329.7142857142857,2647.5,466477.57812500006,102.64285714285714,12691.209677419354,12317.397660818711,220.55016181229772,19147.454268292684,9304.668674698794,4057.832278481013,47142.92857142857,1630.1966292134832,4463.780120481927,2002.1844660194176,5071.786163522012,7035.866666666667,13504.20634920635,21601.383647798742,10245.87593728698,10861.245674740485,176.36477987421384,12691.209677419354,1432.2712418300653,608.4770114942529,10736.622807017544,3695.444059976932,157100.37650602407,1702.1929824561405,51795.97953216374,2386.449438202247,117.22222222222224,120.98615916955016,777.0363321799308,12274.009146341465,2611.25,2242.446601941748,168764.57142857145,627.2222222222222,40608.41346153846,2002.1844660194176,157.46855345911948,2647.5,119015.43674698794,579.0625,329.7142857142857,13121.345911949686,71138.51966292135,207.39062500000003,30698.85714285714,9060.337370242214,1529.7752808988764,3071.4691011235955,46247.0,538.8571428571429,16347.42857142857,23123.5,132.94270833333334,6984.389273356401,12691.209677419354,2499.201741654572,30221.5,15229.451612903224,191.11111111111111,428.0,3578.592162554427,11715.487421383648,19129.33333333333,30221.5,6432.321799307958,41871.69230769231,42.484177215189874,16918.132716049382,3695.444059976932,13504.20634920635,15617.489114658929,117.49826989619376,16569.261006289307,96.59546925566345,678.7183544303797,10146.898432174508,678.7183544303797,131.55526992287918,47142.92857142857,79091.23475609756,736.1797752808989,1180.5379746835442,29354.53846153846,45481.42307692308,21438.1875,6965.361445783132,31412.04644412192,10146.898432174508,11791.8,15617.489114658929,1677.5078125000002,5456.578050443081,14626.074074074077,94285.85714285714,248.6394601542416,5456.578050443081,26081.56401384083,63142.24137931035,12317.397660818711,12317.397660818711,608.4770114942529,11.175616835994196,10861.245674740485,2566.6935483870966,11555.9375,678.7183544303797,1529.7752808988764,1432.2712418300653,518.4642857142858,8104.40251572327,63142.24137931035,3798.928571428572,4057.832278481013,1526.2857142857142,2854.854368932039,3403.344867358708,9306.611111111111,538.8571428571429,13671.296296296296,117.22222222222224,343.1394601542417],\"xaxis\":\"x\",\"y\":[4785.592155979585,378.9394302913829,15365.600809369838,4785.592155979585,4070.255357333951,134757.3771732384,14409.557558767568,5813.845282617682,61516.49587090381,1510.6755468797405,98738.27282520698,1684.2601507265988,1088.107600326137,647.864369876902,4263.755978756964,14168.403568365671,487.3481032940997,4287.792726026477,51779.415544128475,19934.734830917627,12141.409803201397,87747.52966460225,5818.94510670397,377.43130038813075,12797.394868451114,389.0359797814606,19263.507069512012,40163.79013647013,377.43130038813075,23840.068464074022,487.3481032940997,400.48488833402286,126112.56377290396,11223.080944977459,522.1062342859491,415175.9779430929,894.6750043489748,519.4101103091041,2138.6141864173633,19263.507069512012,2790.085070463646,2196.4290704108694,4263.755978756964,2244.514746383961,2963.0052419712265,602.4723068239653,5217.814575086902,410.08771833436947,16913.133647514336,390.2429375597753,489.6064298303322,16311.253798810561,1109.562627058514,2715.348806915594,2281.9990642021894,6353.789558047462,37073.75373050823,11896.354947646087,38882.42895370776,797.0601491084628,703.1493477716122,388.482661364839,481.8437277075316,3747.013377347842,16794.12337436424,1571.4700357476117,2257.4161561731,37073.75373050823,43146.57407027896,76460.52042984031,5813.845282617682,433.93372568151483,50253.31197444373,2781.8507793892813,2963.0052419712265,450.36262704088375,323624.6828685297,2286.347850033103,959.8167805278175,1463.9293549917084,25867.060321463283,388.50455558540244,3461.34813854915,10658.874464205835,48286.68591679114,4070.54807420052,1208.1895902424544,13863.416557439898,3913.805102706469,1545.439458867743,48286.68591679114,664.1495092797293,12797.394868451114,415.88888264864494,24308.715376885695,17460.789585310762,13878.585909613186,2138.6141864173633,3258.406190332729,5925.265679815453,1797.2371240684145,47713.131047358365,51804.0528093027,1099.5606427530397,40683.104312073505,8508.45844428347,1571.3535403980425,505.4911026847705,213132.54291644722,4263.755978756964,2570.353838719455,3926.506181336219,3317.1418553360736,54758.90192605476,20585.396531692135,377.43130038813075,406.91804655949966,120990.40817472423,4395.853637740449,19387.390319584494,2412.686086825897,19699.926384269966,12552.025861937103,7368.391717428835,390.09029678781303,2262.905415567944,11572.521113313724,56931.5110267007,15545.52362930884,1239.906233381196,17460.789585310762,1926.1552505875645,10673.158532726991,9907.699090223898,454.81639486308137,6355.618165841122,19934.734830917627,2858.35606741023,2834.2664976391197,665.469888483012,11592.823783640883,16177.143075924467,12543.725418313918,317.63569157648635,2257.4161561731,15201.471014315619,45645.23591676211,8323.62833082682,376.8168895165831,213132.54291644722,16698.569941711827,663.3571920970296,20710.099320853376,346.28222029939593,513.9332229141203,323.60536008687086,323624.6828685297,1301.212252741579,16913.133647514336,505.4911026847705,632.0783029798903,778.1760841513764,2635.5625663138067,632.0783029798903,76574.72476531091,11492.974658748008,1239.906233381196,5163.94089137517,494.71513791291954,11527.578799029196,19176.79696136619,8979.131225405608,415175.9779430929,2754.6914615362707,2567.0739167219367,378.9394302913829,3747.013377347842,2138.6141864173633,3913.805102706469,10871.491666793676,2516.669574686992,18752.149151877966,636.9170394143573,8323.62833082682,17785.632844249434,2407.321312452001,1686.4843440391876,1051.2986748894868,49435.50780135217,610.4363857799107,960.0492155948752,346.28222029939593,7141.329503589847,8187.405610699661,1148.288091129662,1686.4843440391876,324.6947768318741,320.7835266802878,639820.6545150073,3317.1418553360736,703.1493477716122,340.3871184326995,1531.8394329987157,1686.4843440391876,9236.305456033022,16619.83993793382,463.3944227377964,4287.792726026477,3364.1619784229933,2019.1544585081797,2715.348806915594,5163.94089137517,10185.63650295469,436.06660273854754,26293.480165383102,4218.370391131691,76460.52042984031,7915.880111460361,25978.487349635714,2375.22637673234,489.7605752448112,376.8168895165831,3756.2286447454576,1797.2371240684145,3060.8574440447824,1384.0395692783663,1535.8631900519042,6162.533552625073,2276.6020512892946,14800.536096877271,1535.8631900519042,7368.391717428835,340.3871184326995,40283.97684677175,325.06940680600604,1695.6732046475156,1028.5053850528614,51779.415544128475,92018.35013115287,505.4911026847705,18752.149151877966,3667.712553203018,5968.182975715722,34939.27854355379,6424.274585977019,992.354894936303,30596.638581578518,639820.6545150073,65413.624508456036,324.82371754886964,3747.013377347842,16698.569941711827,19387.390319584494,19387.390319584494,1148.288091129662,6305.285970941779,492.1628050669814,346.28222029939593,14168.403568365671,663.3571920970296,4218.370391131691,45766.272960899,37175.69967364701,369.63985077669076,22891.92825541111,2754.6914615362707,405.6544524204279,319.54816919005003,8698.931074864986,5689.033000226852,11752.173931839643,26293.480165383102,8187.405610699661,117110.65135820676,90877.76737128492,8855.335527589015,3229.016296504339,703.1493477716122,319.72742324233695,1604.2554168141014,3769.9108375996702,396.30197122916206,959.8167805278175,632.0783029798903,999.9729800621147,731503.1707028296,17075.133945943726,5082.686543693957,2107.5040932279385,22891.92825541111,4219.676962291559,5818.94510670397,61516.49587090381,3617.0754679045876,5813.845282617682,7141.329503589847,932.7370475860574,346.28222029939593,415175.9779430929,4555.635369302835,3885.8140646224183,769.1955699368395,953.6647702716481,8337.819513819635,5163.94089137517,48246.709882894014,2276.6020512892946,2886.664844581823,5968.182975715722,7368.391717428835,378.9394302913829,63573.70285359673,40074.17801737161,2276.6020512892946,3138.4978117622,8151.559019833033,319.72742324233695,2286.347850033103,340.3871184326995,172828.89934304,2407.321312452001,410.08048522816836,747.818087951304,407.0672088952674,1088.107600326137,341.517050774033,8698.931074864986,667.3272676429804,39725.42364104348,10673.158532726991,65413.624508456036,959.8167805278175,13195.11592262944,4785.592155979585,2834.2664976391197,15201.471014315619,2963.0052419712265,2425.884027039221,98738.27282520698,1997.7384398419902,115873.73047883328,489.0303345318461,10673.158532726991,9457.064447629586,137428.5757499219,449.0579108608325,1301.212252741579,1085751.3630379443,1502.4134405828336,1684.2601507265988,22891.92825541111,992.354894936303,2244.514746383961,31550.15438840883,316.0906893952995,90462.3574812228,37318.37237669782,639820.6545150073,45645.23591676211,1684.2601507265988,30596.638581578518,56931.5110267007,76460.52042984031,12141.409803201397,95635.30862544454,839.3500692330289,316.62245060704845,13195.11592262944,389.0359797814606,41802.402147140514,19263.507069512012,747.818087951304,1535.8631900519042,2790.085070463646,803.8781088470796,29908.894508749334,648.872490598366,2858.35606741023,415175.9779430929,409.7821718132848,12543.725418313918,17394.662595168105,522.1062342859491,12141.409803201397,9740.51689224938,5199.4234913728005,45766.272960899,2567.0739167219367,4831.715916465941,2417.5880406486763,5082.686543693957,3697.7340063828487,13472.111607949182,20652.753180263422,11602.10423834572,12797.394868451114,471.44073265789405,12543.725418313918,1695.6732046475156,932.7370475860574,10980.322568203586,4555.635369302835,117110.65135820676,2094.8448847085992,54758.90192605476,1826.31018819446,421.9954832228255,410.08771833436947,978.2266656454766,12141.409803201397,4796.262256309382,2236.064155834039,135924.7927322855,1041.1204286449113,40221.84276082843,2273.3851256392354,461.10966099955675,2858.35606741023,143848.60557702882,953.6647702716481,648.872490598366,12664.975331868296,45645.23591676211,489.7605752448112,29908.894508749334,8151.559019833033,2427.747778574022,2262.905415567944,23840.068464074022,866.7970691016989,8187.405610699661,26195.686125920434,449.0579108608325,6353.789558047462,12679.428015781728,2715.348806915594,15684.810589735318,12543.725418313918,504.4494927620649,703.1493477716122,4250.960414621229,11896.354947646087,25978.487349635714,15684.810589735318,5875.699548389586,41802.402147140514,347.0090494475603,16909.279272683176,4555.635369302835,15368.26716660901,16108.716384819742,407.0672088952674,16698.569941711827,388.482661364839,993.553834752781,10573.002781125038,971.4284514069652,450.36262704088375,45766.272960899,76574.72476531091,774.5157686911542,1443.732437768414,19176.79696136619,39725.42364104348,24308.715376885695,8706.164457317907,32091.295039989156,11492.974658748008,13508.076892121393,16108.716384819742,1797.2371240684145,5826.8479598607,15545.52362930884,45766.272960899,438.4351253031718,5826.8479598607,22891.92825541111,65413.624508456036,12552.025861937103,12552.025861937103,932.7370475860574,316.0906893952995,12797.394868451114,2860.1717851882872,11807.964454280216,1123.9090100996427,1913.5805743985902,1816.3382332975395,647.864369876902,8323.62833082682,65413.624508456036,5403.795319141717,4218.370391131691,1531.8394329987157,3111.527273406017,4219.676962291559,11223.080944977459,866.7970691016989,13722.736128528055,436.06660273854754,489.0303345318461],\"yaxis\":\"y\",\"type\":\"scatter\"}], {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Actual\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prediction\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"autosize\":false,\"width\":800,\"height\":500,\"title\":{\"text\":\"R2: 0.869\"}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('239a6e0b-8482-49c8-9619-ebde564b91cd'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; }); 4.2.3 Image Preprocessing \u00b6 back to top Image preprocessing is beyond the scope of this session. We cover this topic in General Applications of Neural Networks . For now, know that there is a wealth of considerations for how to handle images, and they all fit within the realm of feature engineering. 4.3 Transformed Features \u00b6 back to top Transformed features, are features that we would like to augment based on their relationship within their own distribution or to other (allegedly) independent data within our training set. e.g. we're not deriving new features based on some empirical knowledge of the data, rather we are changing them due to statistical properties that we can assess based on the data itself. 4.3.1 Skewness \u00b6 back to top Skewed data can lead to imbalances in our model prediction. Why? Skewed values in the distribution will bias the mean. When assigning weights to this input feature, therefore, the model will give preferential treatment to these values. To demonstrate, I'm going to use scipy to create some skewed data. from scipy.stats import skewnorm a = 10 x = np . linspace ( skewnorm . ppf ( 0.01 , a ), skewnorm . ppf ( 0.99 , a ), 100 ) plt . plot ( x , skewnorm . pdf ( x , a ), 'r-' , lw = 5 , alpha = 0.6 , label = 'skewnorm pdf' ) [<matplotlib.lines.Line2D at 0x7fa48d14a130>] We can now generate a random population based on this distribution r = skewnorm . rvs ( a , size = 1000 ) plt . hist ( r ) (array([113., 267., 225., 172., 116., 62., 26., 13., 2., 4.]), array([-0.19733964, 0.15303313, 0.50340589, 0.85377866, 1.20415142, 1.55452419, 1.90489696, 2.25526972, 2.60564249, 2.95601526, 3.30638802]), <BarContainer object of 10 artists>) Unskewed data will return something close to 0 from calling df.skew() . When dealing with actual data, we can use df.skew() to determine whether we should transform our data. x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) x [ 'Skewed Data' ] . skew () 0.9141902067398219 There are a handful of ways to deal with skewed data: log transform square root transform Box-Cox transform Let's try the first two print ( 'square root transformed skew: {:.4f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) print ( 'log transformed skew: {:.4f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . legend () square root transformed skew: 0.0561 log transformed skew: -1.6916 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in sqrt /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log <matplotlib.legend.Legend at 0x7fa49157d340> We see we didn't get much traction with the log transform, and the log transform will not be able to handle 0 values, and so we will sometimes have to code exceptions for those. Box-Cox is often a good route to go, but it has the added restriction that the data has to all be above 0. Let's create a new distribution with this added restriction a = 6 r = skewnorm . rvs ( a , size = 1000 ) r = [ i for i in r if i > 0 ] plt . hist ( r ) (array([220., 277., 182., 127., 66., 39., 17., 5., 4., 2.]), array([2.17150536e-03, 3.88613862e-01, 7.75056219e-01, 1.16149858e+00, 1.54794093e+00, 1.93438329e+00, 2.32082565e+00, 2.70726800e+00, 3.09371036e+00, 3.48015272e+00, 3.86659507e+00]), <BarContainer object of 10 artists>) from scipy import stats x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ], alpha = 0.5 , label = 'box-cox: {:.2f} ' . format ( pd . DataFrame ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ])[ 0 ] . skew ())) ax . legend () <matplotlib.legend.Legend at 0x7fa492058f40> \ud83c\udfcb\ufe0f Exercise 3: Transform data from a gamma distribution \u00b6 Repeat section 2.3.1, this time synthesizing a gamma distribution and transforming it. Which transformation best reduces the skew? Do this for a dataset that does not contain values at or below 0. # code cell for exercise 3 from scipy.stats import gamma a = 6 r = gamma . rvs ( a , size = 1000 ) r = [ i for i in r if i > 0 ] x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ], alpha = 0.5 , label = 'box-cox: {:.2f} ' . format ( pd . DataFrame ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ])[ 0 ] . skew ())) ax . legend () <matplotlib.legend.Legend at 0x7fa49111dd30> 4.3.2 Colinearity \u00b6 back to top Colinearity can also affect the performance of your machine learning model. In particular, if features are colinear, it can be easy for your model to overfit to your training dataset. This is often mitigated by regularization. If you're curious you can read more about it on this discussion from StackExchange . We will still explore it explicitly here by calculating the Variance Inflation Factor (VIF) on some hypothetical data. VIF = \\frac{1}{1-R^2} Usually we are concerned about data with a VIF above 10 4.3.2.1 Detecting Colinearity \u00b6 back to top from statsmodels.stats.outliers_influence import variance_inflation_factor Step 1: Make some data # we can throttle the error rate random . seed ( 42 ) # x2 will be sqrt of x1 plus some error def func ( x , err ): return x ** .5 + ( err * random . randint ( - 1 , 1 ) * random . random () * x ) x0 = range ( 100 ) x1 = [ func ( i , .05 ) for i in x0 ] # HIGH degree of colinearity with x0 x2 = [ func ( i , 1 ) for i in x0 ] # MED degree of colinearity with x0 x3 = [ random . randint ( 0 , 100 ) for i in x0 ] # NO degree of colinearity with x0 # take a look fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) ax . plot ( x0 , x1 , label = 'x1' ) ax . plot ( x0 , x2 , label = 'x2' ) ax . plot ( x0 , x3 , label = 'x3' ) ax . legend () <matplotlib.legend.Legend at 0x7fa49121ca90> To calculate the colinearities I'm going to aggregate these x's into a dataframe: colin = pd . DataFrame ([ x0 , x1 , x2 , x3 ]) . T colin . columns = [ 'x0' , 'x1' , 'x2' , 'x3' ] colin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 0 0.0 0.000000 0.000000 29.0 1 1.0 1.013751 0.721523 28.0 2 2.0 1.400260 1.414214 3.0 3 3.0 1.630546 -0.438007 84.0 4 4.0 2.017388 4.304847 24.0 Step 2: Calculate VIF factors # calculate VIF factors vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( colin . values , i ) for i in range ( colin . shape [ 1 ])] vif [ \"features\" ] = colin . columns Step 3: Inspect VIF factors # inspect VIF factors display ( vif ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 12.555415 x0 1 15.823872 x1 2 1.030609 x2 3 3.559468 x3 In this case, we may remove either x0 or x1 from the dataset. 4.3.2.2 Fixing Colinearity \u00b6 back to top It is good to aknowledge where colinearity exists as this will influence the interpretability of your model. In most cases, however, it won't have a heavy influence on the performance of your model. A simple method of dealing with colinearity, is to remove the highest VIF features from your model, iteratively, assessing the performance and determining whether to keep the variable or not. Another method is to create some linear combination of the correlated variables. This is encapsulated in the section on dimensionality reduction. 4.3.3 Normalization \u00b6 back to top Many learning algorithms require zero mean and unit variance to behave optimally. Sklearn preprocessing library contains a very usefull class, StandardScaler for handling this automatically for us. from sklearn.preprocessing import StandardScaler scaler = StandardScaler () normed = scaler . fit_transform ( colin ) colin [[ 'x0' , 'x1' , 'x2' , 'x3' ]] . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'> pd . DataFrame ( normed , columns = [[ 'x0' , 'x1' , 'x2' , 'x3' ]]) . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'> \ud83c\udfcb\ufe0f Exercise 4: Normalization affect on VIF \u00b6 In the above, we saw how to scale and center variables. How does this affect VIF? Calculate the VIF for the scaled-centered data # Code Cell for Exercise 4 vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( normed , i ) for i in range ( normed . shape [ 1 ])] vif [ \"features\" ] = colin . columns display ( vif ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 3.286048 x0 1 3.296881 x1 2 1.015805 x2 3 1.035537 x3 4.3.4 Dimensionality Reduction \u00b6 back to top Dimensionality reduction is an awesome way to do feature engineering. It is very commonly used. Because it is also an unsupervised machine learning technique, we will visit this topic in that section. 4.4 Missing Data \u00b6 back to top We will often have missing data in our datasets. How do we deal with this? Let's start by making some data with missing data. We'll use a numpy nan datatype to do this from numpy import nan X = np . array ([[ nan , 0 , 3 ], [ 3 , 7 , 9 ], [ 3 , 5 , 2 ], [ 4 , nan , 6 ], [ 8 , 8 , 1 ]]) y = np . array ([ 14 , 16 , - 1 , 8 , - 5 ]) 4.4.1 Imputation \u00b6 back to top A very common strategy is to impute or fill in the missing data, based on basic statistical descriptions of the feature column (mode, mean, and median) from sklearn.impute import SimpleImputer # strategy = 'mean' will replace nan's with mean value # of the column # others are median and most_frequent (mode) imp = SimpleImputer ( strategy = 'mean' ) X2 = imp . fit_transform ( X ) X2 array([[4.5, 0. , 3. ], [3. , 7. , 9. ], [3. , 5. , 2. ], [4. , 5. , 6. ], [8. , 8. , 1. ]]) 4.4.2 Other Strategies \u00b6 back to top Depending on the severity of missing data, you will sometimes opt to remove the whole column, or perhaps apply some simple learning to fill in the missing data. This is a great article on more advanced strategies for handling missing data. References \u00b6 back to top * Box Cox * Multicolinearity * Missing Data","title":"SOLN S4 Feature Engineering"},{"location":"solutions/SOLN_S4_Feature_Engineering/#data-science-foundations-session-4-feature-engineering","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we talked about model pipelines and conveniently began with a suitable set of input data. In the real world, this is hardly ever the case! What is constant is this: at the end of the day, our models need numbers. Not only this, but a suitable set of numbers. What does that mean? The answer to that question is the subject of our session today.","title":"Data Science Foundations, Session 4: Feature Engineering"},{"location":"solutions/SOLN_S4_Feature_Engineering/#40-preparing-environment-and-importing-data","text":"back to top","title":"4.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_S4_Feature_Engineering/#401-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats as stats from scipy.stats import gamma from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor import seaborn as sns ; sns . set () from sklearn.datasets import load_iris from sklearn.metrics import mean_squared_error , r2_score","title":"4.0.1 Import Packages"},{"location":"solutions/SOLN_S4_Feature_Engineering/#402-load-dataset","text":"back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) orders = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_orders.csv' ) time_cols = [ i for i in orders . columns if '/' in i ] margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 dfcat = margin . columns [: - 2 ] dfcat Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') orders . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer 1/2020 2/2020 3/2020 4/2020 5/2020 6/2020 7/2020 8/2020 9/2020 10/2020 11/2020 12/2020 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146","title":"4.0.2 Load Dataset"},{"location":"solutions/SOLN_S4_Feature_Engineering/#41-categorical-features","text":"back to top At the end of the day, our algorithms operate on numerical values. How do you get from a series of string values to numerical values? margin [ 'Customer' ] . unique () array(['Slugworth', 'Perk-a-Cola', 'Fickelgruber', 'Zebrabar', \"Dandy's Candies\"], dtype=object) A naive way to do it would be to assign a number to every entry 'Slugworth' = 1 'Perk-a-Cola' = 2 'Dandy's Candies' = 3 but we would inadvertently end up with some weird mathematical relationships between these variables, e.g. Dandy's Candies - Perk-a-Cola = Slugworth (3 - 2 = 1). A work around for this is to think multi-dimensionally we express our categorical values as vectors in a hyperspace where they cannot be expressed in terms of one another, i.e. they are orthogonal 'Slugworth' = [1,0,0] 'Perk-a-Cola' = [0,1,0] 'Dandy's Candies' = [0,0,1] such a scheme, in machine learning vernacular, is termed one-hot encoding.","title":"4.1 Categorical Features"},{"location":"solutions/SOLN_S4_Feature_Engineering/#411-one-hot-encoding","text":"back to top sklearn has a couple useful libraries for one-hot encoding. let's start with the OneHotEncoder class in its preprocessing library from sklearn.preprocessing import OneHotEncoder # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ 'Customer' ] . values . reshape ( - 1 , 1 ) # fit our encoder to this data enc . fit ( X_cat ) OneHotEncoder() After fitting our encoder, we can then use this object to create our training array. # as a reference here's our original data display ( X_cat [: 10 ]) print ( X_cat . shape , end = ' \\n\\n ' ) onehotlabels = enc . transform ( X_cat ) . toarray () print ( onehotlabels . shape , end = ' \\n\\n ' ) # And here is our new data onehotlabels [: 10 ] array([['Slugworth'], ['Slugworth'], ['Perk-a-Cola'], ['Fickelgruber'], ['Fickelgruber'], ['Fickelgruber'], ['Slugworth'], ['Zebrabar'], ['Slugworth'], ['Zebrabar']], dtype=object) (1668, 1) (1668, 5) array([[0., 0., 0., 1., 0.], [0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]]) We have our customer information one-hot encoded, we need to do this for all our variables and concatenate them with our regular numerical variables in our original dataframe. # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ dfcat ] . values # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [ \"KG\" ] print ( X_num . shape ) X = np . concatenate (( onehotlabels , X_num . values . reshape ( - 1 , 1 )), axis = 1 ) X . shape (1668,) (1668, 119) And now we grab our EBITDA (margin) data for prediction y = margin [ \"EBITDA/KG\" ]","title":"4.1.1 One-Hot Encoding"},{"location":"solutions/SOLN_S4_Feature_Engineering/#exercise-1-create-a-simple-linear-model","text":"Using the X and y sets, use train_test_split and LinearRegression to make a baseline model based on what we've learned so far. Assess your model performance visually by plottying y_test vs y_test_pred # Cell for Exercise 1 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 ) model = LinearRegression () model . fit ( X_train , y_train ) y_test_pred = model . predict ( X_test ) fig , ax = plt . subplots ( figsize = ( 5 , 5 )) ax . plot ( y_test , y_test_pred , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7fa4b01f90d0>]","title":"\ud83c\udfcb\ufe0f Exercise 1: Create a simple linear model"},{"location":"solutions/SOLN_S4_Feature_Engineering/#question-1","text":"How can we assess the relative feature importance of the features in our model? We could be tempted to inspect the coefficients ( linear.coef_ ) of our model to evaluate the relative feature importance, but in order to do this our features need to be scaled (so that the relative coefficient sizes are meaningful). What other issues might there be (think categorical vs continuous variables).","title":"\ud83d\ude4b Question 1:"},{"location":"solutions/SOLN_S4_Feature_Engineering/#42-derived-features","text":"back to top Can we recall an example of where we've seen this previously? That's right earlier on in our session on model selection and validation we derived some polynomial features to create our polynomial model using the linear regression class in sklearn. We actually see this a lot in engineering, where we will describe log relationships or some other transformation of the original variable. Actually let me see if I can find an example in my handy BSL... concentration profiles in continous stirred tank vs plug flow reactors. Notice the y-axis is log scale. Thanks Bird, Stewart, Lightfoot! Can we think of other examples where we would like to derive features from our input data?","title":"4.2 Derived Features"},{"location":"solutions/SOLN_S4_Feature_Engineering/#421-creating-polynomials","text":"back to top Let's revisit our example from the previous session, right before we introduced Grid Search in sklearn # from Model Selection and Validation, 1.2.1 from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) in the above, we use sklearn's convenient tool, make_pipeline to join together the preprocessing tool PolynomialFeatures and the basic model LinearRegression . Let's take a look at what PolynomialFeatures does to some simple data x = np . arange ( 1 , 11 ) y = x ** 3 print ( x ) print ( y ) [ 1 2 3 4 5 6 7 8 9 10] [ 1 8 27 64 125 216 343 512 729 1000] features = PolynomialFeatures ( degree = 3 ) X2 = features . fit_transform ( x . reshape ( - 1 , 1 )) we see our new feature set contains our original features, plus new features up to the nth-degree polynomial we set when creating the features object from PolynomialFeatures print ( X2 ) [[ 1. 1. 1. 1.] [ 1. 2. 4. 8.] [ 1. 3. 9. 27.] [ 1. 4. 16. 64.] [ 1. 5. 25. 125.] [ 1. 6. 36. 216.] [ 1. 7. 49. 343.] [ 1. 8. 64. 512.] [ 1. 9. 81. 729.] [ 1. 10. 100. 1000.]] model = LinearRegression () . fit ( X2 , y ) yhat = model . predict ( X2 ) plt . scatter ( x , y ) plt . plot ( x , yhat );","title":"4.2.1 Creating Polynomials"},{"location":"solutions/SOLN_S4_Feature_Engineering/#422-dealing-with-time-series","text":"back to top Often, we will be dealing with time series data, whether its data generated by machinery, reactors, or sales and customers. In the following we discuss some simple practices for dealing with time series data.","title":"4.2.2 Dealing with Time Series"},{"location":"solutions/SOLN_S4_Feature_Engineering/#4221-enrichment-fast-fourier-transform","text":"back to top Sometimes we'll want to create a more sophisticated transformation of our input data. As engineers, this can often have to do with some empirical knowledge we understand about our process. When working with equipment and machinery, we will often want to convert a signal from the time to frequency domain. Let's cover how we can do that with numpy! [img src]( https://www.nti-audio.com/en/support/know-how/fast-fourier-transform-fft#:~:text=The%20%22Fast%20Fourier%20Transform%22%20(,frequency%20information%20about%20the%20signal .) What I've drawn here in the following is called a square-wave signal t = np . linspace ( 0 , 1 , 501 ) # FFT should be given an integer number of cycles so we leave out last sample t = t [: - 1 ] f = 5 # linear frequency in Hz w = f * 2 * np . pi # radial frequency h = 4 # height of square wave amp = 4 * h / np . pi s = amp * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) # here is the call to numpy FFT F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ], d = t [ 1 ]) # reorder frequency spectrum and frequency bins with 0 Hz at the center F = np . fft . fftshift ( F ) freq = np . fft . fftshift ( freq ) # scale frequency spectrum to correct amplitude F = F / t . size # amplitudes amps = [ max ( np . sin ( w * t )), max ( np . sin ( w * t * 3 ) / 3 ), max ( np . sin ( w * t * 5 ) / 5 )] fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) ax [ 0 ] . set_xlim ( 0 , 1 ) ax [ 0 ] . set_xlabel ( 'Time (s)' ) # tells us about the amplitude of the component at the # corresponding frequency. Multiplied by two because the # signal power is split between (-) and (+) frequency branches # of FFT, but we're only visualizing the (+) branch magnitude = 2 * np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , 30 ) ax [ 1 ] . set_xlabel ( 'Frequency (Hz)' ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain')","title":"\ud83c\udf52 4.2.2.1 Enrichment: Fast Fourier Transform"},{"location":"solutions/SOLN_S4_Feature_Engineering/#4222-rolling-windows","text":"back to top to see an example of this dataset in action visit this link One powerful technique for dealing with time series data, is to create a rolling window of features based on the historical data. The proper window size can usually be determined by trial and error, or constraints around access to the data itself. In the above gif, we have a window size of 7. What that means is for whatever time step units we are in (that could be minutes, days, months, etc.) we will have 7 of them included in a single instance or observation. This instance or observation is then interpreted by our model and used to assess the target value, typically the quantity in the very next time step after the window (the green bar in the gif). Let's take an example with the orders data tidy_orders = orders . melt ( id_vars = orders . columns [: 6 ], var_name = 'Date' , value_name = 'KG' ) display ( tidy_orders . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 In the next exercise, we are going to attempt to predict an order amount, based on the previous order history. We will scrub all categorical labels and only use historical amounts to inform our models. In effect the data that the model will see will look like the following: fig , ax = plt . subplots ( 3 , 2 , figsize = ( 10 , 20 )) indices = np . argwhere ( ax ) color_dict = { 0 : 'tab:blue' , 1 : 'tab:green' , 2 : 'tab:orange' , 3 : 'tab:red' , 4 : 'tab:pink' , 5 : 'tab:brown' } for index , customer in enumerate ( tidy_orders . Customer . unique ()): orders . loc [ orders . Customer == customer ] . iloc [:, 6 :] . reset_index () . T . plot ( c = color_dict [ index ], legend = False , ax = ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]]) ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]] . set_title ( customer ) What we may notice is that there is very little noise or drift in our order history, but there is certainly some periodicity. The question is can we use a linear model to predict the next order amount based on these history orders?","title":"4.2.2.2 Rolling Windows"},{"location":"solutions/SOLN_S4_Feature_Engineering/#exercise-2-optimize-rolling-window-size-for-customer-forecasts","text":"For this exercise, you will use the process_data function below to help you optimize the window size for predicting the order quantity in any given month. You will train a LinearRegression model. create a model using a window size of 3 and predict the order quantity for the month immediately following the window create a model for window sizes 1-11 and report the \\(R^2\\) for each model def process_data ( Xy , time_cols = 12 , window = 3 , remove_null = False ): \"\"\" This function splits your time series data into the proper windows Parameters ---------- Xy: array The input data. If there are non-time series columns, assumes they are on the left and time columns are on the right. time_cols: int The number of time columns, default 12 window: int The time window size, default 3 Returns ------- X_: array The independent variables, includes time and non-time series columns with the new window y_: array The dependent variable, selected from the time columns at the end of the window labels: The time series labels, can be used in subsequent plot \"\"\" # separate the non-time series columns X_cat = Xy [:,: - time_cols ] # select the columns to apply the sweeping window X = Xy [:, - time_cols :] X_ = [] y = [] for i in range ( X . shape [ 1 ] - window ): # after attaching the current window to the non-time series # columns, add it to a growing list X_ . append ( np . concatenate (( X_cat , X [:, i : i + window ]), axis = 1 )) # add the next time delta after the window to the list of y # values y . append ( X [:, i + window ]) # X_ is 3D: [number of replicates from sweeping window, # length of input data, # size of new feature with categories and time] # we want to reshape X_ so that the replicates due to the sweeping window is # a part of the same dimension as the instances of the input data X_ = np . array ( X_ ) . reshape ( X . shape [ 0 ] * np . array ( X_ ) . shape [ 0 ], window + X_cat . shape [ 1 ]) y = np . array ( y ) . reshape ( X . shape [ 0 ] * np . array ( y ) . shape [ 0 ],) if remove_null : # remove training data where the target is 0 (may be unfair advantage) X_ = X_ [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] y = y [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] # create labels that show the previous month values used to train the model labels = [] for row in X_ : labels . append ( \"X: {} \" . format ( np . array2string ( row [ - window :] . astype ( float ) . round ()))) return X_ , y , labels # Code Cell for Exercise 2 kg_month_data = orders . values [:, 6 :] # use kg_month_data and the function process_data to create your X, y arrays # then use train_test_split to create train and test portions # USE y_test and y_pred for your actual and true test data # change only window parameter in process_data() print ( \"window R2\" ) for window in range ( 1 , 12 ): ###################### ### YOUR CODE HERE ### ###################### X , y , labels = process_data ( kg_month_data , time_cols = 12 , window = 3 ) features = PolynomialFeatures ( degree = 4 ) X2 = features . fit_transform ( X ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.6 ) model = LinearRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( \" {} , \\t {:.2f} \" . format ( window , r2_score ( y_test , y_pred ))) window R2 1, 0.97 2, 0.88 3, 0.80 4, 0.96 5, 0.90 6, 0.86 7, 0.61 8, 0.89 9, 0.76 10, 0.74 11, 0.87 #### RUN AFTER EXERCISE 2.2.2.2.1 #### fig = px . scatter ( x = y_test , y = y_pred , labels = { \"y\" : \"Prediction\" , \"x\" : \"Actual\" }) fig . update_layout ( autosize = False , width = 800 , height = 500 , title = 'R2: {:.3f} ' . format ( r2_score ( y_test , y_pred )) ) require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"239a6e0b-8482-49c8-9619-ebde564b91cd\")) { Plotly.newPlot( \"239a6e0b-8482-49c8-9619-ebde564b91cd\", [{\"hovertemplate\":\"Actual=%{x}<br>Prediction=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4528.380503144654,64.01384083044982,15617.489114658929,4528.380503144654,5698.392857142857,124348.89322916669,21939.111111111117,5712.33671988389,64983.42138364781,1483.5,93302.72435897436,1236.25,773.5849056603774,518.4642857142858,3798.928571428572,13671.296296296296,188.7692307692308,4057.832278481013,101149.25,14626.074074074077,12274.009146341465,141456.76315789475,4793.829296424452,74.78571428571429,10861.245674740485,86.24595469255664,19129.33333333333,39307.04113924051,74.78571428571429,46247.0,188.7692307692308,85.32258064516128,90677.41228070176,9306.611111111111,220.55016181229772,466477.57812500006,1158.125,220.55016181229772,1901.1290322580644,19129.33333333333,2637.8993710691825,1868.867924528302,3798.928571428572,2252.261437908497,5222.5,181.5552699228792,5038.387096774193,120.98615916955016,10146.898432174508,85.32258064516128,286.6666666666667,18613.222222222223,818.1891025641025,2499.201741654572,2010.9477124183009,6984.389273356401,42704.25889967639,11715.487421383648,39307.04113924051,440.859375,428.0,96.59546925566345,102.64285714285714,3569.0359477124184,13671.296296296296,773.5849056603774,2010.9477124183009,42704.25889967639,40608.41346153846,83347.75862068967,10796.31640058055,74.78571428571429,80801.72807017544,2540.0,5222.5,131.55526992287918,299024.0885416667,3882.680981595092,895.9082278481012,1180.5379746835442,15617.489114658929,85.32258064516128,6185.59509202454,16749.13157894737,47142.92857142857,3786.966463414634,936.280487804878,15244.632812500002,3272.80163599182,1271.904761904762,47142.92857142857,356.6666666666667,10861.245674740485,108.64285714285714,21438.1875,16918.132716049382,7892.142857142857,1901.1290322580644,2647.5,5038.387096774193,1677.5078125000002,39307.04113924051,51795.97953216374,916.3717948717948,41871.69230769231,12870.766853932584,1271.904761904762,181.5552699228792,211540.7575757576,3798.928571428572,2054.328561690525,3578.592162554427,2854.854368932039,51795.97953216374,11791.8,149.57142857142858,103.3653846153846,119015.43674698794,2499.201741654572,19286.94968553459,2185.3410740203194,19569.69230769231,12317.397660818711,6965.361445783132,86.24595469255664,3071.4691011235955,13867.125,50574.625,14626.074074074077,1005.4545454545454,16918.132716049382,2543.106741573034,10245.87593728698,8608.709677419354,150.3654970760234,3517.9333333333334,14626.074074074077,3177.0,2499.201741654572,345.6428571428572,10736.622807017544,13799.769319492503,12691.209677419354,11.175616835994196,2010.9477124183009,10736.622807017544,71138.51966292135,8104.40251572327,62.168396770472896,211540.7575757576,16569.261006289307,445.54838709677415,16918.132716049382,42.484177215189874,150.3654970760234,11.175616835994196,299024.0885416667,579.0625,10146.898432174508,181.5552699228792,494.57142857142856,411.1303344867359,2355.267295597484,494.57142857142856,79091.23475609756,10146.898432174508,1005.4545454545454,5038.387096774193,191.11111111111111,9304.668674698794,19569.69230769231,8250.491573033707,466477.57812500006,3048.0,1630.1966292134832,64.01384083044982,3569.0359477124184,1901.1290322580644,3272.80163599182,9772.200520833334,2185.3410740203194,28694.0,314.3203883495146,8104.40251572327,18557.572327044025,2231.547169811321,1432.2712418300653,773.5849056603774,40735.10802469136,314.3203883495146,471.91011235955057,42.484177215189874,9194.277108433736,8173.714285714285,837.8787878787879,1604.1437908496732,22.50980392156863,14.663461538461538,663176.1973875181,2854.854368932039,428.0,43.64516129032258,1526.2857142857142,1432.2712418300653,5456.578050443081,22331.935185185182,150.3654970760234,4057.832278481013,1868.867924528302,1630.1966292134832,4723.49129172714,5038.387096774193,19364.70552147239,117.22222222222224,15110.75,4057.832278481013,83347.75862068967,7892.142857142857,19129.33333333333,1968.8904494382025,207.39062500000003,62.168396770472896,3578.592162554427,1677.5078125000002,2499.201741654572,1254.4444444444443,1236.25,3578.592162554427,1992.4528301886792,14626.074074074077,1236.25,6965.361445783132,36.37096774193548,40735.10802469136,20.098039215686278,1432.2712418300653,538.8571428571429,101149.25,93302.72435897436,181.5552699228792,28694.0,2054.328561690525,5872.384615384615,31412.04644412192,5872.384615384615,854.952380952381,31412.04644412192,1253403.0130624091,63142.24137931035,20.098039215686278,3569.0359477124184,16569.261006289307,19286.94968553459,19286.94968553459,837.8787878787879,3665.809768637532,108.64285714285714,56.07911392405064,13671.296296296296,371.2903225806451,4057.832278481013,47142.92857142857,21438.1875,54.833333333333336,26081.56401384083,2540.0,115.76923076923076,16.423076923076923,8608.709677419354,4463.780120481927,8250.491573033707,15110.75,8173.714285714285,157100.37650602407,104499.0512820513,9076.930817610064,2611.25,428.0,14.663461538461538,2694.0,3569.0359477124184,86.24595469255664,678.7183544303797,494.57142857142856,712.4603174603175,663176.1973875181,10245.87593728698,5071.786163522012,1868.867924528302,26081.56401384083,3403.344867358708,4793.829296424452,64983.42138364781,3272.80163599182,5712.33671988389,9194.277108433736,608.4770114942529,42.484177215189874,466477.57812500006,3695.444059976932,3517.9333333333334,345.6428571428572,579.0625,7892.142857142857,5038.387096774193,45601.61516853933,1992.4528301886792,2647.5,5872.384615384615,6965.361445783132,64.01384083044982,45601.61516853933,23123.5,1992.4528301886792,2540.0,9060.337370242214,14.663461538461538,3882.680981595092,36.37096774193548,193984.2734375,2231.547169811321,108.64285714285714,329.7142857142857,117.49826989619376,773.5849056603774,36.37096774193548,8608.709677419354,371.2903225806451,45481.42307692308,10245.87593728698,63142.24137931035,678.7183544303797,11555.9375,4528.380503144654,2499.201741654572,10736.622807017544,5222.5,1901.1290322580644,93302.72435897436,1702.1929824561405,114208.8534107402,343.1394601542417,10245.87593728698,8173.714285714285,140637.14285714287,132.94270833333334,579.0625,663176.1973875181,1180.5379746835442,1236.25,26081.56401384083,854.952380952381,2010.9477124183009,30698.85714285714,11.175616835994196,90677.41228070176,38128.80258899677,663176.1973875181,71138.51966292135,1236.25,31412.04644412192,50574.625,83347.75862068967,12274.009146341465,90677.41228070176,808.2857142857143,11.175616835994196,11555.9375,86.24595469255664,41871.69230769231,19129.33333333333,329.7142857142857,1236.25,2637.8993710691825,579.7777777777778,30698.85714285714,329.7142857142857,2647.5,466477.57812500006,102.64285714285714,12691.209677419354,12317.397660818711,220.55016181229772,19147.454268292684,9304.668674698794,4057.832278481013,47142.92857142857,1630.1966292134832,4463.780120481927,2002.1844660194176,5071.786163522012,7035.866666666667,13504.20634920635,21601.383647798742,10245.87593728698,10861.245674740485,176.36477987421384,12691.209677419354,1432.2712418300653,608.4770114942529,10736.622807017544,3695.444059976932,157100.37650602407,1702.1929824561405,51795.97953216374,2386.449438202247,117.22222222222224,120.98615916955016,777.0363321799308,12274.009146341465,2611.25,2242.446601941748,168764.57142857145,627.2222222222222,40608.41346153846,2002.1844660194176,157.46855345911948,2647.5,119015.43674698794,579.0625,329.7142857142857,13121.345911949686,71138.51966292135,207.39062500000003,30698.85714285714,9060.337370242214,1529.7752808988764,3071.4691011235955,46247.0,538.8571428571429,16347.42857142857,23123.5,132.94270833333334,6984.389273356401,12691.209677419354,2499.201741654572,30221.5,15229.451612903224,191.11111111111111,428.0,3578.592162554427,11715.487421383648,19129.33333333333,30221.5,6432.321799307958,41871.69230769231,42.484177215189874,16918.132716049382,3695.444059976932,13504.20634920635,15617.489114658929,117.49826989619376,16569.261006289307,96.59546925566345,678.7183544303797,10146.898432174508,678.7183544303797,131.55526992287918,47142.92857142857,79091.23475609756,736.1797752808989,1180.5379746835442,29354.53846153846,45481.42307692308,21438.1875,6965.361445783132,31412.04644412192,10146.898432174508,11791.8,15617.489114658929,1677.5078125000002,5456.578050443081,14626.074074074077,94285.85714285714,248.6394601542416,5456.578050443081,26081.56401384083,63142.24137931035,12317.397660818711,12317.397660818711,608.4770114942529,11.175616835994196,10861.245674740485,2566.6935483870966,11555.9375,678.7183544303797,1529.7752808988764,1432.2712418300653,518.4642857142858,8104.40251572327,63142.24137931035,3798.928571428572,4057.832278481013,1526.2857142857142,2854.854368932039,3403.344867358708,9306.611111111111,538.8571428571429,13671.296296296296,117.22222222222224,343.1394601542417],\"xaxis\":\"x\",\"y\":[4785.592155979585,378.9394302913829,15365.600809369838,4785.592155979585,4070.255357333951,134757.3771732384,14409.557558767568,5813.845282617682,61516.49587090381,1510.6755468797405,98738.27282520698,1684.2601507265988,1088.107600326137,647.864369876902,4263.755978756964,14168.403568365671,487.3481032940997,4287.792726026477,51779.415544128475,19934.734830917627,12141.409803201397,87747.52966460225,5818.94510670397,377.43130038813075,12797.394868451114,389.0359797814606,19263.507069512012,40163.79013647013,377.43130038813075,23840.068464074022,487.3481032940997,400.48488833402286,126112.56377290396,11223.080944977459,522.1062342859491,415175.9779430929,894.6750043489748,519.4101103091041,2138.6141864173633,19263.507069512012,2790.085070463646,2196.4290704108694,4263.755978756964,2244.514746383961,2963.0052419712265,602.4723068239653,5217.814575086902,410.08771833436947,16913.133647514336,390.2429375597753,489.6064298303322,16311.253798810561,1109.562627058514,2715.348806915594,2281.9990642021894,6353.789558047462,37073.75373050823,11896.354947646087,38882.42895370776,797.0601491084628,703.1493477716122,388.482661364839,481.8437277075316,3747.013377347842,16794.12337436424,1571.4700357476117,2257.4161561731,37073.75373050823,43146.57407027896,76460.52042984031,5813.845282617682,433.93372568151483,50253.31197444373,2781.8507793892813,2963.0052419712265,450.36262704088375,323624.6828685297,2286.347850033103,959.8167805278175,1463.9293549917084,25867.060321463283,388.50455558540244,3461.34813854915,10658.874464205835,48286.68591679114,4070.54807420052,1208.1895902424544,13863.416557439898,3913.805102706469,1545.439458867743,48286.68591679114,664.1495092797293,12797.394868451114,415.88888264864494,24308.715376885695,17460.789585310762,13878.585909613186,2138.6141864173633,3258.406190332729,5925.265679815453,1797.2371240684145,47713.131047358365,51804.0528093027,1099.5606427530397,40683.104312073505,8508.45844428347,1571.3535403980425,505.4911026847705,213132.54291644722,4263.755978756964,2570.353838719455,3926.506181336219,3317.1418553360736,54758.90192605476,20585.396531692135,377.43130038813075,406.91804655949966,120990.40817472423,4395.853637740449,19387.390319584494,2412.686086825897,19699.926384269966,12552.025861937103,7368.391717428835,390.09029678781303,2262.905415567944,11572.521113313724,56931.5110267007,15545.52362930884,1239.906233381196,17460.789585310762,1926.1552505875645,10673.158532726991,9907.699090223898,454.81639486308137,6355.618165841122,19934.734830917627,2858.35606741023,2834.2664976391197,665.469888483012,11592.823783640883,16177.143075924467,12543.725418313918,317.63569157648635,2257.4161561731,15201.471014315619,45645.23591676211,8323.62833082682,376.8168895165831,213132.54291644722,16698.569941711827,663.3571920970296,20710.099320853376,346.28222029939593,513.9332229141203,323.60536008687086,323624.6828685297,1301.212252741579,16913.133647514336,505.4911026847705,632.0783029798903,778.1760841513764,2635.5625663138067,632.0783029798903,76574.72476531091,11492.974658748008,1239.906233381196,5163.94089137517,494.71513791291954,11527.578799029196,19176.79696136619,8979.131225405608,415175.9779430929,2754.6914615362707,2567.0739167219367,378.9394302913829,3747.013377347842,2138.6141864173633,3913.805102706469,10871.491666793676,2516.669574686992,18752.149151877966,636.9170394143573,8323.62833082682,17785.632844249434,2407.321312452001,1686.4843440391876,1051.2986748894868,49435.50780135217,610.4363857799107,960.0492155948752,346.28222029939593,7141.329503589847,8187.405610699661,1148.288091129662,1686.4843440391876,324.6947768318741,320.7835266802878,639820.6545150073,3317.1418553360736,703.1493477716122,340.3871184326995,1531.8394329987157,1686.4843440391876,9236.305456033022,16619.83993793382,463.3944227377964,4287.792726026477,3364.1619784229933,2019.1544585081797,2715.348806915594,5163.94089137517,10185.63650295469,436.06660273854754,26293.480165383102,4218.370391131691,76460.52042984031,7915.880111460361,25978.487349635714,2375.22637673234,489.7605752448112,376.8168895165831,3756.2286447454576,1797.2371240684145,3060.8574440447824,1384.0395692783663,1535.8631900519042,6162.533552625073,2276.6020512892946,14800.536096877271,1535.8631900519042,7368.391717428835,340.3871184326995,40283.97684677175,325.06940680600604,1695.6732046475156,1028.5053850528614,51779.415544128475,92018.35013115287,505.4911026847705,18752.149151877966,3667.712553203018,5968.182975715722,34939.27854355379,6424.274585977019,992.354894936303,30596.638581578518,639820.6545150073,65413.624508456036,324.82371754886964,3747.013377347842,16698.569941711827,19387.390319584494,19387.390319584494,1148.288091129662,6305.285970941779,492.1628050669814,346.28222029939593,14168.403568365671,663.3571920970296,4218.370391131691,45766.272960899,37175.69967364701,369.63985077669076,22891.92825541111,2754.6914615362707,405.6544524204279,319.54816919005003,8698.931074864986,5689.033000226852,11752.173931839643,26293.480165383102,8187.405610699661,117110.65135820676,90877.76737128492,8855.335527589015,3229.016296504339,703.1493477716122,319.72742324233695,1604.2554168141014,3769.9108375996702,396.30197122916206,959.8167805278175,632.0783029798903,999.9729800621147,731503.1707028296,17075.133945943726,5082.686543693957,2107.5040932279385,22891.92825541111,4219.676962291559,5818.94510670397,61516.49587090381,3617.0754679045876,5813.845282617682,7141.329503589847,932.7370475860574,346.28222029939593,415175.9779430929,4555.635369302835,3885.8140646224183,769.1955699368395,953.6647702716481,8337.819513819635,5163.94089137517,48246.709882894014,2276.6020512892946,2886.664844581823,5968.182975715722,7368.391717428835,378.9394302913829,63573.70285359673,40074.17801737161,2276.6020512892946,3138.4978117622,8151.559019833033,319.72742324233695,2286.347850033103,340.3871184326995,172828.89934304,2407.321312452001,410.08048522816836,747.818087951304,407.0672088952674,1088.107600326137,341.517050774033,8698.931074864986,667.3272676429804,39725.42364104348,10673.158532726991,65413.624508456036,959.8167805278175,13195.11592262944,4785.592155979585,2834.2664976391197,15201.471014315619,2963.0052419712265,2425.884027039221,98738.27282520698,1997.7384398419902,115873.73047883328,489.0303345318461,10673.158532726991,9457.064447629586,137428.5757499219,449.0579108608325,1301.212252741579,1085751.3630379443,1502.4134405828336,1684.2601507265988,22891.92825541111,992.354894936303,2244.514746383961,31550.15438840883,316.0906893952995,90462.3574812228,37318.37237669782,639820.6545150073,45645.23591676211,1684.2601507265988,30596.638581578518,56931.5110267007,76460.52042984031,12141.409803201397,95635.30862544454,839.3500692330289,316.62245060704845,13195.11592262944,389.0359797814606,41802.402147140514,19263.507069512012,747.818087951304,1535.8631900519042,2790.085070463646,803.8781088470796,29908.894508749334,648.872490598366,2858.35606741023,415175.9779430929,409.7821718132848,12543.725418313918,17394.662595168105,522.1062342859491,12141.409803201397,9740.51689224938,5199.4234913728005,45766.272960899,2567.0739167219367,4831.715916465941,2417.5880406486763,5082.686543693957,3697.7340063828487,13472.111607949182,20652.753180263422,11602.10423834572,12797.394868451114,471.44073265789405,12543.725418313918,1695.6732046475156,932.7370475860574,10980.322568203586,4555.635369302835,117110.65135820676,2094.8448847085992,54758.90192605476,1826.31018819446,421.9954832228255,410.08771833436947,978.2266656454766,12141.409803201397,4796.262256309382,2236.064155834039,135924.7927322855,1041.1204286449113,40221.84276082843,2273.3851256392354,461.10966099955675,2858.35606741023,143848.60557702882,953.6647702716481,648.872490598366,12664.975331868296,45645.23591676211,489.7605752448112,29908.894508749334,8151.559019833033,2427.747778574022,2262.905415567944,23840.068464074022,866.7970691016989,8187.405610699661,26195.686125920434,449.0579108608325,6353.789558047462,12679.428015781728,2715.348806915594,15684.810589735318,12543.725418313918,504.4494927620649,703.1493477716122,4250.960414621229,11896.354947646087,25978.487349635714,15684.810589735318,5875.699548389586,41802.402147140514,347.0090494475603,16909.279272683176,4555.635369302835,15368.26716660901,16108.716384819742,407.0672088952674,16698.569941711827,388.482661364839,993.553834752781,10573.002781125038,971.4284514069652,450.36262704088375,45766.272960899,76574.72476531091,774.5157686911542,1443.732437768414,19176.79696136619,39725.42364104348,24308.715376885695,8706.164457317907,32091.295039989156,11492.974658748008,13508.076892121393,16108.716384819742,1797.2371240684145,5826.8479598607,15545.52362930884,45766.272960899,438.4351253031718,5826.8479598607,22891.92825541111,65413.624508456036,12552.025861937103,12552.025861937103,932.7370475860574,316.0906893952995,12797.394868451114,2860.1717851882872,11807.964454280216,1123.9090100996427,1913.5805743985902,1816.3382332975395,647.864369876902,8323.62833082682,65413.624508456036,5403.795319141717,4218.370391131691,1531.8394329987157,3111.527273406017,4219.676962291559,11223.080944977459,866.7970691016989,13722.736128528055,436.06660273854754,489.0303345318461],\"yaxis\":\"y\",\"type\":\"scatter\"}], {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Actual\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prediction\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"autosize\":false,\"width\":800,\"height\":500,\"title\":{\"text\":\"R2: 0.869\"}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('239a6e0b-8482-49c8-9619-ebde564b91cd'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; });","title":"\ud83c\udfcb\ufe0f Exercise 2: Optimize Rolling Window Size for Customer Forecasts"},{"location":"solutions/SOLN_S4_Feature_Engineering/#423-image-preprocessing","text":"back to top Image preprocessing is beyond the scope of this session. We cover this topic in General Applications of Neural Networks . For now, know that there is a wealth of considerations for how to handle images, and they all fit within the realm of feature engineering.","title":"4.2.3 Image Preprocessing"},{"location":"solutions/SOLN_S4_Feature_Engineering/#43-transformed-features","text":"back to top Transformed features, are features that we would like to augment based on their relationship within their own distribution or to other (allegedly) independent data within our training set. e.g. we're not deriving new features based on some empirical knowledge of the data, rather we are changing them due to statistical properties that we can assess based on the data itself.","title":"4.3 Transformed Features"},{"location":"solutions/SOLN_S4_Feature_Engineering/#431-skewness","text":"back to top Skewed data can lead to imbalances in our model prediction. Why? Skewed values in the distribution will bias the mean. When assigning weights to this input feature, therefore, the model will give preferential treatment to these values. To demonstrate, I'm going to use scipy to create some skewed data. from scipy.stats import skewnorm a = 10 x = np . linspace ( skewnorm . ppf ( 0.01 , a ), skewnorm . ppf ( 0.99 , a ), 100 ) plt . plot ( x , skewnorm . pdf ( x , a ), 'r-' , lw = 5 , alpha = 0.6 , label = 'skewnorm pdf' ) [<matplotlib.lines.Line2D at 0x7fa48d14a130>] We can now generate a random population based on this distribution r = skewnorm . rvs ( a , size = 1000 ) plt . hist ( r ) (array([113., 267., 225., 172., 116., 62., 26., 13., 2., 4.]), array([-0.19733964, 0.15303313, 0.50340589, 0.85377866, 1.20415142, 1.55452419, 1.90489696, 2.25526972, 2.60564249, 2.95601526, 3.30638802]), <BarContainer object of 10 artists>) Unskewed data will return something close to 0 from calling df.skew() . When dealing with actual data, we can use df.skew() to determine whether we should transform our data. x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) x [ 'Skewed Data' ] . skew () 0.9141902067398219 There are a handful of ways to deal with skewed data: log transform square root transform Box-Cox transform Let's try the first two print ( 'square root transformed skew: {:.4f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) print ( 'log transformed skew: {:.4f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . legend () square root transformed skew: 0.0561 log transformed skew: -1.6916 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in sqrt /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log <matplotlib.legend.Legend at 0x7fa49157d340> We see we didn't get much traction with the log transform, and the log transform will not be able to handle 0 values, and so we will sometimes have to code exceptions for those. Box-Cox is often a good route to go, but it has the added restriction that the data has to all be above 0. Let's create a new distribution with this added restriction a = 6 r = skewnorm . rvs ( a , size = 1000 ) r = [ i for i in r if i > 0 ] plt . hist ( r ) (array([220., 277., 182., 127., 66., 39., 17., 5., 4., 2.]), array([2.17150536e-03, 3.88613862e-01, 7.75056219e-01, 1.16149858e+00, 1.54794093e+00, 1.93438329e+00, 2.32082565e+00, 2.70726800e+00, 3.09371036e+00, 3.48015272e+00, 3.86659507e+00]), <BarContainer object of 10 artists>) from scipy import stats x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ], alpha = 0.5 , label = 'box-cox: {:.2f} ' . format ( pd . DataFrame ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ])[ 0 ] . skew ())) ax . legend () <matplotlib.legend.Legend at 0x7fa492058f40>","title":"4.3.1 Skewness"},{"location":"solutions/SOLN_S4_Feature_Engineering/#exercise-3-transform-data-from-a-gamma-distribution","text":"Repeat section 2.3.1, this time synthesizing a gamma distribution and transforming it. Which transformation best reduces the skew? Do this for a dataset that does not contain values at or below 0. # code cell for exercise 3 from scipy.stats import gamma a = 6 r = gamma . rvs ( a , size = 1000 ) r = [ i for i in r if i > 0 ] x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ], alpha = 0.5 , label = 'box-cox: {:.2f} ' . format ( pd . DataFrame ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ])[ 0 ] . skew ())) ax . legend () <matplotlib.legend.Legend at 0x7fa49111dd30>","title":"\ud83c\udfcb\ufe0f Exercise 3: Transform data from a gamma distribution"},{"location":"solutions/SOLN_S4_Feature_Engineering/#432-colinearity","text":"back to top Colinearity can also affect the performance of your machine learning model. In particular, if features are colinear, it can be easy for your model to overfit to your training dataset. This is often mitigated by regularization. If you're curious you can read more about it on this discussion from StackExchange . We will still explore it explicitly here by calculating the Variance Inflation Factor (VIF) on some hypothetical data. VIF = \\frac{1}{1-R^2} Usually we are concerned about data with a VIF above 10","title":"4.3.2 Colinearity"},{"location":"solutions/SOLN_S4_Feature_Engineering/#4321-detecting-colinearity","text":"back to top from statsmodels.stats.outliers_influence import variance_inflation_factor Step 1: Make some data # we can throttle the error rate random . seed ( 42 ) # x2 will be sqrt of x1 plus some error def func ( x , err ): return x ** .5 + ( err * random . randint ( - 1 , 1 ) * random . random () * x ) x0 = range ( 100 ) x1 = [ func ( i , .05 ) for i in x0 ] # HIGH degree of colinearity with x0 x2 = [ func ( i , 1 ) for i in x0 ] # MED degree of colinearity with x0 x3 = [ random . randint ( 0 , 100 ) for i in x0 ] # NO degree of colinearity with x0 # take a look fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) ax . plot ( x0 , x1 , label = 'x1' ) ax . plot ( x0 , x2 , label = 'x2' ) ax . plot ( x0 , x3 , label = 'x3' ) ax . legend () <matplotlib.legend.Legend at 0x7fa49121ca90> To calculate the colinearities I'm going to aggregate these x's into a dataframe: colin = pd . DataFrame ([ x0 , x1 , x2 , x3 ]) . T colin . columns = [ 'x0' , 'x1' , 'x2' , 'x3' ] colin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 0 0.0 0.000000 0.000000 29.0 1 1.0 1.013751 0.721523 28.0 2 2.0 1.400260 1.414214 3.0 3 3.0 1.630546 -0.438007 84.0 4 4.0 2.017388 4.304847 24.0 Step 2: Calculate VIF factors # calculate VIF factors vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( colin . values , i ) for i in range ( colin . shape [ 1 ])] vif [ \"features\" ] = colin . columns Step 3: Inspect VIF factors # inspect VIF factors display ( vif ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 12.555415 x0 1 15.823872 x1 2 1.030609 x2 3 3.559468 x3 In this case, we may remove either x0 or x1 from the dataset.","title":"4.3.2.1 Detecting Colinearity"},{"location":"solutions/SOLN_S4_Feature_Engineering/#4322-fixing-colinearity","text":"back to top It is good to aknowledge where colinearity exists as this will influence the interpretability of your model. In most cases, however, it won't have a heavy influence on the performance of your model. A simple method of dealing with colinearity, is to remove the highest VIF features from your model, iteratively, assessing the performance and determining whether to keep the variable or not. Another method is to create some linear combination of the correlated variables. This is encapsulated in the section on dimensionality reduction.","title":"4.3.2.2 Fixing Colinearity"},{"location":"solutions/SOLN_S4_Feature_Engineering/#433-normalization","text":"back to top Many learning algorithms require zero mean and unit variance to behave optimally. Sklearn preprocessing library contains a very usefull class, StandardScaler for handling this automatically for us. from sklearn.preprocessing import StandardScaler scaler = StandardScaler () normed = scaler . fit_transform ( colin ) colin [[ 'x0' , 'x1' , 'x2' , 'x3' ]] . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'> pd . DataFrame ( normed , columns = [[ 'x0' , 'x1' , 'x2' , 'x3' ]]) . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'>","title":"4.3.3 Normalization"},{"location":"solutions/SOLN_S4_Feature_Engineering/#exercise-4-normalization-affect-on-vif","text":"In the above, we saw how to scale and center variables. How does this affect VIF? Calculate the VIF for the scaled-centered data # Code Cell for Exercise 4 vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( normed , i ) for i in range ( normed . shape [ 1 ])] vif [ \"features\" ] = colin . columns display ( vif ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 3.286048 x0 1 3.296881 x1 2 1.015805 x2 3 1.035537 x3","title":"\ud83c\udfcb\ufe0f Exercise 4: Normalization affect on VIF"},{"location":"solutions/SOLN_S4_Feature_Engineering/#434-dimensionality-reduction","text":"back to top Dimensionality reduction is an awesome way to do feature engineering. It is very commonly used. Because it is also an unsupervised machine learning technique, we will visit this topic in that section.","title":"4.3.4 Dimensionality Reduction"},{"location":"solutions/SOLN_S4_Feature_Engineering/#44-missing-data","text":"back to top We will often have missing data in our datasets. How do we deal with this? Let's start by making some data with missing data. We'll use a numpy nan datatype to do this from numpy import nan X = np . array ([[ nan , 0 , 3 ], [ 3 , 7 , 9 ], [ 3 , 5 , 2 ], [ 4 , nan , 6 ], [ 8 , 8 , 1 ]]) y = np . array ([ 14 , 16 , - 1 , 8 , - 5 ])","title":"4.4 Missing Data"},{"location":"solutions/SOLN_S4_Feature_Engineering/#441-imputation","text":"back to top A very common strategy is to impute or fill in the missing data, based on basic statistical descriptions of the feature column (mode, mean, and median) from sklearn.impute import SimpleImputer # strategy = 'mean' will replace nan's with mean value # of the column # others are median and most_frequent (mode) imp = SimpleImputer ( strategy = 'mean' ) X2 = imp . fit_transform ( X ) X2 array([[4.5, 0. , 3. ], [3. , 7. , 9. ], [3. , 5. , 2. ], [4. , 5. , 6. ], [8. , 8. , 1. ]])","title":"4.4.1 Imputation"},{"location":"solutions/SOLN_S4_Feature_Engineering/#442-other-strategies","text":"back to top Depending on the severity of missing data, you will sometimes opt to remove the whole column, or perhaps apply some simple learning to fill in the missing data. This is a great article on more advanced strategies for handling missing data.","title":"4.4.2 Other Strategies"},{"location":"solutions/SOLN_S4_Feature_Engineering/#references","text":"back to top * Box Cox * Multicolinearity * Missing Data","title":"References"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/","text":"Data Science Foundations, Session 5: Unsupervised Learning: Clustering and Dimensionality Reduction \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we began our discussion on feature engineering and ended with a sneak peak into dimensionality reduction. This last topic deserves a whole session on its own because its use case is not limited to feature engineering! It can also be used as a tool for visualization, for noise filtering, and much more. In this session we discuss dimensionality reduction along with other unsupervised learning methods. Up until now, the only learning estimators we've looked at were supervised ones: estimators that predict labels based on training data. Here, however, we are interested in uncovering aspects of the data without reference to any known labels. The usefulness for these learners will become immediately apparent when we revist our wine quality models from Course 1, Session 7! 5.0 Preparing Environment and Importing Data \u00b6 back to top 5.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score , silhouette_score , calinski_harabasz_score from sklearn.mixture import GaussianMixture from sklearn.impute import SimpleImputer from scipy.spatial.distance import cdist import seaborn as sns ; sns . set () import copy 5.0.2 Load and Process Dataset \u00b6 back to top wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y_wine = X . pop ( target ) X = imp . fit_transform ( X ) X_wine = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X_wine [:, 2 :] = scaler . fit_transform ( X_wine [:, 2 :]) wine = pd . DataFrame ( X_wine , columns = cols ) wine [ target ] = y wine . dropna ( inplace = True ) wine [ 'quality_label' ] = wine [ 'quality' ] . apply ( lambda x : 'low' if x <= 5 else 'med' if x <= 7 else 'high' ) wine [ 'type_encoding' ] = wine [ 'type' ] . map ({ 'red' : 0 , 'white' : 1 }) wine [ 'quality_encoding' ] = wine [ 'quality_label' ] . map ({ 'low' : 0 , 'med' : 1 , 'high' : 2 }) wine . columns = wine . columns . str . replace ( ' ' , '_' ) features = list ( wine . columns [ 1 : - 1 ] . values ) features . remove ( 'quality_label' ) features . remove ( 'quality' ) 5.1 Principal Component Analysis \u00b6 back to top Principle Component Analysis or PCA is one of the most wide spread implementations of dimensionality reduction. In PCA, we find the principle components, or linear recombinations of the dimensions of the data, that best explain the variance of the data. There are mathematical arguments abound for describing how we analytically solve for the principle components and how they relate to other concepts in mathematics (like pythagorean theorem). We'll sidestep that conversation for now, and proceed to our pragmatic demonstrations, as we have done in the past. 5.1.1 The Covariance Matrix \u00b6 back to top In the cell below, we have plotted acidity and density from our familiar wine dataset. fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) wine . loc [ wine [ 'red' ] == 1 ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> X = wine . loc [ wine [ 'red' ] == 1 ][[ 'fixed acidity' , 'density' ]] . values X [: 5 ] array([[0.14156636, 0.9978 ], [0.45029132, 0.9968 ], [0.45029132, 0.997 ], [3.07445349, 0.998 ], [0.14156636, 0.9978 ]]) The principal components, are actually the eigenvectors of the covariance matrix of the standardized data. We will perform this by hand to get an understanding. First we standardize the data from sklearn.preprocessing import StandardScaler X_std = StandardScaler () . fit_transform ( X ) # note I've already done this in 5.0.2 Then we compute the covariance matrix. There is a nice demonstration of computing covariance on stats quest . The covariance can be expressed as: cov(X,Y) = \\frac{1}{n^2}\\sum\\sum(x_i - x_j)(y_i - y_j) \\;\\;\\;\\;\\;\\sf eq. 1 Every \\((x_i - x_j)(y_i - y_j)\\) is the area described by the rectangle between points \\(i\\) and \\(j\\), and we if we deem to color positive changes as red and negative ones as blue, we get a picture like the following: When we view all the pairwise interactions in aggregate we get a sense of how the areas of rectangles made by each observation influence the covariance: As a side note, the covariance term is the numerator in the pearsons correlation we covered last week: \\rho_{x,y} = \\frac{cov(X,Y)}{\\sigma_x\\sigma_y} \\;\\;\\;\\;\\;\\sf eq. 2 import numpy as np mean_vec = np . mean ( X_std , axis = 0 ) cov_mat = ( X_std - mean_vec ) . T . dot (( X_std - mean_vec )) / ( X_std . shape [ 0 ] - 1 ) print ( 'Covariance matrix \\n %s ' % cov_mat ) Covariance matrix [[1.00062578 0.66847772] [0.66847772 1.00062578]] As it is a square symmetric matrix, it can be diagonalized by choosing a new orthogonal coordinate system, given by its eigenvectors. For this particular set of wine data, we will see that the corresponding diagonalized matrix will look like: \\begin{bmatrix} 1.67 & 0 \\\\ 0 & 0.33 \\end{bmatrix} Here's the clincher , what the 0's in this square matrix mean is that in this new coordinate system, there is no covariance between features, and the proportion between variances in this new coordinate system can simply be determined by observing the ratio of their eigenvalues (the values located on the diagonal). Armed with this knowledge, we can now proceed to determine the eigenvalues and eigenvectors of the covariance matrix produced from \\(eq. 1\\). Projecting onto the eigenvectors will yield data in a coordinate system that has no covariance, and the explained variance along each coordinate is captured by the eigenvalues. \ud83c\udf2d 5.1.2 Enrichment: Deriving the Eigenvectors and Eigenvalues \u00b6 The principal components are found mathematically by determining the eigenvectors of the covariance matrix and sorting them by their egienvalues, i.e. their explained variance. There is a mathematical proof 1 , 2 for why this works, but we will not cover that here. So how do we calculate eigenvalues and their correpsonding eigenvectors? This question is usually posited in the following format: A \\cdot v = \\lambda \\cdot v \\;\\;\\;\\;\\;\\sf eq. 3 In this equation A is an n-by-n matrix (our covariance matrix in this case), v is a non-zero n-by-1 vector and \u03bb is a scalar (which may be either real or complex). Any value of \u03bb for which this equation has a solution is known as an eigenvalue of the matrix A. In other words v, is an eigenvector of A if there exists a scalar value such that \\(A \\cdot v\\) and \\(\\lambda \\cdot v\\) will yield the same result In high school or college, we might've solved for these eigenvalues and eigenvectors by hand using simple, integer-bound matrices of A. Here's an example, for this matrix: \\begin{bmatrix} -6 & 3 \\\\ 4 & 5\\end{bmatrix} an eigenvector is: \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} with a corresponding eigenvalue of 6. Taking the requisite dot products for each side of eq. 3, \\(A v\\) gives us: \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} and \\(\\lambda v\\): 6 \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24\\end{bmatrix} 5.1.2.1: Find the Eigenvalues \u00b6 The trick that is employed to decompose these equality statements is to multiply the right hand side of eq. 3 by an identity matrix and then subtract this quantity from both sides of the equation. In the case of \\(v\\) being non-zero, this becomes the determinant : | A - \\lambda I | = 0 In the case of our simple example \\begin{vmatrix} \\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\end{vmatrix} = 0 simplifies to \\begin{vmatrix} -6-\\lambda & 3 \\\\ 4 & 5-\\lambda \\end{vmatrix} = 0 writing out the determinant (-6-\\lambda)(5-\\lambda) - 3 x 4 = 0 gives the quadratic equation \\lambda^2 + \\lambda - 42 = 0 and solving for \\(\\lambda\\) \\lambda = -7 \\space or \\space 6 from scipy.optimize import fsolve , leastsq A = np . array ([[ - 6 , 3 ], [ 4 , 5 ]]) I = np . array ([[ 1 , 0 ],[ 0 , 1 ]]) # define the determinant def det ( lamb ): \"\"\" A: the covariance matrix I: the identity matrix \"\"\" return ( A [ 0 , 0 ] - lamb ) * ( A [ 1 , 1 ] - lamb ) - ( A [ 0 , 1 ] * A [ 1 , 0 ]) root = fsolve ( det , [ - 10 , 10 ]) print ( np . isclose ( det ( root ), [ 0.0 , 0.0 ])) print ( root ) [ True True] [-7. 6.] 5.1.2.2: Find the Eigenvectors \u00b6 We find the eigenvector for each corresponding eigenvalue one at a time \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 6 \\begin{bmatrix} x \\\\ y \\end{bmatrix} multiplying out gives the system of equations -6x + 3y = 6x 4x + 5y = 6y bringing to the left hand side -12x + 3y = 0 4x - 1y = 0 solving for either equation yeilds \\(y = 4x\\) so the eigenvector is \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} def eig ( vec ): \"\"\" A: the covariance matrix lamb: the eigen value \"\"\" return [ A [ 0 , 0 ] * vec [ 0 ] + A [ 0 , 1 ] * vec [ 1 ] - lamb * vec [ 0 ], A [ 1 , 0 ] * vec [ 0 ] + A [ 1 , 1 ] * vec [ 1 ] - lamb * vec [ 1 ]] lamb = round ( root [ 0 ]) vector = fsolve ( eig , [ 1 , - 4 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () lamb = round ( root [ 1 ]) vector = fsolve ( eig , [ 1 , 10 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () -7 [ True True] [-3. 1.] 6 [ True True] [1. 4.] # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( A ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.9486833 -0.24253563] [ 0.31622777 -0.9701425 ]] Eigenvalues [-7. 6.] 5.1.3 Projecting onto the Principal Components \u00b6 To complete our principal component analysis, we need to project our data onto the eigenvectors of the covariance matrix. We can oobtain the eigenvectors and corresponding eigenvalues using np or scipy . Here I've completed the task with np : cov_mat = np . cov ( X_std . T ) # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( cov_mat ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.70710678 -0.70710678] [ 0.70710678 -0.70710678]] Eigenvalues [0.33214806 1.6691035 ] And now we are just going to sort our vectors by their eigenvalues to get the proper order of principal components: # Make a list of (eigenvalue, eigenvector) tuples eig_pairs = [( np . abs ( eig_vals [ i ]), eig_vecs [:, i ]) for i in range ( len ( eig_vals ))] # Sort the (eigenvalue, eigenvector) tuples from high to low eig_pairs . sort ( key = lambda x : x [ 0 ], reverse = True ) # Visually confirm that the list is correctly sorted by decreasing eigenvalues print ( 'Eigenvalues in descending order:' ) for i in eig_pairs : print ( i [ 0 ]) Eigenvalues in descending order: 1.669103500110071 0.3321480643454986 eig_pairs [(1.669103500110071, array([-0.70710678, -0.70710678])), (0.3321480643454986, array([-0.70710678, 0.70710678]))] For those of us who are familiar with eigenvectors, we should recognize that they are necessarily orthogonal to one another. This is good to know from the PCA point of view, because we wouldn't want the variance along one vector to be explained by another. Let's plot these vectors along with the standardized data: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 7 )) ax . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . set_aspect ( 'equal' ) ax . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) (-2.138871623907465, 4.356979103463171) We indeed see that these vectors are orthogonal. For further discussion on the topic of PCA and how it relates to concepts like RSS and Pythagorean Theorem I suggest reading the grandparent, spouse, daughter parable Continuing on with our task of projecting the data onto our principal components, in order to project our data onto the PCs I'll need to reshape eig_pairs : matrix_w = np . hstack (( eig_pairs [ 0 ][ 1 ] . reshape ( 2 , 1 ), eig_pairs [ 1 ][ 1 ] . reshape ( 2 , 1 ))) print ( 'Matrix W: \\n ' , matrix_w ) Matrix W: [[-0.70710678 -0.70710678] [-0.70710678 0.70710678]] And now taking the dot product: Y = X_std . dot ( matrix_w ) plt . scatter ( Y [:, 0 ], Y [:, 1 ]) <matplotlib.collections.PathCollection at 0x7f99baa40190> We see that our data is dispersed nicely along these PCs. 5.1.4 Cumulative Explained Variance \u00b6 Often we will need to decide just how many principal components are enough, especially with high dimensional data containing many colinear variables. To assist with this, data scientists will plot the cumulative explained variance. The explained variance is captured by the eigenvalues (this is why we sort by the eigenvalues in the first place). tot = sum ( eig_vals ) var_exp = [( i / tot ) * 100 for i in sorted ( eig_vals , reverse = True )] cum_var_exp = np . cumsum ( var_exp ) with plt . style . context ( 'seaborn-whitegrid' ): plt . figure ( figsize = ( 7 , 4 )) plt . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'individual explained variance' ) plt . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'cumulative explained variance' ) plt . ylabel ( 'Explained variance ratio' ) plt . xlabel ( 'Principal components' ) plt . legend ( loc = 'center right' ) plt . tight_layout () 5.1.5 PCA with Scikit-Learn \u00b6 But we can avoid the fancy footwork and do all this in sklearn! from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) We see that the values we get are the same as for the hand-calculated eigenvalues and vectors print ( pca . components_ ) [[ 0.70710678 0.70710678] [ 0.70710678 -0.70710678]] And the eigenvalues are under pca.explained_variance_ print ( pca . explained_variance_ ) [1.6691035 0.33214806] 5.1.6 PCA as Dimensionality Reduction \u00b6 back to top One obvious use case for PCA, is to drop the dimensions with lowest explained variance. Continuing with our acidity/density example this will look like the following from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . set_aspect ( 'equal' ) ax [ 0 ] . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax [ 0 ] . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) ax [ 0 ] . set_ylabel ( 'Normalized density' ) ax [ 0 ] . set_xlabel ( 'Normalized acidity' ) ax [ 1 ] . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'Individual' ) ax [ 1 ] . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'Cumulative' ) ax [ 1 ] . set_ylabel ( 'Explained variance ratio' ) ax [ 1 ] . set_xlabel ( 'Principal components' ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f99baaf0100> We can capture 80% of the explained variance along just the first principal component. What does this projection look like? # we set our components to 1 pca = PCA ( n_components = 1 ) pca . fit ( X_std ) # we then project the data onto the first PC # and then rebroadcast this transformation # back onto the orginal dimensions to see # what this looks like in terms of acidity/density X_pca = pca . inverse_transform ( pca . transform ( X_std )) # original data plt . scatter ( X_std [:, 0 ], X_std [:, 1 ], alpha = 0.2 ) # projected data plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.8 ) <matplotlib.collections.PathCollection at 0x7f99bd36a9a0> 5.1.7 PCA for visualization \u00b6 back to top For classification tasks, PCA lends itself as a useful method for seeing how the classes separate on the highest variance dimensions of the data without consideration to the classes themselves, i.e. do our classes seperate out according to the other variables in the dataset. With our wine dataset, we see that the wine types fall out nicely along the first two principal components X_std = StandardScaler () . fit_transform ( wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = wine [ 'white' ] . values , edgecolor = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC') \ud83c\udf2d 5.1.8 Enrichment: PCA as Outlier Removal and Noise Filtering \u00b6 back to top In some cases, it can be advantageous to use PCA as a method for outlier removal. There are many caveats to this that we will not discuss here. But for those who are interested, this is a great overview article. As for noise filteration, Vanderplas' DS handbook has a good section on the topic as does Guido/Muller's Intro to ML with Python by the same pusblisher. 5.1.9 PCA for Feature Engineering \u00b6 back to top Finally, PCA is actually a commonly used preprocessing technique for supervised machine learning models. In the next exercise, our goal will be to use PCA to generate a new set of features, to feed into our linear model. \ud83c\udfcb\ufe0f Exercise 1: PCA as Preprocessing for Models \u00b6 Using the wine data, select any number of the first principal components and attemp to predict density for the red wine data. Consier that the maximum number of principal components are: print ( f \"Max principal components: { X . shape [ 1 ] } \" ) Max principal components: 14 # Code Cell for Exercise 1 ################################################################################ ##### CREATE A SKLEARN-PCA OBJECT, FIT AND TRANSFORM TO THE WINE DATA ########## ################################################################################ pca = PCA ( n_components = 12 ) pca . fit ( X_wine ) X_pca = pca . transform ( X_wine ) ################################################################################ ############################## DO NOT CHANGE BELOW ############################ ################################################################################ plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = wine [ 'white' ] . values , edgecolor = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) plt . show () model = LinearRegression () X_train , X_test , y_train , y_test = train_test_split ( X_pca , y_wine , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( r2_score ( y_test , y_pred )) print ( r2_score ( y_train , model . predict ( X_train ))) 0.9634516142421967 0.953295487875815 5.2 K-Means Clustering \u00b6 back to top We now embark on a second class of unsupervised learning techinques: clustering. The K-means algorithm works under two assumptions: * every cluster can be defined by an arithmetic mean or cluster center * each point is closer to one arithmetic center than the other centers Let's turn back to our wine dataset: X_std = StandardScaler () . fit_transform ( X_wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC') It is fairly obvious under these two dimensions that there are two clusters. The K-means algorithm automatically pics this out from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = 2 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); wa-la! Our simple unsupervised algorithm does a pretty good job of differentiating between the red and white wine clusters. 5.2.1 The Algorithm: Expectation-Maximization \u00b6 back to top We won't go too far into the implementation of the underlying algorithm here. In its basic steps, however, it performs the following: Initialize cluster centers (random guess) Then repeat: E-Step: assign points to the nearest center (arithmetic distance) M-step: set the new center point for each cluster according to the mean of it's datapoint members More information on K-means algorithm can be explored here 5.2.2 Limitations \u00b6 back to top A few brief notes on limitations: the global optimum may not be achieved (no guarantee of finding the overall best solution) the number of clusters must be guessed beforehand cluster boundaries are unavoidably linear and the cluster assignments are unavoidably circular can be slow for large datasets cluster assignments are non probabilistic 3 and 5 motivate our next section, Gaussian Mixture Models 5.2.3 Determining K with the Elbow Method \u00b6 The elbow method is a popular technique for determining the value of k . It involves looping through a range of k 's and assessing some goodness of fit metric. Intuitively, we might presume those metrics involve some measure of the distance of datapoints to their cluster centers. We have options: Distortion: distance ( typically Euclidean ) from the cluster centers averaged across the respective clusters. Inertia: the sum of squared distances of samples to their closest cluster center. Silhouette: calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of The Silhouette Coefficient for a sample is (b-a) / max(b-a) best value is 1 worst value is -1 Calinski Harabasz Score or Variance Ratio: the ratio between within-cluster dispersion and between-cluster dispersion should sound familiar to our ANOVA discussion higher is better And there are many other methods of evaluating cluster assignment performance \ud83d\ude4b\u200d\u2640\ufe0f Question 1 \u00b6 What is the primary difference between Distortion, Intertia vs Silhouette, Calinksi? distortions = [] inertias = [] silhouette = [] variance = [] for k in range ( 1 , 10 ): kmeans = KMeans ( n_clusters = k ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) labels = kmeans . labels_ distortions . append ( sum ( np . min ( cdist ( X_pca , kmeans . cluster_centers_ , 'euclidean' ), axis = 1 )) / X . shape [ 0 ]) inertias . append ( kmeans . inertia_ ) if k > 1 : silhouette . append ( silhouette_score ( X_pca , labels , metric = 'euclidean' )) variance . append ( calinski_harabasz_score ( X_pca , labels )) We note in the following plots that inertia and distortion asymptotically improve with higher k (because they are unregularized) whereas silhouette and calinski metrics are penalized for inter-cluster relatability fig , [[ ax1 , ax2 ], [ ax3 , ax4 ]] = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) ax1 . plot ( range ( 1 , 10 ), distortions ) ax2 . plot ( range ( 1 , 10 ), inertias ) ax3 . plot ( range ( 2 , 10 ), silhouette ) ax4 . plot ( range ( 2 , 10 ), variance ) [<matplotlib.lines.Line2D at 0x7f99b02e5430>] kmeans = KMeans ( n_clusters = 3 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); 5.3 Gaussian Mixture Models \u00b6 back to top in the simplest case, GMMs can be used in the same way as K-means from sklearn.mixture import GaussianMixture gmm = GaussianMixture ( n_components = 2 ) . fit ( X_pca ) labels = gmm . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , alpha = 0.2 , edgecolor = 'grey' ); But because GMM is a probablistic model, we can visualize the certainty with which we assign these clusters. probs = gmm . predict_proba ( X_pca ) print ( probs [ 5 : 20 ] . round ( 3 )) [[1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.]] # convert probs to 1 dimension probs . max ( 1 ) array([1. , 0.99999994, 0.99999999, ..., 1. , 1. , 0.99999189]) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = probs . max ( 1 ), s = 40 , cmap = 'Blues' , alpha = 0.5 , edgecolor = 'grey' ); 5.3.1 Generalizing E-M for GMMs \u00b6 back to top The algorithm for GMMs is very similar to K-means, but now the EM steps are probablistic 1 2 Initialize cluster centers (random guess) Then repeat: E-Step: assign points their probability of belonging to every cluster M-step: set the new center point for each cluster according to the probabilities of all datapoints 5.3.2 GMMs as a Data Generator \u00b6 back to top One particularly fascinating application of GMMs is to use them to generate new data that is similar to the data on which they are modeled. Let's take an example. We're going to use sklearn make_circles function to create some arbitrary data that has a complex relationship along two different axes. # some helper functions borrowed from Jake Vanderplas with a few minor tweaks # https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html from matplotlib.patches import Ellipse def draw_ellipse ( position , covariance , ax = None , ** kwargs ): \"\"\"Draw an ellipse with a given position and covariance\"\"\" ax = ax or plt . gca () # Convert covariance to principal axes if covariance . shape == ( 2 , 2 ): U , s , Vt = np . linalg . svd ( covariance ) angle = np . degrees ( np . arctan2 ( U [ 1 , 0 ], U [ 0 , 0 ])) width , height = 2 * np . sqrt ( s ) else : angle = 0 width , height = 2 * np . sqrt ( covariance ) # Draw the Ellipse for nsig in range ( 1 , 4 ): ax . add_patch ( Ellipse ( position , nsig * width , nsig * height , angle , ** kwargs )) def plot_gmm ( gmm , X , label = True , ax = None , data_alpha = 1 ): ax = ax or plt . gca () labels = gmm . fit ( X ) . predict ( X ) if label : ax . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , zorder = 2 , alpha = data_alpha ) else : ax . scatter ( X [:, 0 ], X [:, 1 ], s = 40 , zorder = 2 , alpha = data_alpha ) ax . axis ( 'equal' ) w_factor = 0.2 / gmm . weights_ . max () for pos , covar , w in zip ( gmm . means_ , gmm . covariances_ , gmm . weights_ ): draw_ellipse ( pos , covar , alpha = w * w_factor ) from sklearn.datasets import make_circles as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ]); As a side note, as a clustering model, the GMM is not particularly useful: gmm2 = GaussianMixture ( n_components = 2 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm2 , X ) But if we use the GMM as a density estimator on the underlying data, rather than as a clustering algorithm, we find a completely different use case: gmm16 = GaussianMixture ( n_components = 16 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm16 , X , label = False ) Now, with the distributions drawn, we can assemble entirely new data: Xnew = gmm16 . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); 5.3.2.1 Determining the number of components \u00b6 back to top Let's think back to session 1 on model selection. How might we determine the best number of components? A couple analytic approaches that we have not much yet discussed, are the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The important thing to note from each of these is that they penalize added complexity to the models, and we would like to minimize their estimated prediction error. n_components = np . arange ( 1 , 42 ) models = [ GaussianMixture ( n , covariance_type = 'full' , random_state = 42 ) . fit ( X ) for n in n_components ] plt . plot ( n_components , [ m . bic ( X ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( X ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); plt . ylabel ( 'est. prediction error' ) Text(0, 0.5, 'est. prediction error') Both AIC and BIC capture a local minimum on the order of 6 Gaussians. This is where the model captures the overall circular modality of the data. However it totally misses the circumscribed nature of the two circles. Increasing this number to 30-40 gaussians captures this feature of the data. The AIC score reflects this while the BIC score (although captures a local minima in the area) does not define this as a global optimum. gmmNew = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmmNew , X , label = True , data_alpha = 0 ) Xnew = gmmNew . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); \ud83c\udfcb\ufe0f Exercise 2: Determine Number of Components for Circular Moons \u00b6 Repeat the above, this time using sklearn.datasets.make_moons # Code Cell for Exercise 2 from sklearn.datasets import make_moons as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) n_components = np . arange ( 1 , 42 ) models = [ GaussianMixture ( n , covariance_type = 'full' , random_state = 42 ) . fit ( X ) for n in n_components ] plt . plot ( n_components , [ m . bic ( X ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( X ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); plt . ylabel ( 'est. prediction error' ) Text(0, 0.5, 'est. prediction error') gmm_moon = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm_moon , X ) Xnew = gmm_moon . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); References \u00b6 PCA \u00b6 Intuitive PCA PCA and Eigenvectors/values GMM \u00b6 GMMs Explained Derive GMM Exercise","title":"SOLN S5 Unsupervised Learning"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#data-science-foundations-session-5-unsupervised-learning-clustering-and-dimensionality-reduction","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we began our discussion on feature engineering and ended with a sneak peak into dimensionality reduction. This last topic deserves a whole session on its own because its use case is not limited to feature engineering! It can also be used as a tool for visualization, for noise filtering, and much more. In this session we discuss dimensionality reduction along with other unsupervised learning methods. Up until now, the only learning estimators we've looked at were supervised ones: estimators that predict labels based on training data. Here, however, we are interested in uncovering aspects of the data without reference to any known labels. The usefulness for these learners will become immediately apparent when we revist our wine quality models from Course 1, Session 7!","title":"Data Science Foundations, Session 5: Unsupervised Learning: Clustering and Dimensionality Reduction"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#50-preparing-environment-and-importing-data","text":"back to top","title":"5.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#501-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score , silhouette_score , calinski_harabasz_score from sklearn.mixture import GaussianMixture from sklearn.impute import SimpleImputer from scipy.spatial.distance import cdist import seaborn as sns ; sns . set () import copy","title":"5.0.1 Import Packages"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#502-load-and-process-dataset","text":"back to top wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y_wine = X . pop ( target ) X = imp . fit_transform ( X ) X_wine = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X_wine [:, 2 :] = scaler . fit_transform ( X_wine [:, 2 :]) wine = pd . DataFrame ( X_wine , columns = cols ) wine [ target ] = y wine . dropna ( inplace = True ) wine [ 'quality_label' ] = wine [ 'quality' ] . apply ( lambda x : 'low' if x <= 5 else 'med' if x <= 7 else 'high' ) wine [ 'type_encoding' ] = wine [ 'type' ] . map ({ 'red' : 0 , 'white' : 1 }) wine [ 'quality_encoding' ] = wine [ 'quality_label' ] . map ({ 'low' : 0 , 'med' : 1 , 'high' : 2 }) wine . columns = wine . columns . str . replace ( ' ' , '_' ) features = list ( wine . columns [ 1 : - 1 ] . values ) features . remove ( 'quality_label' ) features . remove ( 'quality' )","title":"5.0.2 Load and Process Dataset"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#51-principal-component-analysis","text":"back to top Principle Component Analysis or PCA is one of the most wide spread implementations of dimensionality reduction. In PCA, we find the principle components, or linear recombinations of the dimensions of the data, that best explain the variance of the data. There are mathematical arguments abound for describing how we analytically solve for the principle components and how they relate to other concepts in mathematics (like pythagorean theorem). We'll sidestep that conversation for now, and proceed to our pragmatic demonstrations, as we have done in the past.","title":"5.1 Principal Component Analysis"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#511-the-covariance-matrix","text":"back to top In the cell below, we have plotted acidity and density from our familiar wine dataset. fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) wine . loc [ wine [ 'red' ] == 1 ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> X = wine . loc [ wine [ 'red' ] == 1 ][[ 'fixed acidity' , 'density' ]] . values X [: 5 ] array([[0.14156636, 0.9978 ], [0.45029132, 0.9968 ], [0.45029132, 0.997 ], [3.07445349, 0.998 ], [0.14156636, 0.9978 ]]) The principal components, are actually the eigenvectors of the covariance matrix of the standardized data. We will perform this by hand to get an understanding. First we standardize the data from sklearn.preprocessing import StandardScaler X_std = StandardScaler () . fit_transform ( X ) # note I've already done this in 5.0.2 Then we compute the covariance matrix. There is a nice demonstration of computing covariance on stats quest . The covariance can be expressed as: cov(X,Y) = \\frac{1}{n^2}\\sum\\sum(x_i - x_j)(y_i - y_j) \\;\\;\\;\\;\\;\\sf eq. 1 Every \\((x_i - x_j)(y_i - y_j)\\) is the area described by the rectangle between points \\(i\\) and \\(j\\), and we if we deem to color positive changes as red and negative ones as blue, we get a picture like the following: When we view all the pairwise interactions in aggregate we get a sense of how the areas of rectangles made by each observation influence the covariance: As a side note, the covariance term is the numerator in the pearsons correlation we covered last week: \\rho_{x,y} = \\frac{cov(X,Y)}{\\sigma_x\\sigma_y} \\;\\;\\;\\;\\;\\sf eq. 2 import numpy as np mean_vec = np . mean ( X_std , axis = 0 ) cov_mat = ( X_std - mean_vec ) . T . dot (( X_std - mean_vec )) / ( X_std . shape [ 0 ] - 1 ) print ( 'Covariance matrix \\n %s ' % cov_mat ) Covariance matrix [[1.00062578 0.66847772] [0.66847772 1.00062578]] As it is a square symmetric matrix, it can be diagonalized by choosing a new orthogonal coordinate system, given by its eigenvectors. For this particular set of wine data, we will see that the corresponding diagonalized matrix will look like: \\begin{bmatrix} 1.67 & 0 \\\\ 0 & 0.33 \\end{bmatrix} Here's the clincher , what the 0's in this square matrix mean is that in this new coordinate system, there is no covariance between features, and the proportion between variances in this new coordinate system can simply be determined by observing the ratio of their eigenvalues (the values located on the diagonal). Armed with this knowledge, we can now proceed to determine the eigenvalues and eigenvectors of the covariance matrix produced from \\(eq. 1\\). Projecting onto the eigenvectors will yield data in a coordinate system that has no covariance, and the explained variance along each coordinate is captured by the eigenvalues.","title":"5.1.1 The Covariance Matrix"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#512-enrichment-deriving-the-eigenvectors-and-eigenvalues","text":"The principal components are found mathematically by determining the eigenvectors of the covariance matrix and sorting them by their egienvalues, i.e. their explained variance. There is a mathematical proof 1 , 2 for why this works, but we will not cover that here. So how do we calculate eigenvalues and their correpsonding eigenvectors? This question is usually posited in the following format: A \\cdot v = \\lambda \\cdot v \\;\\;\\;\\;\\;\\sf eq. 3 In this equation A is an n-by-n matrix (our covariance matrix in this case), v is a non-zero n-by-1 vector and \u03bb is a scalar (which may be either real or complex). Any value of \u03bb for which this equation has a solution is known as an eigenvalue of the matrix A. In other words v, is an eigenvector of A if there exists a scalar value such that \\(A \\cdot v\\) and \\(\\lambda \\cdot v\\) will yield the same result In high school or college, we might've solved for these eigenvalues and eigenvectors by hand using simple, integer-bound matrices of A. Here's an example, for this matrix: \\begin{bmatrix} -6 & 3 \\\\ 4 & 5\\end{bmatrix} an eigenvector is: \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} with a corresponding eigenvalue of 6. Taking the requisite dot products for each side of eq. 3, \\(A v\\) gives us: \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} and \\(\\lambda v\\): 6 \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24\\end{bmatrix}","title":"\ud83c\udf2d 5.1.2 Enrichment: Deriving the Eigenvectors and Eigenvalues"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#5121-find-the-eigenvalues","text":"The trick that is employed to decompose these equality statements is to multiply the right hand side of eq. 3 by an identity matrix and then subtract this quantity from both sides of the equation. In the case of \\(v\\) being non-zero, this becomes the determinant : | A - \\lambda I | = 0 In the case of our simple example \\begin{vmatrix} \\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\end{vmatrix} = 0 simplifies to \\begin{vmatrix} -6-\\lambda & 3 \\\\ 4 & 5-\\lambda \\end{vmatrix} = 0 writing out the determinant (-6-\\lambda)(5-\\lambda) - 3 x 4 = 0 gives the quadratic equation \\lambda^2 + \\lambda - 42 = 0 and solving for \\(\\lambda\\) \\lambda = -7 \\space or \\space 6 from scipy.optimize import fsolve , leastsq A = np . array ([[ - 6 , 3 ], [ 4 , 5 ]]) I = np . array ([[ 1 , 0 ],[ 0 , 1 ]]) # define the determinant def det ( lamb ): \"\"\" A: the covariance matrix I: the identity matrix \"\"\" return ( A [ 0 , 0 ] - lamb ) * ( A [ 1 , 1 ] - lamb ) - ( A [ 0 , 1 ] * A [ 1 , 0 ]) root = fsolve ( det , [ - 10 , 10 ]) print ( np . isclose ( det ( root ), [ 0.0 , 0.0 ])) print ( root ) [ True True] [-7. 6.]","title":"5.1.2.1: Find the Eigenvalues"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#5122-find-the-eigenvectors","text":"We find the eigenvector for each corresponding eigenvalue one at a time \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 6 \\begin{bmatrix} x \\\\ y \\end{bmatrix} multiplying out gives the system of equations -6x + 3y = 6x 4x + 5y = 6y bringing to the left hand side -12x + 3y = 0 4x - 1y = 0 solving for either equation yeilds \\(y = 4x\\) so the eigenvector is \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} def eig ( vec ): \"\"\" A: the covariance matrix lamb: the eigen value \"\"\" return [ A [ 0 , 0 ] * vec [ 0 ] + A [ 0 , 1 ] * vec [ 1 ] - lamb * vec [ 0 ], A [ 1 , 0 ] * vec [ 0 ] + A [ 1 , 1 ] * vec [ 1 ] - lamb * vec [ 1 ]] lamb = round ( root [ 0 ]) vector = fsolve ( eig , [ 1 , - 4 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () lamb = round ( root [ 1 ]) vector = fsolve ( eig , [ 1 , 10 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () -7 [ True True] [-3. 1.] 6 [ True True] [1. 4.] # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( A ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.9486833 -0.24253563] [ 0.31622777 -0.9701425 ]] Eigenvalues [-7. 6.]","title":"5.1.2.2: Find the Eigenvectors"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#513-projecting-onto-the-principal-components","text":"To complete our principal component analysis, we need to project our data onto the eigenvectors of the covariance matrix. We can oobtain the eigenvectors and corresponding eigenvalues using np or scipy . Here I've completed the task with np : cov_mat = np . cov ( X_std . T ) # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( cov_mat ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.70710678 -0.70710678] [ 0.70710678 -0.70710678]] Eigenvalues [0.33214806 1.6691035 ] And now we are just going to sort our vectors by their eigenvalues to get the proper order of principal components: # Make a list of (eigenvalue, eigenvector) tuples eig_pairs = [( np . abs ( eig_vals [ i ]), eig_vecs [:, i ]) for i in range ( len ( eig_vals ))] # Sort the (eigenvalue, eigenvector) tuples from high to low eig_pairs . sort ( key = lambda x : x [ 0 ], reverse = True ) # Visually confirm that the list is correctly sorted by decreasing eigenvalues print ( 'Eigenvalues in descending order:' ) for i in eig_pairs : print ( i [ 0 ]) Eigenvalues in descending order: 1.669103500110071 0.3321480643454986 eig_pairs [(1.669103500110071, array([-0.70710678, -0.70710678])), (0.3321480643454986, array([-0.70710678, 0.70710678]))] For those of us who are familiar with eigenvectors, we should recognize that they are necessarily orthogonal to one another. This is good to know from the PCA point of view, because we wouldn't want the variance along one vector to be explained by another. Let's plot these vectors along with the standardized data: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 7 )) ax . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . set_aspect ( 'equal' ) ax . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) (-2.138871623907465, 4.356979103463171) We indeed see that these vectors are orthogonal. For further discussion on the topic of PCA and how it relates to concepts like RSS and Pythagorean Theorem I suggest reading the grandparent, spouse, daughter parable Continuing on with our task of projecting the data onto our principal components, in order to project our data onto the PCs I'll need to reshape eig_pairs : matrix_w = np . hstack (( eig_pairs [ 0 ][ 1 ] . reshape ( 2 , 1 ), eig_pairs [ 1 ][ 1 ] . reshape ( 2 , 1 ))) print ( 'Matrix W: \\n ' , matrix_w ) Matrix W: [[-0.70710678 -0.70710678] [-0.70710678 0.70710678]] And now taking the dot product: Y = X_std . dot ( matrix_w ) plt . scatter ( Y [:, 0 ], Y [:, 1 ]) <matplotlib.collections.PathCollection at 0x7f99baa40190> We see that our data is dispersed nicely along these PCs.","title":"5.1.3 Projecting onto the Principal Components"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#514-cumulative-explained-variance","text":"Often we will need to decide just how many principal components are enough, especially with high dimensional data containing many colinear variables. To assist with this, data scientists will plot the cumulative explained variance. The explained variance is captured by the eigenvalues (this is why we sort by the eigenvalues in the first place). tot = sum ( eig_vals ) var_exp = [( i / tot ) * 100 for i in sorted ( eig_vals , reverse = True )] cum_var_exp = np . cumsum ( var_exp ) with plt . style . context ( 'seaborn-whitegrid' ): plt . figure ( figsize = ( 7 , 4 )) plt . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'individual explained variance' ) plt . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'cumulative explained variance' ) plt . ylabel ( 'Explained variance ratio' ) plt . xlabel ( 'Principal components' ) plt . legend ( loc = 'center right' ) plt . tight_layout ()","title":"5.1.4 Cumulative Explained Variance"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#515-pca-with-scikit-learn","text":"But we can avoid the fancy footwork and do all this in sklearn! from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) We see that the values we get are the same as for the hand-calculated eigenvalues and vectors print ( pca . components_ ) [[ 0.70710678 0.70710678] [ 0.70710678 -0.70710678]] And the eigenvalues are under pca.explained_variance_ print ( pca . explained_variance_ ) [1.6691035 0.33214806]","title":"5.1.5 PCA with Scikit-Learn"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#516-pca-as-dimensionality-reduction","text":"back to top One obvious use case for PCA, is to drop the dimensions with lowest explained variance. Continuing with our acidity/density example this will look like the following from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . set_aspect ( 'equal' ) ax [ 0 ] . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax [ 0 ] . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) ax [ 0 ] . set_ylabel ( 'Normalized density' ) ax [ 0 ] . set_xlabel ( 'Normalized acidity' ) ax [ 1 ] . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'Individual' ) ax [ 1 ] . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'Cumulative' ) ax [ 1 ] . set_ylabel ( 'Explained variance ratio' ) ax [ 1 ] . set_xlabel ( 'Principal components' ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f99baaf0100> We can capture 80% of the explained variance along just the first principal component. What does this projection look like? # we set our components to 1 pca = PCA ( n_components = 1 ) pca . fit ( X_std ) # we then project the data onto the first PC # and then rebroadcast this transformation # back onto the orginal dimensions to see # what this looks like in terms of acidity/density X_pca = pca . inverse_transform ( pca . transform ( X_std )) # original data plt . scatter ( X_std [:, 0 ], X_std [:, 1 ], alpha = 0.2 ) # projected data plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.8 ) <matplotlib.collections.PathCollection at 0x7f99bd36a9a0>","title":"5.1.6 PCA as Dimensionality Reduction"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#517-pca-for-visualization","text":"back to top For classification tasks, PCA lends itself as a useful method for seeing how the classes separate on the highest variance dimensions of the data without consideration to the classes themselves, i.e. do our classes seperate out according to the other variables in the dataset. With our wine dataset, we see that the wine types fall out nicely along the first two principal components X_std = StandardScaler () . fit_transform ( wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = wine [ 'white' ] . values , edgecolor = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC')","title":"5.1.7 PCA for visualization"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#518-enrichment-pca-as-outlier-removal-and-noise-filtering","text":"back to top In some cases, it can be advantageous to use PCA as a method for outlier removal. There are many caveats to this that we will not discuss here. But for those who are interested, this is a great overview article. As for noise filteration, Vanderplas' DS handbook has a good section on the topic as does Guido/Muller's Intro to ML with Python by the same pusblisher.","title":"\ud83c\udf2d 5.1.8 Enrichment: PCA as Outlier Removal and Noise Filtering"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#519-pca-for-feature-engineering","text":"back to top Finally, PCA is actually a commonly used preprocessing technique for supervised machine learning models. In the next exercise, our goal will be to use PCA to generate a new set of features, to feed into our linear model.","title":"5.1.9 PCA for Feature Engineering"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#exercise-1-pca-as-preprocessing-for-models","text":"Using the wine data, select any number of the first principal components and attemp to predict density for the red wine data. Consier that the maximum number of principal components are: print ( f \"Max principal components: { X . shape [ 1 ] } \" ) Max principal components: 14 # Code Cell for Exercise 1 ################################################################################ ##### CREATE A SKLEARN-PCA OBJECT, FIT AND TRANSFORM TO THE WINE DATA ########## ################################################################################ pca = PCA ( n_components = 12 ) pca . fit ( X_wine ) X_pca = pca . transform ( X_wine ) ################################################################################ ############################## DO NOT CHANGE BELOW ############################ ################################################################################ plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = wine [ 'white' ] . values , edgecolor = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) plt . show () model = LinearRegression () X_train , X_test , y_train , y_test = train_test_split ( X_pca , y_wine , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( r2_score ( y_test , y_pred )) print ( r2_score ( y_train , model . predict ( X_train ))) 0.9634516142421967 0.953295487875815","title":"\ud83c\udfcb\ufe0f Exercise 1: PCA as Preprocessing for Models"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#52-k-means-clustering","text":"back to top We now embark on a second class of unsupervised learning techinques: clustering. The K-means algorithm works under two assumptions: * every cluster can be defined by an arithmetic mean or cluster center * each point is closer to one arithmetic center than the other centers Let's turn back to our wine dataset: X_std = StandardScaler () . fit_transform ( X_wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC') It is fairly obvious under these two dimensions that there are two clusters. The K-means algorithm automatically pics this out from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = 2 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); wa-la! Our simple unsupervised algorithm does a pretty good job of differentiating between the red and white wine clusters.","title":"5.2 K-Means Clustering"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#521-the-algorithm-expectation-maximization","text":"back to top We won't go too far into the implementation of the underlying algorithm here. In its basic steps, however, it performs the following: Initialize cluster centers (random guess) Then repeat: E-Step: assign points to the nearest center (arithmetic distance) M-step: set the new center point for each cluster according to the mean of it's datapoint members More information on K-means algorithm can be explored here","title":"5.2.1 The Algorithm: Expectation-Maximization"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#522-limitations","text":"back to top A few brief notes on limitations: the global optimum may not be achieved (no guarantee of finding the overall best solution) the number of clusters must be guessed beforehand cluster boundaries are unavoidably linear and the cluster assignments are unavoidably circular can be slow for large datasets cluster assignments are non probabilistic 3 and 5 motivate our next section, Gaussian Mixture Models","title":"5.2.2 Limitations"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#523-determining-k-with-the-elbow-method","text":"The elbow method is a popular technique for determining the value of k . It involves looping through a range of k 's and assessing some goodness of fit metric. Intuitively, we might presume those metrics involve some measure of the distance of datapoints to their cluster centers. We have options: Distortion: distance ( typically Euclidean ) from the cluster centers averaged across the respective clusters. Inertia: the sum of squared distances of samples to their closest cluster center. Silhouette: calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of The Silhouette Coefficient for a sample is (b-a) / max(b-a) best value is 1 worst value is -1 Calinski Harabasz Score or Variance Ratio: the ratio between within-cluster dispersion and between-cluster dispersion should sound familiar to our ANOVA discussion higher is better And there are many other methods of evaluating cluster assignment performance","title":"5.2.3 Determining K with the Elbow Method"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#question-1","text":"What is the primary difference between Distortion, Intertia vs Silhouette, Calinksi? distortions = [] inertias = [] silhouette = [] variance = [] for k in range ( 1 , 10 ): kmeans = KMeans ( n_clusters = k ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) labels = kmeans . labels_ distortions . append ( sum ( np . min ( cdist ( X_pca , kmeans . cluster_centers_ , 'euclidean' ), axis = 1 )) / X . shape [ 0 ]) inertias . append ( kmeans . inertia_ ) if k > 1 : silhouette . append ( silhouette_score ( X_pca , labels , metric = 'euclidean' )) variance . append ( calinski_harabasz_score ( X_pca , labels )) We note in the following plots that inertia and distortion asymptotically improve with higher k (because they are unregularized) whereas silhouette and calinski metrics are penalized for inter-cluster relatability fig , [[ ax1 , ax2 ], [ ax3 , ax4 ]] = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) ax1 . plot ( range ( 1 , 10 ), distortions ) ax2 . plot ( range ( 1 , 10 ), inertias ) ax3 . plot ( range ( 2 , 10 ), silhouette ) ax4 . plot ( range ( 2 , 10 ), variance ) [<matplotlib.lines.Line2D at 0x7f99b02e5430>] kmeans = KMeans ( n_clusters = 3 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 );","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#53-gaussian-mixture-models","text":"back to top in the simplest case, GMMs can be used in the same way as K-means from sklearn.mixture import GaussianMixture gmm = GaussianMixture ( n_components = 2 ) . fit ( X_pca ) labels = gmm . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , alpha = 0.2 , edgecolor = 'grey' ); But because GMM is a probablistic model, we can visualize the certainty with which we assign these clusters. probs = gmm . predict_proba ( X_pca ) print ( probs [ 5 : 20 ] . round ( 3 )) [[1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.]] # convert probs to 1 dimension probs . max ( 1 ) array([1. , 0.99999994, 0.99999999, ..., 1. , 1. , 0.99999189]) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = probs . max ( 1 ), s = 40 , cmap = 'Blues' , alpha = 0.5 , edgecolor = 'grey' );","title":"5.3 Gaussian Mixture Models"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#531-generalizing-e-m-for-gmms","text":"back to top The algorithm for GMMs is very similar to K-means, but now the EM steps are probablistic 1 2 Initialize cluster centers (random guess) Then repeat: E-Step: assign points their probability of belonging to every cluster M-step: set the new center point for each cluster according to the probabilities of all datapoints","title":"5.3.1 Generalizing E-M for GMMs"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#532-gmms-as-a-data-generator","text":"back to top One particularly fascinating application of GMMs is to use them to generate new data that is similar to the data on which they are modeled. Let's take an example. We're going to use sklearn make_circles function to create some arbitrary data that has a complex relationship along two different axes. # some helper functions borrowed from Jake Vanderplas with a few minor tweaks # https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html from matplotlib.patches import Ellipse def draw_ellipse ( position , covariance , ax = None , ** kwargs ): \"\"\"Draw an ellipse with a given position and covariance\"\"\" ax = ax or plt . gca () # Convert covariance to principal axes if covariance . shape == ( 2 , 2 ): U , s , Vt = np . linalg . svd ( covariance ) angle = np . degrees ( np . arctan2 ( U [ 1 , 0 ], U [ 0 , 0 ])) width , height = 2 * np . sqrt ( s ) else : angle = 0 width , height = 2 * np . sqrt ( covariance ) # Draw the Ellipse for nsig in range ( 1 , 4 ): ax . add_patch ( Ellipse ( position , nsig * width , nsig * height , angle , ** kwargs )) def plot_gmm ( gmm , X , label = True , ax = None , data_alpha = 1 ): ax = ax or plt . gca () labels = gmm . fit ( X ) . predict ( X ) if label : ax . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , zorder = 2 , alpha = data_alpha ) else : ax . scatter ( X [:, 0 ], X [:, 1 ], s = 40 , zorder = 2 , alpha = data_alpha ) ax . axis ( 'equal' ) w_factor = 0.2 / gmm . weights_ . max () for pos , covar , w in zip ( gmm . means_ , gmm . covariances_ , gmm . weights_ ): draw_ellipse ( pos , covar , alpha = w * w_factor ) from sklearn.datasets import make_circles as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ]); As a side note, as a clustering model, the GMM is not particularly useful: gmm2 = GaussianMixture ( n_components = 2 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm2 , X ) But if we use the GMM as a density estimator on the underlying data, rather than as a clustering algorithm, we find a completely different use case: gmm16 = GaussianMixture ( n_components = 16 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm16 , X , label = False ) Now, with the distributions drawn, we can assemble entirely new data: Xnew = gmm16 . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]);","title":"5.3.2 GMMs as a Data Generator"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#5321-determining-the-number-of-components","text":"back to top Let's think back to session 1 on model selection. How might we determine the best number of components? A couple analytic approaches that we have not much yet discussed, are the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The important thing to note from each of these is that they penalize added complexity to the models, and we would like to minimize their estimated prediction error. n_components = np . arange ( 1 , 42 ) models = [ GaussianMixture ( n , covariance_type = 'full' , random_state = 42 ) . fit ( X ) for n in n_components ] plt . plot ( n_components , [ m . bic ( X ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( X ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); plt . ylabel ( 'est. prediction error' ) Text(0, 0.5, 'est. prediction error') Both AIC and BIC capture a local minimum on the order of 6 Gaussians. This is where the model captures the overall circular modality of the data. However it totally misses the circumscribed nature of the two circles. Increasing this number to 30-40 gaussians captures this feature of the data. The AIC score reflects this while the BIC score (although captures a local minima in the area) does not define this as a global optimum. gmmNew = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmmNew , X , label = True , data_alpha = 0 ) Xnew = gmmNew . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]);","title":"5.3.2.1 Determining the number of components"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#exercise-2-determine-number-of-components-for-circular-moons","text":"Repeat the above, this time using sklearn.datasets.make_moons # Code Cell for Exercise 2 from sklearn.datasets import make_moons as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) n_components = np . arange ( 1 , 42 ) models = [ GaussianMixture ( n , covariance_type = 'full' , random_state = 42 ) . fit ( X ) for n in n_components ] plt . plot ( n_components , [ m . bic ( X ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( X ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); plt . ylabel ( 'est. prediction error' ) Text(0, 0.5, 'est. prediction error') gmm_moon = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm_moon , X ) Xnew = gmm_moon . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]);","title":"\ud83c\udfcb\ufe0f Exercise 2: Determine Number of Components for Circular Moons"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#references","text":"","title":"References"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#pca","text":"Intuitive PCA PCA and Eigenvectors/values","title":"PCA"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#gmm","text":"GMMs Explained Derive GMM Exercise","title":"GMM"},{"location":"solutions/SOLN_S6_Bagging/","text":"Data Science Foundations Session 6: Bagging Decision Trees and Random Forests \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're going back to the topic of supervised learning models. These models however, belong to a special class of methods called bagging, or bootstrap aggregation. Bagging is an ensemble learning method. In this method, many weak classifiers cast their votes in a general election for the final prediction. The weak learners that random forests are made of, are called decision trees. 6.0 Preparing Environment and Importing Data \u00b6 back to top 6.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score from sklearn.model_selection import train_test_split from sklearn import metrics 6.0.2 Load Dataset \u00b6 back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) print ( margin . shape , end = ' \\n\\n ' ) display ( margin . head ()) (1668, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 We're going to recreate the same operations we employed in Session 4, Feature Engineering: # identify categorical columns cat_cols = margin . columns [: 7 ] # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ cat_cols ] # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [[ 'KG' ]] X_truf = np . concatenate (( onehotlabels , X_num . values ), axis = 1 ) # grab our y data y_truf = margin [ 'EBITDA/KG' ] . values Lastly, to create a classification task, we're going to identify high, med, and low value products: print ( 'bad less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .25 )), end = ' \\n\\n ' ) print ( 'low less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .5 )), end = ' \\n\\n ' ) print ( 'med less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .75 )), end = ' \\n\\n ' ) pd . DataFrame ( margin [ margin . columns [ - 2 ]]) . boxplot ( showfliers = False ) bad less than: 0.12 low less than: 0.22 med less than: 0.35 <AxesSubplot:> margin [ 'profitability' ] = margin [ margin . columns [ - 1 ]] . apply ( lambda x : 'bad' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .25 ) else 'low' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .50 ) else 'med' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .75 ) else 'high' ) margin [ 'profitability' ] . hist () <AxesSubplot:> class_profit = { 'bad' : 0 , 'low' : 1 , 'med' : 2 , 'high' : 3 } y_truf_class = margin [ 'profitability' ] . map ( class_profit ) . values margin [ 'profitability_encoding' ] = y_truf_class margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG profitability profitability_encoding 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 high 3 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 med 2 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 low 1 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 med 2 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 high 3 6.1 Decision Trees \u00b6 back to top In essence, a decision tree is a series of binary questions. Let's begin this discussion by talking about how we make decision trees in sklearn. 6.1.1 Creating a Decision Tree \u00b6 back to top from sklearn import tree X = [[ 0 , 0 ], [ 1 , 1 ]] y = [ 0 , 1 ] clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) After fitting the model we can use the predict method to show the output for a sample clf . predict ([[ 2. , 2. ]]) array([1]) Similar to what we saw with GMMs, we also have access to the probabilities of the outcomes: clf . predict_proba ([[ 2. , 2. ]]) array([[0., 1.]]) Let's now go on to using visual strategies to interpreting trees. 6.1.2 Interpreting a Decision Tree \u00b6 back to top Throughout today, we will discuss many ways to view both a single tree and a random forest of trees. 6.1.2.1 Node & Branch Diagram \u00b6 back to top We can visualize the decision tree: tree . plot_tree ( clf ) [Text(0.5, 0.75, 'X[0] <= 0.5\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'), Text(0.25, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'), Text(0.75, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]')] or, more prettily: import graphviz dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph The gini label, also known as Gini impurity , is a measure of how often a sample passing through the node would be incorrectly labeled if it was randomly assigned a label based on the proportion of all labels passing through the node. So it is a measure of the progress of our tree. Let's take a more complex example from sklearn.datasets import make_classification as gen X , y = gen ( random_state = 42 ) Let's inspect our generated data: print ( X . shape ) print ( y . shape ) y [: 5 ] # a binary classification (100, 20) (100,) array([0, 0, 1, 1, 0]) And now let's train our tree: clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) How do we interpret this graph? dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph Can we confirm the observations in the tree by manually inspecting X and y? y [ X [:, 10 ] < .203 ] array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) We can confirm the gini score of the top left node by hand... scr = [] for j in range ( 1000 ): y_pred = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] y_true = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] scr . append ( mean_squared_error ( y_pred , y_true )) np . mean ( scr ) 0.10817307692307693 Let's take a look at this with our truffle dataset Vary the parameter max_depth what do you notice? Does the term greedy mean anything to you? Do nodes higher in the tree change based on decisions lower in the tree? clf = tree . DecisionTreeClassifier ( max_depth = 1 ) clf . fit ( X_truf , y_truf_class ) DecisionTreeClassifier(max_depth=1) And now lets look at the graph: dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph What is X[4] ??? # It's those tasty sponge cake truffles! enc . get_feature_names_out ()[ 4 ] 'Base Cake_Sponge' This is one great aspect of decision trees, their interpretability . We will perform this analysis again, for now, let's proceed with simpler datasets while exploring the features of decision trees. 6.1.2.1 Decision Boundaries \u00b6 back to top Let's make some random blobs from sklearn.datasets import make_blobs as gen X , y = gen ( random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f1fefd8b130> Let's call up our Classifier again, this time setting the max_depth to two clf = tree . DecisionTreeClassifier ( max_depth = 2 , random_state = 42 ) clf = clf . fit ( X , y ) # Parameters plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) <matplotlib.collections.PathCollection at 0x7f1feef32280> dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph We can see from the output of this graph, that the tree attempts to create the class boundaries as far from the cluster centers as possible. What happens when these clusters overlap? X , y = gen ( random_state = 42 , cluster_std = 3 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f1feeea0e80> Let's go ahead and write our plot into a function def plot_tree ( X , clf ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return plt We see that the boundaries mislabel some points fig = plot_tree ( X , clf ) 6.1.3 Overfitting a Decision Tree \u00b6 back to top Let's increase the max_depth clf = tree . DecisionTreeClassifier ( max_depth = 5 , random_state = 42 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> What we notice is that while the model accurately predicts the training data, we see some spurious labels, noteably the trailing purple bar that extends into the otherwise green region of the data. This is a well known fact about decision trees, that they tend to overfit their training data. In fact, this is a major motivation for why decision trees, a weak classifier, are conveniently packaged into ensembles. We combine the idea of bootstrapping, with decision trees, to come up with an overall better classifier. \ud83c\udfcb\ufe0f Exercise 1: Minimize Overfitting \u00b6 Repeat 6.1.3 with different max_depth settings, also read the docstring and play with any other hyperparameters available to you. What settings do you feel minimize overfitting? The documentation for DecisionTreeClassifier may be helpful # Code Cell for 1 ################################################################################ ##### CHANGE THE HYPERPARAMETERS IN THE CALL TO DECISIONTREECLASSIFIER ######### ################################################################################ clf = tree . DecisionTreeClassifier ( random_state = 42 , max_depth = None , min_samples_split = 3 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = None , max_leaf_nodes = None , min_impurity_decrease = 0.1 , class_weight = None , ccp_alpha = 0.0 ,) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> 6.2 Random Forests and Bagging \u00b6 back to top 6.2.1 What is Bagging? \u00b6 back to top Bagging , or Bootstrap AGGregation is the process of creating subsets of your data and training separate models on them, and using the aggregate votes of the models to make a final prediction. Bootstrapping is a topic in and of itself that we will just touch on here. Without going through the statistical rigor of proof, bootstrapping, or sampling from your observations with replacement, simulates having drawn additional data from the true population. We use this method to create many new datasets that are then used to train separate learners in parallel. This overall approach is called Bagging . A Random Forest is an instance of bagging where the separate learners are decision trees. 6.2.2 Random Forests for Classification \u00b6 back to top from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier tree = DecisionTreeClassifier () bag = BaggingClassifier ( tree , n_estimators = 10 , max_samples = 0.8 , random_state = 1 ) bag . fit ( X , y ) plot_tree ( X , bag ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> In the above, we have bootstrapped by providing each individual tree with 80% of the population data. In practice, Random Forests can achieve even better results by randomizing how the individual classifiers are constructed. In fact there are many unique methods of training individual trees and you can learn more about them here . This randomness is done automatically in sklearn's RandomForestClassifier from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier ( n_estimators = 10 , random_state = 2 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> 6.2.2.1 Interpreting a Random Forest \u00b6 back to top Let's revisit our truffle dataset again, this time with random forests # fit the model clf = RandomForestClassifier ( n_estimators = 100 , min_samples_leaf = 6 ) clf = clf . fit ( X_truf , y_truf_class ) We get a fairly high accuracy when our min_samples_leaf is low and an accuracy that leaves room for improvement when min_samples_leaf is high. This indicates to us the model may be prown to overfitting if we are not careful: accuracy_score ( clf . predict ( X_truf ), y_truf_class ) 0.6133093525179856 We can grab the original feature names with get_feature_names_out() : feats = enc . get_feature_names_out () The feature importances are stored in clf.feature_importances_ . These are calculated from the Mean Decrease in Impurity or MDI also called the Gini Importance . It is the sum of the number of nodes across all trees that include the feature, weighted by the number of samples passing through the node. One downside of estimating feature importance in this way is that it doesn't play well with highly cardinal features (features with many unique values such as mailing addresses, are highly cardinal features) len ( feats ) 118 # grab feature importances imp = clf . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in clf . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ feats , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.098864 0.090298 2 Base Cake_Chiffon 0.058403 0.049159 3 Base Cake_Butter 0.049615 0.046537 4 Base Cake_Pound 0.047093 0.043291 5 Primary Flavor_Butter Toffee 0.03565 0.038741 I'm going to use plotly to create this chart: px . bar ( feat [: 20 ], x = 'feature' , y = 'importance' , error_y = 'std' , title = 'Feature Importance' ) require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"2b50d292-b065-42c3-9980-738d478ac8b2\")) { Plotly.newPlot( \"2b50d292-b065-42c3-9980-738d478ac8b2\", [{\"alignmentgroup\":\"True\",\"error_y\":{\"array\":[0.09029809907413336,0.0491594926001353,0.04653692392065348,0.04329103819043059,0.03874140277043045,0.0349780684180197,0.030094493632483902,0.03311853070594179,0.03369534638488077,0.023175605457143568,0.02003645368097637,0.02649675589259294,0.011595438604605596,0.02247515253748258,0.01375825943123077,0.019922394040156593,0.011655486749852109,0.01266580704339592,0.015074673768077048,0.015049527462185765]},\"hovertemplate\":\"feature=%{x}<br>importance=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"Base Cake_Sponge\",\"Base Cake_Chiffon\",\"Base Cake_Butter\",\"Base Cake_Pound\",\"Primary Flavor_Butter Toffee\",\"Base Cake_Cheese\",\"Primary Flavor_Doughnut\",\"Secondary Flavor_Egg Nog\",\"Color Group_Olive\",\"Truffle Type_Candy Outer\",\"Color Group_White\",\"Secondary Flavor_Black Cherry\",\"Customer_Zebrabar\",\"Base Cake_Tiramisu\",\"Customer_Slugworth\",\"Truffle Type_Chocolate Outer\",\"Customer_Perk-a-Cola\",\"Customer_Dandy's Candies\",\"Color Group_Opal\",\"Secondary Flavor_Apricot\"],\"xaxis\":\"x\",\"y\":[0.0988642579163017,0.0584025133239696,0.049615450872702874,0.04709340666985859,0.03565014121102607,0.02844217305880782,0.027599939763576282,0.02585556386092948,0.02204011487187889,0.020269190892063724,0.019095262999859298,0.018956174795772437,0.016652674359145594,0.016599891454384605,0.0163798681859097,0.014287195738793366,0.013875453357030356,0.013555160346052895,0.013433004919153784,0.012081891079920398],\"yaxis\":\"y\",\"type\":\"bar\"}], {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"feature\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"importance\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Feature Importance\"},\"barmode\":\"relative\"}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('2b50d292-b065-42c3-9980-738d478ac8b2'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; }); \ud83d\ude4b\u200d\u2640\ufe0f Question 1: Feature Importance and Cardinality \u00b6 How does feature importance change in the above plot when we change the minimum leaf size from 6 to 1? \ud83d\ude4b\u200d Question 2: Compare to Moods Median \u00b6 We can then go and look at the different EBITDAs when selecting for each of these features. What do you notice as the primary difference between these results and those from Session 2: Inferential Statistics Exercise 1, Part C when we ran Mood's Median test on this same data? feat . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.098864 0.090298 2 Base Cake_Chiffon 0.058403 0.049159 3 Base Cake_Butter 0.049615 0.046537 4 Base Cake_Pound 0.047093 0.043291 5 Primary Flavor_Butter Toffee 0.03565 0.038741 for feature in feat . iloc [: 10 , 0 ]: group = feature . split ( '_' )[ 0 ] sel = \" \" . join ( feature . split ( '_' )[ 1 :]) pos = margin . loc [( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () neg = margin . loc [ ~ ( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () print ( group + \": \" + sel ) print ( \" \\t with: {:.2f} \" . format ( pos )) print ( \" \\t without: {:.2f} \" . format ( neg )) Base Cake: Sponge with: 0.70 without: 0.20 Base Cake: Chiffon with: 0.13 without: 0.24 Base Cake: Butter with: 0.14 without: 0.26 Base Cake: Pound with: 0.24 without: 0.20 Primary Flavor: Butter Toffee with: 0.46 without: 0.21 Base Cake: Cheese with: 0.44 without: 0.21 Primary Flavor: Doughnut with: 0.38 without: 0.20 Secondary Flavor: Egg Nog with: 0.23 without: 0.21 Color Group: Olive with: 0.67 without: 0.21 Truffle Type: Candy Outer with: 0.20 without: 0.22 6.2.3 Random Forests for Regression \u00b6 back to top from sklearn.ensemble import RandomForestRegressor clf = RandomForestRegressor ( n_estimators = 10 ) Because our labels on our blob data were numerical, we can apply and view the estimator in the same way: clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> I want to revisit a dataset we brought up in Session 2 on feature engineering: t = np . linspace ( 0 , 5 , 200 ) w = 5 h = 4 s = 4 * h / np . pi * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ]) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) # tells us about the amplitude of the component at the # corresponding frequency magnitude = np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , .15 ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain') Let's see if a random forest regression model can capture the wave behavior of the time-series data clf = RandomForestRegressor ( n_estimators = 10 ) clf . fit ( t . reshape ( - 1 , 1 ), s ) RandomForestRegressor(n_estimators=10) t2 = np . linspace ( 0 , 10 , 400 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . plot ( t , s ) ax . plot ( t2 , clf . predict ( t2 . reshape ( - 1 , 1 ))) [<matplotlib.lines.Line2D at 0x7f1fe536df10>] Nice! without specifying any perdiodicity, the random forest does a good job of embedding this periodicity in the final output. \ud83c\udfcb\ufe0f Exercise 2: Practice with Random Forests \u00b6 With the wine dataset: predict: density create a learning curve of train/test score vs model complexity for your random forest model(s) I have provided the cleaned dataset as well as starter code for training the model and making parity plots Do not change the following 3 cells: wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ 'density' ] = y model = RandomForestRegressor ( n_estimators = 65 , criterion = 'squared_error' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , bootstrap = True , oob_score = False , n_jobs = None , random_state = None , verbose = 0 , warm_start = False , ccp_alpha = 0.0 , max_samples = None ,) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax . set_ylabel ( 'Predicted' ) ax . set_xlabel ( 'Actual' ) ax_ . set_xlabel ( 'Actual' ) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.972') Compare these results with our linear model from Lab 3. Recall that we can quickly grab the names of the paramters in our sklearn model: RandomForestRegressor () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Cell for Exercise 2 X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) r2 = [] for n_estimators in range ( 1 , 100 ): model = RandomForestRegressor ( n_estimators = n_estimators ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) r2 . append ([ r2_score ( y_train , model . predict ( X_train )), r2_score ( y_test , model . predict ( X_test ))]) score = np . array ( r2 ) score1 = score [:, 0 ] score2 = score [:, 1 ] fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . plot ( range ( 1 , 100 ), score1 , ls = '' , marker = '.' , color = 'blue' , label = 'Train' ) ax . plot ( range ( 1 , 100 ), score2 , ls = '' , marker = 'o' , color = 'red' , label = 'Test' ) ax . set_title ( \"Scores with Increasing Model Complexity\" ) ax . set_xlabel ( \"Trees in the Forest\" ) ax . set_ylabel ( \"$R^2$\" ) ax . legend () <matplotlib.legend.Legend at 0x7f1fe5063f40>","title":"SOLN S6 Bagging"},{"location":"solutions/SOLN_S6_Bagging/#data-science-foundations-session-6-bagging-decision-trees-and-random-forests","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're going back to the topic of supervised learning models. These models however, belong to a special class of methods called bagging, or bootstrap aggregation. Bagging is an ensemble learning method. In this method, many weak classifiers cast their votes in a general election for the final prediction. The weak learners that random forests are made of, are called decision trees.","title":"Data Science Foundations  Session 6: Bagging  Decision Trees and Random Forests"},{"location":"solutions/SOLN_S6_Bagging/#60-preparing-environment-and-importing-data","text":"back to top","title":"6.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_S6_Bagging/#601-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score from sklearn.model_selection import train_test_split from sklearn import metrics","title":"6.0.1 Import Packages"},{"location":"solutions/SOLN_S6_Bagging/#602-load-dataset","text":"back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) print ( margin . shape , end = ' \\n\\n ' ) display ( margin . head ()) (1668, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 We're going to recreate the same operations we employed in Session 4, Feature Engineering: # identify categorical columns cat_cols = margin . columns [: 7 ] # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ cat_cols ] # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [[ 'KG' ]] X_truf = np . concatenate (( onehotlabels , X_num . values ), axis = 1 ) # grab our y data y_truf = margin [ 'EBITDA/KG' ] . values Lastly, to create a classification task, we're going to identify high, med, and low value products: print ( 'bad less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .25 )), end = ' \\n\\n ' ) print ( 'low less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .5 )), end = ' \\n\\n ' ) print ( 'med less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .75 )), end = ' \\n\\n ' ) pd . DataFrame ( margin [ margin . columns [ - 2 ]]) . boxplot ( showfliers = False ) bad less than: 0.12 low less than: 0.22 med less than: 0.35 <AxesSubplot:> margin [ 'profitability' ] = margin [ margin . columns [ - 1 ]] . apply ( lambda x : 'bad' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .25 ) else 'low' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .50 ) else 'med' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .75 ) else 'high' ) margin [ 'profitability' ] . hist () <AxesSubplot:> class_profit = { 'bad' : 0 , 'low' : 1 , 'med' : 2 , 'high' : 3 } y_truf_class = margin [ 'profitability' ] . map ( class_profit ) . values margin [ 'profitability_encoding' ] = y_truf_class margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG profitability profitability_encoding 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 high 3 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 med 2 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 low 1 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 med 2 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 high 3","title":"6.0.2 Load Dataset"},{"location":"solutions/SOLN_S6_Bagging/#61-decision-trees","text":"back to top In essence, a decision tree is a series of binary questions. Let's begin this discussion by talking about how we make decision trees in sklearn.","title":"6.1 Decision Trees"},{"location":"solutions/SOLN_S6_Bagging/#611-creating-a-decision-tree","text":"back to top from sklearn import tree X = [[ 0 , 0 ], [ 1 , 1 ]] y = [ 0 , 1 ] clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) After fitting the model we can use the predict method to show the output for a sample clf . predict ([[ 2. , 2. ]]) array([1]) Similar to what we saw with GMMs, we also have access to the probabilities of the outcomes: clf . predict_proba ([[ 2. , 2. ]]) array([[0., 1.]]) Let's now go on to using visual strategies to interpreting trees.","title":"6.1.1 Creating a Decision Tree"},{"location":"solutions/SOLN_S6_Bagging/#612-interpreting-a-decision-tree","text":"back to top Throughout today, we will discuss many ways to view both a single tree and a random forest of trees.","title":"6.1.2 Interpreting a Decision Tree"},{"location":"solutions/SOLN_S6_Bagging/#6121-node-branch-diagram","text":"back to top We can visualize the decision tree: tree . plot_tree ( clf ) [Text(0.5, 0.75, 'X[0] <= 0.5\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'), Text(0.25, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'), Text(0.75, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]')] or, more prettily: import graphviz dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph The gini label, also known as Gini impurity , is a measure of how often a sample passing through the node would be incorrectly labeled if it was randomly assigned a label based on the proportion of all labels passing through the node. So it is a measure of the progress of our tree. Let's take a more complex example from sklearn.datasets import make_classification as gen X , y = gen ( random_state = 42 ) Let's inspect our generated data: print ( X . shape ) print ( y . shape ) y [: 5 ] # a binary classification (100, 20) (100,) array([0, 0, 1, 1, 0]) And now let's train our tree: clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) How do we interpret this graph? dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph Can we confirm the observations in the tree by manually inspecting X and y? y [ X [:, 10 ] < .203 ] array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) We can confirm the gini score of the top left node by hand... scr = [] for j in range ( 1000 ): y_pred = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] y_true = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] scr . append ( mean_squared_error ( y_pred , y_true )) np . mean ( scr ) 0.10817307692307693 Let's take a look at this with our truffle dataset Vary the parameter max_depth what do you notice? Does the term greedy mean anything to you? Do nodes higher in the tree change based on decisions lower in the tree? clf = tree . DecisionTreeClassifier ( max_depth = 1 ) clf . fit ( X_truf , y_truf_class ) DecisionTreeClassifier(max_depth=1) And now lets look at the graph: dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph What is X[4] ??? # It's those tasty sponge cake truffles! enc . get_feature_names_out ()[ 4 ] 'Base Cake_Sponge' This is one great aspect of decision trees, their interpretability . We will perform this analysis again, for now, let's proceed with simpler datasets while exploring the features of decision trees.","title":"6.1.2.1 Node &amp; Branch Diagram"},{"location":"solutions/SOLN_S6_Bagging/#6121-decision-boundaries","text":"back to top Let's make some random blobs from sklearn.datasets import make_blobs as gen X , y = gen ( random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f1fefd8b130> Let's call up our Classifier again, this time setting the max_depth to two clf = tree . DecisionTreeClassifier ( max_depth = 2 , random_state = 42 ) clf = clf . fit ( X , y ) # Parameters plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) <matplotlib.collections.PathCollection at 0x7f1feef32280> dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph We can see from the output of this graph, that the tree attempts to create the class boundaries as far from the cluster centers as possible. What happens when these clusters overlap? X , y = gen ( random_state = 42 , cluster_std = 3 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f1feeea0e80> Let's go ahead and write our plot into a function def plot_tree ( X , clf ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return plt We see that the boundaries mislabel some points fig = plot_tree ( X , clf )","title":"6.1.2.1 Decision Boundaries"},{"location":"solutions/SOLN_S6_Bagging/#613-overfitting-a-decision-tree","text":"back to top Let's increase the max_depth clf = tree . DecisionTreeClassifier ( max_depth = 5 , random_state = 42 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> What we notice is that while the model accurately predicts the training data, we see some spurious labels, noteably the trailing purple bar that extends into the otherwise green region of the data. This is a well known fact about decision trees, that they tend to overfit their training data. In fact, this is a major motivation for why decision trees, a weak classifier, are conveniently packaged into ensembles. We combine the idea of bootstrapping, with decision trees, to come up with an overall better classifier.","title":"6.1.3 Overfitting a Decision Tree"},{"location":"solutions/SOLN_S6_Bagging/#exercise-1-minimize-overfitting","text":"Repeat 6.1.3 with different max_depth settings, also read the docstring and play with any other hyperparameters available to you. What settings do you feel minimize overfitting? The documentation for DecisionTreeClassifier may be helpful # Code Cell for 1 ################################################################################ ##### CHANGE THE HYPERPARAMETERS IN THE CALL TO DECISIONTREECLASSIFIER ######### ################################################################################ clf = tree . DecisionTreeClassifier ( random_state = 42 , max_depth = None , min_samples_split = 3 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = None , max_leaf_nodes = None , min_impurity_decrease = 0.1 , class_weight = None , ccp_alpha = 0.0 ,) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'>","title":"\ud83c\udfcb\ufe0f Exercise 1: Minimize Overfitting"},{"location":"solutions/SOLN_S6_Bagging/#62-random-forests-and-bagging","text":"back to top","title":"6.2 Random Forests and Bagging"},{"location":"solutions/SOLN_S6_Bagging/#621-what-is-bagging","text":"back to top Bagging , or Bootstrap AGGregation is the process of creating subsets of your data and training separate models on them, and using the aggregate votes of the models to make a final prediction. Bootstrapping is a topic in and of itself that we will just touch on here. Without going through the statistical rigor of proof, bootstrapping, or sampling from your observations with replacement, simulates having drawn additional data from the true population. We use this method to create many new datasets that are then used to train separate learners in parallel. This overall approach is called Bagging . A Random Forest is an instance of bagging where the separate learners are decision trees.","title":"6.2.1 What is Bagging?"},{"location":"solutions/SOLN_S6_Bagging/#622-random-forests-for-classification","text":"back to top from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier tree = DecisionTreeClassifier () bag = BaggingClassifier ( tree , n_estimators = 10 , max_samples = 0.8 , random_state = 1 ) bag . fit ( X , y ) plot_tree ( X , bag ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> In the above, we have bootstrapped by providing each individual tree with 80% of the population data. In practice, Random Forests can achieve even better results by randomizing how the individual classifiers are constructed. In fact there are many unique methods of training individual trees and you can learn more about them here . This randomness is done automatically in sklearn's RandomForestClassifier from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier ( n_estimators = 10 , random_state = 2 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'>","title":"6.2.2 Random Forests for Classification"},{"location":"solutions/SOLN_S6_Bagging/#6221-interpreting-a-random-forest","text":"back to top Let's revisit our truffle dataset again, this time with random forests # fit the model clf = RandomForestClassifier ( n_estimators = 100 , min_samples_leaf = 6 ) clf = clf . fit ( X_truf , y_truf_class ) We get a fairly high accuracy when our min_samples_leaf is low and an accuracy that leaves room for improvement when min_samples_leaf is high. This indicates to us the model may be prown to overfitting if we are not careful: accuracy_score ( clf . predict ( X_truf ), y_truf_class ) 0.6133093525179856 We can grab the original feature names with get_feature_names_out() : feats = enc . get_feature_names_out () The feature importances are stored in clf.feature_importances_ . These are calculated from the Mean Decrease in Impurity or MDI also called the Gini Importance . It is the sum of the number of nodes across all trees that include the feature, weighted by the number of samples passing through the node. One downside of estimating feature importance in this way is that it doesn't play well with highly cardinal features (features with many unique values such as mailing addresses, are highly cardinal features) len ( feats ) 118 # grab feature importances imp = clf . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in clf . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ feats , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.098864 0.090298 2 Base Cake_Chiffon 0.058403 0.049159 3 Base Cake_Butter 0.049615 0.046537 4 Base Cake_Pound 0.047093 0.043291 5 Primary Flavor_Butter Toffee 0.03565 0.038741 I'm going to use plotly to create this chart: px . bar ( feat [: 20 ], x = 'feature' , y = 'importance' , error_y = 'std' , title = 'Feature Importance' ) require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"2b50d292-b065-42c3-9980-738d478ac8b2\")) { Plotly.newPlot( \"2b50d292-b065-42c3-9980-738d478ac8b2\", [{\"alignmentgroup\":\"True\",\"error_y\":{\"array\":[0.09029809907413336,0.0491594926001353,0.04653692392065348,0.04329103819043059,0.03874140277043045,0.0349780684180197,0.030094493632483902,0.03311853070594179,0.03369534638488077,0.023175605457143568,0.02003645368097637,0.02649675589259294,0.011595438604605596,0.02247515253748258,0.01375825943123077,0.019922394040156593,0.011655486749852109,0.01266580704339592,0.015074673768077048,0.015049527462185765]},\"hovertemplate\":\"feature=%{x}<br>importance=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"Base Cake_Sponge\",\"Base Cake_Chiffon\",\"Base Cake_Butter\",\"Base Cake_Pound\",\"Primary Flavor_Butter Toffee\",\"Base Cake_Cheese\",\"Primary Flavor_Doughnut\",\"Secondary Flavor_Egg Nog\",\"Color Group_Olive\",\"Truffle Type_Candy Outer\",\"Color Group_White\",\"Secondary Flavor_Black Cherry\",\"Customer_Zebrabar\",\"Base Cake_Tiramisu\",\"Customer_Slugworth\",\"Truffle Type_Chocolate Outer\",\"Customer_Perk-a-Cola\",\"Customer_Dandy's Candies\",\"Color Group_Opal\",\"Secondary Flavor_Apricot\"],\"xaxis\":\"x\",\"y\":[0.0988642579163017,0.0584025133239696,0.049615450872702874,0.04709340666985859,0.03565014121102607,0.02844217305880782,0.027599939763576282,0.02585556386092948,0.02204011487187889,0.020269190892063724,0.019095262999859298,0.018956174795772437,0.016652674359145594,0.016599891454384605,0.0163798681859097,0.014287195738793366,0.013875453357030356,0.013555160346052895,0.013433004919153784,0.012081891079920398],\"yaxis\":\"y\",\"type\":\"bar\"}], {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"feature\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"importance\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Feature Importance\"},\"barmode\":\"relative\"}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('2b50d292-b065-42c3-9980-738d478ac8b2'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; });","title":"6.2.2.1 Interpreting a Random Forest"},{"location":"solutions/SOLN_S6_Bagging/#question-1-feature-importance-and-cardinality","text":"How does feature importance change in the above plot when we change the minimum leaf size from 6 to 1?","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: Feature Importance and Cardinality"},{"location":"solutions/SOLN_S6_Bagging/#question-2-compare-to-moods-median","text":"We can then go and look at the different EBITDAs when selecting for each of these features. What do you notice as the primary difference between these results and those from Session 2: Inferential Statistics Exercise 1, Part C when we ran Mood's Median test on this same data? feat . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.098864 0.090298 2 Base Cake_Chiffon 0.058403 0.049159 3 Base Cake_Butter 0.049615 0.046537 4 Base Cake_Pound 0.047093 0.043291 5 Primary Flavor_Butter Toffee 0.03565 0.038741 for feature in feat . iloc [: 10 , 0 ]: group = feature . split ( '_' )[ 0 ] sel = \" \" . join ( feature . split ( '_' )[ 1 :]) pos = margin . loc [( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () neg = margin . loc [ ~ ( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () print ( group + \": \" + sel ) print ( \" \\t with: {:.2f} \" . format ( pos )) print ( \" \\t without: {:.2f} \" . format ( neg )) Base Cake: Sponge with: 0.70 without: 0.20 Base Cake: Chiffon with: 0.13 without: 0.24 Base Cake: Butter with: 0.14 without: 0.26 Base Cake: Pound with: 0.24 without: 0.20 Primary Flavor: Butter Toffee with: 0.46 without: 0.21 Base Cake: Cheese with: 0.44 without: 0.21 Primary Flavor: Doughnut with: 0.38 without: 0.20 Secondary Flavor: Egg Nog with: 0.23 without: 0.21 Color Group: Olive with: 0.67 without: 0.21 Truffle Type: Candy Outer with: 0.20 without: 0.22","title":"\ud83d\ude4b\u200d Question 2: Compare to Moods Median"},{"location":"solutions/SOLN_S6_Bagging/#623-random-forests-for-regression","text":"back to top from sklearn.ensemble import RandomForestRegressor clf = RandomForestRegressor ( n_estimators = 10 ) Because our labels on our blob data were numerical, we can apply and view the estimator in the same way: clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> I want to revisit a dataset we brought up in Session 2 on feature engineering: t = np . linspace ( 0 , 5 , 200 ) w = 5 h = 4 s = 4 * h / np . pi * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ]) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) # tells us about the amplitude of the component at the # corresponding frequency magnitude = np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , .15 ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain') Let's see if a random forest regression model can capture the wave behavior of the time-series data clf = RandomForestRegressor ( n_estimators = 10 ) clf . fit ( t . reshape ( - 1 , 1 ), s ) RandomForestRegressor(n_estimators=10) t2 = np . linspace ( 0 , 10 , 400 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . plot ( t , s ) ax . plot ( t2 , clf . predict ( t2 . reshape ( - 1 , 1 ))) [<matplotlib.lines.Line2D at 0x7f1fe536df10>] Nice! without specifying any perdiodicity, the random forest does a good job of embedding this periodicity in the final output.","title":"6.2.3 Random Forests for Regression"},{"location":"solutions/SOLN_S6_Bagging/#exercise-2-practice-with-random-forests","text":"With the wine dataset: predict: density create a learning curve of train/test score vs model complexity for your random forest model(s) I have provided the cleaned dataset as well as starter code for training the model and making parity plots Do not change the following 3 cells: wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ 'density' ] = y model = RandomForestRegressor ( n_estimators = 65 , criterion = 'squared_error' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , bootstrap = True , oob_score = False , n_jobs = None , random_state = None , verbose = 0 , warm_start = False , ccp_alpha = 0.0 , max_samples = None ,) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax . set_ylabel ( 'Predicted' ) ax . set_xlabel ( 'Actual' ) ax_ . set_xlabel ( 'Actual' ) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.972') Compare these results with our linear model from Lab 3. Recall that we can quickly grab the names of the paramters in our sklearn model: RandomForestRegressor () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Cell for Exercise 2 X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) r2 = [] for n_estimators in range ( 1 , 100 ): model = RandomForestRegressor ( n_estimators = n_estimators ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) r2 . append ([ r2_score ( y_train , model . predict ( X_train )), r2_score ( y_test , model . predict ( X_test ))]) score = np . array ( r2 ) score1 = score [:, 0 ] score2 = score [:, 1 ] fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . plot ( range ( 1 , 100 ), score1 , ls = '' , marker = '.' , color = 'blue' , label = 'Train' ) ax . plot ( range ( 1 , 100 ), score2 , ls = '' , marker = 'o' , color = 'red' , label = 'Test' ) ax . set_title ( \"Scores with Increasing Model Complexity\" ) ax . set_xlabel ( \"Trees in the Forest\" ) ax . set_ylabel ( \"$R^2$\" ) ax . legend () <matplotlib.legend.Legend at 0x7f1fe5063f40>","title":"\ud83c\udfcb\ufe0f Exercise 2: Practice with Random Forests"},{"location":"solutions/SOLN_X1_Thinking_Data/","text":"Data Science Foundations Extras 1: Thinking Data \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to take our newfound knowledge from the course, and practice how we can leverage data to build predictive models. We'll start with a feature engineering problem on some dummy data. This will get us thinking creatively about problem solving. We will then pivot over to an Airbnb dataset . After performing some general, exploratory data analysis, we will solve the following business case: Airbnb is interested in using historical list prices from their airbnb hosts, to make pricing suggestions to new hosts. How can we use this existing datset to assist with this price listing suggestion? Prepare Environment and Import Data \u00b6 back to top # basic packages import pandas as pd import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , silhouette_score , calinski_harabasz_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score Warm Up \u00b6 Add aditional feature(s) to X to predict y with a model limited to a linear classification boundary from sklearn.datasets import make_circles X , y = make_circles ( random_state = 42 , noise = .01 ) relabel = dict ( zip ([ 0 , 1 , 2 , 3 ],[ 0 , 1 , 0 , 1 ])) y = np . vectorize ( relabel . get )( y ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f211fc48550> X2 = ( X ** 2 ) . sum ( axis = 1 ) X_ = np . hstack (( X , X2 . reshape ( - 1 , 1 ))) We can separate: px . scatter_3d ( x = X_ [:, 0 ], y = X_ [:, 1 ], z = X_ [:, 2 ], color = y ) and now predict model = LogisticRegression () model . fit ( X_ , y ) y_pred = model . predict ( X_ ) r2_score ( y , y_pred ) 1.0 Build a Baseline \u00b6 Exploratory Data Analysis \u00b6 which columns are numerical, string; which contain nans/nulls; what is the VIF between features airbnb = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/datasets/main/datasets/airbnb/AB_NYC_2019.csv\" ) airbnb . shape (48895, 16) airbnb . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 0 2539 Clean & quiet apt home by the park 2787 John Brooklyn Kensington 40.64749 -73.97237 Private room 149 1 9 2018-10-19 0.21 6 365 1 2595 Skylit Midtown Castle 2845 Jennifer Manhattan Midtown 40.75362 -73.98377 Entire home/apt 225 1 45 2019-05-21 0.38 2 355 2 3647 THE VILLAGE OF HARLEM....NEW YORK ! 4632 Elisabeth Manhattan Harlem 40.80902 -73.94190 Private room 150 3 0 NaN NaN 1 365 3 3831 Cozy Entire Floor of Brownstone 4869 LisaRoxanne Brooklyn Clinton Hill 40.68514 -73.95976 Entire home/apt 89 1 270 2019-07-05 4.64 1 194 4 5022 Entire Apt: Spacious Studio/Loft by central park 7192 Laura Manhattan East Harlem 40.79851 -73.94399 Entire home/apt 80 10 9 2018-11-19 0.10 1 0 airbnb . dtypes id int64 name object host_id int64 host_name object neighbourhood_group object neighbourhood object latitude float64 longitude float64 room_type object price int64 minimum_nights int64 number_of_reviews int64 last_review object reviews_per_month float64 calculated_host_listings_count int64 availability_365 int64 dtype: object airbnb . isnull () . sum ( axis = 0 ) id 0 name 16 host_id 0 host_name 21 neighbourhood_group 0 neighbourhood 0 latitude 0 longitude 0 room_type 0 price 0 minimum_nights 0 number_of_reviews 0 last_review 10052 reviews_per_month 10052 calculated_host_listings_count 0 availability_365 0 dtype: int64 airbnb . nunique () id 48895 name 47905 host_id 37457 host_name 11452 neighbourhood_group 5 neighbourhood 221 latitude 19048 longitude 14718 room_type 3 price 674 minimum_nights 109 number_of_reviews 394 last_review 1764 reviews_per_month 937 calculated_host_listings_count 47 availability_365 366 dtype: int64 plt . figure ( figsize = ( 10 , 6 )) sns . scatterplot ( x = airbnb . longitude , y = airbnb . latitude , hue = airbnb . neighbourhood_group ) plt . ioff () <matplotlib.pyplot._IoffContext at 0x7f211d15bb20> X = airbnb . copy () reviews_per_month has some 'nans' X_num = X . select_dtypes ( exclude = 'object' ) X_num . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 48890 36484665 8232441 40.67853 -73.94995 70 2 0 NaN 2 9 48891 36485057 6570630 40.70184 -73.93317 40 4 0 NaN 2 36 48892 36485431 23492952 40.81475 -73.94867 115 10 0 NaN 1 27 48893 36485609 30985759 40.75751 -73.99112 55 1 0 NaN 6 2 48894 36487245 68119814 40.76404 -73.98933 90 7 0 NaN 1 23 X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 149 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 225 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 150 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 89 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 80 10 9 0.10 1 0 X_num . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 count 4.889500e+04 4.889500e+04 48895.000000 48895.000000 48895.000000 48895.000000 48895.000000 38843.000000 48895.000000 48895.000000 mean 1.901714e+07 6.762001e+07 40.728949 -73.952170 152.720687 7.029962 23.274466 1.373221 7.143982 112.781327 std 1.098311e+07 7.861097e+07 0.054530 0.046157 240.154170 20.510550 44.550582 1.680442 32.952519 131.622289 min 2.539000e+03 2.438000e+03 40.499790 -74.244420 0.000000 1.000000 0.000000 0.010000 1.000000 0.000000 25% 9.471945e+06 7.822033e+06 40.690100 -73.983070 69.000000 1.000000 1.000000 0.190000 1.000000 0.000000 50% 1.967728e+07 3.079382e+07 40.723070 -73.955680 106.000000 3.000000 5.000000 0.720000 1.000000 45.000000 75% 2.915218e+07 1.074344e+08 40.763115 -73.936275 175.000000 5.000000 24.000000 2.020000 2.000000 227.000000 max 3.648724e+07 2.743213e+08 40.913060 -73.712990 10000.000000 1250.000000 629.000000 58.500000 327.000000 365.000000 X . dropna ( inplace = True ) X_num = X . select_dtypes ( exclude = 'object' ) vif = [ variance_inflation_factor ( X_num . values , i ) for i in range ( X_num . shape [ 1 ])] pd . DataFrame ( vif , index = X_num . columns ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 id 2.180074 host_id 2.836905 latitude 0.775769 longitude 425502.981678 price 1.012423 minimum_nights 1.039144 number_of_reviews 2.348200 reviews_per_month 2.314318 calculated_host_listings_count 1.067389 availability_365 1.139558 X_num . drop ( 'longitude' , axis = 1 , inplace = True ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy X_num .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 149 1 9 0.21 6 365 1 2595 2845 40.75362 225 1 45 0.38 2 355 3 3831 4869 40.68514 89 1 270 4.64 1 194 4 5022 7192 40.79851 80 10 9 0.10 1 0 5 5099 7322 40.74767 200 3 74 0.59 1 129 ... ... ... ... ... ... ... ... ... ... 48782 36425863 83554966 40.78099 129 1 1 1.00 1 147 48790 36427429 257683179 40.75104 45 1 1 1.00 6 339 48799 36438336 211644523 40.54179 235 1 1 1.00 1 87 48805 36442252 273841667 40.80787 100 1 2 2.00 1 40 48852 36455809 74162901 40.69805 30 1 1 1.00 1 1 38821 rows \u00d7 9 columns vif = [ variance_inflation_factor ( X_num . values , i ) for i in range ( X_num . shape [ 1 ])] pd . DataFrame ( vif , index = X_num . columns ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 id 8.424770 host_id 2.827543 latitude 7.297302 price 1.538975 minimum_nights 1.157468 number_of_reviews 3.215893 reviews_per_month 3.858006 calculated_host_listings_count 1.106414 availability_365 2.035592 Feature Engineering \u00b6 Say we want to predict pricing, using an ML model. How would you build your features? Based on the number of null values, what would you do with the last_review and reviews_per_month column? X = airbnb . copy () y = X . pop ( 'price' ) X_cat = X . select_dtypes ( include = 'object' ) X_cat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name host_name neighbourhood_group neighbourhood room_type last_review 0 Clean & quiet apt home by the park John Brooklyn Kensington Private room 2018-10-19 1 Skylit Midtown Castle Jennifer Manhattan Midtown Entire home/apt 2019-05-21 2 THE VILLAGE OF HARLEM....NEW YORK ! Elisabeth Manhattan Harlem Private room NaN 3 Cozy Entire Floor of Brownstone LisaRoxanne Brooklyn Clinton Hill Entire home/apt 2019-07-05 4 Entire Apt: Spacious Studio/Loft by central park Laura Manhattan East Harlem Entire home/apt 2018-11-19 based on the number of unique columns, we may want to remove name , host_name , and last_review X_cat . nunique () name 47905 host_name 11452 neighbourhood_group 5 neighbourhood 221 room_type 3 last_review 1764 dtype: int64 X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () And now we deal with the numerical columns X_num = X . select_dtypes ( exclude = 'object' ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 10 9 0.10 1 0 both id and host_id will be highly cardinal without telling us much about the behavior of unseen data. We should remove them. We'll also drop the columns with nans for now X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_num = X_num . dropna ( axis = 1 ) X_enc_df = pd . DataFrame ( X_enc , columns = enc . get_feature_names_out ()) X_feat = pd . concat (( X_enc_df , X_num ), axis = 1 ) X_feat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } neighbourhood_group_Bronx neighbourhood_group_Brooklyn neighbourhood_group_Manhattan neighbourhood_group_Queens neighbourhood_group_Staten Island neighbourhood_Allerton neighbourhood_Arden Heights neighbourhood_Arrochar neighbourhood_Arverne neighbourhood_Astoria ... neighbourhood_Woodside room_type_Entire home/apt room_type_Private room room_type_Shared room latitude longitude minimum_nights number_of_reviews calculated_host_listings_count availability_365 0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 1.0 0.0 40.64749 -73.97237 1 9 6 365 1 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 40.75362 -73.98377 1 45 2 355 2 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 1.0 0.0 40.80902 -73.94190 3 0 1 365 3 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 40.68514 -73.95976 1 270 1 194 4 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 40.79851 -73.94399 10 9 1 0 5 rows \u00d7 235 columns Feature Transformation \u00b6 What features do you think will cause the most problems if untransformed? Scale and Center all but the target variable, price scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (48895, 235) (48895,) Model Baseline \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = LinearRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) r2_score ( y_train , model . predict ( X_train )) 0.11264603204210533 r2_score ( y_test , y_pred ) -1.563294115330747e+17 model = RandomForestRegressor () model . fit ( X_train , y_train ) r2_score ( y_train , model . predict ( X_train )) 0.8597830223730762 r2_score ( y_test , model . predict ( X_test )) 0.10233675407266163 both of these results from the LinearRegression and RandomForest models indicate overfitting Back to Feature Engineering \u00b6 \ud83c\udf1f - keep this feature \ud83d\udca1 - interesting behavior discovered \ud83d\udc4e - don't keep this feature \ud83d\udd2e - try for next time To try: drop nan rows not columns remove outliers (filter by group) PCA of one hot encoded vectors (will help with linear model) transform 'last review date' (str) into 'days since last review' (number) \ud83c\udf1f NaNs - Drop Row-wise \u00b6 X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } latitude longitude minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 40.64749 -73.97237 1 9 0.21 6 365 1 40.75362 -73.98377 1 45 0.38 2 355 3 40.68514 -73.95976 1 270 4.64 1 194 4 40.79851 -73.94399 10 9 0.10 1 0 5 40.74767 -73.97500 3 74 0.59 1 129 X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38821, 233) (38821,) Train/Eval \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestRegressor ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train R2: { r2_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test R2: { r2_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train R2: 0.88 Test R2: 0.23 \ud83d\udca1 Outliers - by Borough \u00b6 X = airbnb . copy () X = X . dropna ( axis = 0 ) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = X [ 'neighbourhood_group' ], y = X [ 'price' ], ax = ax ) # ax.set_ylim(0, 1500) <AxesSubplot:xlabel='neighbourhood_group', ylabel='price'> fig , ax = plt . subplots ( figsize = ( 15 , 10 )) sns . kdeplot ( hue = X [ 'neighbourhood_group' ], x = X [ 'price' ], ax = ax ) <AxesSubplot:xlabel='price', ylabel='Density'> X = X . loc [ X . groupby ( 'neighbourhood_group' ) . apply ( lambda x : x [ 'price' ] < ( x [ 'price' ] . std () * 3 )) . unstack ( level = 0 ) . any ( axis = 1 )] fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 20 , 10 )) sns . boxplot ( x = X [ 'neighbourhood_group' ], y = X [ 'price' ], ax = ax ) sns . kdeplot ( hue = X [ 'neighbourhood_group' ], x = X [ 'price' ], ax = ax_ ) <AxesSubplot:xlabel='price', ylabel='Density'> y = X . pop ( 'price' ) X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38309, 232) (38309,) Train/Eval \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestRegressor ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train R2: { r2_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test R2: { r2_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train R2: 0.93 Test R2: 0.52 fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax . plot ( y_train , model . predict ( X_train ), ls = '' , marker = ',' ) ax_ . plot ( y_test , model . predict ( X_test ), ls = '' , marker = ',' ) [<matplotlib.lines.Line2D at 0x7f211ce29c40>] \ud83c\udf1f Bin Prices, Classifier Model \u00b6 X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) . values Y = y . reshape ( - 1 , 1 ) labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 y = labels X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38821, 233) (38821,) Train/Eval \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 1.00 Test Acc: 0.60 y_pred = model . predict ( X_train ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) precision recall f1-score support 1 1.00 1.00 1.00 7986 2 1.00 1.00 1.00 7594 3 1.00 1.00 1.00 7878 4 1.00 1.00 1.00 7598 accuracy 1.00 31056 macro avg 1.00 1.00 1.00 31056 weighted avg 1.00 1.00 1.00 31056 <AxesSubplot:> y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) precision recall f1-score support 1 0.70 0.80 0.74 1998 2 0.49 0.44 0.46 1846 3 0.50 0.47 0.48 1986 4 0.66 0.67 0.66 1935 accuracy 0.60 7765 macro avg 0.59 0.59 0.59 7765 weighted avg 0.59 0.60 0.59 7765 <AxesSubplot:> \ud83d\udc4e Cluster Prices, Classifier Model \u00b6 X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) . values Y = y . reshape ( - 1 , 1 ) distortions = [] inertias = [] silhouette = [] variance = [] krange = 20 for k in range ( 1 , krange ): kmeans = KMeans ( n_clusters = k ) kmeans . fit ( Y ) y_kmeans = kmeans . predict ( Y ) labels = kmeans . labels_ distortions . append ( sum ( np . min ( cdist ( Y , kmeans . cluster_centers_ , 'euclidean' ), axis = 1 )) / Y . shape [ 0 ]) inertias . append ( kmeans . inertia_ ) if k > 1 : silhouette . append ( silhouette_score ( Y , labels , metric = 'euclidean' )) variance . append ( calinski_harabasz_score ( Y , labels )) fig , [[ ax1 , ax2 ], [ ax3 , ax4 ]] = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) ax1 . plot ( range ( 1 , krange ), distortions ) ax2 . plot ( range ( 1 , krange ), inertias ) ax3 . plot ( range ( 2 , krange ), silhouette ) ax4 . plot ( range ( 2 , krange ), variance ) [<matplotlib.lines.Line2D at 0x7f211ca89100>] kmeans = KMeans ( n_clusters = 5 ) kmeans . fit ( Y ) y_kmeans = kmeans . predict ( Y ) labels = kmeans . labels_ ks = kmeans . cluster_centers_ ks = ks . flatten () ks = np . sort ( ks ) ks array([ 87.29374822, 230.74992332, 647.13125 , 2728.375 , 8749.75 ]) edges = ( np . diff ( ks ) / 2 + ks [: - 1 ]) . astype ( int ) bins = [] for idx , edge in enumerate ( edges ): if idx == 0 : bins . append ( f \"0- { edge } \" ) elif idx < len ( edges ): bins . append ( f \" { edges [ idx - 1 ] } - { edge } \" ) bins . append ( f \" { edge } +\" ) bins ['0-159', '159-438', '438-1687', '1687-5739', '5739+'] pd . DataFrame ( labels ) . value_counts ( sort = False ) 0 9651 1 8 2 961 3 28153 4 48 dtype: int64 y = labels X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38821, 233) (38821,) Train/Eval \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 1.00 Test Acc: 0.81 y_pred = model . predict ( X_train ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) precision recall f1-score support 0 1.00 1.00 1.00 7687 1 1.00 1.00 1.00 7 2 1.00 1.00 1.00 762 3 1.00 1.00 1.00 22561 4 1.00 1.00 1.00 39 accuracy 1.00 31056 macro avg 1.00 1.00 1.00 31056 weighted avg 1.00 1.00 1.00 31056 <AxesSubplot:> y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) precision recall f1-score support 0 0.64 0.60 0.62 1964 1 0.00 0.00 0.00 1 2 0.71 0.14 0.23 199 3 0.86 0.91 0.88 5592 4 0.67 0.22 0.33 9 accuracy 0.81 7765 macro avg 0.58 0.37 0.41 7765 weighted avg 0.80 0.81 0.80 7765 <AxesSubplot:> \ud83c\udf1f PCA, Feature Reduction \u00b6 The results in Bin Price, Classifier Model indicate overfitting. Let's see if we can reduce the cardinality of our One Hot features X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) . values Y = y . reshape ( - 1 , 1 ) bins = 10 quantiles = bins + 1 labels = y . copy () for idx , quant in enumerate ( np . linspace ( 0 , 1 , quantiles )): if idx == 0 : prev_quant = quant continue if idx == 1 : labels [ labels <= np . quantile ( y , quant )] = 1 elif quant < 1 : labels [( labels > np . quantile ( y , prev_quant )) & ( labels <= np . quantile ( y , quant ))] = idx else : labels [( labels > np . quantile ( y , prev_quant ))] = idx prev_quant = quant print ([ np . quantile ( y , quant ) for quant in np . linspace ( 0 , 1 , quantiles )]) y = labels X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () pca = PCA ( n_components = 3 ) X_pca = pca . fit_transform ( X_enc ) print ( pca . explained_variance_ ) scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_pca , X_std_num )) print ( X_std . shape ) print ( y . shape ) [0.0, 49.0, 60.0, 75.0, 90.0, 101.0, 125.0, 150.0, 190.0, 250.0, 10000.0] [0.52595687 0.42901998 0.16673031] (38821, 10) (38821,) Train/Eval \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) y_pred = model . predict ( X_train ) print ( \"Training Confusion Matrix\" ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) plt . show () y_pred = model . predict ( X_test ) print ( \"Testing Confusion Matrix\" ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) Train Acc: 1.00 Test Acc: 0.32 Training Confusion Matrix precision recall f1-score support 1 1.00 1.00 1.00 3148 2 1.00 1.00 1.00 3241 3 1.00 1.00 1.00 3458 4 1.00 1.00 1.00 3075 5 1.00 1.00 1.00 2658 6 1.00 1.00 1.00 3198 7 1.00 1.00 1.00 3426 8 1.00 1.00 1.00 2759 9 1.00 1.00 1.00 3274 10 1.00 1.00 1.00 2819 accuracy 1.00 31056 macro avg 1.00 1.00 1.00 31056 weighted avg 1.00 1.00 1.00 31056 Testing Confusion Matrix precision recall f1-score support 1 0.51 0.56 0.54 840 2 0.36 0.41 0.39 794 3 0.30 0.31 0.30 824 4 0.26 0.23 0.25 782 5 0.17 0.15 0.16 604 6 0.24 0.23 0.24 813 7 0.25 0.26 0.25 842 8 0.25 0.19 0.22 736 9 0.32 0.35 0.33 818 10 0.46 0.48 0.47 712 accuracy 0.32 7765 macro avg 0.31 0.32 0.31 7765 weighted avg 0.32 0.32 0.32 7765 <AxesSubplot:> \ud83d\udd2e Last Review Date, PCA for Outlier Removal, Impute \u00b6 If I wanted to spend more time on this: remove outliers with PCA the issue with our outlier removal previously, is that we are conditioning on y . As we can't know y in a production setting, this makes our model suspetible to underdetecting true, high-y value signal removing outliers based on the input, X is prefered, and we might try this with PCA turn last_review_date into a number (counts of days) this would change a string (to be one hot encoded) column to a number column (avoids curse of dimensionality) impute missing values we're currently omitting about 20% of our data points, it may give us a boost to impute or otherwise estimate these missing values Hyperparameter Optimization \u00b6 Round 1 \u00b6 We'll start with a broad, shallow search (few trees) param_grid = { 'bootstrap' : [ True , False ], 'criterion' : [ 'gini' , 'entropy' ], 'min_samples_split' : [ 2 , 4 , 6 ], 'min_samples_leaf' : [ 1 , 3 , 5 ], 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'class_weight' : [ 'balanced' , 'balanced_subsample' , None ], 'n_estimators' : [ 1 , 5 ]} grid = GridSearchCV ( RandomForestClassifier (), param_grid , cv = 5 , n_jobs =- 1 , verbose = 3 ) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) print ( grid . best_params_ ) {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 5} model = grid . best_estimator_ model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) y_pred = model . predict ( X_train ) print ( \"Training Confusion Matrix\" ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) plt . show () y_pred = model . predict ( X_test ) print ( \"Testing Confusion Matrix\" ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) Train Acc: 0.79 Test Acc: 0.30 Training Confusion Matrix precision recall f1-score support 1 0.80 0.89 0.84 3148 2 0.79 0.83 0.81 3241 3 0.82 0.74 0.78 3458 4 0.77 0.76 0.77 3075 5 0.74 0.79 0.77 2658 6 0.79 0.74 0.76 3198 7 0.81 0.71 0.76 3426 8 0.77 0.83 0.80 2759 9 0.81 0.77 0.79 3274 10 0.80 0.86 0.83 2819 accuracy 0.79 31056 macro avg 0.79 0.79 0.79 31056 weighted avg 0.79 0.79 0.79 31056 Testing Confusion Matrix precision recall f1-score support 1 0.49 0.58 0.53 840 2 0.33 0.37 0.35 794 3 0.28 0.24 0.26 824 4 0.23 0.21 0.22 782 5 0.17 0.21 0.18 604 6 0.24 0.20 0.22 813 7 0.23 0.20 0.21 842 8 0.23 0.22 0.22 736 9 0.29 0.27 0.28 818 10 0.41 0.47 0.44 712 accuracy 0.30 7765 macro avg 0.29 0.30 0.29 7765 weighted avg 0.29 0.30 0.29 7765 <AxesSubplot:> gs_results = pd . DataFrame ( grid . cv_results_ ) params = list ( gs_results . columns [ gs_results . columns . str . contains ( 'param' )] . values ) params . pop ( - 1 ) display ( gs_results . head ()) print ( params ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_fit_time std_fit_time mean_score_time std_score_time param_bootstrap param_class_weight param_criterion param_max_features param_min_samples_leaf param_min_samples_split ... split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score split5_test_score split6_test_score mean_test_score std_test_score rank_test_score 0 0.118039 0.002019 0.004001 0.000212 True balanced gini auto 1 2 ... 0.230787 0.235745 0.239351 0.232139 0.247520 0.238278 0.222272 0.235156 0.007317 610 1 0.518118 0.006517 0.012629 0.000356 True balanced gini auto 1 2 ... 0.262114 0.268425 0.253324 0.281271 0.273670 0.272543 0.259693 0.267291 0.008820 309 2 0.113497 0.004069 0.003702 0.000130 True balanced gini auto 1 4 ... 0.231463 0.231012 0.236196 0.249268 0.243012 0.240532 0.229937 0.237345 0.006712 567 3 0.482682 0.004648 0.012108 0.000515 True balanced gini auto 1 4 ... 0.268875 0.277890 0.272481 0.275186 0.275248 0.274121 0.272543 0.273764 0.002630 269 4 0.104504 0.004581 0.003597 0.000109 True balanced gini auto 1 6 ... 0.232590 0.227406 0.237773 0.246338 0.249549 0.232191 0.238954 0.237829 0.007357 559 5 rows \u00d7 22 columns ['param_bootstrap', 'param_class_weight', 'param_criterion', 'param_max_features', 'param_min_samples_leaf', 'param_min_samples_split', 'param_n_estimators'] target = 'mean_test_score' moodsdf = pd . DataFrame () for col in params : for truff in gs_results [ col ] . unique (): try : group = gs_results . loc [ gs_results [ col ] == truff ][ target ] pop = gs_results . loc [ ~ ( gs_results [ col ] == truff )][ target ] stat , p , m , table = stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , table ]) . T ]) except : print ( col , truff ) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'table' ] moodsdf [ 'p_value' ] = moodsdf [ 'p_value' ] . astype ( float ) print ( moodsdf . shape ) confidence_level = 0.05 moodsdf = moodsdf . loc [( moodsdf [ 'p_value' ] < confidence_level )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( \"Clearing high p-value...\" ) print ( moodsdf . shape ) param_class_weight None (17, 9) Clearing high p-value... (2, 9) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 0 param_n_estimators 1 644.006173 4.494276e-142 0.260964 0.243444 0.243093 324 [[0, 324], [324, 0]] 1 param_n_estimators 5 644.006173 4.494276e-142 0.260964 0.280889 0.281234 324 [[324, 0], [0, 324]] for param in params : sns . boxplot ( x = gs_results [ param ], y = gs_results [ target ]) plt . show () Round 2 \u00b6 Let's take those best parameters and dig a little deaper print ( grid . best_params_ ) {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 5} param_grid = { 'bootstrap' : [ True , False ], 'criterion' : [ 'gini' , 'entropy' ]} grid = GridSearchCV ( RandomForestClassifier ( min_samples_leaf = 5 , min_samples_split = 2 , max_features = 'sqrt' , class_weight = 'balanced' , n_estimators = 100 ), param_grid , cv = 5 , n_jobs =- 1 , verbose = 2 ) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) Fitting 5 folds for each of 4 candidates, totalling 20 fits {'bootstrap': False, 'criterion': 'entropy'} gs_results2 = pd . DataFrame ( grid . cv_results_ ) params = list ( gs_results2 . columns [ gs_results2 . columns . str . contains ( 'param' )] . values ) params . pop ( - 1 ) display ( gs_results2 . head ()) print ( params ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_fit_time std_fit_time mean_score_time std_score_time param_bootstrap param_criterion params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score 0 20.133523 0.372951 0.581618 0.020547 True gini {'bootstrap': True, 'criterion': 'gini'} 0.306825 0.305909 0.309612 0.314281 0.307036 0.308733 0.003035 2 1 52.458941 0.532249 0.603756 0.138301 True entropy {'bootstrap': True, 'criterion': 'entropy'} 0.311494 0.297859 0.314925 0.312188 0.304460 0.308185 0.006211 4 2 31.101482 0.656385 0.763153 0.090014 False gini {'bootstrap': False, 'criterion': 'gini'} 0.306665 0.303494 0.314603 0.313798 0.302689 0.308250 0.005044 3 3 53.006182 7.189164 0.337193 0.088254 False entropy {'bootstrap': False, 'criterion': 'entropy'} 0.307147 0.303333 0.317501 0.313476 0.307841 0.309860 0.005010 1 ['param_bootstrap', 'param_criterion'] for param in params : sns . boxplot ( x = gs_results [ param ], y = gs_results [ target ]) plt . show () Round 3 \u00b6 And now tune model complexity # Cell for Exercise 2 r2 = [] for n_estimators in range ( 10 , 100 , 10 ): model = RandomForestClassifier ( n_estimators = n_estimators , bootstrap = False , criterion = 'entropy' , min_samples_leaf = 5 , min_samples_split = 2 , max_features = 'sqrt' , class_weight = 'balanced' ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) r2 . append ([ r2_score ( y_train , model . predict ( X_train )), r2_score ( y_test , model . predict ( X_test ))]) score = np . array ( r2 ) score1 = score [:, 0 ] score2 = score [:, 1 ] fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . plot ( range ( 10 , 100 , 10 ), score1 , ls = '' , marker = '.' , color = 'blue' , label = 'Train' ) ax . plot ( range ( 10 , 100 , 10 ), score2 , ls = '' , marker = 'o' , color = 'red' , label = 'Test' ) ax . set_title ( \"Scores with Increasing Model Complexity\" ) ax . set_xlabel ( \"Trees in the Forest\" ) ax . set_ylabel ( \"$R^2$\" ) ax . legend () <matplotlib.legend.Legend at 0x7f20e6d99a00> model = grid . best_estimator_ model . n_estimators = 80 model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) y_pred = model . predict ( X_train ) print ( \"Training Confusion Matrix\" ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) plt . show () y_pred = model . predict ( X_test ) print ( \"Testing Confusion Matrix\" ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) Train Acc: 0.90 Test Acc: 0.32 Training Confusion Matrix precision recall f1-score support 1 0.86 0.95 0.90 3148 2 0.88 0.91 0.90 3241 3 0.92 0.86 0.89 3458 4 0.90 0.88 0.89 3075 5 0.87 0.91 0.89 2658 6 0.91 0.86 0.89 3198 7 0.93 0.85 0.89 3426 8 0.90 0.93 0.91 2759 9 0.91 0.89 0.90 3274 10 0.88 0.94 0.91 2819 accuracy 0.90 31056 macro avg 0.90 0.90 0.90 31056 weighted avg 0.90 0.90 0.90 31056 Testing Confusion Matrix precision recall f1-score support 1 0.51 0.60 0.55 840 2 0.34 0.38 0.36 794 3 0.30 0.26 0.28 824 4 0.27 0.24 0.26 782 5 0.17 0.21 0.19 604 6 0.25 0.22 0.23 813 7 0.25 0.21 0.23 842 8 0.25 0.24 0.24 736 9 0.33 0.30 0.31 818 10 0.44 0.54 0.48 712 accuracy 0.32 7765 macro avg 0.31 0.32 0.31 7765 weighted avg 0.31 0.32 0.32 7765 <AxesSubplot:> After all that work we don't get much lift from the random forest with default hyperparameters Conclusion \u00b6 The Final Classification Model \u00b6 Final model had an F1 score ranging from 19-55% depending on class and a total accuracy of 32% This model could be used to suggest a price band for would-be airbnb hosts in NYC; or a price estimator to assess how changes in listing attributes wil affect price. A potential pitfall could be that new airbnb hosts will not have many total reviews or high variance in the reviews per month. We can currate price signal from the available feature inputs: neighbourhood_group neighbourhood longitude / latitude room_type price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 What worked: dropping nans row-wise allowed us to keep the reviews_per_month column, which gave us an \\(R^2\\) boost of 10% converting from a regression problem to a classification problem allowed us to deal with the long, high-price tail converting one hot encoded vectors to the first principal components kept us from overfitting (although this was not important for the random forrest model) More to try: change last_review_date from datetime or str to int use PCA for outlier removal based on the input data X imput missing values for reviews_per_month to capture an additional 10,000 datapoints Additional Strategies \u00b6 Removing outliers based on the target variable, price , could also be a valid strategy. If we were to employ the model, we would have to be transparent that it should be used to predict prices in the sub $700 range, which is most of the Airbnb business in NYC anyway. At the end of the day, our decisions about model creation need to serve the business need.","title":"SOLN X1 Thinking Data"},{"location":"solutions/SOLN_X1_Thinking_Data/#data-science-foundations-extras-1-thinking-data","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to take our newfound knowledge from the course, and practice how we can leverage data to build predictive models. We'll start with a feature engineering problem on some dummy data. This will get us thinking creatively about problem solving. We will then pivot over to an Airbnb dataset . After performing some general, exploratory data analysis, we will solve the following business case: Airbnb is interested in using historical list prices from their airbnb hosts, to make pricing suggestions to new hosts. How can we use this existing datset to assist with this price listing suggestion?","title":"Data Science Foundations  Extras 1: Thinking Data"},{"location":"solutions/SOLN_X1_Thinking_Data/#prepare-environment-and-import-data","text":"back to top # basic packages import pandas as pd import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , silhouette_score , calinski_harabasz_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score","title":"Prepare Environment and Import Data"},{"location":"solutions/SOLN_X1_Thinking_Data/#warm-up","text":"Add aditional feature(s) to X to predict y with a model limited to a linear classification boundary from sklearn.datasets import make_circles X , y = make_circles ( random_state = 42 , noise = .01 ) relabel = dict ( zip ([ 0 , 1 , 2 , 3 ],[ 0 , 1 , 0 , 1 ])) y = np . vectorize ( relabel . get )( y ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f211fc48550> X2 = ( X ** 2 ) . sum ( axis = 1 ) X_ = np . hstack (( X , X2 . reshape ( - 1 , 1 ))) We can separate: px . scatter_3d ( x = X_ [:, 0 ], y = X_ [:, 1 ], z = X_ [:, 2 ], color = y ) and now predict model = LogisticRegression () model . fit ( X_ , y ) y_pred = model . predict ( X_ ) r2_score ( y , y_pred ) 1.0","title":"Warm Up"},{"location":"solutions/SOLN_X1_Thinking_Data/#build-a-baseline","text":"","title":"Build a Baseline"},{"location":"solutions/SOLN_X1_Thinking_Data/#exploratory-data-analysis","text":"which columns are numerical, string; which contain nans/nulls; what is the VIF between features airbnb = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/datasets/main/datasets/airbnb/AB_NYC_2019.csv\" ) airbnb . shape (48895, 16) airbnb . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 0 2539 Clean & quiet apt home by the park 2787 John Brooklyn Kensington 40.64749 -73.97237 Private room 149 1 9 2018-10-19 0.21 6 365 1 2595 Skylit Midtown Castle 2845 Jennifer Manhattan Midtown 40.75362 -73.98377 Entire home/apt 225 1 45 2019-05-21 0.38 2 355 2 3647 THE VILLAGE OF HARLEM....NEW YORK ! 4632 Elisabeth Manhattan Harlem 40.80902 -73.94190 Private room 150 3 0 NaN NaN 1 365 3 3831 Cozy Entire Floor of Brownstone 4869 LisaRoxanne Brooklyn Clinton Hill 40.68514 -73.95976 Entire home/apt 89 1 270 2019-07-05 4.64 1 194 4 5022 Entire Apt: Spacious Studio/Loft by central park 7192 Laura Manhattan East Harlem 40.79851 -73.94399 Entire home/apt 80 10 9 2018-11-19 0.10 1 0 airbnb . dtypes id int64 name object host_id int64 host_name object neighbourhood_group object neighbourhood object latitude float64 longitude float64 room_type object price int64 minimum_nights int64 number_of_reviews int64 last_review object reviews_per_month float64 calculated_host_listings_count int64 availability_365 int64 dtype: object airbnb . isnull () . sum ( axis = 0 ) id 0 name 16 host_id 0 host_name 21 neighbourhood_group 0 neighbourhood 0 latitude 0 longitude 0 room_type 0 price 0 minimum_nights 0 number_of_reviews 0 last_review 10052 reviews_per_month 10052 calculated_host_listings_count 0 availability_365 0 dtype: int64 airbnb . nunique () id 48895 name 47905 host_id 37457 host_name 11452 neighbourhood_group 5 neighbourhood 221 latitude 19048 longitude 14718 room_type 3 price 674 minimum_nights 109 number_of_reviews 394 last_review 1764 reviews_per_month 937 calculated_host_listings_count 47 availability_365 366 dtype: int64 plt . figure ( figsize = ( 10 , 6 )) sns . scatterplot ( x = airbnb . longitude , y = airbnb . latitude , hue = airbnb . neighbourhood_group ) plt . ioff () <matplotlib.pyplot._IoffContext at 0x7f211d15bb20> X = airbnb . copy () reviews_per_month has some 'nans' X_num = X . select_dtypes ( exclude = 'object' ) X_num . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 48890 36484665 8232441 40.67853 -73.94995 70 2 0 NaN 2 9 48891 36485057 6570630 40.70184 -73.93317 40 4 0 NaN 2 36 48892 36485431 23492952 40.81475 -73.94867 115 10 0 NaN 1 27 48893 36485609 30985759 40.75751 -73.99112 55 1 0 NaN 6 2 48894 36487245 68119814 40.76404 -73.98933 90 7 0 NaN 1 23 X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 149 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 225 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 150 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 89 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 80 10 9 0.10 1 0 X_num . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 count 4.889500e+04 4.889500e+04 48895.000000 48895.000000 48895.000000 48895.000000 48895.000000 38843.000000 48895.000000 48895.000000 mean 1.901714e+07 6.762001e+07 40.728949 -73.952170 152.720687 7.029962 23.274466 1.373221 7.143982 112.781327 std 1.098311e+07 7.861097e+07 0.054530 0.046157 240.154170 20.510550 44.550582 1.680442 32.952519 131.622289 min 2.539000e+03 2.438000e+03 40.499790 -74.244420 0.000000 1.000000 0.000000 0.010000 1.000000 0.000000 25% 9.471945e+06 7.822033e+06 40.690100 -73.983070 69.000000 1.000000 1.000000 0.190000 1.000000 0.000000 50% 1.967728e+07 3.079382e+07 40.723070 -73.955680 106.000000 3.000000 5.000000 0.720000 1.000000 45.000000 75% 2.915218e+07 1.074344e+08 40.763115 -73.936275 175.000000 5.000000 24.000000 2.020000 2.000000 227.000000 max 3.648724e+07 2.743213e+08 40.913060 -73.712990 10000.000000 1250.000000 629.000000 58.500000 327.000000 365.000000 X . dropna ( inplace = True ) X_num = X . select_dtypes ( exclude = 'object' ) vif = [ variance_inflation_factor ( X_num . values , i ) for i in range ( X_num . shape [ 1 ])] pd . DataFrame ( vif , index = X_num . columns ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 id 2.180074 host_id 2.836905 latitude 0.775769 longitude 425502.981678 price 1.012423 minimum_nights 1.039144 number_of_reviews 2.348200 reviews_per_month 2.314318 calculated_host_listings_count 1.067389 availability_365 1.139558 X_num . drop ( 'longitude' , axis = 1 , inplace = True ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy X_num .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 149 1 9 0.21 6 365 1 2595 2845 40.75362 225 1 45 0.38 2 355 3 3831 4869 40.68514 89 1 270 4.64 1 194 4 5022 7192 40.79851 80 10 9 0.10 1 0 5 5099 7322 40.74767 200 3 74 0.59 1 129 ... ... ... ... ... ... ... ... ... ... 48782 36425863 83554966 40.78099 129 1 1 1.00 1 147 48790 36427429 257683179 40.75104 45 1 1 1.00 6 339 48799 36438336 211644523 40.54179 235 1 1 1.00 1 87 48805 36442252 273841667 40.80787 100 1 2 2.00 1 40 48852 36455809 74162901 40.69805 30 1 1 1.00 1 1 38821 rows \u00d7 9 columns vif = [ variance_inflation_factor ( X_num . values , i ) for i in range ( X_num . shape [ 1 ])] pd . DataFrame ( vif , index = X_num . columns ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 id 8.424770 host_id 2.827543 latitude 7.297302 price 1.538975 minimum_nights 1.157468 number_of_reviews 3.215893 reviews_per_month 3.858006 calculated_host_listings_count 1.106414 availability_365 2.035592","title":"Exploratory Data Analysis"},{"location":"solutions/SOLN_X1_Thinking_Data/#feature-engineering","text":"Say we want to predict pricing, using an ML model. How would you build your features? Based on the number of null values, what would you do with the last_review and reviews_per_month column? X = airbnb . copy () y = X . pop ( 'price' ) X_cat = X . select_dtypes ( include = 'object' ) X_cat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name host_name neighbourhood_group neighbourhood room_type last_review 0 Clean & quiet apt home by the park John Brooklyn Kensington Private room 2018-10-19 1 Skylit Midtown Castle Jennifer Manhattan Midtown Entire home/apt 2019-05-21 2 THE VILLAGE OF HARLEM....NEW YORK ! Elisabeth Manhattan Harlem Private room NaN 3 Cozy Entire Floor of Brownstone LisaRoxanne Brooklyn Clinton Hill Entire home/apt 2019-07-05 4 Entire Apt: Spacious Studio/Loft by central park Laura Manhattan East Harlem Entire home/apt 2018-11-19 based on the number of unique columns, we may want to remove name , host_name , and last_review X_cat . nunique () name 47905 host_name 11452 neighbourhood_group 5 neighbourhood 221 room_type 3 last_review 1764 dtype: int64 X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () And now we deal with the numerical columns X_num = X . select_dtypes ( exclude = 'object' ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 10 9 0.10 1 0 both id and host_id will be highly cardinal without telling us much about the behavior of unseen data. We should remove them. We'll also drop the columns with nans for now X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_num = X_num . dropna ( axis = 1 ) X_enc_df = pd . DataFrame ( X_enc , columns = enc . get_feature_names_out ()) X_feat = pd . concat (( X_enc_df , X_num ), axis = 1 ) X_feat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } neighbourhood_group_Bronx neighbourhood_group_Brooklyn neighbourhood_group_Manhattan neighbourhood_group_Queens neighbourhood_group_Staten Island neighbourhood_Allerton neighbourhood_Arden Heights neighbourhood_Arrochar neighbourhood_Arverne neighbourhood_Astoria ... neighbourhood_Woodside room_type_Entire home/apt room_type_Private room room_type_Shared room latitude longitude minimum_nights number_of_reviews calculated_host_listings_count availability_365 0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 1.0 0.0 40.64749 -73.97237 1 9 6 365 1 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 40.75362 -73.98377 1 45 2 355 2 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 1.0 0.0 40.80902 -73.94190 3 0 1 365 3 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 40.68514 -73.95976 1 270 1 194 4 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 40.79851 -73.94399 10 9 1 0 5 rows \u00d7 235 columns","title":"Feature Engineering"},{"location":"solutions/SOLN_X1_Thinking_Data/#feature-transformation","text":"What features do you think will cause the most problems if untransformed? Scale and Center all but the target variable, price scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (48895, 235) (48895,)","title":"Feature Transformation"},{"location":"solutions/SOLN_X1_Thinking_Data/#model-baseline","text":"X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = LinearRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) r2_score ( y_train , model . predict ( X_train )) 0.11264603204210533 r2_score ( y_test , y_pred ) -1.563294115330747e+17 model = RandomForestRegressor () model . fit ( X_train , y_train ) r2_score ( y_train , model . predict ( X_train )) 0.8597830223730762 r2_score ( y_test , model . predict ( X_test )) 0.10233675407266163 both of these results from the LinearRegression and RandomForest models indicate overfitting","title":"Model Baseline"},{"location":"solutions/SOLN_X1_Thinking_Data/#back-to-feature-engineering","text":"\ud83c\udf1f - keep this feature \ud83d\udca1 - interesting behavior discovered \ud83d\udc4e - don't keep this feature \ud83d\udd2e - try for next time To try: drop nan rows not columns remove outliers (filter by group) PCA of one hot encoded vectors (will help with linear model) transform 'last review date' (str) into 'days since last review' (number)","title":"Back to Feature Engineering"},{"location":"solutions/SOLN_X1_Thinking_Data/#nans-drop-row-wise","text":"X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } latitude longitude minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 40.64749 -73.97237 1 9 0.21 6 365 1 40.75362 -73.98377 1 45 0.38 2 355 3 40.68514 -73.95976 1 270 4.64 1 194 4 40.79851 -73.94399 10 9 0.10 1 0 5 40.74767 -73.97500 3 74 0.59 1 129 X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38821, 233) (38821,)","title":"\ud83c\udf1f NaNs - Drop Row-wise"},{"location":"solutions/SOLN_X1_Thinking_Data/#traineval","text":"X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestRegressor ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train R2: { r2_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test R2: { r2_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train R2: 0.88 Test R2: 0.23","title":"Train/Eval"},{"location":"solutions/SOLN_X1_Thinking_Data/#outliers-by-borough","text":"X = airbnb . copy () X = X . dropna ( axis = 0 ) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = X [ 'neighbourhood_group' ], y = X [ 'price' ], ax = ax ) # ax.set_ylim(0, 1500) <AxesSubplot:xlabel='neighbourhood_group', ylabel='price'> fig , ax = plt . subplots ( figsize = ( 15 , 10 )) sns . kdeplot ( hue = X [ 'neighbourhood_group' ], x = X [ 'price' ], ax = ax ) <AxesSubplot:xlabel='price', ylabel='Density'> X = X . loc [ X . groupby ( 'neighbourhood_group' ) . apply ( lambda x : x [ 'price' ] < ( x [ 'price' ] . std () * 3 )) . unstack ( level = 0 ) . any ( axis = 1 )] fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 20 , 10 )) sns . boxplot ( x = X [ 'neighbourhood_group' ], y = X [ 'price' ], ax = ax ) sns . kdeplot ( hue = X [ 'neighbourhood_group' ], x = X [ 'price' ], ax = ax_ ) <AxesSubplot:xlabel='price', ylabel='Density'> y = X . pop ( 'price' ) X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38309, 232) (38309,)","title":"\ud83d\udca1 Outliers - by Borough"},{"location":"solutions/SOLN_X1_Thinking_Data/#traineval_1","text":"X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestRegressor ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train R2: { r2_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test R2: { r2_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train R2: 0.93 Test R2: 0.52 fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax . plot ( y_train , model . predict ( X_train ), ls = '' , marker = ',' ) ax_ . plot ( y_test , model . predict ( X_test ), ls = '' , marker = ',' ) [<matplotlib.lines.Line2D at 0x7f211ce29c40>]","title":"Train/Eval"},{"location":"solutions/SOLN_X1_Thinking_Data/#bin-prices-classifier-model","text":"X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) . values Y = y . reshape ( - 1 , 1 ) labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 y = labels X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38821, 233) (38821,)","title":"\ud83c\udf1f Bin Prices, Classifier Model"},{"location":"solutions/SOLN_X1_Thinking_Data/#traineval_2","text":"X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 1.00 Test Acc: 0.60 y_pred = model . predict ( X_train ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) precision recall f1-score support 1 1.00 1.00 1.00 7986 2 1.00 1.00 1.00 7594 3 1.00 1.00 1.00 7878 4 1.00 1.00 1.00 7598 accuracy 1.00 31056 macro avg 1.00 1.00 1.00 31056 weighted avg 1.00 1.00 1.00 31056 <AxesSubplot:> y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) precision recall f1-score support 1 0.70 0.80 0.74 1998 2 0.49 0.44 0.46 1846 3 0.50 0.47 0.48 1986 4 0.66 0.67 0.66 1935 accuracy 0.60 7765 macro avg 0.59 0.59 0.59 7765 weighted avg 0.59 0.60 0.59 7765 <AxesSubplot:>","title":"Train/Eval"},{"location":"solutions/SOLN_X1_Thinking_Data/#cluster-prices-classifier-model","text":"X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) . values Y = y . reshape ( - 1 , 1 ) distortions = [] inertias = [] silhouette = [] variance = [] krange = 20 for k in range ( 1 , krange ): kmeans = KMeans ( n_clusters = k ) kmeans . fit ( Y ) y_kmeans = kmeans . predict ( Y ) labels = kmeans . labels_ distortions . append ( sum ( np . min ( cdist ( Y , kmeans . cluster_centers_ , 'euclidean' ), axis = 1 )) / Y . shape [ 0 ]) inertias . append ( kmeans . inertia_ ) if k > 1 : silhouette . append ( silhouette_score ( Y , labels , metric = 'euclidean' )) variance . append ( calinski_harabasz_score ( Y , labels )) fig , [[ ax1 , ax2 ], [ ax3 , ax4 ]] = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) ax1 . plot ( range ( 1 , krange ), distortions ) ax2 . plot ( range ( 1 , krange ), inertias ) ax3 . plot ( range ( 2 , krange ), silhouette ) ax4 . plot ( range ( 2 , krange ), variance ) [<matplotlib.lines.Line2D at 0x7f211ca89100>] kmeans = KMeans ( n_clusters = 5 ) kmeans . fit ( Y ) y_kmeans = kmeans . predict ( Y ) labels = kmeans . labels_ ks = kmeans . cluster_centers_ ks = ks . flatten () ks = np . sort ( ks ) ks array([ 87.29374822, 230.74992332, 647.13125 , 2728.375 , 8749.75 ]) edges = ( np . diff ( ks ) / 2 + ks [: - 1 ]) . astype ( int ) bins = [] for idx , edge in enumerate ( edges ): if idx == 0 : bins . append ( f \"0- { edge } \" ) elif idx < len ( edges ): bins . append ( f \" { edges [ idx - 1 ] } - { edge } \" ) bins . append ( f \" { edge } +\" ) bins ['0-159', '159-438', '438-1687', '1687-5739', '5739+'] pd . DataFrame ( labels ) . value_counts ( sort = False ) 0 9651 1 8 2 961 3 28153 4 48 dtype: int64 y = labels X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38821, 233) (38821,)","title":"\ud83d\udc4e Cluster Prices, Classifier Model"},{"location":"solutions/SOLN_X1_Thinking_Data/#traineval_3","text":"X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 1.00 Test Acc: 0.81 y_pred = model . predict ( X_train ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) precision recall f1-score support 0 1.00 1.00 1.00 7687 1 1.00 1.00 1.00 7 2 1.00 1.00 1.00 762 3 1.00 1.00 1.00 22561 4 1.00 1.00 1.00 39 accuracy 1.00 31056 macro avg 1.00 1.00 1.00 31056 weighted avg 1.00 1.00 1.00 31056 <AxesSubplot:> y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) precision recall f1-score support 0 0.64 0.60 0.62 1964 1 0.00 0.00 0.00 1 2 0.71 0.14 0.23 199 3 0.86 0.91 0.88 5592 4 0.67 0.22 0.33 9 accuracy 0.81 7765 macro avg 0.58 0.37 0.41 7765 weighted avg 0.80 0.81 0.80 7765 <AxesSubplot:>","title":"Train/Eval"},{"location":"solutions/SOLN_X1_Thinking_Data/#pca-feature-reduction","text":"The results in Bin Price, Classifier Model indicate overfitting. Let's see if we can reduce the cardinality of our One Hot features X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) . values Y = y . reshape ( - 1 , 1 ) bins = 10 quantiles = bins + 1 labels = y . copy () for idx , quant in enumerate ( np . linspace ( 0 , 1 , quantiles )): if idx == 0 : prev_quant = quant continue if idx == 1 : labels [ labels <= np . quantile ( y , quant )] = 1 elif quant < 1 : labels [( labels > np . quantile ( y , prev_quant )) & ( labels <= np . quantile ( y , quant ))] = idx else : labels [( labels > np . quantile ( y , prev_quant ))] = idx prev_quant = quant print ([ np . quantile ( y , quant ) for quant in np . linspace ( 0 , 1 , quantiles )]) y = labels X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () pca = PCA ( n_components = 3 ) X_pca = pca . fit_transform ( X_enc ) print ( pca . explained_variance_ ) scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_pca , X_std_num )) print ( X_std . shape ) print ( y . shape ) [0.0, 49.0, 60.0, 75.0, 90.0, 101.0, 125.0, 150.0, 190.0, 250.0, 10000.0] [0.52595687 0.42901998 0.16673031] (38821, 10) (38821,)","title":"\ud83c\udf1f PCA, Feature Reduction"},{"location":"solutions/SOLN_X1_Thinking_Data/#traineval_4","text":"X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) y_pred = model . predict ( X_train ) print ( \"Training Confusion Matrix\" ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) plt . show () y_pred = model . predict ( X_test ) print ( \"Testing Confusion Matrix\" ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) Train Acc: 1.00 Test Acc: 0.32 Training Confusion Matrix precision recall f1-score support 1 1.00 1.00 1.00 3148 2 1.00 1.00 1.00 3241 3 1.00 1.00 1.00 3458 4 1.00 1.00 1.00 3075 5 1.00 1.00 1.00 2658 6 1.00 1.00 1.00 3198 7 1.00 1.00 1.00 3426 8 1.00 1.00 1.00 2759 9 1.00 1.00 1.00 3274 10 1.00 1.00 1.00 2819 accuracy 1.00 31056 macro avg 1.00 1.00 1.00 31056 weighted avg 1.00 1.00 1.00 31056 Testing Confusion Matrix precision recall f1-score support 1 0.51 0.56 0.54 840 2 0.36 0.41 0.39 794 3 0.30 0.31 0.30 824 4 0.26 0.23 0.25 782 5 0.17 0.15 0.16 604 6 0.24 0.23 0.24 813 7 0.25 0.26 0.25 842 8 0.25 0.19 0.22 736 9 0.32 0.35 0.33 818 10 0.46 0.48 0.47 712 accuracy 0.32 7765 macro avg 0.31 0.32 0.31 7765 weighted avg 0.32 0.32 0.32 7765 <AxesSubplot:>","title":"Train/Eval"},{"location":"solutions/SOLN_X1_Thinking_Data/#last-review-date-pca-for-outlier-removal-impute","text":"If I wanted to spend more time on this: remove outliers with PCA the issue with our outlier removal previously, is that we are conditioning on y . As we can't know y in a production setting, this makes our model suspetible to underdetecting true, high-y value signal removing outliers based on the input, X is prefered, and we might try this with PCA turn last_review_date into a number (counts of days) this would change a string (to be one hot encoded) column to a number column (avoids curse of dimensionality) impute missing values we're currently omitting about 20% of our data points, it may give us a boost to impute or otherwise estimate these missing values","title":"\ud83d\udd2e Last Review Date, PCA for Outlier Removal, Impute"},{"location":"solutions/SOLN_X1_Thinking_Data/#hyperparameter-optimization","text":"","title":"Hyperparameter Optimization"},{"location":"solutions/SOLN_X1_Thinking_Data/#round-1","text":"We'll start with a broad, shallow search (few trees) param_grid = { 'bootstrap' : [ True , False ], 'criterion' : [ 'gini' , 'entropy' ], 'min_samples_split' : [ 2 , 4 , 6 ], 'min_samples_leaf' : [ 1 , 3 , 5 ], 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'class_weight' : [ 'balanced' , 'balanced_subsample' , None ], 'n_estimators' : [ 1 , 5 ]} grid = GridSearchCV ( RandomForestClassifier (), param_grid , cv = 5 , n_jobs =- 1 , verbose = 3 ) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) print ( grid . best_params_ ) {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 5} model = grid . best_estimator_ model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) y_pred = model . predict ( X_train ) print ( \"Training Confusion Matrix\" ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) plt . show () y_pred = model . predict ( X_test ) print ( \"Testing Confusion Matrix\" ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) Train Acc: 0.79 Test Acc: 0.30 Training Confusion Matrix precision recall f1-score support 1 0.80 0.89 0.84 3148 2 0.79 0.83 0.81 3241 3 0.82 0.74 0.78 3458 4 0.77 0.76 0.77 3075 5 0.74 0.79 0.77 2658 6 0.79 0.74 0.76 3198 7 0.81 0.71 0.76 3426 8 0.77 0.83 0.80 2759 9 0.81 0.77 0.79 3274 10 0.80 0.86 0.83 2819 accuracy 0.79 31056 macro avg 0.79 0.79 0.79 31056 weighted avg 0.79 0.79 0.79 31056 Testing Confusion Matrix precision recall f1-score support 1 0.49 0.58 0.53 840 2 0.33 0.37 0.35 794 3 0.28 0.24 0.26 824 4 0.23 0.21 0.22 782 5 0.17 0.21 0.18 604 6 0.24 0.20 0.22 813 7 0.23 0.20 0.21 842 8 0.23 0.22 0.22 736 9 0.29 0.27 0.28 818 10 0.41 0.47 0.44 712 accuracy 0.30 7765 macro avg 0.29 0.30 0.29 7765 weighted avg 0.29 0.30 0.29 7765 <AxesSubplot:> gs_results = pd . DataFrame ( grid . cv_results_ ) params = list ( gs_results . columns [ gs_results . columns . str . contains ( 'param' )] . values ) params . pop ( - 1 ) display ( gs_results . head ()) print ( params ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_fit_time std_fit_time mean_score_time std_score_time param_bootstrap param_class_weight param_criterion param_max_features param_min_samples_leaf param_min_samples_split ... split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score split5_test_score split6_test_score mean_test_score std_test_score rank_test_score 0 0.118039 0.002019 0.004001 0.000212 True balanced gini auto 1 2 ... 0.230787 0.235745 0.239351 0.232139 0.247520 0.238278 0.222272 0.235156 0.007317 610 1 0.518118 0.006517 0.012629 0.000356 True balanced gini auto 1 2 ... 0.262114 0.268425 0.253324 0.281271 0.273670 0.272543 0.259693 0.267291 0.008820 309 2 0.113497 0.004069 0.003702 0.000130 True balanced gini auto 1 4 ... 0.231463 0.231012 0.236196 0.249268 0.243012 0.240532 0.229937 0.237345 0.006712 567 3 0.482682 0.004648 0.012108 0.000515 True balanced gini auto 1 4 ... 0.268875 0.277890 0.272481 0.275186 0.275248 0.274121 0.272543 0.273764 0.002630 269 4 0.104504 0.004581 0.003597 0.000109 True balanced gini auto 1 6 ... 0.232590 0.227406 0.237773 0.246338 0.249549 0.232191 0.238954 0.237829 0.007357 559 5 rows \u00d7 22 columns ['param_bootstrap', 'param_class_weight', 'param_criterion', 'param_max_features', 'param_min_samples_leaf', 'param_min_samples_split', 'param_n_estimators'] target = 'mean_test_score' moodsdf = pd . DataFrame () for col in params : for truff in gs_results [ col ] . unique (): try : group = gs_results . loc [ gs_results [ col ] == truff ][ target ] pop = gs_results . loc [ ~ ( gs_results [ col ] == truff )][ target ] stat , p , m , table = stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , table ]) . T ]) except : print ( col , truff ) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'table' ] moodsdf [ 'p_value' ] = moodsdf [ 'p_value' ] . astype ( float ) print ( moodsdf . shape ) confidence_level = 0.05 moodsdf = moodsdf . loc [( moodsdf [ 'p_value' ] < confidence_level )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( \"Clearing high p-value...\" ) print ( moodsdf . shape ) param_class_weight None (17, 9) Clearing high p-value... (2, 9) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 0 param_n_estimators 1 644.006173 4.494276e-142 0.260964 0.243444 0.243093 324 [[0, 324], [324, 0]] 1 param_n_estimators 5 644.006173 4.494276e-142 0.260964 0.280889 0.281234 324 [[324, 0], [0, 324]] for param in params : sns . boxplot ( x = gs_results [ param ], y = gs_results [ target ]) plt . show ()","title":"Round 1"},{"location":"solutions/SOLN_X1_Thinking_Data/#round-2","text":"Let's take those best parameters and dig a little deaper print ( grid . best_params_ ) {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 5} param_grid = { 'bootstrap' : [ True , False ], 'criterion' : [ 'gini' , 'entropy' ]} grid = GridSearchCV ( RandomForestClassifier ( min_samples_leaf = 5 , min_samples_split = 2 , max_features = 'sqrt' , class_weight = 'balanced' , n_estimators = 100 ), param_grid , cv = 5 , n_jobs =- 1 , verbose = 2 ) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) Fitting 5 folds for each of 4 candidates, totalling 20 fits {'bootstrap': False, 'criterion': 'entropy'} gs_results2 = pd . DataFrame ( grid . cv_results_ ) params = list ( gs_results2 . columns [ gs_results2 . columns . str . contains ( 'param' )] . values ) params . pop ( - 1 ) display ( gs_results2 . head ()) print ( params ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_fit_time std_fit_time mean_score_time std_score_time param_bootstrap param_criterion params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score 0 20.133523 0.372951 0.581618 0.020547 True gini {'bootstrap': True, 'criterion': 'gini'} 0.306825 0.305909 0.309612 0.314281 0.307036 0.308733 0.003035 2 1 52.458941 0.532249 0.603756 0.138301 True entropy {'bootstrap': True, 'criterion': 'entropy'} 0.311494 0.297859 0.314925 0.312188 0.304460 0.308185 0.006211 4 2 31.101482 0.656385 0.763153 0.090014 False gini {'bootstrap': False, 'criterion': 'gini'} 0.306665 0.303494 0.314603 0.313798 0.302689 0.308250 0.005044 3 3 53.006182 7.189164 0.337193 0.088254 False entropy {'bootstrap': False, 'criterion': 'entropy'} 0.307147 0.303333 0.317501 0.313476 0.307841 0.309860 0.005010 1 ['param_bootstrap', 'param_criterion'] for param in params : sns . boxplot ( x = gs_results [ param ], y = gs_results [ target ]) plt . show ()","title":"Round 2"},{"location":"solutions/SOLN_X1_Thinking_Data/#round-3","text":"And now tune model complexity # Cell for Exercise 2 r2 = [] for n_estimators in range ( 10 , 100 , 10 ): model = RandomForestClassifier ( n_estimators = n_estimators , bootstrap = False , criterion = 'entropy' , min_samples_leaf = 5 , min_samples_split = 2 , max_features = 'sqrt' , class_weight = 'balanced' ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) r2 . append ([ r2_score ( y_train , model . predict ( X_train )), r2_score ( y_test , model . predict ( X_test ))]) score = np . array ( r2 ) score1 = score [:, 0 ] score2 = score [:, 1 ] fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . plot ( range ( 10 , 100 , 10 ), score1 , ls = '' , marker = '.' , color = 'blue' , label = 'Train' ) ax . plot ( range ( 10 , 100 , 10 ), score2 , ls = '' , marker = 'o' , color = 'red' , label = 'Test' ) ax . set_title ( \"Scores with Increasing Model Complexity\" ) ax . set_xlabel ( \"Trees in the Forest\" ) ax . set_ylabel ( \"$R^2$\" ) ax . legend () <matplotlib.legend.Legend at 0x7f20e6d99a00> model = grid . best_estimator_ model . n_estimators = 80 model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) y_pred = model . predict ( X_train ) print ( \"Training Confusion Matrix\" ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) plt . show () y_pred = model . predict ( X_test ) print ( \"Testing Confusion Matrix\" ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) Train Acc: 0.90 Test Acc: 0.32 Training Confusion Matrix precision recall f1-score support 1 0.86 0.95 0.90 3148 2 0.88 0.91 0.90 3241 3 0.92 0.86 0.89 3458 4 0.90 0.88 0.89 3075 5 0.87 0.91 0.89 2658 6 0.91 0.86 0.89 3198 7 0.93 0.85 0.89 3426 8 0.90 0.93 0.91 2759 9 0.91 0.89 0.90 3274 10 0.88 0.94 0.91 2819 accuracy 0.90 31056 macro avg 0.90 0.90 0.90 31056 weighted avg 0.90 0.90 0.90 31056 Testing Confusion Matrix precision recall f1-score support 1 0.51 0.60 0.55 840 2 0.34 0.38 0.36 794 3 0.30 0.26 0.28 824 4 0.27 0.24 0.26 782 5 0.17 0.21 0.19 604 6 0.25 0.22 0.23 813 7 0.25 0.21 0.23 842 8 0.25 0.24 0.24 736 9 0.33 0.30 0.31 818 10 0.44 0.54 0.48 712 accuracy 0.32 7765 macro avg 0.31 0.32 0.31 7765 weighted avg 0.31 0.32 0.32 7765 <AxesSubplot:> After all that work we don't get much lift from the random forest with default hyperparameters","title":"Round 3"},{"location":"solutions/SOLN_X1_Thinking_Data/#conclusion","text":"","title":"Conclusion"},{"location":"solutions/SOLN_X1_Thinking_Data/#the-final-classification-model","text":"Final model had an F1 score ranging from 19-55% depending on class and a total accuracy of 32% This model could be used to suggest a price band for would-be airbnb hosts in NYC; or a price estimator to assess how changes in listing attributes wil affect price. A potential pitfall could be that new airbnb hosts will not have many total reviews or high variance in the reviews per month. We can currate price signal from the available feature inputs: neighbourhood_group neighbourhood longitude / latitude room_type price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 What worked: dropping nans row-wise allowed us to keep the reviews_per_month column, which gave us an \\(R^2\\) boost of 10% converting from a regression problem to a classification problem allowed us to deal with the long, high-price tail converting one hot encoded vectors to the first principal components kept us from overfitting (although this was not important for the random forrest model) More to try: change last_review_date from datetime or str to int use PCA for outlier removal based on the input data X imput missing values for reviews_per_month to capture an additional 10,000 datapoints","title":"The Final Classification Model"},{"location":"solutions/SOLN_X1_Thinking_Data/#additional-strategies","text":"Removing outliers based on the target variable, price , could also be a valid strategy. If we were to employ the model, we would have to be transparent that it should be used to predict prices in the sub $700 range, which is most of the Airbnb business in NYC anyway. At the end of the day, our decisions about model creation need to serve the business need.","title":"Additional Strategies"}]}