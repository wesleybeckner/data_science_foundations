{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science Foundations \u00b6 Welcome to Data Science Foundations, my name is Wesley Visit the github repo to access all materials and code for this class This is part II of a multi-part lesson path. Checkout Python Foundations for part I and General Applications of Neural Networks for part III Access the solutions if you get stuck The recommended schedule for this material: Day (2.5 hrs/day) Modules 1 Session 1: Regression and Analysis Lab 1: Descriptive Statistics Data Hunt 2 Session 2: Inferential Statistics Lab 2: Inferential Statistics Data Hunt 3 Session 3: Model Selection and Validation Project Part 1: Statistical Analysis of Tic-Tac-Toe 4 Session 4: Feature Engineering Lab 3: Practice with Feature Engineering 5 Session 5: Unsupervised Learning Project Part 2: Heuristical Tic-Tac-Toe Agents 6 Session 6: Bagging Lab 4: Practice with Supervised Learners 7 Session 7: Boosting Lab 5: Practice with Writing Unit Tests Project Part 3: 1-Step Look Ahead Agents 8 Project Part 4: N-Step Look Ahead Agents Happy learning \ud83e\uddd1\u200d\ud83c\udfeb","title":"About"},{"location":"#data-science-foundations","text":"Welcome to Data Science Foundations, my name is Wesley Visit the github repo to access all materials and code for this class This is part II of a multi-part lesson path. Checkout Python Foundations for part I and General Applications of Neural Networks for part III Access the solutions if you get stuck The recommended schedule for this material: Day (2.5 hrs/day) Modules 1 Session 1: Regression and Analysis Lab 1: Descriptive Statistics Data Hunt 2 Session 2: Inferential Statistics Lab 2: Inferential Statistics Data Hunt 3 Session 3: Model Selection and Validation Project Part 1: Statistical Analysis of Tic-Tac-Toe 4 Session 4: Feature Engineering Lab 3: Practice with Feature Engineering 5 Session 5: Unsupervised Learning Project Part 2: Heuristical Tic-Tac-Toe Agents 6 Session 6: Bagging Lab 4: Practice with Supervised Learners 7 Session 7: Boosting Lab 5: Practice with Writing Unit Tests Project Part 3: 1-Step Look Ahead Agents 8 Project Part 4: N-Step Look Ahead Agents Happy learning \ud83e\uddd1\u200d\ud83c\udfeb","title":"Data Science Foundations"},{"location":"S1_Regression_and_Analysis/","text":"Data Science Foundations Session 1: Regression and Analysis \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at fitting data to a curve using regression . We will also look at using regression to make predictions for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error. 1.0 Preparing Environment and Importing Data \u00b6 back to top 1.0.1 Import Packages \u00b6 back to top # Import pandas, pyplot, ipywidgets import pandas as pd from matplotlib import pyplot as plt from ipywidgets import interact # Import Scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score # for enrichment topics import seaborn as sns import numpy as np 1.0.2 Load Dataset \u00b6 back to top For our discussion on regression and descriptive statistics today we will use a well known dataset of different wines and their quality ratings df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) df . shape (6497, 13) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 1.1 What is regression? \u00b6 It is the process of finding a relationship between dependent and independent variables to find trends in data. This abstract definition means that you have one variable (the dependent variable) which depends on one or more variables (the independent variables). One of the reasons for which we want to regress data is to understand whether there is a trend between two variables. Housing Prices Example We can imagine this scenario with housing prices. Envision a mixed dataset of continuous and discrete independent variables. Some features could be continuous, floating point values like location ranking and housing condition. Others could be discrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a regression model. 1.2 Linear regression fitting with scikit-learn \u00b6 \ud83c\udfcb\ufe0f Exercise 1: rudimentary EDA \u00b6 What does the data look like? Recall how to visualize data in a pandas dataframe for every column calculate the: skew: df.skew() kurtosis: df.kurtosis() pearsons correlation with the dependent variable: df.corr() number of missing entries df.isnull() and organize this into a new dataframe note: pearsons is just one type of correlation, another correlation available to us is spearman which differs from pearsons in that it depends on ranked values rather than their direct quantities, you can read more here df . isnull () . sum () type 0 fixed acidity 10 volatile acidity 8 citric acid 3 residual sugar 2 chlorides 2 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 9 sulphates 4 alcohol 0 quality 0 dtype: int64 # Cell for Exercise 1 # part A # using df.<method> define the following four variables with the results from # skew(), kurtosis(), corr() (while selecting for quality), and isnull() # for isnull() you'll notice the return is a dataframe of booleans. we would # like to simply know the number of null values for each column. change the # return of isnull() using the sum() method along the columns # skew = # kurt = # pear = # null = # part B # on line 13, put these results in a list using square brackets and call # pd.DataFrame on the list to make your new DataFrame! store it under the # variable name dff # part C # take the transpose of this DataFrame using dff.T. reassign dff to this copy # part D # set the column names to 'skew', 'kurtosis', 'pearsons _quality', and # 'null count' using dff.columns # Now return dff to the output to view your hand work # dff # uncomment this line I have gone ahead and repeated this exercise with the red vs white wine types: red = df . loc [ df [ 'type' ] == 'red' ] wht = df . loc [ df [ 'type' ] == 'white' ] def get_summary ( df ): skew = df . skew () kurt = df . kurtosis () pear = df . corr ()[ 'quality' ] null = df . isnull () . sum () med = df . median () men = df . mean () dff = pd . DataFrame ([ skew , kurt , pear , null , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'pearsons _quality' , 'null count' , 'median' , 'mean' ] return dff dffr = get_summary ( red ) dffw = get_summary ( wht ) desc = pd . concat ([ dffr , dffw ], keys = [ 'red' , 'white' ]) /tmp/ipykernel_1419/2387423026.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. skew = df.skew() /tmp/ipykernel_1419/2387423026.py:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. kurt = df.kurtosis() /tmp/ipykernel_1419/2387423026.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. med = df.median() /tmp/ipykernel_1419/2387423026.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. men = df.mean() desc .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count median mean red fixed acidity 0.982192 1.132624 0.123834 2.0 7.90000 8.322104 volatile acidity 0.672862 1.226846 -0.390858 1.0 0.52000 0.527738 citric acid 0.317891 -0.788476 0.226917 1.0 0.26000 0.271145 residual sugar 4.540655 28.617595 0.013732 0.0 2.20000 2.538806 chlorides 5.680347 41.715787 -0.128907 0.0 0.07900 0.087467 free sulfur dioxide 1.250567 2.023562 -0.050656 0.0 14.00000 15.874922 total sulfur dioxide 1.515531 3.809824 -0.185100 0.0 38.00000 46.467792 density 0.071288 0.934079 -0.174919 0.0 0.99675 0.996747 pH 0.194803 0.814690 -0.057094 2.0 3.31000 3.310864 sulphates 2.429115 11.712632 0.251685 2.0 0.62000 0.658078 alcohol 0.860829 0.200029 0.476166 0.0 10.20000 10.422983 quality 0.217802 0.296708 1.000000 0.0 6.00000 5.636023 type NaN NaN NaN 0.0 NaN NaN white fixed acidity 0.647981 2.176560 -0.114032 8.0 6.80000 6.855532 volatile acidity 1.578595 5.095526 -0.194976 7.0 0.26000 0.278252 citric acid 1.284217 6.182036 -0.009194 2.0 0.32000 0.334250 residual sugar 1.076601 3.469536 -0.097373 2.0 5.20000 6.393250 chlorides 5.023412 37.560847 -0.210181 2.0 0.04300 0.045778 free sulfur dioxide 1.406745 11.466342 0.008158 0.0 34.00000 35.308085 total sulfur dioxide 0.390710 0.571853 -0.174737 0.0 134.00000 138.360657 density 0.977773 9.793807 -0.307123 0.0 0.99374 0.994027 pH 0.458402 0.532552 0.098858 7.0 3.18000 3.188203 sulphates 0.977361 1.589847 0.053690 2.0 0.47000 0.489835 alcohol 0.487342 -0.698425 0.435575 0.0 10.40000 10.514267 quality 0.155796 0.216526 1.000000 0.0 6.00000 5.877909 type NaN NaN NaN 0.0 NaN NaN def my_fig ( metric = desc . columns ): fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) pd . DataFrame ( desc [ metric ]) . unstack ()[ metric ] . T . plot ( kind = 'barh' , ax = ax ) # interact(my_fig) \ud83d\ude4b Question 1: Discussion Around EDA Plot \u00b6 What do we think of this plot? metric = mean , the cholrides values metric = kurtosis , residual sugar metric = pearsons _quality , magnitudes and directions How to improve the plot, what other plots would we like to see? For instance, what if we were really curious about the high kurtosis for chlorides content? What more would we like to glean about the distribution of chloride content? # we can use df.describe() to take a look at the quantile values and min/max df [ 'chlorides' ] . describe () count 6495.000000 mean 0.056042 std 0.035036 min 0.009000 25% 0.038000 50% 0.047000 75% 0.065000 max 0.611000 Name: chlorides, dtype: float64 # and see how these values appear in a KDE fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) df [ 'chlorides' ] . plot ( kind = 'kde' , ax = ax ) ax . set_xlim ( 0 , .61 ) (0.0, 0.61) # lastly we may want to look at the raw values themselves. We can sort them # too view outliers df [ 'chlorides' ] . sort_values ( ascending = False )[: 50 ] 5156 0.611 5049 0.610 5004 0.467 4979 0.464 5590 0.422 6268 0.415 6270 0.415 5652 0.415 6217 0.414 5949 0.414 5349 0.413 6158 0.403 4981 0.401 5628 0.387 6063 0.369 4915 0.368 5067 0.360 5179 0.358 484 0.346 5189 0.343 4917 0.341 5124 0.337 4940 0.332 1217 0.301 687 0.290 4473 0.271 5079 0.270 6272 0.267 5138 0.263 1865 0.255 5466 0.250 1034 0.244 5674 0.243 5675 0.241 683 0.240 1638 0.239 5045 0.236 6456 0.235 6468 0.230 5465 0.226 5464 0.226 5564 0.222 2186 0.217 5996 0.216 6333 0.214 5206 0.214 6332 0.214 5205 0.213 4497 0.212 1835 0.211 Name: chlorides, dtype: float64 1.2.2 Visualizing the data set - motivating regression analysis \u00b6 In order to demonstrate simple linear regression with this dataset we will look at two particular features: fixed acidity and density . We can create a scatter plot of fixed acidity vs density for the red wine in the dataset using df.plot() and see that there appears to be a general trend between the two features: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) df . loc [ df [ 'type' ] == 'red' ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> Now the question is: How do we quantify this trend? 1.2.3 Estimating the regression coefficients \u00b6 It looks like density increases with fixed acidity following a line, maybe something like y(x)= m \\cdot x + b \\;\\;\\;\\;\\;\\;\\;\\; \\sf{eq. 1} with \\( y=\\sf density \\), \\(x=\\sf fixed \\space acidity\\), and \\(m\\) the slope and \\(b\\) the intercept. To solve the problem, we need to find the values of \\(b\\) and \\(m\\) in equation 1 to best fit the data. This is called linear regression . In linear regression our goal is to minimize the error between computed values of positions \\(y^{\\sf calc}(x_i)\\equiv y^{\\sf calc}_i\\) and known values \\(y^{\\sf exact}(x_i)\\equiv y^{\\sf exact}_i\\), i.e. find \\(b\\) and \\(m\\) which lead to lowest value of \\epsilon (m,b) =SS_{\\sf res}=\\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - m\\cdot x_i - b \\right)^2\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sf{eq. 2} Otherwise known as the residual sum of squares To find out more see e.g. https://en.wikipedia.org/wiki/Simple_linear_regression \ud83d\ude4b Question 2: linear regression loss function \u00b6 Do we always want m and b to be large positive numbers so as to minimize eq. 2? Luckily scikit-learn contains many functions related to regression including linear regression . The function we will use is called LinearRegression() . # Create linear regression object model = linear_model . LinearRegression () # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red [ 'density' ] . values . reshape ( - 1 , 1 ) y = red [ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) print ( red [ 'density' ] . values . shape , red [ 'fixed acidity' ] . values . shape ) print ( x . shape , y . shape ) (1599,) (1599,) (1599, 1) (1599, 1) What happens when we try to fit the data as is? # Fit to the data # model.fit(x, y) \ud83c\udfcb\ufe0f Exercise 2: drop Null Values (and practice pandas operations) \u00b6 Let's look back at our dataset description dataframe above, what do we notice, what contains null values? There are several strategies for dealing with null values. For now let's take the simplest case, and drop rows in our dataframe that contain null # Cell for Exercise 2 # For this templated exercise you are going to complete everything in one line # of code, but we are going to break it up into steps. So for each part (A, B, # etc.) paste your answer from the previous part to begin (your opertaions will # read from left to right) # step A # select the 'density' and 'fixed acidity' columns of red. make sure the return # is a dataframe # step B # now use the dropna() method on axis 0 (the rows) to drop any null values # step B # select column 'density' # step C # select the values # step D # reshape the result with an empty second dimension using .reshape() and store # the result under variable x # repeat the same process with 'fixed acidity' and variable y Now that we have our x and y arrays we can fit using ScikitLearn x = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values . reshape ( - 1 , 1 ) y = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) \ud83d\ude4b Question 3: why do we drop null values across both columns? \u00b6 Notice in the above cell how we selected both density and fixed acidity before calling dropna ? Why did we do that? Why didn't we just select density in the x variable case and fixed acidity in the y variable case? # Fit to the data model . fit ( x , y ) # Extract the values of interest m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # Print the slope m and intercept b print ( 'Scikit learn - Slope: ' , m , 'Intercept: ' , b ) Scikit learn - Slope: 616.01314280661 Intercept: -605.6880086750523 \ud83c\udfcb\ufe0f Exercise 3: calculating y_pred \u00b6 Estimate the values of \\(y\\) by using your fitted parameters. Hint: Use your model.coef_ and model.intercept_ parameters to estimate y_pred following equation 1 # define y_pred in terms of m, x, and b # y_pred = # uncomment the following lines! # fig, ax = plt.subplots(1,1, figsize=(10,10)) # ax.plot(x, y_pred, ls='', marker='*') # ax.plot(x, y, ls='', marker='.') We can also return predictions directly with the model object using the predict() method note: it is great to get in the habit of utilizing model outputs this way, as the API will be similar across all scikit-learn models (and sometimes models in other libraries as well!) # Another way to get this is using the model.predict function y_pred = model . predict ( x ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x , y_pred , ls = '' , marker = '*' ) ax . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7fdd881875e0>] 1.3 Error and topics of model fitting (assessing model accuracy) \u00b6 1.3.1 Measuring the quality of fit \u00b6 1.3.1.1 Mean Squared Error \u00b6 The plot in Section 1.2.3 looks good, but numerically what is our error? What is the mean value of $\\epsilon$, i.e. the Mean Squared Error (MSE) ? {\\sf MSE}=\\epsilon_{\\sf ave} = \\frac{\\sum_{i=1}^{N_{\\sf times}}\\left(y^{\\sf exact}_i - m\\cdot t_i - b \\right)^2}{N_{\\sf times}}\\;\\;\\;\\;\\;\\sf eq. 3 # The mean squared error print ( 'Mean squared error: %.2f ' % mean_squared_error ( y , y_pred )) Mean squared error: 1.68 1.3.1.2 R-square \u00b6 Another way to measure error is the regression score, \\(R^2\\). \\(R^2\\) is generally defined as the ratio of the total sum of squares \\(SS_{\\sf tot}\\) to the residual sum of squares \\(SS_{\\sf res}\\): SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\; \\sf eq. 4 SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2\\;\\;\\;\\;\\; \\sf eq. 5 R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}} \\;\\;\\;\\;\\;\\; \\sf eq. 6 In eq. 4, \\(\\bar{y}=\\sum_i y^{\\sf exact}_i/N\\) is the average value of y for \\(N\\) points. The best value of \\(R^2\\) is 1 but it can also take a negative value if the error is large. See all the different regression metrics here . \ud83d\ude4b Question 4: lets understand \\(R^2\\) \u00b6 Do we need a large value of \\(SS_{\\sf tot}\\) to minimize \\(R^2\\) - is this something which we have the power to control? # Print the coefficient of determination - 1 is perfect prediction print ( 'Coefficient of determination: %.2f ' % r2_score ( y , y_pred )) Coefficient of determination: 0.45 1.3.2 Corollaries with classification models \u00b6 For classification tasks, we typically assess accuracy vs MSE or R-square, since we are dealing with categorical rather than numerical predictions. What is accuracy? It is defined as the ratio of True assignments to all assignments. For a binary positive/negative classification task this can be written as the following: Acc = \\frac{T_p + T_n}{F_p + F_n + T_p + T_n} Where \\(T\\) is True, \\(F\\) is false, \\(p\\) is positive, \\(n\\) is negative Just as a quick example, we can perform this type of task on our wine dataset by predicting on quality, which is a discrete 3-9 quality score: y_train = df [ 'type' ] . values . reshape ( - 1 , 1 ) x_train = df [ 'quality' ] . values . reshape ( - 1 , 1 ) # train a logistic regression model on the training set from sklearn.linear_model import LogisticRegression # instantiate model logreg = LogisticRegression () # fit model logreg . fit ( x_train , y_train ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) LogisticRegression() # make class predictions for the testing set y_pred_class = logreg . predict ( x_train ) # calculate accuracy from sklearn import metrics print ( metrics . accuracy_score ( y_train , y_pred_class )) 0.7538864091118977 1.3.3 Beyond a single input feature \u00b6 ( also: quick appreciative beat for folding in domain area expertise into our models and features ) The acidity of the wine (the dependent variable v) could depend on: potassium from the soil (increases alkalinity) unripe grapes (increases acidity) grapes grown in colder climates or reduced sunshine create less sugar (increases acidity) preprocessing such as adding tartaric acid to the grape juice before fermentation (increases acidity) malolactic fermentation (reduces acidity) + others So in our lab today we will look at folding in additional variables in our dataset into the model 1.4 Multivariate regression \u00b6 Let's now turn our attention to wine quality. The value we aim to predict or evaluate is the quality of each wine in our dataset. This is our dependent variable. We will look at how this is related to the 12 other independent variables, also known as input features . We're going to do this with only the red wine data red . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 4898 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 4899 red 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5 4900 red 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5 4901 red 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6 4902 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 1.4.1 Linear regression with all input fields \u00b6 For this example, notice we have a categorical data variable in the 'type' column. We will ignore this for now, and only work with our red wines. In the future we will discuss how to deal with categorical variable such as this in a mathematical representation. # this is a list of all our features or independent variables features = list ( red . columns [ 1 :]) # we're going to remove our target or dependent variable, density from this # list features . remove ( 'density' ) # now we define X and y according to these lists of names X = red . dropna ( axis = 0 )[ features ] . values y = red . dropna ( axis = 0 )[ 'density' ] . values # we will talk about scaling/centering our data at a later time X = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) red . isnull () . sum ( axis = 0 ) # we are getting rid of some nasty nulls! type 0 fixed acidity 2 volatile acidity 1 citric acid 1 residual sugar 0 chlorides 0 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 2 sulphates 2 alcohol 0 quality 0 dtype: int64 # Create linear regression object - note that we are using all the input features model = linear_model . LinearRegression () model . fit ( X , y ) y_calc = model . predict ( X ) Let's see what the coefficients look like ... print ( \"Fit coefficients: \\n \" , model . coef_ , \" \\n Number of coefficients:\" , len ( model . coef_ )) Fit coefficients: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] Number of coefficients: 11 We have 11 !!! That's because we are regressing respect to all 11 independent variables !!! So now, y_{\\sf calc}= m_1x_1 +\\, m_2x_2 \\,+ \\,m_3x_3 \\,+\\,... \\,+ \\,b =\\sum_{i=1}^{13}m_i x_i + b\\;\\;\\;\\;\\; \\sf eq. 7 print ( \"We have 11 slopes / weights: \\n\\n \" , model . coef_ ) print ( \" \\n And one intercept: \" , model . intercept_ ) We have 11 slopes / weights: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] And one intercept: 0.9967517451349656 # This size should match the number of columns in X if len ( X [ 0 ]) == len ( model . coef_ ): print ( \"All good! The number of coefficients matches the number of input features.\" ) else : print ( \"Hmm .. something strange is going on.\" ) All good! The number of coefficients matches the number of input features. \ud83c\udfcb\ufe0f Exercise 4: evaluate the error \u00b6 Let's evaluate the error by computing the MSE and \\(R^2\\) metrics (see eq. 3 and 6). # The mean squared error # part A # calculate the MSE using mean_squared_error() # mse = # part B # calculate the R square using r2_score() # r2 = # print('Mean squared error: {:.2f}'.format(mse)) # print('Coefficient of determination: {:.2f}'.format(r2)) \ud83c\udfcb\ufe0f Exercise 5: make a plot of y actual vs y predicted \u00b6 We can also look at how well the computed values match the true values graphically by generating a scatterplot. # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style # plt.plot() plt . title ( \"Linear regression - computed values on entire data set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () \ud83c\udf52 1.4.2 Enrichment : Splitting into train and test sets \u00b6 note: more of this topic is covered in Model Selection and Validation To see whether we can predict, we will carry out our regression only on a part, 80%, of the full data set. This part is called the training data. We will then test the trained model to predict the rest of the data, 20% - the test data. The function which fits won't see the test data until it has to predict it. We start by splitting out data using scikit-learn's train_test_split() function: X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 42 ) Now we check the size of y_train and y_test , the sum should be the size of y! If this works then we move on and carry out regression but we only use the training data! if len ( y_test ) + len ( y_train ) == len ( y ): print ( 'All good, ready to to go and regress! \\n ' ) # Carry out linear regression print ( 'Running linear regression algorithm on the training set \\n ' ) model = linear_model . LinearRegression () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_pred_test = model . predict ( X_test ) All good, ready to to go and regress! Running linear regression algorithm on the training set Fit coefficients and intercept: [ 1.62385613e-03 1.10578142e-04 7.75216492e-07 5.87755741e-04 7.65190323e-05 -1.03490059e-04 8.87357873e-05 7.79083342e-04 2.23534769e-04 -9.99858829e-04 5.85256438e-06] 0.9967531628434799 Now we can plot our predicted values to see how accurate we are in predicting. We will generate a scatterplot and computing the MSE and \\(R^2\\) metrics of error. sns . scatterplot ( x = y_pred_test , y = y_test , color = \"mediumvioletred\" , s = 50 ) plt . title ( \"Linear regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2e ' % mean_squared_error ( y_test , y_pred_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_pred_test )) Mean squared error: 5.45e-07 Coefficient of determination: 0.87 1.4.2.1 Other data considerations \u00b6 Do we need all the independent variables? Topics of interential statistics covered in a couple sessions Can we output integer quality scores? Topics of non-binary classification tasks covered in week 4 \ud83c\udf52 1.4.3 Enrichment : Other regression algorithms \u00b6 There are many other regression algorithms the two we want to highlight here are Ridge, LASSO, and Elastic Net. They differ by an added term to the loss function. Let's review. Eq. 2 expanded to multivariate form yields: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 for Ridge regression, we add a regularization term known as L2 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}\\beta_{j}^2 for LASSO (Least Absolute Shrinkage and Selection Operator) we add L1 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}|\\beta_{j}| The key difference here is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. Elastic Net is a combination of these two regularization methods. model = linear_model . Ridge () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_calc_test = model . predict ( X_test ) Fit coefficients and intercept: [ 1.61930554e-03 1.11227142e-04 2.64709094e-06 5.87271456e-04 7.58510569e-05 -1.02851782e-04 8.76686650e-05 7.75641517e-04 2.23315063e-04 -9.98653815e-04 5.26839010e-06] 0.9967531358810221 sns . scatterplot ( x = y_calc_test , y = y_test , color = \"lightseagreen\" , s = 50 ) plt . title ( \"Ridge regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2f ' % mean_squared_error ( y_test , y_calc_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_calc_test )) Mean squared error: 0.00 Coefficient of determination: 0.87 \ud83c\udfcb\ufe0f Exercise 6: Tune Hyperparameter for Ridge Regression \u00b6 Use the docstring to peak into the hyperparameters for Ridge Regression. What is the optimal value of lambda? Plot the \\(\\beta\\) values vs \\(\\lambda\\) from the results of your analysis # cell for exercise 3 out_lambdas = [] out_coefs = [] out_scores = [] for i in range ( 10 ): lambdas = [] coefs = [] scores = [] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) for lamb in range ( 1 , int ( 5e3 ), 20 ): model = linear_model . Ridge ( alpha = lamb ) model . fit ( X_train , y_train ) lambdas . append ( lamb ) coefs . append ( model . coef_ ) scores . append ( r2_score ( y_test , model . predict ( X_test ))) # print('MSE: %.4f' % mean_squared_error(y_test, model.predict(X_test))) # print('R2: %.4f' % r2_score(y_test, model.predict(X_test))) out_lambdas . append ( lambdas ) out_coefs . append ( coefs ) out_scores . append ( scores ) coef_means = np . array ( out_coefs ) . mean ( axis = 0 ) coef_stds = np . array ( out_coefs ) . std ( axis = 0 ) results_means = pd . DataFrame ( coef_means , columns = features ) results_stds = pd . DataFrame ( coef_stds , columns = features ) results_means [ 'lambda' ] = [ i for i in lambdas ] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) for feat in features : ax . errorbar ([ i for i in lambdas ], results_means [ feat ], yerr = results_stds [ feat ], label = feat ) # results.plot('lambda', 'scores', ax=ax[1]) ax . legend () <matplotlib.legend.Legend at 0x7fdd72270fd0> results = pd . DataFrame ( coefs , columns = features ) results [ 'lambda' ] = [ i for i in lambdas ] results [ 'scores' ] = scores fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) for feat in features : results . plot ( 'lambda' , feat , ax = ax [ 0 ]) results . plot ( 'lambda' , 'scores' , ax = ax [ 1 ]) <AxesSubplot:xlabel='lambda'> \ud83c\udf52 1.5 Enrichment : Additional Regression Exercises \u00b6 Problem 1) Number and choice of input features \u00b6 Load the red wine dataset and evaluate how the linear regression predictions changes as you change the number and choice of input features . The total number of columns in X is 11 and each column represents a specific input feature. Estimate the MSE print ( X_train . shape ) (1274, 11) If you want to use the first 5 features you could proceed as following: X_train_five = X_train [:, 0 : 5 ] X_test_five = X_test [:, 0 : 5 ] Check that the new variables have the shape your expect print ( X_train_five . shape ) print ( X_test_five . shape ) (1274, 5) (319, 5) Now you can use these to train your linear regression model and repeat for different numbers or sets of input features! Note that you do not need to change the output feature! It's size is independent from the number of input features, yet recall that its length is the same as the number of values per input feature. Questions to think about while you work on this problem - How many input feature variables does one need? Is there a maximum or minimum number? - Could one input feature variable be better than the rest? - What if values are missing for one of the input feature variables - is it still worth using it? - Can you use L1 or L2 to determine these optimum features more quickly? Problem 2) Type of regression algorithm \u00b6 Try using other types of linear regression methods on the wine dataset: the LASSO model and the Elastic net model which are described by the sklearn.linear_model.ElasticNet() sklearn.linear_model.Lasso() scikit-learn functions. For more detail see ElasticNet and Lasso . Questions to think about while you work on this problem - How does the error change with each model? - Which model seems to perform best? - How can you optimize the hyperparameter, \\(\\lambda\\) - Does one model do better than the other at determining which input features are more important? - How about non linear regression / what if the data does not follow a line? - How do the bias and variance change for each model from sklearn.linear_model import ElasticNet from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.linear_model import LinearRegression for model in [ ElasticNet , Lasso , Ridge , LinearRegression ]: model = model () model . fit ( X_train , y_train ) print ( str ( model )) print ( 'Mean squared error: %.ef' % mean_squared_error ( y_test , model . predict ( X_test ))) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , model . predict ( X_test ))) print () ElasticNet() Mean squared error: 4e-06f Coefficient of determination: -0.00 Lasso() Mean squared error: 4e-06f Coefficient of determination: -0.00 Ridge() Mean squared error: 7e-07f Coefficient of determination: 0.83 LinearRegression() Mean squared error: 7e-07f Coefficient of determination: 0.83 References \u00b6 Linear Regression To find out more see simple linear regression scikit-learn Scikit-learn Linear regression in scikit-learn Metrics of error The Boston dataset Pearson correlation To find out more see pearson Irreducible error, bias and variance Great Coursera videos here and here","title":"Regression and Analysis"},{"location":"S1_Regression_and_Analysis/#data-science-foundations-session-1-regression-and-analysis","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at fitting data to a curve using regression . We will also look at using regression to make predictions for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error.","title":"Data Science Foundations  Session 1: Regression and Analysis"},{"location":"S1_Regression_and_Analysis/#10-preparing-environment-and-importing-data","text":"back to top","title":"1.0 Preparing Environment and Importing Data"},{"location":"S1_Regression_and_Analysis/#101-import-packages","text":"back to top # Import pandas, pyplot, ipywidgets import pandas as pd from matplotlib import pyplot as plt from ipywidgets import interact # Import Scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score # for enrichment topics import seaborn as sns import numpy as np","title":"1.0.1 Import Packages"},{"location":"S1_Regression_and_Analysis/#102-load-dataset","text":"back to top For our discussion on regression and descriptive statistics today we will use a well known dataset of different wines and their quality ratings df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) df . shape (6497, 13) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6","title":"1.0.2 Load Dataset"},{"location":"S1_Regression_and_Analysis/#11-what-is-regression","text":"It is the process of finding a relationship between dependent and independent variables to find trends in data. This abstract definition means that you have one variable (the dependent variable) which depends on one or more variables (the independent variables). One of the reasons for which we want to regress data is to understand whether there is a trend between two variables. Housing Prices Example We can imagine this scenario with housing prices. Envision a mixed dataset of continuous and discrete independent variables. Some features could be continuous, floating point values like location ranking and housing condition. Others could be discrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a regression model.","title":"1.1 What is regression?"},{"location":"S1_Regression_and_Analysis/#12-linear-regression-fitting-with-scikit-learn","text":"","title":"1.2  Linear regression fitting with scikit-learn"},{"location":"S1_Regression_and_Analysis/#exercise-1-rudimentary-eda","text":"What does the data look like? Recall how to visualize data in a pandas dataframe for every column calculate the: skew: df.skew() kurtosis: df.kurtosis() pearsons correlation with the dependent variable: df.corr() number of missing entries df.isnull() and organize this into a new dataframe note: pearsons is just one type of correlation, another correlation available to us is spearman which differs from pearsons in that it depends on ranked values rather than their direct quantities, you can read more here df . isnull () . sum () type 0 fixed acidity 10 volatile acidity 8 citric acid 3 residual sugar 2 chlorides 2 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 9 sulphates 4 alcohol 0 quality 0 dtype: int64 # Cell for Exercise 1 # part A # using df.<method> define the following four variables with the results from # skew(), kurtosis(), corr() (while selecting for quality), and isnull() # for isnull() you'll notice the return is a dataframe of booleans. we would # like to simply know the number of null values for each column. change the # return of isnull() using the sum() method along the columns # skew = # kurt = # pear = # null = # part B # on line 13, put these results in a list using square brackets and call # pd.DataFrame on the list to make your new DataFrame! store it under the # variable name dff # part C # take the transpose of this DataFrame using dff.T. reassign dff to this copy # part D # set the column names to 'skew', 'kurtosis', 'pearsons _quality', and # 'null count' using dff.columns # Now return dff to the output to view your hand work # dff # uncomment this line I have gone ahead and repeated this exercise with the red vs white wine types: red = df . loc [ df [ 'type' ] == 'red' ] wht = df . loc [ df [ 'type' ] == 'white' ] def get_summary ( df ): skew = df . skew () kurt = df . kurtosis () pear = df . corr ()[ 'quality' ] null = df . isnull () . sum () med = df . median () men = df . mean () dff = pd . DataFrame ([ skew , kurt , pear , null , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'pearsons _quality' , 'null count' , 'median' , 'mean' ] return dff dffr = get_summary ( red ) dffw = get_summary ( wht ) desc = pd . concat ([ dffr , dffw ], keys = [ 'red' , 'white' ]) /tmp/ipykernel_1419/2387423026.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. skew = df.skew() /tmp/ipykernel_1419/2387423026.py:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. kurt = df.kurtosis() /tmp/ipykernel_1419/2387423026.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. med = df.median() /tmp/ipykernel_1419/2387423026.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. men = df.mean() desc .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count median mean red fixed acidity 0.982192 1.132624 0.123834 2.0 7.90000 8.322104 volatile acidity 0.672862 1.226846 -0.390858 1.0 0.52000 0.527738 citric acid 0.317891 -0.788476 0.226917 1.0 0.26000 0.271145 residual sugar 4.540655 28.617595 0.013732 0.0 2.20000 2.538806 chlorides 5.680347 41.715787 -0.128907 0.0 0.07900 0.087467 free sulfur dioxide 1.250567 2.023562 -0.050656 0.0 14.00000 15.874922 total sulfur dioxide 1.515531 3.809824 -0.185100 0.0 38.00000 46.467792 density 0.071288 0.934079 -0.174919 0.0 0.99675 0.996747 pH 0.194803 0.814690 -0.057094 2.0 3.31000 3.310864 sulphates 2.429115 11.712632 0.251685 2.0 0.62000 0.658078 alcohol 0.860829 0.200029 0.476166 0.0 10.20000 10.422983 quality 0.217802 0.296708 1.000000 0.0 6.00000 5.636023 type NaN NaN NaN 0.0 NaN NaN white fixed acidity 0.647981 2.176560 -0.114032 8.0 6.80000 6.855532 volatile acidity 1.578595 5.095526 -0.194976 7.0 0.26000 0.278252 citric acid 1.284217 6.182036 -0.009194 2.0 0.32000 0.334250 residual sugar 1.076601 3.469536 -0.097373 2.0 5.20000 6.393250 chlorides 5.023412 37.560847 -0.210181 2.0 0.04300 0.045778 free sulfur dioxide 1.406745 11.466342 0.008158 0.0 34.00000 35.308085 total sulfur dioxide 0.390710 0.571853 -0.174737 0.0 134.00000 138.360657 density 0.977773 9.793807 -0.307123 0.0 0.99374 0.994027 pH 0.458402 0.532552 0.098858 7.0 3.18000 3.188203 sulphates 0.977361 1.589847 0.053690 2.0 0.47000 0.489835 alcohol 0.487342 -0.698425 0.435575 0.0 10.40000 10.514267 quality 0.155796 0.216526 1.000000 0.0 6.00000 5.877909 type NaN NaN NaN 0.0 NaN NaN def my_fig ( metric = desc . columns ): fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) pd . DataFrame ( desc [ metric ]) . unstack ()[ metric ] . T . plot ( kind = 'barh' , ax = ax ) # interact(my_fig)","title":"\ud83c\udfcb\ufe0f Exercise 1: rudimentary EDA"},{"location":"S1_Regression_and_Analysis/#question-1-discussion-around-eda-plot","text":"What do we think of this plot? metric = mean , the cholrides values metric = kurtosis , residual sugar metric = pearsons _quality , magnitudes and directions How to improve the plot, what other plots would we like to see? For instance, what if we were really curious about the high kurtosis for chlorides content? What more would we like to glean about the distribution of chloride content? # we can use df.describe() to take a look at the quantile values and min/max df [ 'chlorides' ] . describe () count 6495.000000 mean 0.056042 std 0.035036 min 0.009000 25% 0.038000 50% 0.047000 75% 0.065000 max 0.611000 Name: chlorides, dtype: float64 # and see how these values appear in a KDE fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) df [ 'chlorides' ] . plot ( kind = 'kde' , ax = ax ) ax . set_xlim ( 0 , .61 ) (0.0, 0.61) # lastly we may want to look at the raw values themselves. We can sort them # too view outliers df [ 'chlorides' ] . sort_values ( ascending = False )[: 50 ] 5156 0.611 5049 0.610 5004 0.467 4979 0.464 5590 0.422 6268 0.415 6270 0.415 5652 0.415 6217 0.414 5949 0.414 5349 0.413 6158 0.403 4981 0.401 5628 0.387 6063 0.369 4915 0.368 5067 0.360 5179 0.358 484 0.346 5189 0.343 4917 0.341 5124 0.337 4940 0.332 1217 0.301 687 0.290 4473 0.271 5079 0.270 6272 0.267 5138 0.263 1865 0.255 5466 0.250 1034 0.244 5674 0.243 5675 0.241 683 0.240 1638 0.239 5045 0.236 6456 0.235 6468 0.230 5465 0.226 5464 0.226 5564 0.222 2186 0.217 5996 0.216 6333 0.214 5206 0.214 6332 0.214 5205 0.213 4497 0.212 1835 0.211 Name: chlorides, dtype: float64","title":"\ud83d\ude4b Question 1: Discussion Around EDA Plot"},{"location":"S1_Regression_and_Analysis/#122-visualizing-the-data-set-motivating-regression-analysis","text":"In order to demonstrate simple linear regression with this dataset we will look at two particular features: fixed acidity and density . We can create a scatter plot of fixed acidity vs density for the red wine in the dataset using df.plot() and see that there appears to be a general trend between the two features: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) df . loc [ df [ 'type' ] == 'red' ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> Now the question is: How do we quantify this trend?","title":"1.2.2 Visualizing the data set - motivating regression analysis"},{"location":"S1_Regression_and_Analysis/#123-estimating-the-regression-coefficients","text":"It looks like density increases with fixed acidity following a line, maybe something like y(x)= m \\cdot x + b \\;\\;\\;\\;\\;\\;\\;\\; \\sf{eq. 1} with \\( y=\\sf density \\), \\(x=\\sf fixed \\space acidity\\), and \\(m\\) the slope and \\(b\\) the intercept. To solve the problem, we need to find the values of \\(b\\) and \\(m\\) in equation 1 to best fit the data. This is called linear regression . In linear regression our goal is to minimize the error between computed values of positions \\(y^{\\sf calc}(x_i)\\equiv y^{\\sf calc}_i\\) and known values \\(y^{\\sf exact}(x_i)\\equiv y^{\\sf exact}_i\\), i.e. find \\(b\\) and \\(m\\) which lead to lowest value of \\epsilon (m,b) =SS_{\\sf res}=\\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - m\\cdot x_i - b \\right)^2\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sf{eq. 2} Otherwise known as the residual sum of squares To find out more see e.g. https://en.wikipedia.org/wiki/Simple_linear_regression","title":"1.2.3 Estimating the regression coefficients"},{"location":"S1_Regression_and_Analysis/#question-2-linear-regression-loss-function","text":"Do we always want m and b to be large positive numbers so as to minimize eq. 2? Luckily scikit-learn contains many functions related to regression including linear regression . The function we will use is called LinearRegression() . # Create linear regression object model = linear_model . LinearRegression () # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red [ 'density' ] . values . reshape ( - 1 , 1 ) y = red [ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) print ( red [ 'density' ] . values . shape , red [ 'fixed acidity' ] . values . shape ) print ( x . shape , y . shape ) (1599,) (1599,) (1599, 1) (1599, 1) What happens when we try to fit the data as is? # Fit to the data # model.fit(x, y)","title":"\ud83d\ude4b Question 2: linear regression loss function"},{"location":"S1_Regression_and_Analysis/#exercise-2-drop-null-values-and-practice-pandas-operations","text":"Let's look back at our dataset description dataframe above, what do we notice, what contains null values? There are several strategies for dealing with null values. For now let's take the simplest case, and drop rows in our dataframe that contain null # Cell for Exercise 2 # For this templated exercise you are going to complete everything in one line # of code, but we are going to break it up into steps. So for each part (A, B, # etc.) paste your answer from the previous part to begin (your opertaions will # read from left to right) # step A # select the 'density' and 'fixed acidity' columns of red. make sure the return # is a dataframe # step B # now use the dropna() method on axis 0 (the rows) to drop any null values # step B # select column 'density' # step C # select the values # step D # reshape the result with an empty second dimension using .reshape() and store # the result under variable x # repeat the same process with 'fixed acidity' and variable y Now that we have our x and y arrays we can fit using ScikitLearn x = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values . reshape ( - 1 , 1 ) y = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'fixed acidity' ] . values . reshape ( - 1 , 1 )","title":"\ud83c\udfcb\ufe0f Exercise 2: drop Null Values (and practice pandas operations)"},{"location":"S1_Regression_and_Analysis/#question-3-why-do-we-drop-null-values-across-both-columns","text":"Notice in the above cell how we selected both density and fixed acidity before calling dropna ? Why did we do that? Why didn't we just select density in the x variable case and fixed acidity in the y variable case? # Fit to the data model . fit ( x , y ) # Extract the values of interest m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # Print the slope m and intercept b print ( 'Scikit learn - Slope: ' , m , 'Intercept: ' , b ) Scikit learn - Slope: 616.01314280661 Intercept: -605.6880086750523","title":"\ud83d\ude4b Question 3: why do we drop null values across both columns?"},{"location":"S1_Regression_and_Analysis/#exercise-3-calculating-y_pred","text":"Estimate the values of \\(y\\) by using your fitted parameters. Hint: Use your model.coef_ and model.intercept_ parameters to estimate y_pred following equation 1 # define y_pred in terms of m, x, and b # y_pred = # uncomment the following lines! # fig, ax = plt.subplots(1,1, figsize=(10,10)) # ax.plot(x, y_pred, ls='', marker='*') # ax.plot(x, y, ls='', marker='.') We can also return predictions directly with the model object using the predict() method note: it is great to get in the habit of utilizing model outputs this way, as the API will be similar across all scikit-learn models (and sometimes models in other libraries as well!) # Another way to get this is using the model.predict function y_pred = model . predict ( x ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x , y_pred , ls = '' , marker = '*' ) ax . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7fdd881875e0>]","title":"\ud83c\udfcb\ufe0f Exercise 3: calculating y_pred"},{"location":"S1_Regression_and_Analysis/#13-error-and-topics-of-model-fitting-assessing-model-accuracy","text":"","title":"1.3 Error and topics of model fitting (assessing model accuracy)"},{"location":"S1_Regression_and_Analysis/#131-measuring-the-quality-of-fit","text":"","title":"1.3.1 Measuring the quality of fit"},{"location":"S1_Regression_and_Analysis/#1311-mean-squared-error","text":"The plot in Section 1.2.3 looks good, but numerically what is our error? What is the mean value of $\\epsilon$, i.e. the Mean Squared Error (MSE) ? {\\sf MSE}=\\epsilon_{\\sf ave} = \\frac{\\sum_{i=1}^{N_{\\sf times}}\\left(y^{\\sf exact}_i - m\\cdot t_i - b \\right)^2}{N_{\\sf times}}\\;\\;\\;\\;\\;\\sf eq. 3 # The mean squared error print ( 'Mean squared error: %.2f ' % mean_squared_error ( y , y_pred )) Mean squared error: 1.68","title":"1.3.1.1 Mean Squared Error"},{"location":"S1_Regression_and_Analysis/#1312-r-square","text":"Another way to measure error is the regression score, \\(R^2\\). \\(R^2\\) is generally defined as the ratio of the total sum of squares \\(SS_{\\sf tot}\\) to the residual sum of squares \\(SS_{\\sf res}\\): SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\; \\sf eq. 4 SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2\\;\\;\\;\\;\\; \\sf eq. 5 R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}} \\;\\;\\;\\;\\;\\; \\sf eq. 6 In eq. 4, \\(\\bar{y}=\\sum_i y^{\\sf exact}_i/N\\) is the average value of y for \\(N\\) points. The best value of \\(R^2\\) is 1 but it can also take a negative value if the error is large. See all the different regression metrics here .","title":"1.3.1.2 R-square"},{"location":"S1_Regression_and_Analysis/#question-4-lets-understand-r2","text":"Do we need a large value of \\(SS_{\\sf tot}\\) to minimize \\(R^2\\) - is this something which we have the power to control? # Print the coefficient of determination - 1 is perfect prediction print ( 'Coefficient of determination: %.2f ' % r2_score ( y , y_pred )) Coefficient of determination: 0.45","title":"\ud83d\ude4b Question 4: lets understand \\(R^2\\)"},{"location":"S1_Regression_and_Analysis/#132-corollaries-with-classification-models","text":"For classification tasks, we typically assess accuracy vs MSE or R-square, since we are dealing with categorical rather than numerical predictions. What is accuracy? It is defined as the ratio of True assignments to all assignments. For a binary positive/negative classification task this can be written as the following: Acc = \\frac{T_p + T_n}{F_p + F_n + T_p + T_n} Where \\(T\\) is True, \\(F\\) is false, \\(p\\) is positive, \\(n\\) is negative Just as a quick example, we can perform this type of task on our wine dataset by predicting on quality, which is a discrete 3-9 quality score: y_train = df [ 'type' ] . values . reshape ( - 1 , 1 ) x_train = df [ 'quality' ] . values . reshape ( - 1 , 1 ) # train a logistic regression model on the training set from sklearn.linear_model import LogisticRegression # instantiate model logreg = LogisticRegression () # fit model logreg . fit ( x_train , y_train ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) LogisticRegression() # make class predictions for the testing set y_pred_class = logreg . predict ( x_train ) # calculate accuracy from sklearn import metrics print ( metrics . accuracy_score ( y_train , y_pred_class )) 0.7538864091118977","title":"1.3.2 Corollaries with classification models"},{"location":"S1_Regression_and_Analysis/#133-beyond-a-single-input-feature","text":"( also: quick appreciative beat for folding in domain area expertise into our models and features ) The acidity of the wine (the dependent variable v) could depend on: potassium from the soil (increases alkalinity) unripe grapes (increases acidity) grapes grown in colder climates or reduced sunshine create less sugar (increases acidity) preprocessing such as adding tartaric acid to the grape juice before fermentation (increases acidity) malolactic fermentation (reduces acidity) + others So in our lab today we will look at folding in additional variables in our dataset into the model","title":"1.3.3 Beyond a single input feature"},{"location":"S1_Regression_and_Analysis/#14-multivariate-regression","text":"Let's now turn our attention to wine quality. The value we aim to predict or evaluate is the quality of each wine in our dataset. This is our dependent variable. We will look at how this is related to the 12 other independent variables, also known as input features . We're going to do this with only the red wine data red . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 4898 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 4899 red 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5 4900 red 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5 4901 red 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6 4902 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5","title":"1.4 Multivariate regression"},{"location":"S1_Regression_and_Analysis/#141-linear-regression-with-all-input-fields","text":"For this example, notice we have a categorical data variable in the 'type' column. We will ignore this for now, and only work with our red wines. In the future we will discuss how to deal with categorical variable such as this in a mathematical representation. # this is a list of all our features or independent variables features = list ( red . columns [ 1 :]) # we're going to remove our target or dependent variable, density from this # list features . remove ( 'density' ) # now we define X and y according to these lists of names X = red . dropna ( axis = 0 )[ features ] . values y = red . dropna ( axis = 0 )[ 'density' ] . values # we will talk about scaling/centering our data at a later time X = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) red . isnull () . sum ( axis = 0 ) # we are getting rid of some nasty nulls! type 0 fixed acidity 2 volatile acidity 1 citric acid 1 residual sugar 0 chlorides 0 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 2 sulphates 2 alcohol 0 quality 0 dtype: int64 # Create linear regression object - note that we are using all the input features model = linear_model . LinearRegression () model . fit ( X , y ) y_calc = model . predict ( X ) Let's see what the coefficients look like ... print ( \"Fit coefficients: \\n \" , model . coef_ , \" \\n Number of coefficients:\" , len ( model . coef_ )) Fit coefficients: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] Number of coefficients: 11 We have 11 !!! That's because we are regressing respect to all 11 independent variables !!! So now, y_{\\sf calc}= m_1x_1 +\\, m_2x_2 \\,+ \\,m_3x_3 \\,+\\,... \\,+ \\,b =\\sum_{i=1}^{13}m_i x_i + b\\;\\;\\;\\;\\; \\sf eq. 7 print ( \"We have 11 slopes / weights: \\n\\n \" , model . coef_ ) print ( \" \\n And one intercept: \" , model . intercept_ ) We have 11 slopes / weights: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] And one intercept: 0.9967517451349656 # This size should match the number of columns in X if len ( X [ 0 ]) == len ( model . coef_ ): print ( \"All good! The number of coefficients matches the number of input features.\" ) else : print ( \"Hmm .. something strange is going on.\" ) All good! The number of coefficients matches the number of input features.","title":"1.4.1 Linear regression with all input fields"},{"location":"S1_Regression_and_Analysis/#exercise-4-evaluate-the-error","text":"Let's evaluate the error by computing the MSE and \\(R^2\\) metrics (see eq. 3 and 6). # The mean squared error # part A # calculate the MSE using mean_squared_error() # mse = # part B # calculate the R square using r2_score() # r2 = # print('Mean squared error: {:.2f}'.format(mse)) # print('Coefficient of determination: {:.2f}'.format(r2))","title":"\ud83c\udfcb\ufe0f Exercise 4: evaluate the error"},{"location":"S1_Regression_and_Analysis/#exercise-5-make-a-plot-of-y-actual-vs-y-predicted","text":"We can also look at how well the computed values match the true values graphically by generating a scatterplot. # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style # plt.plot() plt . title ( \"Linear regression - computed values on entire data set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show ()","title":"\ud83c\udfcb\ufe0f Exercise 5: make a plot of y actual vs y predicted"},{"location":"S1_Regression_and_Analysis/#142-enrichment-splitting-into-train-and-test-sets","text":"note: more of this topic is covered in Model Selection and Validation To see whether we can predict, we will carry out our regression only on a part, 80%, of the full data set. This part is called the training data. We will then test the trained model to predict the rest of the data, 20% - the test data. The function which fits won't see the test data until it has to predict it. We start by splitting out data using scikit-learn's train_test_split() function: X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 42 ) Now we check the size of y_train and y_test , the sum should be the size of y! If this works then we move on and carry out regression but we only use the training data! if len ( y_test ) + len ( y_train ) == len ( y ): print ( 'All good, ready to to go and regress! \\n ' ) # Carry out linear regression print ( 'Running linear regression algorithm on the training set \\n ' ) model = linear_model . LinearRegression () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_pred_test = model . predict ( X_test ) All good, ready to to go and regress! Running linear regression algorithm on the training set Fit coefficients and intercept: [ 1.62385613e-03 1.10578142e-04 7.75216492e-07 5.87755741e-04 7.65190323e-05 -1.03490059e-04 8.87357873e-05 7.79083342e-04 2.23534769e-04 -9.99858829e-04 5.85256438e-06] 0.9967531628434799 Now we can plot our predicted values to see how accurate we are in predicting. We will generate a scatterplot and computing the MSE and \\(R^2\\) metrics of error. sns . scatterplot ( x = y_pred_test , y = y_test , color = \"mediumvioletred\" , s = 50 ) plt . title ( \"Linear regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2e ' % mean_squared_error ( y_test , y_pred_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_pred_test )) Mean squared error: 5.45e-07 Coefficient of determination: 0.87","title":"\ud83c\udf52 1.4.2 Enrichment: Splitting into train and test sets"},{"location":"S1_Regression_and_Analysis/#1421-other-data-considerations","text":"Do we need all the independent variables? Topics of interential statistics covered in a couple sessions Can we output integer quality scores? Topics of non-binary classification tasks covered in week 4","title":"1.4.2.1 Other data considerations"},{"location":"S1_Regression_and_Analysis/#143-enrichment-other-regression-algorithms","text":"There are many other regression algorithms the two we want to highlight here are Ridge, LASSO, and Elastic Net. They differ by an added term to the loss function. Let's review. Eq. 2 expanded to multivariate form yields: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 for Ridge regression, we add a regularization term known as L2 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}\\beta_{j}^2 for LASSO (Least Absolute Shrinkage and Selection Operator) we add L1 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}|\\beta_{j}| The key difference here is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. Elastic Net is a combination of these two regularization methods. model = linear_model . Ridge () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_calc_test = model . predict ( X_test ) Fit coefficients and intercept: [ 1.61930554e-03 1.11227142e-04 2.64709094e-06 5.87271456e-04 7.58510569e-05 -1.02851782e-04 8.76686650e-05 7.75641517e-04 2.23315063e-04 -9.98653815e-04 5.26839010e-06] 0.9967531358810221 sns . scatterplot ( x = y_calc_test , y = y_test , color = \"lightseagreen\" , s = 50 ) plt . title ( \"Ridge regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2f ' % mean_squared_error ( y_test , y_calc_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_calc_test )) Mean squared error: 0.00 Coefficient of determination: 0.87","title":"\ud83c\udf52 1.4.3 Enrichment: Other regression algorithms"},{"location":"S1_Regression_and_Analysis/#exercise-6-tune-hyperparameter-for-ridge-regression","text":"Use the docstring to peak into the hyperparameters for Ridge Regression. What is the optimal value of lambda? Plot the \\(\\beta\\) values vs \\(\\lambda\\) from the results of your analysis # cell for exercise 3 out_lambdas = [] out_coefs = [] out_scores = [] for i in range ( 10 ): lambdas = [] coefs = [] scores = [] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) for lamb in range ( 1 , int ( 5e3 ), 20 ): model = linear_model . Ridge ( alpha = lamb ) model . fit ( X_train , y_train ) lambdas . append ( lamb ) coefs . append ( model . coef_ ) scores . append ( r2_score ( y_test , model . predict ( X_test ))) # print('MSE: %.4f' % mean_squared_error(y_test, model.predict(X_test))) # print('R2: %.4f' % r2_score(y_test, model.predict(X_test))) out_lambdas . append ( lambdas ) out_coefs . append ( coefs ) out_scores . append ( scores ) coef_means = np . array ( out_coefs ) . mean ( axis = 0 ) coef_stds = np . array ( out_coefs ) . std ( axis = 0 ) results_means = pd . DataFrame ( coef_means , columns = features ) results_stds = pd . DataFrame ( coef_stds , columns = features ) results_means [ 'lambda' ] = [ i for i in lambdas ] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) for feat in features : ax . errorbar ([ i for i in lambdas ], results_means [ feat ], yerr = results_stds [ feat ], label = feat ) # results.plot('lambda', 'scores', ax=ax[1]) ax . legend () <matplotlib.legend.Legend at 0x7fdd72270fd0> results = pd . DataFrame ( coefs , columns = features ) results [ 'lambda' ] = [ i for i in lambdas ] results [ 'scores' ] = scores fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) for feat in features : results . plot ( 'lambda' , feat , ax = ax [ 0 ]) results . plot ( 'lambda' , 'scores' , ax = ax [ 1 ]) <AxesSubplot:xlabel='lambda'>","title":"\ud83c\udfcb\ufe0f Exercise 6: Tune Hyperparameter for Ridge Regression"},{"location":"S1_Regression_and_Analysis/#15-enrichment-additional-regression-exercises","text":"","title":"\ud83c\udf52 1.5 Enrichment: Additional Regression Exercises"},{"location":"S1_Regression_and_Analysis/#problem-1-number-and-choice-of-input-features","text":"Load the red wine dataset and evaluate how the linear regression predictions changes as you change the number and choice of input features . The total number of columns in X is 11 and each column represents a specific input feature. Estimate the MSE print ( X_train . shape ) (1274, 11) If you want to use the first 5 features you could proceed as following: X_train_five = X_train [:, 0 : 5 ] X_test_five = X_test [:, 0 : 5 ] Check that the new variables have the shape your expect print ( X_train_five . shape ) print ( X_test_five . shape ) (1274, 5) (319, 5) Now you can use these to train your linear regression model and repeat for different numbers or sets of input features! Note that you do not need to change the output feature! It's size is independent from the number of input features, yet recall that its length is the same as the number of values per input feature. Questions to think about while you work on this problem - How many input feature variables does one need? Is there a maximum or minimum number? - Could one input feature variable be better than the rest? - What if values are missing for one of the input feature variables - is it still worth using it? - Can you use L1 or L2 to determine these optimum features more quickly?","title":"Problem 1) Number and choice of input features"},{"location":"S1_Regression_and_Analysis/#problem-2-type-of-regression-algorithm","text":"Try using other types of linear regression methods on the wine dataset: the LASSO model and the Elastic net model which are described by the sklearn.linear_model.ElasticNet() sklearn.linear_model.Lasso() scikit-learn functions. For more detail see ElasticNet and Lasso . Questions to think about while you work on this problem - How does the error change with each model? - Which model seems to perform best? - How can you optimize the hyperparameter, \\(\\lambda\\) - Does one model do better than the other at determining which input features are more important? - How about non linear regression / what if the data does not follow a line? - How do the bias and variance change for each model from sklearn.linear_model import ElasticNet from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.linear_model import LinearRegression for model in [ ElasticNet , Lasso , Ridge , LinearRegression ]: model = model () model . fit ( X_train , y_train ) print ( str ( model )) print ( 'Mean squared error: %.ef' % mean_squared_error ( y_test , model . predict ( X_test ))) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , model . predict ( X_test ))) print () ElasticNet() Mean squared error: 4e-06f Coefficient of determination: -0.00 Lasso() Mean squared error: 4e-06f Coefficient of determination: -0.00 Ridge() Mean squared error: 7e-07f Coefficient of determination: 0.83 LinearRegression() Mean squared error: 7e-07f Coefficient of determination: 0.83","title":"Problem 2) Type of regression algorithm"},{"location":"S1_Regression_and_Analysis/#references","text":"Linear Regression To find out more see simple linear regression scikit-learn Scikit-learn Linear regression in scikit-learn Metrics of error The Boston dataset Pearson correlation To find out more see pearson Irreducible error, bias and variance Great Coursera videos here and here","title":"References"},{"location":"S2_Inferential_Statistics/","text":"Data Science Foundations Session 2: Inferential Statistics \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at the utility of EDA combined with inferential statistics. 2.0 Preparing Environment and Importing Data \u00b6 back to top 2.0.1 Import Packages \u00b6 back to top # The modules we've seen before import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns # our stats modules import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy 2.0.2 Load Dataset \u00b6 back to top For this session, we will use dummy datasets from sklearn. df = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... ... ... ... ... ... 1663 Tiramisu Chocolate Outer Doughnut Pear Amethyst Fickelgruber 12/2020 38128.802589 0.420111 1664 Tiramisu Chocolate Outer Doughnut Pear Burgundy Zebrabar 12/2020 108.642857 0.248659 1665 Tiramisu Chocolate Outer Doughnut Pear Teal Zebrabar 12/2020 3517.933333 0.378501 1666 Tiramisu Chocolate Outer Doughnut Rock and Rye Amethyst Slugworth 12/2020 10146.898432 0.213149 1667 Tiramisu Chocolate Outer Doughnut Rock and Rye Burgundy Zebrabar 12/2020 1271.904762 0.431813 1668 rows \u00d7 9 columns descriptors = df . columns [: - 2 ] for col in df . columns [: - 2 ]: print ( col ) print ( df [ col ] . unique ()) print () Base Cake ['Butter' 'Cheese' 'Chiffon' 'Pound' 'Sponge' 'Tiramisu'] Truffle Type ['Candy Outer' 'Chocolate Outer' 'Jelly Filled'] Primary Flavor ['Butter Pecan' 'Ginger Lime' 'Margarita' 'Pear' 'Pink Lemonade' 'Raspberry Ginger Ale' 'Sassafras' 'Spice' 'Wild Cherry Cream' 'Cream Soda' 'Horchata' 'Kettle Corn' 'Lemon Bar' 'Orange Pineapple\\tP' 'Plum' 'Orange' 'Butter Toffee' 'Lemon' 'Acai Berry' 'Apricot' 'Birch Beer' 'Cherry Cream Spice' 'Creme de Menthe' 'Fruit Punch' 'Ginger Ale' 'Grand Mariner' 'Orange Brandy' 'Pecan' 'Toasted Coconut' 'Watermelon' 'Wintergreen' 'Vanilla' 'Bavarian Cream' 'Black Licorice' 'Caramel Cream' 'Cheesecake' 'Cherry Cola' 'Coffee' 'Irish Cream' 'Lemon Custard' 'Mango' 'Sour' 'Amaretto' 'Blueberry' 'Butter Milk' 'Chocolate Mint' 'Coconut' 'Dill Pickle' 'Gingersnap' 'Chocolate' 'Doughnut'] Secondary Flavor ['Toffee' 'Banana' 'Rum' 'Tutti Frutti' 'Vanilla' 'Mixed Berry' 'Whipped Cream' 'Apricot' 'Passion Fruit' 'Peppermint' 'Dill Pickle' 'Black Cherry' 'Wild Cherry Cream' 'Papaya' 'Mango' 'Cucumber' 'Egg Nog' 'Pear' 'Rock and Rye' 'Tangerine' 'Apple' 'Black Currant' 'Kiwi' 'Lemon' 'Hazelnut' 'Butter Rum' 'Fuzzy Navel' 'Mojito' 'Ginger Beer'] Color Group ['Taupe' 'Amethyst' 'Burgundy' 'White' 'Black' 'Opal' 'Citrine' 'Rose' 'Slate' 'Teal' 'Tiffany' 'Olive'] Customer ['Slugworth' 'Perk-a-Cola' 'Fickelgruber' 'Zebrabar' \"Dandy's Candies\"] Date ['1/2020' '2/2020' '3/2020' '4/2020' '5/2020' '6/2020' '7/2020' '8/2020' '9/2020' '10/2020' '11/2020' '12/2020'] 2.1 Many Flavors of Statistical Tests \u00b6 img src Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (\u201cinferences\u201d) from that data. With inferential statistics, you take data from samples and make generalizations about a population statshowto Non-parametric tests Moods Median (Comparison of Medians) Kruskal-Wallis (Comparison of Medians, compare to ANOVA ) Mann Whitney (Rank order test, compare to T-test ) Comparison of means T-test Independent Equal Variances (students T-test) Unequal Variances (Welch's T-test) Dependent Comparison of variances Analysis of Variance (ANOVA) One Way ANOVA Two Way ANOVA MANOVA Factorial ANOVA When do I use each of these? We will talk about this as we proceed through the examples. This page from minitab has good rules of thumb on the subject. 2.1.1 What is Mood's Median? \u00b6 You can use Chi-Square to test for a goodness of fit (whether a sample of data represents a distribution) or whether two variables are related (using a contingency table, which we will create below!) A special case of Pearon's Chi-Squared Test: We create a table that counts the observations above and below the global median for two or more groups . We then perform a chi-squared test of significance on this contingency table Null hypothesis: the Medians are all equal The chi-square test statistic: x^2 = \\sum{\\frac{(O-E)^2}{E}} Where \\(O\\) is the observed frequency and \\(E\\) is the expected frequency. Let's take an example , say we have three shifts with the following production rates: np . random . seed ( 7 ) shift_one = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_two = [ round ( i ) for i in np . random . normal ( 21 , 3 , 10 )] print ( shift_one ) print ( shift_two ) [21, 15, 16, 17, 14, 16, 16, 11, 19, 18] [19, 20, 23, 20, 20, 17, 23, 21, 22, 16] stat , p , m , table = scipy . stats . median_test ( shift_one , shift_two , correction = False ) what is median_test returning? print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 7.20 p-value of the test: 0.007 the grand median: 18.5 Let's evaluate that test statistic ourselves by taking a look at the contingency table: table array([[2, 8], [8, 2]]) This is easier to make sense of if we order the shift times shift_one . sort () shift_one [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] When we look at shift one, we see that 8 values are at or below the grand median. shift_two . sort () shift_two [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] For shift two, only two are at or below the grand median. Since the sample sizes are the same, the expected value for both groups is the same, 5 above and 5 below the grand median. The chi-square is then: X^2 = \\frac{(2-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(2-5)^2}{5} ( 2 - 5 ) ** 2 / 5 + ( 8 - 5 ) ** 2 / 5 + ( 8 - 5 ) ** 2 / 5 + ( 2 - 5 ) ** 2 / 5 7.2 Our p-value, or the probability of observing the null-hypothsis, is under 0.05 (at 0.007). We can conclude that these shift performances were drawn under seperate distributions. For comparison, let's do this analysis again with shifts of equal performances np . random . seed ( 3 ) shift_three = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_four = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] stat , p , m , table = scipy . stats . median_test ( shift_three , shift_four , correction = False ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000 the grand median: 15.5 and the shift raw values: shift_three . sort () shift_four . sort () print ( shift_three ) print ( shift_four ) [10, 14, 15, 15, 15, 16, 16, 16, 17, 21] [11, 12, 13, 14, 15, 16, 19, 19, 19, 21] table array([[5, 5], [5, 5]]) 2.1.2 When to Use Mood's? \u00b6 Mood's Median Test is highly flexible but has the following assumptions: Considers only one categorical factor Response variable is continuous (our shift rates) Data does not need to be normally distributed But the distributions are similarly shaped Sample sizes can be unequal and small (less than 20 observations) Other considerations: Not as powerful as Kruskal-Wallis Test but still useful for small sample sizes or when there are outliers \ud83c\udfcb\ufe0f Exercise 1: Use Mood's Median Test \u00b6 Part A Perform moods median test on Base Cake (Categorical Variable) and EBITDA/KG (Continuous Variable) in Truffle data \u00b6 We're also going to get some practice with pandas groupby. # what is returned by this groupby? gp = df . groupby ( 'Base Cake' ) How do we find out? We could iterate through it: # seems to be a tuple of some sort for i in gp : print ( i ) break ('Butter', Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns]) # the first object appears to be the group print ( i [ 0 ]) # the second object appears to be the df belonging to that group print ( i [ 1 ]) Butter Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns] going back to our diagram from our earlier pandas session. It looks like whenever we split in the groupby method, we create separate dataframes as well as their group label: Ok, so we know gp is separate dataframes. How do we turn them into arrays to then pass to median_test ? # complete this for loop for i , j in gp : pass # turn 'EBITDA/KG' of j into an array using the .values attribute # print this to the screen After you've completed the previous step, turn this into a list comprehension and pass the result to a variable called margins # complete the code below # margins = [# YOUR LIST COMPREHENSION HERE] Remember the list unpacking we did for the tic tac toe project? We're going to do the same thing here. Unpack the margins list for median_test and run the cell below! # complete the following line # stat, p, m, table = scipy.stats.median_test(<UNPACK MARGINS HERE>, correction=False) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.2e} \" . format ( p )) print ( \"the grand median: {:.2e} \" . format ( m )) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.00e+00 the grand median: 1.55e+01 Part B View the distributions of the data using matplotlib and seaborn \u00b6 What a fantastic statistical result we found! Can we affirm our result with some visualizations? I hope so! Create a boxplot below using pandas. In your call to df.boxplot() the by parameter should be set to Base Cake and the column parameter should be set to EBITDA/KG # YOUR BOXPLOT HERE For comparison, I've shown the boxplot below using seaborn! fig , ax = plt . subplots ( figsize = ( 10 , 7 )) ax = sns . boxplot ( x = 'Base Cake' , y = 'EBITDA/KG' , data = df , color = '#A0cbe8' ) Part C Perform Moods Median on all the other groups \u00b6 # Recall the other descriptors we have descriptors Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') for desc in descriptors : # YOUR CODE FORM MARGINS BELOW # margins = [<YOUR LIST COMPREHENSION>] # UNPACK MARGINS INTO MEDIAN_TEST # stat, p, m, table = scipy.stats.median_test(<YOUR UNPACKING METHOD>, correction=False) print ( desc ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:e} \" . format ( p )) print ( \"the grand median: {} \" . format ( m ), end = ' \\n\\n ' ) Base Cake The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Truffle Type The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Primary Flavor The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Secondary Flavor The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Color Group The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Customer The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Date The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Part D Many boxplots \u00b6 And finally, we will confirm these visually. Complete the Boxplot for each group: for desc in descriptors : fig , ax = plt . subplots ( figsize = ( 10 , 5 )) # sns.boxplot(x=<YOUR X VARIABLE HERE>, y='EBITDA/KG', data=df, color='#A0cbe8', ax=ax) 2.1.3 What is a T-test? \u00b6 There are 1-sample and 2-sample T-tests (note: we would use a 1-sample T-test just to determine if the sample mean is equal to a hypothesized population mean) Within 2-sample T-tests we have independent and dependent T-tests (uncorrelated or correlated samples) For independent, two-sample T-tests: Equal variance (or pooled) T-test scipy.stats.ttest_ind(equal_var=True) also called Student's T-test Unequal variance T-test scipy.stats.ttest_ind(equal_var=False) also called Welch's T-test For dependent T-tests: Paired (or correlated) T-test scipy.stats.ttest_rel ex : patient symptoms before and after treatment A full discussion on T-tests is outside the scope of this session, but we can refer to wikipedia for more information, including formulas on how each statistic is computed: student's T-test 2.1.3.1 Demonstration of T-tests \u00b6 back to top We'll assume our shifts are of equal variance and proceed with the appropriate independent two-sample T-test... print ( shift_one ) print ( shift_two ) [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] To calculate the T-test, we follow a slightly different statistical formula: T=\\frac{\\mu_1 - \\mu_2}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} where \\(\\mu\\) are the means of the two groups, \\(n\\) are the sample sizes and \\(s\\) is the pooled standard deviation, also known as the cummulative variance (depending on if you square it or not): s= \\sqrt{\\frac{(n_1-1)\\sigma_1^2 + (n_2-1)\\sigma_2^2}{n_1 + n_2 - 2}} where \\(\\sigma\\) are the standard deviations. What you'll notice here is we are combining the two variances, we can only do this if we assume the variances are somewhat equal, this is known as the equal variances t-test. mean_shift_one = np . mean ( shift_one ) mean_shift_two = np . mean ( shift_two ) print ( mean_shift_one , mean_shift_two ) 16.3 20.1 com_var = (( np . sum ([( i - mean_shift_one ) ** 2 for i in shift_one ]) + np . sum ([( i - mean_shift_two ) ** 2 for i in shift_two ])) / ( len ( shift_one ) + len ( shift_two ) - 2 )) print ( com_var ) 6.5 T = ( np . abs ( mean_shift_one - mean_shift_two ) / ( np . sqrt ( com_var / len ( shift_one ) + com_var / len ( shift_two )))) T 3.3328204733667115 We see that this hand-computed result matches that of the scipy module: scipy . stats . ttest_ind ( shift_two , shift_one , equal_var = True ) Ttest_indResult(statistic=3.3328204733667115, pvalue=0.0037029158660758575) 2.1.4 What are F-statistics and the F-test? \u00b6 The F-statistic is simply a ratio of two variances, or the ratio of mean squares mean squares is the estimate of population variance that accounts for the degrees of freedom to compute that estimate. We will explore this in the context of ANOVA 2.1.4.1 What is Analysis of Variance? \u00b6 ANOVA uses the F-test to determine whether the variability between group means is larger than the variability within the groups. If that statistic is large enough, you can conclude that the means of the groups are not equal. The caveat is that ANOVA tells us whether there is a difference in means but it does not tell us where the difference is. To find where the difference is between the groups, we have to conduct post-hoc tests. There are two main types: One-way (one factor) and Two-way (two factor) where factor is an independent variable Ind A Ind B Dep X H 10 X I 12 Y I 11 Y H 20 ANOVA Hypotheses \u00b6 Null hypothesis : group means are equal Alternative hypothesis : at least one group mean is different from the other groups ANOVA Assumptions \u00b6 Residuals (experimental error) are normally distributed (test with Shapiro-Wilk) Homogeneity of variances (variances are equal between groups) (test with Bartlett's) Observations are sampled independently from each other Note: ANOVA assumptions can be checked using test statistics (e.g. Shapiro-Wilk, Bartlett\u2019s, Levene\u2019s test) and the visual approaches such as residual plots (e.g. QQ-plots) and histograms. Steps for ANOVA \u00b6 Check sample sizes: equal observations must be in each group Calculate Sum of Square between groups and within groups (\\(SS_B, SS_E\\)) Calculate Mean Square between groups and within groups (\\(MS_B, MS_E\\)) Calculate F value (\\(MS_B/MS_E\\)) This might be easier to see in a table: Source of Variation degree of freedom (Df) Sum of squares (SS) Mean square (MS) F value Between Groups Df_b = P-1 SS_B MS_B = SS_B / Df_B MS_B / MS_E Within Groups Df_E = P(N-1) SS_E MS_E = SS_E / Df_E total Df_T = PN-1 SS_T Where: SS_B = \\sum_{i}^{P}{(\\bar{y}_i-\\bar{y})^2} SS_E = \\sum_{ik}^{PN}{(\\bar{y}_{ik}-\\bar{y}_i)^2} SS_T = SS_B + SS_E Let's go back to our shift data to take an example: shifts = pd . DataFrame ([ shift_one , shift_two , shift_three , shift_four ]) . T shifts . columns = [ 'A' , 'B' , 'C' , 'D' ] shifts . boxplot () <AxesSubplot:> 2.1.4.2 SNS Boxplot \u00b6 this is another great way to view boxplot data. Notice how sns also shows us the raw data alongside the box and whiskers using a swarmplot . shift_melt = pd . melt ( shifts . reset_index (), id_vars = [ 'index' ], value_vars = [ 'A' , 'B' , 'C' , 'D' ]) shift_melt . columns = [ 'index' , 'shift' , 'rate' ] ax = sns . boxplot ( x = 'shift' , y = 'rate' , data = shift_melt , color = '#A0cbe8' ) ax = sns . swarmplot ( x = \"shift\" , y = \"rate\" , data = shift_melt , color = '#79706e' ) Anyway back to ANOVA... fvalue , pvalue = stats . f_oneway ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( fvalue , pvalue ) 5.599173553719008 0.0029473487978665873 We can get this in the format of the table we saw above: # get ANOVA table import statsmodels.api as sm from statsmodels.formula.api import ols # Ordinary Least Squares (OLS) model model = ols ( 'rate ~ C(shift)' , data = shift_melt ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(shift) 135.5 3.0 5.599174 0.002947 Residual 290.4 36.0 NaN NaN The Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9750654697418213 0.5121709108352661 We can use Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. w , pvalue = stats . bartlett ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( w , pvalue ) 1.3763632854696672 0.711084540821183 2.1.4.3 ANOVA Interpretation \u00b6 The p value form ANOVA analysis is significant ( p < 0.05) and we can conclude there are significant difference between the shifts. But we do not know which shift(s) are different. For this we need to perform a post hoc test. There are a multitude of these that are beyond the scope of this discussion ( Tukey-kramer is one such test) 2.1.5 Putting it all together \u00b6 In summary, there are many statistical tests at our disposal when performing inferential statistical analysis. In times like these, a simple decision tree can be extraordinarily useful! source: scribbr \ud83c\udf52 2.2 Enrichment: Evaluate statistical significance of product margin: a snake in the garden \u00b6 2.2.1 Mood's Median on product descriptors \u00b6 The first issue we run into with moods is... what? Since Mood's is nonparametric, we can easily become overconfident in our results. Let's take an example, continuing with the Truffle Type column. Recall that there are 3 unique Truffle Types: df [ 'Truffle Type' ] . unique () array(['Candy Outer', 'Chocolate Outer', 'Jelly Filled'], dtype=object) We can loop through each group and compute the: Moods test (comparison of medians) Welch's T-test (unequal variances, comparison of means) Shapiro-Wilk test for normality col = 'Truffle Type' moodsdf = pd . DataFrame () confidence_level = 0.01 welch_rej = mood_rej = shapiro_rej = False for truff in df [ col ] . unique (): # for each group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) if p < confidence_level : mood_rej = True median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) print ( \" {} : N= {} \" . format ( truff , size )) print ( \"Moods Median Test\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( stat , p ), end = \" \\n \" ) print ( f \" \\t reject: { mood_rej } \" ) print ( \"Welch's T-Test\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( * scipy . stats . ttest_ind ( group , pop , equal_var = False ))) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue if welchp < confidence_level : welch_rej = True print ( f \" \\t reject: { welch_rej } \" ) print ( \"Shapiro-Wilk\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( * stats . shapiro ( group ))) if stats . shapiro ( group ) . pvalue < confidence_level : shapiro_rej = True print ( f \" \\t reject: { shapiro_rej } \" ) print ( end = \" \\n\\n \" ) moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ col , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] Candy Outer: N=288 Moods Median Test statistic=1.52, pvalue=2.18e-01 reject: False Welch's T-Test statistic=-2.76, pvalue=5.91e-03 reject: True Shapiro-Wilk statistic=0.96, pvalue=2.74e-07 reject: True Chocolate Outer: N=1356 Moods Median Test statistic=6.63, pvalue=1.00e-02 reject: False Welch's T-Test statistic=4.41, pvalue=1.19e-05 reject: True Shapiro-Wilk statistic=0.96, pvalue=6.30e-19 reject: True Jelly Filled: N=24 Moods Median Test statistic=18.64, pvalue=1.58e-05 reject: True Welch's T-Test statistic=-8.41, pvalue=7.93e-09 reject: True Shapiro-Wilk statistic=0.87, pvalue=6.56e-03 reject: True \ud83d\ude4b\u200d\u2640\ufe0f Question 1: Moods Results on Truffle Type \u00b6 What can we say about these results? Recall that: Shapiro-Wilk Null Hypothesis: the distribution is normal Welch's T-test: requires that the distributions be normal Moods test: does not require normality in distributions main conclusions: all the groups are non-normal and therefore welch's test is invalid. We saw that the Welch's test had much lower p-values than the Moods median test. This is good news! It means that our Moods test, while allowing for non-normality, is much more conservative in its test-statistic, and therefore was unable to reject the null hypothesis in the cases of the chocolate outer and candy outer groups We can go ahead and repeat this analysis for all of our product categories: df . columns [: 5 ] Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group'], dtype='object') moodsdf = pd . DataFrame () for col in df . columns [: 5 ]: for truff in df [ col ] . unique (): group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] print ( moodsdf . shape ) (101, 10) moodsdf = moodsdf . loc [( moodsdf [ 'p_value' ] < confidence_level )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( moodsdf . shape ) (57, 10) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Secondary Flavor Wild Cherry Cream 6.798913 0.009121 0.216049 -0.122491 -0.15791 12 0.00001 [[1, 833], [11, 823]] 1 Primary Flavor Lemon Bar 6.798913 0.009121 0.216049 -0.122491 -0.15791 12 0.00001 [[1, 833], [11, 823]] 2 Primary Flavor Orange Pineapple\\tP 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 3 Secondary Flavor Papaya 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 4 Primary Flavor Cherry Cream Spice 10.156401 0.001438 0.216049 0.018702 0.009701 12 0.000001 [[0, 834], [12, 822]] 5 Secondary Flavor Cucumber 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 6 Truffle Type Jelly Filled 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 7 Primary Flavor Orange 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 8 Primary Flavor Toasted Coconut 15.261253 0.000094 0.216049 0.037002 0.028392 24 0.0 [[2, 832], [22, 812]] 9 Primary Flavor Wild Cherry Cream 6.798913 0.009121 0.216049 0.047647 0.028695 12 0.00038 [[1, 833], [11, 823]] 10 Secondary Flavor Apricot 15.261253 0.000094 0.216049 0.060312 0.037422 24 0.0 [[2, 832], [22, 812]] 11 Primary Flavor Kettle Corn 29.062065 0.0 0.216049 0.055452 0.045891 60 0.0 [[9, 825], [51, 783]] 12 Primary Flavor Acai Berry 18.643248 0.000016 0.216049 0.036505 0.049466 24 0.0 [[1, 833], [23, 811]] 13 Primary Flavor Pink Lemonade 10.156401 0.001438 0.216049 0.039862 0.056349 12 0.000011 [[0, 834], [12, 822]] 14 Secondary Flavor Black Cherry 58.900366 0.0 0.216049 0.055975 0.062898 96 0.0 [[11, 823], [85, 749]] 15 Primary Flavor Watermelon 15.261253 0.000094 0.216049 0.04405 0.067896 24 0.0 [[2, 832], [22, 812]] 16 Primary Flavor Sassafras 6.798913 0.009121 0.216049 0.072978 0.074112 12 0.000059 [[1, 833], [11, 823]] 17 Primary Flavor Plum 34.851608 0.0 0.216049 0.084963 0.079993 72 0.0 [[11, 823], [61, 773]] 18 Secondary Flavor Dill Pickle 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 19 Primary Flavor Horchata 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 20 Primary Flavor Lemon Custard 12.217457 0.000473 0.216049 0.079389 0.087969 24 0.000006 [[3, 831], [21, 813]] 21 Primary Flavor Fruit Punch 10.156401 0.001438 0.216049 0.078935 0.090326 12 0.000076 [[0, 834], [12, 822]] 22 Base Cake Chiffon 117.046226 0.0 0.216049 0.127851 0.125775 288 0.0 [[60, 774], [228, 606]] 23 Base Cake Butter 134.36727 0.0 0.216049 0.142082 0.139756 456 0.0 [[122, 712], [334, 500]] 24 Secondary Flavor Banana 10.805348 0.001012 0.216049 0.163442 0.15537 60 0.0 [[17, 817], [43, 791]] 25 Primary Flavor Cream Soda 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 26 Secondary Flavor Peppermint 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 27 Primary Flavor Grand Mariner 10.581767 0.001142 0.216049 0.197463 0.165529 72 0.000829 [[22, 812], [50, 784]] 28 Color Group Amethyst 20.488275 0.000006 0.216049 0.195681 0.167321 300 0.0 [[114, 720], [186, 648]] 29 Color Group Burgundy 10.999677 0.000911 0.216049 0.193048 0.171465 120 0.000406 [[42, 792], [78, 756]] 30 Color Group White 35.76526 0.0 0.216049 0.19 0.177264 432 0.0 [[162, 672], [270, 564]] 31 Primary Flavor Ginger Lime 7.835047 0.005124 0.216049 0.21435 0.183543 84 0.001323 [[29, 805], [55, 779]] 32 Primary Flavor Mango 11.262488 0.000791 0.216049 0.28803 0.245049 132 0.009688 [[85, 749], [47, 787]] 33 Color Group Opal 11.587164 0.000664 0.216049 0.317878 0.259304 324 0.0 [[190, 644], [134, 700]] 34 Secondary Flavor Apple 27.283292 0.0 0.216049 0.326167 0.293876 36 0.001176 [[34, 800], [2, 832]] 35 Secondary Flavor Tangerine 32.626389 0.0 0.216049 0.342314 0.319273 48 0.000113 [[44, 790], [4, 830]] 36 Secondary Flavor Black Currant 34.778391 0.0 0.216049 0.357916 0.332449 36 0.0 [[36, 798], [0, 834]] 37 Secondary Flavor Pear 16.614303 0.000046 0.216049 0.373034 0.33831 60 0.000031 [[46, 788], [14, 820]] 38 Primary Flavor Vanilla 34.778391 0.0 0.216049 0.378053 0.341626 36 0.000001 [[36, 798], [0, 834]] 39 Color Group Citrine 10.156401 0.001438 0.216049 0.390728 0.342512 12 0.001925 [[12, 822], [0, 834]] 40 Color Group Teal 13.539679 0.000234 0.216049 0.323955 0.3446 96 0.00121 [[66, 768], [30, 804]] 41 Base Cake Tiramisu 52.360619 0.0 0.216049 0.388267 0.362102 144 0.0 [[114, 720], [30, 804]] 42 Primary Flavor Doughnut 74.935256 0.0 0.216049 0.439721 0.379361 108 0.0 [[98, 736], [10, 824]] 43 Secondary Flavor Ginger Beer 22.363443 0.000002 0.216049 0.444895 0.382283 24 0.000481 [[24, 810], [0, 834]] 44 Color Group Rose 18.643248 0.000016 0.216049 0.42301 0.407061 24 0.000062 [[23, 811], [1, 833]] 45 Base Cake Cheese 66.804744 0.0 0.216049 0.450934 0.435638 84 0.0 [[79, 755], [5, 829]] 46 Primary Flavor Butter Toffee 60.181468 0.0 0.216049 0.50366 0.456343 60 0.0 [[60, 774], [0, 834]] 47 Color Group Slate 10.156401 0.001438 0.216049 0.540214 0.483138 12 0.000017 [[12, 822], [0, 834]] 48 Primary Flavor Gingersnap 22.363443 0.000002 0.216049 0.643218 0.623627 24 0.0 [[24, 810], [0, 834]] 49 Primary Flavor Dill Pickle 22.363443 0.000002 0.216049 0.642239 0.655779 24 0.0 [[24, 810], [0, 834]] 50 Color Group Olive 44.967537 0.0 0.216049 0.637627 0.670186 60 0.0 [[56, 778], [4, 830]] 51 Primary Flavor Butter Milk 10.156401 0.001438 0.216049 0.699284 0.688601 12 0.0 [[12, 822], [0, 834]] 52 Base Cake Sponge 127.156266 0.0 0.216049 0.698996 0.699355 120 0.0 [[120, 714], [0, 834]] 53 Primary Flavor Chocolate Mint 10.156401 0.001438 0.216049 0.685546 0.699666 12 0.0 [[12, 822], [0, 834]] 54 Primary Flavor Coconut 10.156401 0.001438 0.216049 0.732777 0.717641 12 0.0 [[12, 822], [0, 834]] 55 Primary Flavor Blueberry 22.363443 0.000002 0.216049 0.759643 0.72536 24 0.0 [[24, 810], [0, 834]] 56 Primary Flavor Amaretto 10.156401 0.001438 0.216049 0.782156 0.764845 12 0.0 [[12, 822], [0, 834]] \ud83c\udf52\ud83c\udf52 2.2.2 Enrichment : Broad Analysis of Categories: ANOVA \u00b6 Recall our \"melted\" shift data. It will be useful to think of getting our Truffle data in this format: shift_melt . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index shift rate 0 0 A 15 1 1 A 15 2 2 A 15 3 3 A 16 4 4 A 17 df . columns = df . columns . str . replace ( ' ' , '_' ) df . columns = df . columns . str . replace ( '/' , '_' ) # get ANOVA table # Ordinary Least Squares (OLS) model model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Recall the Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9576056599617004 1.2598073820281984e-21 And the Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. gb = df . groupby ( 'Truffle_Type' )[ 'EBITDA_KG' ] gb <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fafac7cfd10> w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( w , pvalue ) 109.93252546442552 1.344173733366234e-24 Wow it looks like our data is not drawn from a normal distribution! Let's check this for other categories... We can wrap these in a for loop: for col in df . columns [: 5 ]: print ( col ) model = ols ( 'EBITDA_KG ~ C( {} )' . format ( col ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) w , pvalue = stats . shapiro ( model . resid ) print ( \"Shapiro: \" , w , pvalue ) gb = df . groupby ( col )[ 'EBITDA_KG' ] w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( \"Bartlett: \" , w , pvalue ) print () Base_Cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Base_Cake) 39.918103 5.0 314.869955 1.889884e-237 Residual 42.140500 1662.0 NaN NaN Shapiro: 0.9634131193161011 4.1681337029688696e-20 Bartlett: 69.83288886114195 1.1102218566053728e-13 Truffle_Type .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Shapiro: 0.9576056599617004 1.2598073820281984e-21 Bartlett: 109.93252546442552 1.344173733366234e-24 Primary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Primary_Flavor) 50.270639 50.0 51.143649 1.153434e-292 Residual 31.787964 1617.0 NaN NaN Shapiro: 0.948470413684845 9.90281706784179e-24 Bartlett: 210.15130419114894 1.5872504991231547e-21 Secondary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Secondary_Flavor) 15.088382 28.0 13.188089 1.929302e-54 Residual 66.970220 1639.0 NaN NaN Shapiro: 0.9548103213310242 2.649492974953278e-22 Bartlett: 420.6274502894803 1.23730070350945e-71 Color_Group .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Color_Group) 16.079685 11.0 36.689347 6.544980e-71 Residual 65.978918 1656.0 NaN NaN Shapiro: 0.969061017036438 1.8926407335144587e-18 Bartlett: 136.55525281340468 8.164787784033709e-24 \ud83c\udf52\ud83c\udf52 2.2.3 Enrichment : Visual Analysis of Residuals: QQ-Plots \u00b6 This can be distressing and is often why we want visual methods to see what is going on with our data! model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () #create instance of influence influence = model . get_influence () #obtain standardized residuals standardized_residuals = influence . resid_studentized_internal # res.anova_std_residuals are standardized residuals obtained from ANOVA (check above) sm . qqplot ( standardized_residuals , line = '45' ) plt . xlabel ( \"Theoretical Quantiles\" ) plt . ylabel ( \"Standardized Residuals\" ) plt . show () # histogram plt . hist ( model . resid , bins = 'auto' , histtype = 'bar' , ec = 'k' ) plt . xlabel ( \"Residuals\" ) plt . ylabel ( 'Frequency' ) plt . show () We see that a lot of our data is swayed by extremely high and low values, so what can we conclude? You need the right test statistic for the right job, in this case, we are littered with unequal variance in our groupings so we use the moods median and welch (unequal variance t-test) to make conclusions about our data References \u00b6 Renesh Bedre ANOVA Minitab ANOVA Analytics Vidhya ANOVA Renesh Bedre Hypothesis Testing Real Statistics Turkey-kramer Mutual Information","title":"Inferential Statistics"},{"location":"S2_Inferential_Statistics/#data-science-foundations-session-2-inferential-statistics","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at the utility of EDA combined with inferential statistics.","title":"Data Science Foundations  Session 2: Inferential Statistics"},{"location":"S2_Inferential_Statistics/#20-preparing-environment-and-importing-data","text":"back to top","title":"2.0 Preparing Environment and Importing Data"},{"location":"S2_Inferential_Statistics/#201-import-packages","text":"back to top # The modules we've seen before import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns # our stats modules import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy","title":"2.0.1 Import Packages"},{"location":"S2_Inferential_Statistics/#202-load-dataset","text":"back to top For this session, we will use dummy datasets from sklearn. df = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... ... ... ... ... ... 1663 Tiramisu Chocolate Outer Doughnut Pear Amethyst Fickelgruber 12/2020 38128.802589 0.420111 1664 Tiramisu Chocolate Outer Doughnut Pear Burgundy Zebrabar 12/2020 108.642857 0.248659 1665 Tiramisu Chocolate Outer Doughnut Pear Teal Zebrabar 12/2020 3517.933333 0.378501 1666 Tiramisu Chocolate Outer Doughnut Rock and Rye Amethyst Slugworth 12/2020 10146.898432 0.213149 1667 Tiramisu Chocolate Outer Doughnut Rock and Rye Burgundy Zebrabar 12/2020 1271.904762 0.431813 1668 rows \u00d7 9 columns descriptors = df . columns [: - 2 ] for col in df . columns [: - 2 ]: print ( col ) print ( df [ col ] . unique ()) print () Base Cake ['Butter' 'Cheese' 'Chiffon' 'Pound' 'Sponge' 'Tiramisu'] Truffle Type ['Candy Outer' 'Chocolate Outer' 'Jelly Filled'] Primary Flavor ['Butter Pecan' 'Ginger Lime' 'Margarita' 'Pear' 'Pink Lemonade' 'Raspberry Ginger Ale' 'Sassafras' 'Spice' 'Wild Cherry Cream' 'Cream Soda' 'Horchata' 'Kettle Corn' 'Lemon Bar' 'Orange Pineapple\\tP' 'Plum' 'Orange' 'Butter Toffee' 'Lemon' 'Acai Berry' 'Apricot' 'Birch Beer' 'Cherry Cream Spice' 'Creme de Menthe' 'Fruit Punch' 'Ginger Ale' 'Grand Mariner' 'Orange Brandy' 'Pecan' 'Toasted Coconut' 'Watermelon' 'Wintergreen' 'Vanilla' 'Bavarian Cream' 'Black Licorice' 'Caramel Cream' 'Cheesecake' 'Cherry Cola' 'Coffee' 'Irish Cream' 'Lemon Custard' 'Mango' 'Sour' 'Amaretto' 'Blueberry' 'Butter Milk' 'Chocolate Mint' 'Coconut' 'Dill Pickle' 'Gingersnap' 'Chocolate' 'Doughnut'] Secondary Flavor ['Toffee' 'Banana' 'Rum' 'Tutti Frutti' 'Vanilla' 'Mixed Berry' 'Whipped Cream' 'Apricot' 'Passion Fruit' 'Peppermint' 'Dill Pickle' 'Black Cherry' 'Wild Cherry Cream' 'Papaya' 'Mango' 'Cucumber' 'Egg Nog' 'Pear' 'Rock and Rye' 'Tangerine' 'Apple' 'Black Currant' 'Kiwi' 'Lemon' 'Hazelnut' 'Butter Rum' 'Fuzzy Navel' 'Mojito' 'Ginger Beer'] Color Group ['Taupe' 'Amethyst' 'Burgundy' 'White' 'Black' 'Opal' 'Citrine' 'Rose' 'Slate' 'Teal' 'Tiffany' 'Olive'] Customer ['Slugworth' 'Perk-a-Cola' 'Fickelgruber' 'Zebrabar' \"Dandy's Candies\"] Date ['1/2020' '2/2020' '3/2020' '4/2020' '5/2020' '6/2020' '7/2020' '8/2020' '9/2020' '10/2020' '11/2020' '12/2020']","title":"2.0.2 Load Dataset"},{"location":"S2_Inferential_Statistics/#21-many-flavors-of-statistical-tests","text":"img src Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (\u201cinferences\u201d) from that data. With inferential statistics, you take data from samples and make generalizations about a population statshowto Non-parametric tests Moods Median (Comparison of Medians) Kruskal-Wallis (Comparison of Medians, compare to ANOVA ) Mann Whitney (Rank order test, compare to T-test ) Comparison of means T-test Independent Equal Variances (students T-test) Unequal Variances (Welch's T-test) Dependent Comparison of variances Analysis of Variance (ANOVA) One Way ANOVA Two Way ANOVA MANOVA Factorial ANOVA When do I use each of these? We will talk about this as we proceed through the examples. This page from minitab has good rules of thumb on the subject.","title":"2.1 Many Flavors of Statistical Tests"},{"location":"S2_Inferential_Statistics/#211-what-is-moods-median","text":"You can use Chi-Square to test for a goodness of fit (whether a sample of data represents a distribution) or whether two variables are related (using a contingency table, which we will create below!) A special case of Pearon's Chi-Squared Test: We create a table that counts the observations above and below the global median for two or more groups . We then perform a chi-squared test of significance on this contingency table Null hypothesis: the Medians are all equal The chi-square test statistic: x^2 = \\sum{\\frac{(O-E)^2}{E}} Where \\(O\\) is the observed frequency and \\(E\\) is the expected frequency. Let's take an example , say we have three shifts with the following production rates: np . random . seed ( 7 ) shift_one = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_two = [ round ( i ) for i in np . random . normal ( 21 , 3 , 10 )] print ( shift_one ) print ( shift_two ) [21, 15, 16, 17, 14, 16, 16, 11, 19, 18] [19, 20, 23, 20, 20, 17, 23, 21, 22, 16] stat , p , m , table = scipy . stats . median_test ( shift_one , shift_two , correction = False ) what is median_test returning? print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 7.20 p-value of the test: 0.007 the grand median: 18.5 Let's evaluate that test statistic ourselves by taking a look at the contingency table: table array([[2, 8], [8, 2]]) This is easier to make sense of if we order the shift times shift_one . sort () shift_one [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] When we look at shift one, we see that 8 values are at or below the grand median. shift_two . sort () shift_two [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] For shift two, only two are at or below the grand median. Since the sample sizes are the same, the expected value for both groups is the same, 5 above and 5 below the grand median. The chi-square is then: X^2 = \\frac{(2-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(2-5)^2}{5} ( 2 - 5 ) ** 2 / 5 + ( 8 - 5 ) ** 2 / 5 + ( 8 - 5 ) ** 2 / 5 + ( 2 - 5 ) ** 2 / 5 7.2 Our p-value, or the probability of observing the null-hypothsis, is under 0.05 (at 0.007). We can conclude that these shift performances were drawn under seperate distributions. For comparison, let's do this analysis again with shifts of equal performances np . random . seed ( 3 ) shift_three = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_four = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] stat , p , m , table = scipy . stats . median_test ( shift_three , shift_four , correction = False ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000 the grand median: 15.5 and the shift raw values: shift_three . sort () shift_four . sort () print ( shift_three ) print ( shift_four ) [10, 14, 15, 15, 15, 16, 16, 16, 17, 21] [11, 12, 13, 14, 15, 16, 19, 19, 19, 21] table array([[5, 5], [5, 5]])","title":"2.1.1 What is Mood's Median?"},{"location":"S2_Inferential_Statistics/#212-when-to-use-moods","text":"Mood's Median Test is highly flexible but has the following assumptions: Considers only one categorical factor Response variable is continuous (our shift rates) Data does not need to be normally distributed But the distributions are similarly shaped Sample sizes can be unequal and small (less than 20 observations) Other considerations: Not as powerful as Kruskal-Wallis Test but still useful for small sample sizes or when there are outliers","title":"2.1.2 When to Use Mood's?"},{"location":"S2_Inferential_Statistics/#exercise-1-use-moods-median-test","text":"","title":"\ud83c\udfcb\ufe0f Exercise 1: Use Mood's Median Test"},{"location":"S2_Inferential_Statistics/#part-a-perform-moods-median-test-on-base-cake-categorical-variable-and-ebitdakg-continuous-variable-in-truffle-data","text":"We're also going to get some practice with pandas groupby. # what is returned by this groupby? gp = df . groupby ( 'Base Cake' ) How do we find out? We could iterate through it: # seems to be a tuple of some sort for i in gp : print ( i ) break ('Butter', Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns]) # the first object appears to be the group print ( i [ 0 ]) # the second object appears to be the df belonging to that group print ( i [ 1 ]) Butter Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns] going back to our diagram from our earlier pandas session. It looks like whenever we split in the groupby method, we create separate dataframes as well as their group label: Ok, so we know gp is separate dataframes. How do we turn them into arrays to then pass to median_test ? # complete this for loop for i , j in gp : pass # turn 'EBITDA/KG' of j into an array using the .values attribute # print this to the screen After you've completed the previous step, turn this into a list comprehension and pass the result to a variable called margins # complete the code below # margins = [# YOUR LIST COMPREHENSION HERE] Remember the list unpacking we did for the tic tac toe project? We're going to do the same thing here. Unpack the margins list for median_test and run the cell below! # complete the following line # stat, p, m, table = scipy.stats.median_test(<UNPACK MARGINS HERE>, correction=False) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.2e} \" . format ( p )) print ( \"the grand median: {:.2e} \" . format ( m )) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.00e+00 the grand median: 1.55e+01","title":"Part A Perform moods median test on Base Cake (Categorical Variable) and EBITDA/KG (Continuous Variable) in Truffle data"},{"location":"S2_Inferential_Statistics/#part-b-view-the-distributions-of-the-data-using-matplotlib-and-seaborn","text":"What a fantastic statistical result we found! Can we affirm our result with some visualizations? I hope so! Create a boxplot below using pandas. In your call to df.boxplot() the by parameter should be set to Base Cake and the column parameter should be set to EBITDA/KG # YOUR BOXPLOT HERE For comparison, I've shown the boxplot below using seaborn! fig , ax = plt . subplots ( figsize = ( 10 , 7 )) ax = sns . boxplot ( x = 'Base Cake' , y = 'EBITDA/KG' , data = df , color = '#A0cbe8' )","title":"Part B View the distributions of the data using matplotlib and seaborn"},{"location":"S2_Inferential_Statistics/#part-c-perform-moods-median-on-all-the-other-groups","text":"# Recall the other descriptors we have descriptors Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') for desc in descriptors : # YOUR CODE FORM MARGINS BELOW # margins = [<YOUR LIST COMPREHENSION>] # UNPACK MARGINS INTO MEDIAN_TEST # stat, p, m, table = scipy.stats.median_test(<YOUR UNPACKING METHOD>, correction=False) print ( desc ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:e} \" . format ( p )) print ( \"the grand median: {} \" . format ( m ), end = ' \\n\\n ' ) Base Cake The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Truffle Type The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Primary Flavor The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Secondary Flavor The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Color Group The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Customer The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5 Date The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000000e+00 the grand median: 15.5","title":"Part C Perform Moods Median on all the other groups"},{"location":"S2_Inferential_Statistics/#part-d-many-boxplots","text":"And finally, we will confirm these visually. Complete the Boxplot for each group: for desc in descriptors : fig , ax = plt . subplots ( figsize = ( 10 , 5 )) # sns.boxplot(x=<YOUR X VARIABLE HERE>, y='EBITDA/KG', data=df, color='#A0cbe8', ax=ax)","title":"Part D Many boxplots"},{"location":"S2_Inferential_Statistics/#213-what-is-a-t-test","text":"There are 1-sample and 2-sample T-tests (note: we would use a 1-sample T-test just to determine if the sample mean is equal to a hypothesized population mean) Within 2-sample T-tests we have independent and dependent T-tests (uncorrelated or correlated samples) For independent, two-sample T-tests: Equal variance (or pooled) T-test scipy.stats.ttest_ind(equal_var=True) also called Student's T-test Unequal variance T-test scipy.stats.ttest_ind(equal_var=False) also called Welch's T-test For dependent T-tests: Paired (or correlated) T-test scipy.stats.ttest_rel ex : patient symptoms before and after treatment A full discussion on T-tests is outside the scope of this session, but we can refer to wikipedia for more information, including formulas on how each statistic is computed: student's T-test","title":"2.1.3 What is a T-test?"},{"location":"S2_Inferential_Statistics/#2131-demonstration-of-t-tests","text":"back to top We'll assume our shifts are of equal variance and proceed with the appropriate independent two-sample T-test... print ( shift_one ) print ( shift_two ) [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] To calculate the T-test, we follow a slightly different statistical formula: T=\\frac{\\mu_1 - \\mu_2}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} where \\(\\mu\\) are the means of the two groups, \\(n\\) are the sample sizes and \\(s\\) is the pooled standard deviation, also known as the cummulative variance (depending on if you square it or not): s= \\sqrt{\\frac{(n_1-1)\\sigma_1^2 + (n_2-1)\\sigma_2^2}{n_1 + n_2 - 2}} where \\(\\sigma\\) are the standard deviations. What you'll notice here is we are combining the two variances, we can only do this if we assume the variances are somewhat equal, this is known as the equal variances t-test. mean_shift_one = np . mean ( shift_one ) mean_shift_two = np . mean ( shift_two ) print ( mean_shift_one , mean_shift_two ) 16.3 20.1 com_var = (( np . sum ([( i - mean_shift_one ) ** 2 for i in shift_one ]) + np . sum ([( i - mean_shift_two ) ** 2 for i in shift_two ])) / ( len ( shift_one ) + len ( shift_two ) - 2 )) print ( com_var ) 6.5 T = ( np . abs ( mean_shift_one - mean_shift_two ) / ( np . sqrt ( com_var / len ( shift_one ) + com_var / len ( shift_two )))) T 3.3328204733667115 We see that this hand-computed result matches that of the scipy module: scipy . stats . ttest_ind ( shift_two , shift_one , equal_var = True ) Ttest_indResult(statistic=3.3328204733667115, pvalue=0.0037029158660758575)","title":"2.1.3.1 Demonstration of T-tests"},{"location":"S2_Inferential_Statistics/#214-what-are-f-statistics-and-the-f-test","text":"The F-statistic is simply a ratio of two variances, or the ratio of mean squares mean squares is the estimate of population variance that accounts for the degrees of freedom to compute that estimate. We will explore this in the context of ANOVA","title":"2.1.4 What are F-statistics and the F-test?"},{"location":"S2_Inferential_Statistics/#2141-what-is-analysis-of-variance","text":"ANOVA uses the F-test to determine whether the variability between group means is larger than the variability within the groups. If that statistic is large enough, you can conclude that the means of the groups are not equal. The caveat is that ANOVA tells us whether there is a difference in means but it does not tell us where the difference is. To find where the difference is between the groups, we have to conduct post-hoc tests. There are two main types: One-way (one factor) and Two-way (two factor) where factor is an independent variable Ind A Ind B Dep X H 10 X I 12 Y I 11 Y H 20","title":"2.1.4.1 What is Analysis of Variance?"},{"location":"S2_Inferential_Statistics/#anova-hypotheses","text":"Null hypothesis : group means are equal Alternative hypothesis : at least one group mean is different from the other groups","title":"ANOVA Hypotheses"},{"location":"S2_Inferential_Statistics/#anova-assumptions","text":"Residuals (experimental error) are normally distributed (test with Shapiro-Wilk) Homogeneity of variances (variances are equal between groups) (test with Bartlett's) Observations are sampled independently from each other Note: ANOVA assumptions can be checked using test statistics (e.g. Shapiro-Wilk, Bartlett\u2019s, Levene\u2019s test) and the visual approaches such as residual plots (e.g. QQ-plots) and histograms.","title":"ANOVA Assumptions"},{"location":"S2_Inferential_Statistics/#steps-for-anova","text":"Check sample sizes: equal observations must be in each group Calculate Sum of Square between groups and within groups (\\(SS_B, SS_E\\)) Calculate Mean Square between groups and within groups (\\(MS_B, MS_E\\)) Calculate F value (\\(MS_B/MS_E\\)) This might be easier to see in a table: Source of Variation degree of freedom (Df) Sum of squares (SS) Mean square (MS) F value Between Groups Df_b = P-1 SS_B MS_B = SS_B / Df_B MS_B / MS_E Within Groups Df_E = P(N-1) SS_E MS_E = SS_E / Df_E total Df_T = PN-1 SS_T Where: SS_B = \\sum_{i}^{P}{(\\bar{y}_i-\\bar{y})^2} SS_E = \\sum_{ik}^{PN}{(\\bar{y}_{ik}-\\bar{y}_i)^2} SS_T = SS_B + SS_E Let's go back to our shift data to take an example: shifts = pd . DataFrame ([ shift_one , shift_two , shift_three , shift_four ]) . T shifts . columns = [ 'A' , 'B' , 'C' , 'D' ] shifts . boxplot () <AxesSubplot:>","title":"Steps for ANOVA"},{"location":"S2_Inferential_Statistics/#2142-sns-boxplot","text":"this is another great way to view boxplot data. Notice how sns also shows us the raw data alongside the box and whiskers using a swarmplot . shift_melt = pd . melt ( shifts . reset_index (), id_vars = [ 'index' ], value_vars = [ 'A' , 'B' , 'C' , 'D' ]) shift_melt . columns = [ 'index' , 'shift' , 'rate' ] ax = sns . boxplot ( x = 'shift' , y = 'rate' , data = shift_melt , color = '#A0cbe8' ) ax = sns . swarmplot ( x = \"shift\" , y = \"rate\" , data = shift_melt , color = '#79706e' ) Anyway back to ANOVA... fvalue , pvalue = stats . f_oneway ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( fvalue , pvalue ) 5.599173553719008 0.0029473487978665873 We can get this in the format of the table we saw above: # get ANOVA table import statsmodels.api as sm from statsmodels.formula.api import ols # Ordinary Least Squares (OLS) model model = ols ( 'rate ~ C(shift)' , data = shift_melt ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(shift) 135.5 3.0 5.599174 0.002947 Residual 290.4 36.0 NaN NaN The Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9750654697418213 0.5121709108352661 We can use Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. w , pvalue = stats . bartlett ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( w , pvalue ) 1.3763632854696672 0.711084540821183","title":"2.1.4.2 SNS Boxplot"},{"location":"S2_Inferential_Statistics/#2143-anova-interpretation","text":"The p value form ANOVA analysis is significant ( p < 0.05) and we can conclude there are significant difference between the shifts. But we do not know which shift(s) are different. For this we need to perform a post hoc test. There are a multitude of these that are beyond the scope of this discussion ( Tukey-kramer is one such test)","title":"2.1.4.3 ANOVA Interpretation"},{"location":"S2_Inferential_Statistics/#215-putting-it-all-together","text":"In summary, there are many statistical tests at our disposal when performing inferential statistical analysis. In times like these, a simple decision tree can be extraordinarily useful! source: scribbr","title":"2.1.5 Putting it all together"},{"location":"S2_Inferential_Statistics/#22-enrichment-evaluate-statistical-significance-of-product-margin-a-snake-in-the-garden","text":"","title":"\ud83c\udf52 2.2 Enrichment: Evaluate statistical significance of product margin: a snake in the garden"},{"location":"S2_Inferential_Statistics/#221-moods-median-on-product-descriptors","text":"The first issue we run into with moods is... what? Since Mood's is nonparametric, we can easily become overconfident in our results. Let's take an example, continuing with the Truffle Type column. Recall that there are 3 unique Truffle Types: df [ 'Truffle Type' ] . unique () array(['Candy Outer', 'Chocolate Outer', 'Jelly Filled'], dtype=object) We can loop through each group and compute the: Moods test (comparison of medians) Welch's T-test (unequal variances, comparison of means) Shapiro-Wilk test for normality col = 'Truffle Type' moodsdf = pd . DataFrame () confidence_level = 0.01 welch_rej = mood_rej = shapiro_rej = False for truff in df [ col ] . unique (): # for each group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) if p < confidence_level : mood_rej = True median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) print ( \" {} : N= {} \" . format ( truff , size )) print ( \"Moods Median Test\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( stat , p ), end = \" \\n \" ) print ( f \" \\t reject: { mood_rej } \" ) print ( \"Welch's T-Test\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( * scipy . stats . ttest_ind ( group , pop , equal_var = False ))) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue if welchp < confidence_level : welch_rej = True print ( f \" \\t reject: { welch_rej } \" ) print ( \"Shapiro-Wilk\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( * stats . shapiro ( group ))) if stats . shapiro ( group ) . pvalue < confidence_level : shapiro_rej = True print ( f \" \\t reject: { shapiro_rej } \" ) print ( end = \" \\n\\n \" ) moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ col , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] Candy Outer: N=288 Moods Median Test statistic=1.52, pvalue=2.18e-01 reject: False Welch's T-Test statistic=-2.76, pvalue=5.91e-03 reject: True Shapiro-Wilk statistic=0.96, pvalue=2.74e-07 reject: True Chocolate Outer: N=1356 Moods Median Test statistic=6.63, pvalue=1.00e-02 reject: False Welch's T-Test statistic=4.41, pvalue=1.19e-05 reject: True Shapiro-Wilk statistic=0.96, pvalue=6.30e-19 reject: True Jelly Filled: N=24 Moods Median Test statistic=18.64, pvalue=1.58e-05 reject: True Welch's T-Test statistic=-8.41, pvalue=7.93e-09 reject: True Shapiro-Wilk statistic=0.87, pvalue=6.56e-03 reject: True","title":"2.2.1 Mood's Median on product descriptors"},{"location":"S2_Inferential_Statistics/#question-1-moods-results-on-truffle-type","text":"What can we say about these results? Recall that: Shapiro-Wilk Null Hypothesis: the distribution is normal Welch's T-test: requires that the distributions be normal Moods test: does not require normality in distributions main conclusions: all the groups are non-normal and therefore welch's test is invalid. We saw that the Welch's test had much lower p-values than the Moods median test. This is good news! It means that our Moods test, while allowing for non-normality, is much more conservative in its test-statistic, and therefore was unable to reject the null hypothesis in the cases of the chocolate outer and candy outer groups We can go ahead and repeat this analysis for all of our product categories: df . columns [: 5 ] Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group'], dtype='object') moodsdf = pd . DataFrame () for col in df . columns [: 5 ]: for truff in df [ col ] . unique (): group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] print ( moodsdf . shape ) (101, 10) moodsdf = moodsdf . loc [( moodsdf [ 'p_value' ] < confidence_level )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( moodsdf . shape ) (57, 10) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Secondary Flavor Wild Cherry Cream 6.798913 0.009121 0.216049 -0.122491 -0.15791 12 0.00001 [[1, 833], [11, 823]] 1 Primary Flavor Lemon Bar 6.798913 0.009121 0.216049 -0.122491 -0.15791 12 0.00001 [[1, 833], [11, 823]] 2 Primary Flavor Orange Pineapple\\tP 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 3 Secondary Flavor Papaya 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 4 Primary Flavor Cherry Cream Spice 10.156401 0.001438 0.216049 0.018702 0.009701 12 0.000001 [[0, 834], [12, 822]] 5 Secondary Flavor Cucumber 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 6 Truffle Type Jelly Filled 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 7 Primary Flavor Orange 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 8 Primary Flavor Toasted Coconut 15.261253 0.000094 0.216049 0.037002 0.028392 24 0.0 [[2, 832], [22, 812]] 9 Primary Flavor Wild Cherry Cream 6.798913 0.009121 0.216049 0.047647 0.028695 12 0.00038 [[1, 833], [11, 823]] 10 Secondary Flavor Apricot 15.261253 0.000094 0.216049 0.060312 0.037422 24 0.0 [[2, 832], [22, 812]] 11 Primary Flavor Kettle Corn 29.062065 0.0 0.216049 0.055452 0.045891 60 0.0 [[9, 825], [51, 783]] 12 Primary Flavor Acai Berry 18.643248 0.000016 0.216049 0.036505 0.049466 24 0.0 [[1, 833], [23, 811]] 13 Primary Flavor Pink Lemonade 10.156401 0.001438 0.216049 0.039862 0.056349 12 0.000011 [[0, 834], [12, 822]] 14 Secondary Flavor Black Cherry 58.900366 0.0 0.216049 0.055975 0.062898 96 0.0 [[11, 823], [85, 749]] 15 Primary Flavor Watermelon 15.261253 0.000094 0.216049 0.04405 0.067896 24 0.0 [[2, 832], [22, 812]] 16 Primary Flavor Sassafras 6.798913 0.009121 0.216049 0.072978 0.074112 12 0.000059 [[1, 833], [11, 823]] 17 Primary Flavor Plum 34.851608 0.0 0.216049 0.084963 0.079993 72 0.0 [[11, 823], [61, 773]] 18 Secondary Flavor Dill Pickle 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 19 Primary Flavor Horchata 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 20 Primary Flavor Lemon Custard 12.217457 0.000473 0.216049 0.079389 0.087969 24 0.000006 [[3, 831], [21, 813]] 21 Primary Flavor Fruit Punch 10.156401 0.001438 0.216049 0.078935 0.090326 12 0.000076 [[0, 834], [12, 822]] 22 Base Cake Chiffon 117.046226 0.0 0.216049 0.127851 0.125775 288 0.0 [[60, 774], [228, 606]] 23 Base Cake Butter 134.36727 0.0 0.216049 0.142082 0.139756 456 0.0 [[122, 712], [334, 500]] 24 Secondary Flavor Banana 10.805348 0.001012 0.216049 0.163442 0.15537 60 0.0 [[17, 817], [43, 791]] 25 Primary Flavor Cream Soda 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 26 Secondary Flavor Peppermint 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 27 Primary Flavor Grand Mariner 10.581767 0.001142 0.216049 0.197463 0.165529 72 0.000829 [[22, 812], [50, 784]] 28 Color Group Amethyst 20.488275 0.000006 0.216049 0.195681 0.167321 300 0.0 [[114, 720], [186, 648]] 29 Color Group Burgundy 10.999677 0.000911 0.216049 0.193048 0.171465 120 0.000406 [[42, 792], [78, 756]] 30 Color Group White 35.76526 0.0 0.216049 0.19 0.177264 432 0.0 [[162, 672], [270, 564]] 31 Primary Flavor Ginger Lime 7.835047 0.005124 0.216049 0.21435 0.183543 84 0.001323 [[29, 805], [55, 779]] 32 Primary Flavor Mango 11.262488 0.000791 0.216049 0.28803 0.245049 132 0.009688 [[85, 749], [47, 787]] 33 Color Group Opal 11.587164 0.000664 0.216049 0.317878 0.259304 324 0.0 [[190, 644], [134, 700]] 34 Secondary Flavor Apple 27.283292 0.0 0.216049 0.326167 0.293876 36 0.001176 [[34, 800], [2, 832]] 35 Secondary Flavor Tangerine 32.626389 0.0 0.216049 0.342314 0.319273 48 0.000113 [[44, 790], [4, 830]] 36 Secondary Flavor Black Currant 34.778391 0.0 0.216049 0.357916 0.332449 36 0.0 [[36, 798], [0, 834]] 37 Secondary Flavor Pear 16.614303 0.000046 0.216049 0.373034 0.33831 60 0.000031 [[46, 788], [14, 820]] 38 Primary Flavor Vanilla 34.778391 0.0 0.216049 0.378053 0.341626 36 0.000001 [[36, 798], [0, 834]] 39 Color Group Citrine 10.156401 0.001438 0.216049 0.390728 0.342512 12 0.001925 [[12, 822], [0, 834]] 40 Color Group Teal 13.539679 0.000234 0.216049 0.323955 0.3446 96 0.00121 [[66, 768], [30, 804]] 41 Base Cake Tiramisu 52.360619 0.0 0.216049 0.388267 0.362102 144 0.0 [[114, 720], [30, 804]] 42 Primary Flavor Doughnut 74.935256 0.0 0.216049 0.439721 0.379361 108 0.0 [[98, 736], [10, 824]] 43 Secondary Flavor Ginger Beer 22.363443 0.000002 0.216049 0.444895 0.382283 24 0.000481 [[24, 810], [0, 834]] 44 Color Group Rose 18.643248 0.000016 0.216049 0.42301 0.407061 24 0.000062 [[23, 811], [1, 833]] 45 Base Cake Cheese 66.804744 0.0 0.216049 0.450934 0.435638 84 0.0 [[79, 755], [5, 829]] 46 Primary Flavor Butter Toffee 60.181468 0.0 0.216049 0.50366 0.456343 60 0.0 [[60, 774], [0, 834]] 47 Color Group Slate 10.156401 0.001438 0.216049 0.540214 0.483138 12 0.000017 [[12, 822], [0, 834]] 48 Primary Flavor Gingersnap 22.363443 0.000002 0.216049 0.643218 0.623627 24 0.0 [[24, 810], [0, 834]] 49 Primary Flavor Dill Pickle 22.363443 0.000002 0.216049 0.642239 0.655779 24 0.0 [[24, 810], [0, 834]] 50 Color Group Olive 44.967537 0.0 0.216049 0.637627 0.670186 60 0.0 [[56, 778], [4, 830]] 51 Primary Flavor Butter Milk 10.156401 0.001438 0.216049 0.699284 0.688601 12 0.0 [[12, 822], [0, 834]] 52 Base Cake Sponge 127.156266 0.0 0.216049 0.698996 0.699355 120 0.0 [[120, 714], [0, 834]] 53 Primary Flavor Chocolate Mint 10.156401 0.001438 0.216049 0.685546 0.699666 12 0.0 [[12, 822], [0, 834]] 54 Primary Flavor Coconut 10.156401 0.001438 0.216049 0.732777 0.717641 12 0.0 [[12, 822], [0, 834]] 55 Primary Flavor Blueberry 22.363443 0.000002 0.216049 0.759643 0.72536 24 0.0 [[24, 810], [0, 834]] 56 Primary Flavor Amaretto 10.156401 0.001438 0.216049 0.782156 0.764845 12 0.0 [[12, 822], [0, 834]]","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: Moods Results on Truffle Type"},{"location":"S2_Inferential_Statistics/#222-enrichment-broad-analysis-of-categories-anova","text":"Recall our \"melted\" shift data. It will be useful to think of getting our Truffle data in this format: shift_melt . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index shift rate 0 0 A 15 1 1 A 15 2 2 A 15 3 3 A 16 4 4 A 17 df . columns = df . columns . str . replace ( ' ' , '_' ) df . columns = df . columns . str . replace ( '/' , '_' ) # get ANOVA table # Ordinary Least Squares (OLS) model model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Recall the Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9576056599617004 1.2598073820281984e-21 And the Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. gb = df . groupby ( 'Truffle_Type' )[ 'EBITDA_KG' ] gb <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fafac7cfd10> w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( w , pvalue ) 109.93252546442552 1.344173733366234e-24 Wow it looks like our data is not drawn from a normal distribution! Let's check this for other categories... We can wrap these in a for loop: for col in df . columns [: 5 ]: print ( col ) model = ols ( 'EBITDA_KG ~ C( {} )' . format ( col ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) w , pvalue = stats . shapiro ( model . resid ) print ( \"Shapiro: \" , w , pvalue ) gb = df . groupby ( col )[ 'EBITDA_KG' ] w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( \"Bartlett: \" , w , pvalue ) print () Base_Cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Base_Cake) 39.918103 5.0 314.869955 1.889884e-237 Residual 42.140500 1662.0 NaN NaN Shapiro: 0.9634131193161011 4.1681337029688696e-20 Bartlett: 69.83288886114195 1.1102218566053728e-13 Truffle_Type .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Shapiro: 0.9576056599617004 1.2598073820281984e-21 Bartlett: 109.93252546442552 1.344173733366234e-24 Primary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Primary_Flavor) 50.270639 50.0 51.143649 1.153434e-292 Residual 31.787964 1617.0 NaN NaN Shapiro: 0.948470413684845 9.90281706784179e-24 Bartlett: 210.15130419114894 1.5872504991231547e-21 Secondary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Secondary_Flavor) 15.088382 28.0 13.188089 1.929302e-54 Residual 66.970220 1639.0 NaN NaN Shapiro: 0.9548103213310242 2.649492974953278e-22 Bartlett: 420.6274502894803 1.23730070350945e-71 Color_Group .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Color_Group) 16.079685 11.0 36.689347 6.544980e-71 Residual 65.978918 1656.0 NaN NaN Shapiro: 0.969061017036438 1.8926407335144587e-18 Bartlett: 136.55525281340468 8.164787784033709e-24","title":"\ud83c\udf52\ud83c\udf52 2.2.2 Enrichment: Broad Analysis of Categories: ANOVA"},{"location":"S2_Inferential_Statistics/#223-enrichment-visual-analysis-of-residuals-qq-plots","text":"This can be distressing and is often why we want visual methods to see what is going on with our data! model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () #create instance of influence influence = model . get_influence () #obtain standardized residuals standardized_residuals = influence . resid_studentized_internal # res.anova_std_residuals are standardized residuals obtained from ANOVA (check above) sm . qqplot ( standardized_residuals , line = '45' ) plt . xlabel ( \"Theoretical Quantiles\" ) plt . ylabel ( \"Standardized Residuals\" ) plt . show () # histogram plt . hist ( model . resid , bins = 'auto' , histtype = 'bar' , ec = 'k' ) plt . xlabel ( \"Residuals\" ) plt . ylabel ( 'Frequency' ) plt . show () We see that a lot of our data is swayed by extremely high and low values, so what can we conclude? You need the right test statistic for the right job, in this case, we are littered with unequal variance in our groupings so we use the moods median and welch (unequal variance t-test) to make conclusions about our data","title":"\ud83c\udf52\ud83c\udf52 2.2.3 Enrichment: Visual Analysis of Residuals: QQ-Plots"},{"location":"S2_Inferential_Statistics/#references","text":"Renesh Bedre ANOVA Minitab ANOVA Analytics Vidhya ANOVA Renesh Bedre Hypothesis Testing Real Statistics Turkey-kramer Mutual Information","title":"References"},{"location":"S3_Model_Selection_and_Validation/","text":"Data Science Foundations Session 3: Model Selection and Validation \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com At the end of session 1, we saw the basic recipe for creating a supervised machine learning model: Environment setup and importing data Rudimentary exploratory data analysis Feature engineering (these were created for us) Choosing and training a model: choose model choose hyperparameters fit using (training) data predict using (validation) data In session 1, I chose our model and hyperparameters preemptively. How did I do that? In the real world, you won't necessarily have the best intution about how to make these choices. In today's session, we will algorithmize the way we approach choosing and training a model Note: I will import libraries at the beginning of this notebook, as is good practice, but will reimport them as they are used to remind ourselves where each method came from! 3.0 Preparing Environment and Importing Data \u00b6 back to top 3.0.1 Import Packages \u00b6 back to top # Pandas library for the pandas dataframes import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import seaborn as sns import random import scipy.stats as stats from patsy import dmatrices from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import LabelEncoder , StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn import metrics from sklearn.metrics import r2_score , mean_squared_error from sklearn.datasets import load_iris 3.0.2 Load Dataset \u00b6 back to top In session 1 we cursorily discussed why we may need strategies for validating our model. Here we'll discuss it more in depth. I'm going to take a simple example. In the following, I have a dataset that contains some data about flowers. It's a very famous dataset used in the ML world iris = load_iris () X = iris . data y = iris . target print ( X . shape ) print ( y . shape ) (150, 4) (150,) let's go ahead and load our wine dataset as well... wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) wine . dropna ( inplace = True ) wine [ 'quality_label' ] = wine [ 'quality' ] . apply ( lambda x : 'low' if x <= 5 else 'med' if x <= 7 else 'high' ) class_tp = { 'red' : 0 , 'white' : 1 } y_tp = wine [ 'type' ] . map ( class_tp ) wine [ 'type_encoding' ] = y_tp class_ql = { 'low' : 0 , 'med' : 1 , 'high' : 2 } y_ql = wine [ 'quality_label' ] . map ( class_ql ) wine [ 'quality_encoding' ] = y_ql wine . drop ([ 'type' , 'quality_label' , 'quality' ], axis = 1 , inplace = True ) wine . columns = wine . columns . str . replace ( ' ' , '_' ) wine . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide density pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1 3.1 Model Validation \u00b6 back to top doing it the wrong way While we're here, I'm going to introduce a VERY SIMPLE supervised learning method called K-Nearest Neighbors. 3.1.0 K-Nearest Neighbors \u00b6 back to top K-Nearest Neighbors is perhaps the simplest algorithm of them all. It is essentially a lookup table: We select the hyperparameter K, and when assigning a new value a data label, assign it according to, the majority label in the vicinity of the new datapoint. The vicinity being determined by K, the number of nearest neighbors we are going to assess. knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X , y ) KNeighborsRegressor(n_neighbors=1) knn . score ( X , y ) 1.0 Wow! we achieved a model with a perfect score! But is this really how we would expect the model to perform on data it had never seen before? Probably not. How do we actually check the performance of our model? 3.1.1 Holdout Sets \u00b6 back to top The way we account for unseen data, in practice, is to leave a portion of the dataset out for testing. This way, we can estimate how our model will perform on entirely new data it may come across in application. train_size = 0.6 X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = train_size , random_state = 42 ) knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X_train , y_train ) print ( knn . score ( X_test , y_test )) 0.9753593429158111 We see that we get a more reasonable value for our performance! 3.1.2 Data Leakage and Cross-Validation \u00b6 back to top An even more rigorous method to leaving out a single test set, is to perform cross validation. Imagine a situation where we are trying to estimate the best value of K in our KNN algorithm. If we continually train our model with new values of K on our training set, and test with our testing set, \"knowledge\" of our test set values with leak into our model, as we choose the best value for K based on how it performs on our test set (even though we did not train on this test set). We call this phenomenon data leakage . CV or Cross Validation overcomes this by only evaluating our parameters with our training set. img src In this scheme, we don't evaluate our model on the test set until the very end. Rather, we estimate our hyperparameter performances by slicing the training set into cross folds img src from sklearn.model_selection import cross_val_score scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) scores array([0.91666667, 0.81725888, 0.85714286, 1. , 0.91 ]) print ( \" %0.2f accuracy with a standard deviation of %0.3f \" % ( scores . mean (), scores . std ())) 0.90 accuracy with a standard deviation of 0.062 More information on the cross_val_score method in sklearn can be found here An additional topic on cross validation is the extreme leave-one-out validation, you can read more about that here Taking these building blocks, if we wanted to for example optimize the number of nearest neighbors without incurring data leakage. We might take the following approach: for k in range ( 1 , 10 ): knn = KNeighborsRegressor ( n_neighbors = k ) scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) print ( \"k= %0.0f , %0.3f accuracy with a standard deviation of %0.3f \" % ( k , scores . mean (), scores . std ())) k=1, 0.900 accuracy with a standard deviation of 0.062 k=2, 0.925 accuracy with a standard deviation of 0.071 k=3, 0.916 accuracy with a standard deviation of 0.077 k=4, 0.932 accuracy with a standard deviation of 0.046 k=5, 0.928 accuracy with a standard deviation of 0.045 k=6, 0.924 accuracy with a standard deviation of 0.047 k=7, 0.929 accuracy with a standard deviation of 0.041 k=8, 0.924 accuracy with a standard deviation of 0.047 k=9, 0.923 accuracy with a standard deviation of 0.042 k = 4 # select best hyperparameters knn = KNeighborsRegressor ( n_neighbors = k ) knn . fit ( X_train , y_train ) # final train print ( knn . score ( X_test , y_test )) 0.9845995893223819 3.1.3 Bias-Variance Tradeoff \u00b6 back to top This next concept will be most easily understood if we go ahead an make up some data ourselves, I'm going to do that now. # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) Text(0, 0.5, 'Y') Let's fit to just a portion of this data random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] plt . plot ( X_train , y_train , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f03e3ce5730>] Now let's take two extreme scenarios: fitting to these datapoints a linear line and a high order polynomial. Keeping in mind the larger dataset (the population) as well as the (irreducible) error we introduced in our data generating function will really illustrate our point! # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) # plot linear model ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) # plot polynomial model ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has high bias because we are forcing the functional form without much consideration to the underlying data \u2014 we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has low variance with respect to the underlying data. On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has low bias because we don't introduce many constraints on the final form of the model. it is high variance because depending on the underlying training data, the final outcome of the model can change quite drastically! In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further: what happens when we retrain these models on different samples of the data population and let's use this to better understand what we mean by bias and variance what happens when we tie this back in with the irreducible error we introduced to the data generator? and let's use this to better understand irreducible error # we're going to perform this task again, but this time for 5 rounds, training # on different samples of the population data random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) for samples in range ( 5 ): X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) As we can see, depending on what data we train our model on, the high bias model changes relatively slightly, while the high variance model changes a whole awful lot! The high variance model is prone to something we call overfitting . It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the population : # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance models are particuarly prone to the phenomenon of over fitting and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this. x = np . arange ( 20 , 100 ) y = [ func ( t , err = 0 ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f03e399ab50>] random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') This time, our high variance model really gets it ! And this is because the data we trained on actually is a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the bias variance tradeoff \ud83c\udfcb\ufe0f Exercise 1: Quantitatively Define Performance \u00b6 Up until now, we've explored this idea of bias variance tradeoff from a qualitative standpoint. As an exercise, continue with this idea, this time calculating the mean squared error (MSE) and R-square between the model and UNSEEN (non-training data) population data. error metrics r2_score mean_squared_error Do this for a 9 th order polynomial and repeat for population data with low, med, and high degrees of error and small and large training fractions (for 6 total comparisons). Complete the chart below. error training fraction MSE R2 # Code Cell for Exercise 1 from sklearn.preprocessing import PolynomialFeatures random . seed ( 42 ) # function to generate data def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) ################################################################################ ########## CHANGE ERR TO CHANGE THE AMOUNT OF NOISE IN YOUR DATA ############### ################################################################################ # err = <YOUR ERR> # change the error (.1 - 0.9) y_actual = [ func ( t , err ) for t in x ] ################################################################################ ############# CHANGE TRAIN_SIZE TO SAMPLE THE DATA FOR TRAINING ################ ################################################################################ # train_size=<YOUR NUMBER>, # change the training size x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = train_size , random_state = 42 ) # solving our training data with a 9-degree polynomial coefs = np . polyfit ( x_train , y_train , 9 ) # generate y data with 9-degree polynomial model and X_seq y_pred = np . polyval ( coefs , x_test ) ################################################################################ ############################## CALCULATE MSE AND R2 ############################ ################################################################################ mse = mean_squared_error ( y_test , y_pred ) r2 = r2_score ( y_test , y_pred ) # look at results print ( \"mean square error: {:.2f} \" . format ( mse )) print ( \"r2: {:.2f} \" . format ( r2 )) mean square error: 0.08 r2: 0.63 3.1.4 Learning Curves \u00b6 back to top To move from qualitative to quantitative understanding of bias-variance tradeoff we need to introduce some metric for model performance. A good one to use here is R-square, a measure of the degree to which predictions match actual values. We can import a tool from sklearn to calculate this for us. from sklearn.metrics import r2_score 3.1.4.1 Considering Model Complexity \u00b6 back to top In a learning curve, we will typically plot the training and testing scores together, to give a sense of when we have either too much bias or too much variance in our model. I'm going to go ahead and recreate the original data distribution we introduced in 1.1.3 # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f03e387c7f0>] Now let's itteratively introduce more complexity into our model random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score = r2_score ( np . polyval ( coefs , X_train ), y_train ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score , ls = '' , marker = '.' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f03e37859a0> As we see from both plots, the score on the training data increases with added model complexity. Giving us the expected perfect fit when the order is the same as the number of data points! This is part I of our learning curve. Part II consists of plotting the training data score with the testing data score. Something else I'm going to do, is define the training portion of the data as a fraction of the overall dataset . This is a typical practice in machine learning. To keep the comparisons the same as up until now, I will keep this training fraction low at .2 random . seed ( 42 ) # defining my training fraction training_frac = .2 # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) train_indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in train_indices ] test_indices = [ i for i in range ( len ( x )) if i not in train_indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) ax [ 0 ] . plot ( X_test , y_test , ls = '' , marker = '.' , color = 'grey' , alpha = 0.5 ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score1 )) ax [ 1 ] . plot ( complexity , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend ([ 'Train $R^2$' , 'Test $R^2$' ]) ax [ 0 ] . legend () <matplotlib.legend.Legend at 0x7f03e36458e0> As we can see, The 2 nd order polynomial achieves the greatest best test set data \\(R^2\\), while the highest order polynomial achieves the best training set data \\(R^2\\). This learning curve is explanative of what we see generally, namely a divergence after some degree of complexity between training and test set performances. In this case, we would resolve to choose the 2 nd order polynomial as the best model for our data. img src 3.1.4.2 Considering Training Set Size \u00b6 back to top The last piece of the puzzle we require, to fully cover learning curves, is the effect of training data size on the model. Let's explore. random . seed ( 42 ) # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .9 , 50 ): # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax . plot ( training_frac , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax . plot ( training_frac , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax . set_title ( \"9th-order Polynomial Score with Increasing Training Set Size\" ) ax . legend ([ 'Train' , 'Test' ]) ax . set_xlabel ( 'Training Fraction' ) ax . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$') What we see here is a trend that happens generally, as our amount of training data increases, our models handle more complexity. This is because the more data we have, the more we are able to dicipher noise from signal. Our model will ignore noise as it does not generalize well across the other datapoints. Instead, it will will fit to relationships that appear across the entire training dataset. img src \ud83c\udfcb\ufe0f Exercise 2: Visualization \u00b6 Starting with the code below, make a side-by-side plot of a 3 rd degree polynomial and a 9 th degree polynomial. On the x axis slowly increase the training set size, on the y axis plot the scores for the training and test sets. # Code Cell for Exercise 2 random . seed ( 42 ) # create the figure and axes fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .95 , 50 ): # create test and training data x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = training_frac , random_state = 42 ) # solving our training data with a n-degree polynomial coefs = np . polyfit ( x_train , y_train , 9 ) # recording the scores for the training and test sets score1 = r2_score ( np . polyval ( coefs , x_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , x_test ), y_test ) ax . plot ( training_frac , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax . plot ( training_frac , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax . set_title ( \"9th-order Polynomial Score\" ) ax . legend ([ 'Train' , 'Test' ]) ax . set_xlabel ( 'Training Fraction' ) ax . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$') \ud83d\ude4b\u200d\u2640\ufe0f Question 1: In what regions of the plots are we overfitting? \u00b6 Where in these plots is overfitting occuring? Why is it different for each polynomial? 3.2 Model Validation in Practice \u00b6 back to top We will now turn our attention to practical implementation. In practice, there are a wide number of variables (called hyperparameters) to consider when choosing a model. Scikit learn has a useful method called Grid Search that will iterate through every possible combination of a range of hyperparameter settings you provide as input. Before we get started with grid search, we'll need to switch over from our numpy polynomial fit method to one in sklearn. Here, the caveat is our actual model will solve for the coefficients infront of the polynomials. We will engineer the polynomial features ourselves. This is an example of feature engineering which we will revisit in depth in the next session. from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) 3.2.1 Grid Search \u00b6 back to top from sklearn.model_selection import GridSearchCV param_grid = { 'polynomialfeatures__degree' : np . arange ( 10 ), 'linearregression__fit_intercept' : [ True , False ]} grid = GridSearchCV ( PolynomialRegression (), param_grid , cv = 7 ) # create test and training data random . seed ( 42 ) X_train = random . sample ( list ( x ), int ( int ( len ( x )) * .8 )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] grid . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) GridSearchCV(cv=7, estimator=Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()), ('linearregression', LinearRegression())]), param_grid={'linearregression__fit_intercept': [True, False], 'polynomialfeatures__degree': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}) grid . best_params_ {'linearregression__fit_intercept': True, 'polynomialfeatures__degree': 3} to grab the best model from the CV/search outcome. we use grid.best_estimator model = grid . best_estimator_ # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 5 )) ax . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax . plot ( X_train , y_train , ls = '' , marker = '.' ) ax . set_ylim ( min ( y ), max ( y )) ax . set_title ( \"Best Grid Search CV Model\" ) Text(0.5, 1.0, 'Best Grid Search CV Model') \ud83c\udfcb\ufe0f Exercise 3: Grid Search \u00b6 There are two objectives for this exercise: Complete the object param_grid by reading the parameters below in StandardScaler . We would like to include in our grid search whether or not to scale our input features so that they are centered at 0 ( with_mean ) and have unit variance ( with_std ) After you have completed the grid search, create a side-by-side plot of actual versus predicted values for training data (left plot) and testing data (right plot) Include in your plot title the R2 for testing and R2 for training df = wine . copy () y = df . pop ( 'density' ) X = df display ( X . head ()) display ( y . head ()) print ( X . shape ) print ( y . shape ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) def NormalizedRegression ( ** kwargs ): return make_pipeline ( StandardScaler (), LinearRegression ( ** kwargs )) ###################################################################################################### ####################### YOUR EXTRA PARAMETERS GO IN THE DICTIONARY BELOW ############################# ###################################################################################################### param_grid = { 'linearregression__fit_intercept' : [ True , False ]} grid = GridSearchCV ( NormalizedRegression (), param_grid , cv = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 0 1.0010 1 0.9940 2 0.9951 3 0.9956 4 0.9956 Name: density, dtype: float64 (6463, 12) (6463,) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) {'linearregression__fit_intercept': True} model = grid . best_estimator_ ###################################################################################################### ##################################### ADJUST THE PLOT LOGIC BELOW #################################### ###################################################################################################### fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Best Grid Search CV Model\" ) Text(0.5, 1.0, 'Best Grid Search CV Model') References \u00b6 back to top Model Validation \u00b6 cross_val_score leave-one-out","title":"Model Selection and Validation"},{"location":"S3_Model_Selection_and_Validation/#data-science-foundations-session-3-model-selection-and-validation","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com At the end of session 1, we saw the basic recipe for creating a supervised machine learning model: Environment setup and importing data Rudimentary exploratory data analysis Feature engineering (these were created for us) Choosing and training a model: choose model choose hyperparameters fit using (training) data predict using (validation) data In session 1, I chose our model and hyperparameters preemptively. How did I do that? In the real world, you won't necessarily have the best intution about how to make these choices. In today's session, we will algorithmize the way we approach choosing and training a model Note: I will import libraries at the beginning of this notebook, as is good practice, but will reimport them as they are used to remind ourselves where each method came from!","title":"Data Science Foundations  Session 3: Model Selection and Validation"},{"location":"S3_Model_Selection_and_Validation/#30-preparing-environment-and-importing-data","text":"back to top","title":"3.0 Preparing Environment and Importing Data"},{"location":"S3_Model_Selection_and_Validation/#301-import-packages","text":"back to top # Pandas library for the pandas dataframes import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import seaborn as sns import random import scipy.stats as stats from patsy import dmatrices from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import LabelEncoder , StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn import metrics from sklearn.metrics import r2_score , mean_squared_error from sklearn.datasets import load_iris","title":"3.0.1 Import Packages"},{"location":"S3_Model_Selection_and_Validation/#302-load-dataset","text":"back to top In session 1 we cursorily discussed why we may need strategies for validating our model. Here we'll discuss it more in depth. I'm going to take a simple example. In the following, I have a dataset that contains some data about flowers. It's a very famous dataset used in the ML world iris = load_iris () X = iris . data y = iris . target print ( X . shape ) print ( y . shape ) (150, 4) (150,) let's go ahead and load our wine dataset as well... wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) wine . dropna ( inplace = True ) wine [ 'quality_label' ] = wine [ 'quality' ] . apply ( lambda x : 'low' if x <= 5 else 'med' if x <= 7 else 'high' ) class_tp = { 'red' : 0 , 'white' : 1 } y_tp = wine [ 'type' ] . map ( class_tp ) wine [ 'type_encoding' ] = y_tp class_ql = { 'low' : 0 , 'med' : 1 , 'high' : 2 } y_ql = wine [ 'quality_label' ] . map ( class_ql ) wine [ 'quality_encoding' ] = y_ql wine . drop ([ 'type' , 'quality_label' , 'quality' ], axis = 1 , inplace = True ) wine . columns = wine . columns . str . replace ( ' ' , '_' ) wine . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide density pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1","title":"3.0.2 Load Dataset"},{"location":"S3_Model_Selection_and_Validation/#31-model-validation","text":"back to top doing it the wrong way While we're here, I'm going to introduce a VERY SIMPLE supervised learning method called K-Nearest Neighbors.","title":"3.1 Model Validation"},{"location":"S3_Model_Selection_and_Validation/#310-k-nearest-neighbors","text":"back to top K-Nearest Neighbors is perhaps the simplest algorithm of them all. It is essentially a lookup table: We select the hyperparameter K, and when assigning a new value a data label, assign it according to, the majority label in the vicinity of the new datapoint. The vicinity being determined by K, the number of nearest neighbors we are going to assess. knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X , y ) KNeighborsRegressor(n_neighbors=1) knn . score ( X , y ) 1.0 Wow! we achieved a model with a perfect score! But is this really how we would expect the model to perform on data it had never seen before? Probably not. How do we actually check the performance of our model?","title":"3.1.0 K-Nearest Neighbors"},{"location":"S3_Model_Selection_and_Validation/#311-holdout-sets","text":"back to top The way we account for unseen data, in practice, is to leave a portion of the dataset out for testing. This way, we can estimate how our model will perform on entirely new data it may come across in application. train_size = 0.6 X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = train_size , random_state = 42 ) knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X_train , y_train ) print ( knn . score ( X_test , y_test )) 0.9753593429158111 We see that we get a more reasonable value for our performance!","title":"3.1.1 Holdout Sets"},{"location":"S3_Model_Selection_and_Validation/#312-data-leakage-and-cross-validation","text":"back to top An even more rigorous method to leaving out a single test set, is to perform cross validation. Imagine a situation where we are trying to estimate the best value of K in our KNN algorithm. If we continually train our model with new values of K on our training set, and test with our testing set, \"knowledge\" of our test set values with leak into our model, as we choose the best value for K based on how it performs on our test set (even though we did not train on this test set). We call this phenomenon data leakage . CV or Cross Validation overcomes this by only evaluating our parameters with our training set. img src In this scheme, we don't evaluate our model on the test set until the very end. Rather, we estimate our hyperparameter performances by slicing the training set into cross folds img src from sklearn.model_selection import cross_val_score scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) scores array([0.91666667, 0.81725888, 0.85714286, 1. , 0.91 ]) print ( \" %0.2f accuracy with a standard deviation of %0.3f \" % ( scores . mean (), scores . std ())) 0.90 accuracy with a standard deviation of 0.062 More information on the cross_val_score method in sklearn can be found here An additional topic on cross validation is the extreme leave-one-out validation, you can read more about that here Taking these building blocks, if we wanted to for example optimize the number of nearest neighbors without incurring data leakage. We might take the following approach: for k in range ( 1 , 10 ): knn = KNeighborsRegressor ( n_neighbors = k ) scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) print ( \"k= %0.0f , %0.3f accuracy with a standard deviation of %0.3f \" % ( k , scores . mean (), scores . std ())) k=1, 0.900 accuracy with a standard deviation of 0.062 k=2, 0.925 accuracy with a standard deviation of 0.071 k=3, 0.916 accuracy with a standard deviation of 0.077 k=4, 0.932 accuracy with a standard deviation of 0.046 k=5, 0.928 accuracy with a standard deviation of 0.045 k=6, 0.924 accuracy with a standard deviation of 0.047 k=7, 0.929 accuracy with a standard deviation of 0.041 k=8, 0.924 accuracy with a standard deviation of 0.047 k=9, 0.923 accuracy with a standard deviation of 0.042 k = 4 # select best hyperparameters knn = KNeighborsRegressor ( n_neighbors = k ) knn . fit ( X_train , y_train ) # final train print ( knn . score ( X_test , y_test )) 0.9845995893223819","title":"3.1.2 Data Leakage and Cross-Validation"},{"location":"S3_Model_Selection_and_Validation/#313-bias-variance-tradeoff","text":"back to top This next concept will be most easily understood if we go ahead an make up some data ourselves, I'm going to do that now. # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) Text(0, 0.5, 'Y') Let's fit to just a portion of this data random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] plt . plot ( X_train , y_train , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f03e3ce5730>] Now let's take two extreme scenarios: fitting to these datapoints a linear line and a high order polynomial. Keeping in mind the larger dataset (the population) as well as the (irreducible) error we introduced in our data generating function will really illustrate our point! # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) # plot linear model ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) # plot polynomial model ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has high bias because we are forcing the functional form without much consideration to the underlying data \u2014 we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has low variance with respect to the underlying data. On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has low bias because we don't introduce many constraints on the final form of the model. it is high variance because depending on the underlying training data, the final outcome of the model can change quite drastically! In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further: what happens when we retrain these models on different samples of the data population and let's use this to better understand what we mean by bias and variance what happens when we tie this back in with the irreducible error we introduced to the data generator? and let's use this to better understand irreducible error # we're going to perform this task again, but this time for 5 rounds, training # on different samples of the population data random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) for samples in range ( 5 ): X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) As we can see, depending on what data we train our model on, the high bias model changes relatively slightly, while the high variance model changes a whole awful lot! The high variance model is prone to something we call overfitting . It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the population : # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance models are particuarly prone to the phenomenon of over fitting and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this. x = np . arange ( 20 , 100 ) y = [ func ( t , err = 0 ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f03e399ab50>] random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') This time, our high variance model really gets it ! And this is because the data we trained on actually is a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the bias variance tradeoff","title":"3.1.3 Bias-Variance Tradeoff"},{"location":"S3_Model_Selection_and_Validation/#exercise-1-quantitatively-define-performance","text":"Up until now, we've explored this idea of bias variance tradeoff from a qualitative standpoint. As an exercise, continue with this idea, this time calculating the mean squared error (MSE) and R-square between the model and UNSEEN (non-training data) population data. error metrics r2_score mean_squared_error Do this for a 9 th order polynomial and repeat for population data with low, med, and high degrees of error and small and large training fractions (for 6 total comparisons). Complete the chart below. error training fraction MSE R2 # Code Cell for Exercise 1 from sklearn.preprocessing import PolynomialFeatures random . seed ( 42 ) # function to generate data def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) ################################################################################ ########## CHANGE ERR TO CHANGE THE AMOUNT OF NOISE IN YOUR DATA ############### ################################################################################ # err = <YOUR ERR> # change the error (.1 - 0.9) y_actual = [ func ( t , err ) for t in x ] ################################################################################ ############# CHANGE TRAIN_SIZE TO SAMPLE THE DATA FOR TRAINING ################ ################################################################################ # train_size=<YOUR NUMBER>, # change the training size x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = train_size , random_state = 42 ) # solving our training data with a 9-degree polynomial coefs = np . polyfit ( x_train , y_train , 9 ) # generate y data with 9-degree polynomial model and X_seq y_pred = np . polyval ( coefs , x_test ) ################################################################################ ############################## CALCULATE MSE AND R2 ############################ ################################################################################ mse = mean_squared_error ( y_test , y_pred ) r2 = r2_score ( y_test , y_pred ) # look at results print ( \"mean square error: {:.2f} \" . format ( mse )) print ( \"r2: {:.2f} \" . format ( r2 )) mean square error: 0.08 r2: 0.63","title":"\ud83c\udfcb\ufe0f Exercise 1: Quantitatively Define Performance"},{"location":"S3_Model_Selection_and_Validation/#314-learning-curves","text":"back to top To move from qualitative to quantitative understanding of bias-variance tradeoff we need to introduce some metric for model performance. A good one to use here is R-square, a measure of the degree to which predictions match actual values. We can import a tool from sklearn to calculate this for us. from sklearn.metrics import r2_score","title":"3.1.4 Learning Curves"},{"location":"S3_Model_Selection_and_Validation/#3141-considering-model-complexity","text":"back to top In a learning curve, we will typically plot the training and testing scores together, to give a sense of when we have either too much bias or too much variance in our model. I'm going to go ahead and recreate the original data distribution we introduced in 1.1.3 # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f03e387c7f0>] Now let's itteratively introduce more complexity into our model random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score = r2_score ( np . polyval ( coefs , X_train ), y_train ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score , ls = '' , marker = '.' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f03e37859a0> As we see from both plots, the score on the training data increases with added model complexity. Giving us the expected perfect fit when the order is the same as the number of data points! This is part I of our learning curve. Part II consists of plotting the training data score with the testing data score. Something else I'm going to do, is define the training portion of the data as a fraction of the overall dataset . This is a typical practice in machine learning. To keep the comparisons the same as up until now, I will keep this training fraction low at .2 random . seed ( 42 ) # defining my training fraction training_frac = .2 # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) train_indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in train_indices ] test_indices = [ i for i in range ( len ( x )) if i not in train_indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) ax [ 0 ] . plot ( X_test , y_test , ls = '' , marker = '.' , color = 'grey' , alpha = 0.5 ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score1 )) ax [ 1 ] . plot ( complexity , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend ([ 'Train $R^2$' , 'Test $R^2$' ]) ax [ 0 ] . legend () <matplotlib.legend.Legend at 0x7f03e36458e0> As we can see, The 2 nd order polynomial achieves the greatest best test set data \\(R^2\\), while the highest order polynomial achieves the best training set data \\(R^2\\). This learning curve is explanative of what we see generally, namely a divergence after some degree of complexity between training and test set performances. In this case, we would resolve to choose the 2 nd order polynomial as the best model for our data. img src","title":"3.1.4.1 Considering Model Complexity"},{"location":"S3_Model_Selection_and_Validation/#3142-considering-training-set-size","text":"back to top The last piece of the puzzle we require, to fully cover learning curves, is the effect of training data size on the model. Let's explore. random . seed ( 42 ) # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .9 , 50 ): # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax . plot ( training_frac , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax . plot ( training_frac , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax . set_title ( \"9th-order Polynomial Score with Increasing Training Set Size\" ) ax . legend ([ 'Train' , 'Test' ]) ax . set_xlabel ( 'Training Fraction' ) ax . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$') What we see here is a trend that happens generally, as our amount of training data increases, our models handle more complexity. This is because the more data we have, the more we are able to dicipher noise from signal. Our model will ignore noise as it does not generalize well across the other datapoints. Instead, it will will fit to relationships that appear across the entire training dataset. img src","title":"3.1.4.2 Considering Training Set Size"},{"location":"S3_Model_Selection_and_Validation/#exercise-2-visualization","text":"Starting with the code below, make a side-by-side plot of a 3 rd degree polynomial and a 9 th degree polynomial. On the x axis slowly increase the training set size, on the y axis plot the scores for the training and test sets. # Code Cell for Exercise 2 random . seed ( 42 ) # create the figure and axes fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .95 , 50 ): # create test and training data x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = training_frac , random_state = 42 ) # solving our training data with a n-degree polynomial coefs = np . polyfit ( x_train , y_train , 9 ) # recording the scores for the training and test sets score1 = r2_score ( np . polyval ( coefs , x_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , x_test ), y_test ) ax . plot ( training_frac , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax . plot ( training_frac , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax . set_title ( \"9th-order Polynomial Score\" ) ax . legend ([ 'Train' , 'Test' ]) ax . set_xlabel ( 'Training Fraction' ) ax . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$')","title":"\ud83c\udfcb\ufe0f Exercise 2: Visualization"},{"location":"S3_Model_Selection_and_Validation/#question-1-in-what-regions-of-the-plots-are-we-overfitting","text":"Where in these plots is overfitting occuring? Why is it different for each polynomial?","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: In what regions of the plots are we overfitting?"},{"location":"S3_Model_Selection_and_Validation/#32-model-validation-in-practice","text":"back to top We will now turn our attention to practical implementation. In practice, there are a wide number of variables (called hyperparameters) to consider when choosing a model. Scikit learn has a useful method called Grid Search that will iterate through every possible combination of a range of hyperparameter settings you provide as input. Before we get started with grid search, we'll need to switch over from our numpy polynomial fit method to one in sklearn. Here, the caveat is our actual model will solve for the coefficients infront of the polynomials. We will engineer the polynomial features ourselves. This is an example of feature engineering which we will revisit in depth in the next session. from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs ))","title":"3.2 Model Validation in Practice"},{"location":"S3_Model_Selection_and_Validation/#321-grid-search","text":"back to top from sklearn.model_selection import GridSearchCV param_grid = { 'polynomialfeatures__degree' : np . arange ( 10 ), 'linearregression__fit_intercept' : [ True , False ]} grid = GridSearchCV ( PolynomialRegression (), param_grid , cv = 7 ) # create test and training data random . seed ( 42 ) X_train = random . sample ( list ( x ), int ( int ( len ( x )) * .8 )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] grid . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) GridSearchCV(cv=7, estimator=Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()), ('linearregression', LinearRegression())]), param_grid={'linearregression__fit_intercept': [True, False], 'polynomialfeatures__degree': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}) grid . best_params_ {'linearregression__fit_intercept': True, 'polynomialfeatures__degree': 3} to grab the best model from the CV/search outcome. we use grid.best_estimator model = grid . best_estimator_ # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 5 )) ax . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax . plot ( X_train , y_train , ls = '' , marker = '.' ) ax . set_ylim ( min ( y ), max ( y )) ax . set_title ( \"Best Grid Search CV Model\" ) Text(0.5, 1.0, 'Best Grid Search CV Model')","title":"3.2.1 Grid Search"},{"location":"S3_Model_Selection_and_Validation/#exercise-3-grid-search","text":"There are two objectives for this exercise: Complete the object param_grid by reading the parameters below in StandardScaler . We would like to include in our grid search whether or not to scale our input features so that they are centered at 0 ( with_mean ) and have unit variance ( with_std ) After you have completed the grid search, create a side-by-side plot of actual versus predicted values for training data (left plot) and testing data (right plot) Include in your plot title the R2 for testing and R2 for training df = wine . copy () y = df . pop ( 'density' ) X = df display ( X . head ()) display ( y . head ()) print ( X . shape ) print ( y . shape ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) def NormalizedRegression ( ** kwargs ): return make_pipeline ( StandardScaler (), LinearRegression ( ** kwargs )) ###################################################################################################### ####################### YOUR EXTRA PARAMETERS GO IN THE DICTIONARY BELOW ############################# ###################################################################################################### param_grid = { 'linearregression__fit_intercept' : [ True , False ]} grid = GridSearchCV ( NormalizedRegression (), param_grid , cv = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 0 1.0010 1 0.9940 2 0.9951 3 0.9956 4 0.9956 Name: density, dtype: float64 (6463, 12) (6463,) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) {'linearregression__fit_intercept': True} model = grid . best_estimator_ ###################################################################################################### ##################################### ADJUST THE PLOT LOGIC BELOW #################################### ###################################################################################################### fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Best Grid Search CV Model\" ) Text(0.5, 1.0, 'Best Grid Search CV Model')","title":"\ud83c\udfcb\ufe0f Exercise 3: Grid Search"},{"location":"S3_Model_Selection_and_Validation/#references","text":"back to top","title":"References"},{"location":"S3_Model_Selection_and_Validation/#model-validation","text":"cross_val_score leave-one-out","title":"Model Validation"},{"location":"S4_Feature_Engineering/","text":"Data Science Foundations Session 4: Feature Engineering \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we talked about model pipelines and conveniently began with a suitable set of input data. In the real world, this is hardly ever the case! What is constant is this: at the end of the day, our models need numbers. Not only this, but a suitable set of numbers. What does that mean? The answer to that question is the subject of our session today. 4.0 Preparing Environment and Importing Data \u00b6 back to top 4.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats as stats from scipy.stats import gamma from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor import seaborn as sns ; sns . set () from sklearn.datasets import load_iris from sklearn.metrics import mean_squared_error , r2_score 4.0.2 Load Dataset \u00b6 back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) orders = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_orders.csv' ) time_cols = [ i for i in orders . columns if '/' in i ] margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 dfcat = margin . columns [: - 2 ] dfcat Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') orders . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer 1/2020 2/2020 3/2020 4/2020 5/2020 6/2020 7/2020 8/2020 9/2020 10/2020 11/2020 12/2020 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 4.1 Categorical Features \u00b6 back to top At the end of the day, our algorithms operate on numerical values. How do you get from a series of string values to numerical values? margin [ 'Customer' ] . unique () array(['Slugworth', 'Perk-a-Cola', 'Fickelgruber', 'Zebrabar', \"Dandy's Candies\"], dtype=object) A naive way to do it would be to assign a number to every entry 'Slugworth' = 1 'Perk-a-Cola' = 2 'Dandy's Candies' = 3 but we would inadvertently end up with some weird mathematical relationships between these variables, e.g. Dandy's Candies - Perk-a-Cola = Slugworth (3 - 2 = 1). A work around for this is to think multi-dimensionally we express our categorical values as vectors in a hyperspace where they cannot be expressed in terms of one another, i.e. they are orthogonal 'Slugworth' = [1,0,0] 'Perk-a-Cola' = [0,1,0] 'Dandy's Candies' = [0,0,1] such a scheme, in machine learning vernacular, is termed one-hot encoding. 4.1.1 One-Hot Encoding \u00b6 back to top sklearn has a couple useful libraries for one-hot encoding. let's start with the OneHotEncoder class in its preprocessing library from sklearn.preprocessing import OneHotEncoder # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ 'Customer' ] . values . reshape ( - 1 , 1 ) # fit our encoder to this data enc . fit ( X_cat ) OneHotEncoder() After fitting our encoder, we can then use this object to create our training array. # as a reference here's our original data display ( X_cat [: 10 ]) print ( X_cat . shape , end = ' \\n\\n ' ) onehotlabels = enc . transform ( X_cat ) . toarray () print ( onehotlabels . shape , end = ' \\n\\n ' ) # And here is our new data onehotlabels [: 10 ] array([['Slugworth'], ['Slugworth'], ['Perk-a-Cola'], ['Fickelgruber'], ['Fickelgruber'], ['Fickelgruber'], ['Slugworth'], ['Zebrabar'], ['Slugworth'], ['Zebrabar']], dtype=object) (1668, 1) (1668, 5) array([[0., 0., 0., 1., 0.], [0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]]) We have our customer information one-hot encoded, we need to do this for all our variables and concatenate them with our regular numerical variables in our original dataframe. # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ dfcat ] . values # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [ \"KG\" ] print ( X_num . shape ) X = np . concatenate (( onehotlabels , X_num . values . reshape ( - 1 , 1 )), axis = 1 ) X . shape (1668,) (1668, 119) And now we grab our EBITDA (margin) data for prediction y = margin [ \"EBITDA/KG\" ] \ud83c\udfcb\ufe0f Exercise 1: Create a simple linear model \u00b6 Using the X and y sets, use train_test_split and LinearRegression to make a baseline model based on what we've learned so far. Assess your model performance visually by plottying y_test vs y_test_pred # Cell for Exercise 1 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression [<matplotlib.lines.Line2D at 0x7fa4b01f90d0>] \ud83d\ude4b Question 1: \u00b6 How can we assess the relative feature importance of the features in our model? We could be tempted to inspect the coefficients ( linear.coef_ ) of our model to evaluate the relative feature importance, but in order to do this our features need to be scaled (so that the relative coefficient sizes are meaningful). What other issues might there be (think categorical vs continuous variables). 4.2 Derived Features \u00b6 back to top Can we recall an example of where we've seen this previously? That's right earlier on in our session on model selection and validation we derived some polynomial features to create our polynomial model using the linear regression class in sklearn. We actually see this a lot in engineering, where we will describe log relationships or some other transformation of the original variable. Actually let me see if I can find an example in my handy BSL... concentration profiles in continous stirred tank vs plug flow reactors. Notice the y-axis is log scale. Thanks Bird, Stewart, Lightfoot! Can we think of other examples where we would like to derive features from our input data? 4.2.1 Creating Polynomials \u00b6 back to top Let's revisit our example from the previous session, right before we introduced Grid Search in sklearn # from Model Selection and Validation, 1.2.1 from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) in the above, we use sklearn's convenient tool, make_pipeline to join together the preprocessing tool PolynomialFeatures and the basic model LinearRegression . Let's take a look at what PolynomialFeatures does to some simple data x = np . arange ( 1 , 11 ) y = x ** 3 print ( x ) print ( y ) [ 1 2 3 4 5 6 7 8 9 10] [ 1 8 27 64 125 216 343 512 729 1000] features = PolynomialFeatures ( degree = 3 ) X2 = features . fit_transform ( x . reshape ( - 1 , 1 )) we see our new feature set contains our original features, plus new features up to the nth-degree polynomial we set when creating the features object from PolynomialFeatures print ( X2 ) [[ 1. 1. 1. 1.] [ 1. 2. 4. 8.] [ 1. 3. 9. 27.] [ 1. 4. 16. 64.] [ 1. 5. 25. 125.] [ 1. 6. 36. 216.] [ 1. 7. 49. 343.] [ 1. 8. 64. 512.] [ 1. 9. 81. 729.] [ 1. 10. 100. 1000.]] model = LinearRegression () . fit ( X2 , y ) yhat = model . predict ( X2 ) plt . scatter ( x , y ) plt . plot ( x , yhat ); 4.2.2 Dealing with Time Series \u00b6 back to top Often, we will be dealing with time series data, whether its data generated by machinery, reactors, or sales and customers. In the following we discuss some simple practices for dealing with time series data. \ud83c\udf52 4.2.2.1 Enrichment : Fast Fourier Transform \u00b6 Special thanks to Brian Gerwe for his contribution to this section \ud83d\udc68\u200d\ud83c\udf73 back to top Sometimes we'll want to create a more sophisticated transformation of our input data. As engineers, this can often have to do with some empirical knowledge we understand about our process. When working with equipment and machinery, we will often want to convert a signal from the time to frequency domain. Let's cover how we can do that with numpy! img src What I've drawn here in the following is called a square-wave signal t = np . linspace ( 0 , 1 , 501 ) # FFT should be given an integer number of cycles so we leave out last sample t = t [: - 1 ] f = 5 # linear frequency in Hz w = f * 2 * np . pi # radial frequency h = 4 # height of square wave amp = 4 * h / np . pi s = amp * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) # here is the call to numpy FFT F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ], d = t [ 1 ]) # reorder frequency spectrum and frequency bins with 0 Hz at the center F = np . fft . fftshift ( F ) freq = np . fft . fftshift ( freq ) # scale frequency spectrum to correct amplitude F = F / t . size # amplitudes amps = [ max ( np . sin ( w * t )), max ( np . sin ( w * t * 3 ) / 3 ), max ( np . sin ( w * t * 5 ) / 5 )] fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) ax [ 0 ] . set_xlim ( 0 , 1 ) ax [ 0 ] . set_xlabel ( 'Time (s)' ) # tells us about the amplitude of the component at the # corresponding frequency. Multiplied by two because the # signal power is split between (-) and (+) frequency branches # of FFT, but we're only visualizing the (+) branch magnitude = 2 * np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , 30 ) ax [ 1 ] . set_xlabel ( 'Frequency (Hz)' ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain') 4.2.2.2 Rolling Windows \u00b6 back to top to see an example of this dataset in action visit this link One powerful technique for dealing with time series data, is to create a rolling window of features based on the historical data. The proper window size can usually be determined by trial and error, or constraints around access to the data itself. In the above gif, we have a window size of 7. What that means is for whatever time step units we are in (that could be minutes, days, months, etc.) we will have 7 of them included in a single instance or observation. This instance or observation is then interpreted by our model and used to assess the target value, typically the quantity in the very next time step after the window (the green bar in the gif). Let's take an example with the orders data tidy_orders = orders . melt ( id_vars = orders . columns [: 6 ], var_name = 'Date' , value_name = 'KG' ) display ( tidy_orders . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 In the next exercise, we are going to attempt to predict an order amount, based on the previous order history. We will scrub all categorical labels and only use historical amounts to inform our models. In effect the data that the model will see will look like the following: fig , ax = plt . subplots ( 3 , 2 , figsize = ( 10 , 20 )) indices = np . argwhere ( ax ) color_dict = { 0 : 'tab:blue' , 1 : 'tab:green' , 2 : 'tab:orange' , 3 : 'tab:red' , 4 : 'tab:pink' , 5 : 'tab:brown' } for index , customer in enumerate ( tidy_orders . Customer . unique ()): orders . loc [ orders . Customer == customer ] . iloc [:, 6 :] . reset_index () . T . plot ( c = color_dict [ index ], legend = False , ax = ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]]) ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]] . set_title ( customer ) What we may notice is that there is very little noise or drift in our order history, but there is certainly some periodicity. The question is can we use a linear model to predict the next order amount based on these history orders? def process_data ( Xy , time_cols = 12 , window = 3 , remove_null = False ): \"\"\" This function splits your time series data into the proper windows Parameters ---------- Xy: array The input data. If there are non-time series columns, assumes they are on the left and time columns are on the right. time_cols: int The number of time columns, default 12 window: int The time window size, default 3 Returns ------- X_: array The independent variables, includes time and non-time series columns with the new window y_: array The dependent variable, selected from the time columns at the end of the window labels: The time series labels, can be used in subsequent plot \"\"\" # separate the non-time series columns X_cat = Xy [:,: - time_cols ] # select the columns to apply the sweeping window X = Xy [:, - time_cols :] X_ = [] y = [] for i in range ( X . shape [ 1 ] - window ): # after attaching the current window to the non-time series # columns, add it to a growing list X_ . append ( np . concatenate (( X_cat , X [:, i : i + window ]), axis = 1 )) # add the next time delta after the window to the list of y # values y . append ( X [:, i + window ]) # X_ is 3D: [number of replicates from sweeping window, # length of input data, # size of new feature with categories and time] # we want to reshape X_ so that the replicates due to the sweeping window is # a part of the same dimension as the instances of the input data X_ = np . array ( X_ ) . reshape ( X . shape [ 0 ] * np . array ( X_ ) . shape [ 0 ], window + X_cat . shape [ 1 ]) y = np . array ( y ) . reshape ( X . shape [ 0 ] * np . array ( y ) . shape [ 0 ],) if remove_null : # remove training data where the target is 0 (may be unfair advantage) X_ = X_ [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] y = y [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] # create labels that show the previous month values used to train the model labels = [] for row in X_ : labels . append ( \"X: {} \" . format ( np . array2string ( row [ - window :] . astype ( float ) . round ()))) return X_ , y , labels \ud83c\udfcb\ufe0f Exercise 2: Optimize Rolling Window Size for Customer Forecasts \u00b6 For this exercise, you will use the process_data function below to help you optimize the window size for predicting the order quantity in any given month. You will train a LinearRegression model. create a model using a window size of 3 and predict the order quantity for the month immediately following the window create a model for window sizes 1-11 and report the \\(R^2\\) for each model # Cell for Exercise 2 kg_month_data = orders . values [:, 6 :] # use kg_month_data and the function process_data to create your X, y arrays # then use train_test_split to create train and test portions # USE y_test and y_pred for your actual and true test data # change only window parameter in process_data() print ( \"window R2\" ) for window in range ( 1 , 12 ): ###################### ### YOUR CODE HERE ### ###################### pass window R2 1, 0.97 2, 0.88 3, 0.80 4, 0.96 5, 0.90 6, 0.86 7, 0.61 8, 0.89 9, 0.76 10, 0.74 11, 0.87 #### RUN AFTER EXERCISE 2.2.2.2.1 #### fig = px . scatter ( x = y_test , y = y_pred , labels = { \"y\" : \"Prediction\" , \"x\" : \"Actual\" }) fig . update_layout ( autosize = False , width = 800 , height = 500 , title = 'R2: {:.3f} ' . format ( r2_score ( y_test , y_pred )) ) require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"239a6e0b-8482-49c8-9619-ebde564b91cd\")) { Plotly.newPlot( \"239a6e0b-8482-49c8-9619-ebde564b91cd\", [{\"hovertemplate\":\"Actual=%{x}<br>Prediction=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4528.380503144654,64.01384083044982,15617.489114658929,4528.380503144654,5698.392857142857,124348.89322916669,21939.111111111117,5712.33671988389,64983.42138364781,1483.5,93302.72435897436,1236.25,773.5849056603774,518.4642857142858,3798.928571428572,13671.296296296296,188.7692307692308,4057.832278481013,101149.25,14626.074074074077,12274.009146341465,141456.76315789475,4793.829296424452,74.78571428571429,10861.245674740485,86.24595469255664,19129.33333333333,39307.04113924051,74.78571428571429,46247.0,188.7692307692308,85.32258064516128,90677.41228070176,9306.611111111111,220.55016181229772,466477.57812500006,1158.125,220.55016181229772,1901.1290322580644,19129.33333333333,2637.8993710691825,1868.867924528302,3798.928571428572,2252.261437908497,5222.5,181.5552699228792,5038.387096774193,120.98615916955016,10146.898432174508,85.32258064516128,286.6666666666667,18613.222222222223,818.1891025641025,2499.201741654572,2010.9477124183009,6984.389273356401,42704.25889967639,11715.487421383648,39307.04113924051,440.859375,428.0,96.59546925566345,102.64285714285714,3569.0359477124184,13671.296296296296,773.5849056603774,2010.9477124183009,42704.25889967639,40608.41346153846,83347.75862068967,10796.31640058055,74.78571428571429,80801.72807017544,2540.0,5222.5,131.55526992287918,299024.0885416667,3882.680981595092,895.9082278481012,1180.5379746835442,15617.489114658929,85.32258064516128,6185.59509202454,16749.13157894737,47142.92857142857,3786.966463414634,936.280487804878,15244.632812500002,3272.80163599182,1271.904761904762,47142.92857142857,356.6666666666667,10861.245674740485,108.64285714285714,21438.1875,16918.132716049382,7892.142857142857,1901.1290322580644,2647.5,5038.387096774193,1677.5078125000002,39307.04113924051,51795.97953216374,916.3717948717948,41871.69230769231,12870.766853932584,1271.904761904762,181.5552699228792,211540.7575757576,3798.928571428572,2054.328561690525,3578.592162554427,2854.854368932039,51795.97953216374,11791.8,149.57142857142858,103.3653846153846,119015.43674698794,2499.201741654572,19286.94968553459,2185.3410740203194,19569.69230769231,12317.397660818711,6965.361445783132,86.24595469255664,3071.4691011235955,13867.125,50574.625,14626.074074074077,1005.4545454545454,16918.132716049382,2543.106741573034,10245.87593728698,8608.709677419354,150.3654970760234,3517.9333333333334,14626.074074074077,3177.0,2499.201741654572,345.6428571428572,10736.622807017544,13799.769319492503,12691.209677419354,11.175616835994196,2010.9477124183009,10736.622807017544,71138.51966292135,8104.40251572327,62.168396770472896,211540.7575757576,16569.261006289307,445.54838709677415,16918.132716049382,42.484177215189874,150.3654970760234,11.175616835994196,299024.0885416667,579.0625,10146.898432174508,181.5552699228792,494.57142857142856,411.1303344867359,2355.267295597484,494.57142857142856,79091.23475609756,10146.898432174508,1005.4545454545454,5038.387096774193,191.11111111111111,9304.668674698794,19569.69230769231,8250.491573033707,466477.57812500006,3048.0,1630.1966292134832,64.01384083044982,3569.0359477124184,1901.1290322580644,3272.80163599182,9772.200520833334,2185.3410740203194,28694.0,314.3203883495146,8104.40251572327,18557.572327044025,2231.547169811321,1432.2712418300653,773.5849056603774,40735.10802469136,314.3203883495146,471.91011235955057,42.484177215189874,9194.277108433736,8173.714285714285,837.8787878787879,1604.1437908496732,22.50980392156863,14.663461538461538,663176.1973875181,2854.854368932039,428.0,43.64516129032258,1526.2857142857142,1432.2712418300653,5456.578050443081,22331.935185185182,150.3654970760234,4057.832278481013,1868.867924528302,1630.1966292134832,4723.49129172714,5038.387096774193,19364.70552147239,117.22222222222224,15110.75,4057.832278481013,83347.75862068967,7892.142857142857,19129.33333333333,1968.8904494382025,207.39062500000003,62.168396770472896,3578.592162554427,1677.5078125000002,2499.201741654572,1254.4444444444443,1236.25,3578.592162554427,1992.4528301886792,14626.074074074077,1236.25,6965.361445783132,36.37096774193548,40735.10802469136,20.098039215686278,1432.2712418300653,538.8571428571429,101149.25,93302.72435897436,181.5552699228792,28694.0,2054.328561690525,5872.384615384615,31412.04644412192,5872.384615384615,854.952380952381,31412.04644412192,1253403.0130624091,63142.24137931035,20.098039215686278,3569.0359477124184,16569.261006289307,19286.94968553459,19286.94968553459,837.8787878787879,3665.809768637532,108.64285714285714,56.07911392405064,13671.296296296296,371.2903225806451,4057.832278481013,47142.92857142857,21438.1875,54.833333333333336,26081.56401384083,2540.0,115.76923076923076,16.423076923076923,8608.709677419354,4463.780120481927,8250.491573033707,15110.75,8173.714285714285,157100.37650602407,104499.0512820513,9076.930817610064,2611.25,428.0,14.663461538461538,2694.0,3569.0359477124184,86.24595469255664,678.7183544303797,494.57142857142856,712.4603174603175,663176.1973875181,10245.87593728698,5071.786163522012,1868.867924528302,26081.56401384083,3403.344867358708,4793.829296424452,64983.42138364781,3272.80163599182,5712.33671988389,9194.277108433736,608.4770114942529,42.484177215189874,466477.57812500006,3695.444059976932,3517.9333333333334,345.6428571428572,579.0625,7892.142857142857,5038.387096774193,45601.61516853933,1992.4528301886792,2647.5,5872.384615384615,6965.361445783132,64.01384083044982,45601.61516853933,23123.5,1992.4528301886792,2540.0,9060.337370242214,14.663461538461538,3882.680981595092,36.37096774193548,193984.2734375,2231.547169811321,108.64285714285714,329.7142857142857,117.49826989619376,773.5849056603774,36.37096774193548,8608.709677419354,371.2903225806451,45481.42307692308,10245.87593728698,63142.24137931035,678.7183544303797,11555.9375,4528.380503144654,2499.201741654572,10736.622807017544,5222.5,1901.1290322580644,93302.72435897436,1702.1929824561405,114208.8534107402,343.1394601542417,10245.87593728698,8173.714285714285,140637.14285714287,132.94270833333334,579.0625,663176.1973875181,1180.5379746835442,1236.25,26081.56401384083,854.952380952381,2010.9477124183009,30698.85714285714,11.175616835994196,90677.41228070176,38128.80258899677,663176.1973875181,71138.51966292135,1236.25,31412.04644412192,50574.625,83347.75862068967,12274.009146341465,90677.41228070176,808.2857142857143,11.175616835994196,11555.9375,86.24595469255664,41871.69230769231,19129.33333333333,329.7142857142857,1236.25,2637.8993710691825,579.7777777777778,30698.85714285714,329.7142857142857,2647.5,466477.57812500006,102.64285714285714,12691.209677419354,12317.397660818711,220.55016181229772,19147.454268292684,9304.668674698794,4057.832278481013,47142.92857142857,1630.1966292134832,4463.780120481927,2002.1844660194176,5071.786163522012,7035.866666666667,13504.20634920635,21601.383647798742,10245.87593728698,10861.245674740485,176.36477987421384,12691.209677419354,1432.2712418300653,608.4770114942529,10736.622807017544,3695.444059976932,157100.37650602407,1702.1929824561405,51795.97953216374,2386.449438202247,117.22222222222224,120.98615916955016,777.0363321799308,12274.009146341465,2611.25,2242.446601941748,168764.57142857145,627.2222222222222,40608.41346153846,2002.1844660194176,157.46855345911948,2647.5,119015.43674698794,579.0625,329.7142857142857,13121.345911949686,71138.51966292135,207.39062500000003,30698.85714285714,9060.337370242214,1529.7752808988764,3071.4691011235955,46247.0,538.8571428571429,16347.42857142857,23123.5,132.94270833333334,6984.389273356401,12691.209677419354,2499.201741654572,30221.5,15229.451612903224,191.11111111111111,428.0,3578.592162554427,11715.487421383648,19129.33333333333,30221.5,6432.321799307958,41871.69230769231,42.484177215189874,16918.132716049382,3695.444059976932,13504.20634920635,15617.489114658929,117.49826989619376,16569.261006289307,96.59546925566345,678.7183544303797,10146.898432174508,678.7183544303797,131.55526992287918,47142.92857142857,79091.23475609756,736.1797752808989,1180.5379746835442,29354.53846153846,45481.42307692308,21438.1875,6965.361445783132,31412.04644412192,10146.898432174508,11791.8,15617.489114658929,1677.5078125000002,5456.578050443081,14626.074074074077,94285.85714285714,248.6394601542416,5456.578050443081,26081.56401384083,63142.24137931035,12317.397660818711,12317.397660818711,608.4770114942529,11.175616835994196,10861.245674740485,2566.6935483870966,11555.9375,678.7183544303797,1529.7752808988764,1432.2712418300653,518.4642857142858,8104.40251572327,63142.24137931035,3798.928571428572,4057.832278481013,1526.2857142857142,2854.854368932039,3403.344867358708,9306.611111111111,538.8571428571429,13671.296296296296,117.22222222222224,343.1394601542417],\"xaxis\":\"x\",\"y\":[4785.592155979585,378.9394302913829,15365.600809369838,4785.592155979585,4070.255357333951,134757.3771732384,14409.557558767568,5813.845282617682,61516.49587090381,1510.6755468797405,98738.27282520698,1684.2601507265988,1088.107600326137,647.864369876902,4263.755978756964,14168.403568365671,487.3481032940997,4287.792726026477,51779.415544128475,19934.734830917627,12141.409803201397,87747.52966460225,5818.94510670397,377.43130038813075,12797.394868451114,389.0359797814606,19263.507069512012,40163.79013647013,377.43130038813075,23840.068464074022,487.3481032940997,400.48488833402286,126112.56377290396,11223.080944977459,522.1062342859491,415175.9779430929,894.6750043489748,519.4101103091041,2138.6141864173633,19263.507069512012,2790.085070463646,2196.4290704108694,4263.755978756964,2244.514746383961,2963.0052419712265,602.4723068239653,5217.814575086902,410.08771833436947,16913.133647514336,390.2429375597753,489.6064298303322,16311.253798810561,1109.562627058514,2715.348806915594,2281.9990642021894,6353.789558047462,37073.75373050823,11896.354947646087,38882.42895370776,797.0601491084628,703.1493477716122,388.482661364839,481.8437277075316,3747.013377347842,16794.12337436424,1571.4700357476117,2257.4161561731,37073.75373050823,43146.57407027896,76460.52042984031,5813.845282617682,433.93372568151483,50253.31197444373,2781.8507793892813,2963.0052419712265,450.36262704088375,323624.6828685297,2286.347850033103,959.8167805278175,1463.9293549917084,25867.060321463283,388.50455558540244,3461.34813854915,10658.874464205835,48286.68591679114,4070.54807420052,1208.1895902424544,13863.416557439898,3913.805102706469,1545.439458867743,48286.68591679114,664.1495092797293,12797.394868451114,415.88888264864494,24308.715376885695,17460.789585310762,13878.585909613186,2138.6141864173633,3258.406190332729,5925.265679815453,1797.2371240684145,47713.131047358365,51804.0528093027,1099.5606427530397,40683.104312073505,8508.45844428347,1571.3535403980425,505.4911026847705,213132.54291644722,4263.755978756964,2570.353838719455,3926.506181336219,3317.1418553360736,54758.90192605476,20585.396531692135,377.43130038813075,406.91804655949966,120990.40817472423,4395.853637740449,19387.390319584494,2412.686086825897,19699.926384269966,12552.025861937103,7368.391717428835,390.09029678781303,2262.905415567944,11572.521113313724,56931.5110267007,15545.52362930884,1239.906233381196,17460.789585310762,1926.1552505875645,10673.158532726991,9907.699090223898,454.81639486308137,6355.618165841122,19934.734830917627,2858.35606741023,2834.2664976391197,665.469888483012,11592.823783640883,16177.143075924467,12543.725418313918,317.63569157648635,2257.4161561731,15201.471014315619,45645.23591676211,8323.62833082682,376.8168895165831,213132.54291644722,16698.569941711827,663.3571920970296,20710.099320853376,346.28222029939593,513.9332229141203,323.60536008687086,323624.6828685297,1301.212252741579,16913.133647514336,505.4911026847705,632.0783029798903,778.1760841513764,2635.5625663138067,632.0783029798903,76574.72476531091,11492.974658748008,1239.906233381196,5163.94089137517,494.71513791291954,11527.578799029196,19176.79696136619,8979.131225405608,415175.9779430929,2754.6914615362707,2567.0739167219367,378.9394302913829,3747.013377347842,2138.6141864173633,3913.805102706469,10871.491666793676,2516.669574686992,18752.149151877966,636.9170394143573,8323.62833082682,17785.632844249434,2407.321312452001,1686.4843440391876,1051.2986748894868,49435.50780135217,610.4363857799107,960.0492155948752,346.28222029939593,7141.329503589847,8187.405610699661,1148.288091129662,1686.4843440391876,324.6947768318741,320.7835266802878,639820.6545150073,3317.1418553360736,703.1493477716122,340.3871184326995,1531.8394329987157,1686.4843440391876,9236.305456033022,16619.83993793382,463.3944227377964,4287.792726026477,3364.1619784229933,2019.1544585081797,2715.348806915594,5163.94089137517,10185.63650295469,436.06660273854754,26293.480165383102,4218.370391131691,76460.52042984031,7915.880111460361,25978.487349635714,2375.22637673234,489.7605752448112,376.8168895165831,3756.2286447454576,1797.2371240684145,3060.8574440447824,1384.0395692783663,1535.8631900519042,6162.533552625073,2276.6020512892946,14800.536096877271,1535.8631900519042,7368.391717428835,340.3871184326995,40283.97684677175,325.06940680600604,1695.6732046475156,1028.5053850528614,51779.415544128475,92018.35013115287,505.4911026847705,18752.149151877966,3667.712553203018,5968.182975715722,34939.27854355379,6424.274585977019,992.354894936303,30596.638581578518,639820.6545150073,65413.624508456036,324.82371754886964,3747.013377347842,16698.569941711827,19387.390319584494,19387.390319584494,1148.288091129662,6305.285970941779,492.1628050669814,346.28222029939593,14168.403568365671,663.3571920970296,4218.370391131691,45766.272960899,37175.69967364701,369.63985077669076,22891.92825541111,2754.6914615362707,405.6544524204279,319.54816919005003,8698.931074864986,5689.033000226852,11752.173931839643,26293.480165383102,8187.405610699661,117110.65135820676,90877.76737128492,8855.335527589015,3229.016296504339,703.1493477716122,319.72742324233695,1604.2554168141014,3769.9108375996702,396.30197122916206,959.8167805278175,632.0783029798903,999.9729800621147,731503.1707028296,17075.133945943726,5082.686543693957,2107.5040932279385,22891.92825541111,4219.676962291559,5818.94510670397,61516.49587090381,3617.0754679045876,5813.845282617682,7141.329503589847,932.7370475860574,346.28222029939593,415175.9779430929,4555.635369302835,3885.8140646224183,769.1955699368395,953.6647702716481,8337.819513819635,5163.94089137517,48246.709882894014,2276.6020512892946,2886.664844581823,5968.182975715722,7368.391717428835,378.9394302913829,63573.70285359673,40074.17801737161,2276.6020512892946,3138.4978117622,8151.559019833033,319.72742324233695,2286.347850033103,340.3871184326995,172828.89934304,2407.321312452001,410.08048522816836,747.818087951304,407.0672088952674,1088.107600326137,341.517050774033,8698.931074864986,667.3272676429804,39725.42364104348,10673.158532726991,65413.624508456036,959.8167805278175,13195.11592262944,4785.592155979585,2834.2664976391197,15201.471014315619,2963.0052419712265,2425.884027039221,98738.27282520698,1997.7384398419902,115873.73047883328,489.0303345318461,10673.158532726991,9457.064447629586,137428.5757499219,449.0579108608325,1301.212252741579,1085751.3630379443,1502.4134405828336,1684.2601507265988,22891.92825541111,992.354894936303,2244.514746383961,31550.15438840883,316.0906893952995,90462.3574812228,37318.37237669782,639820.6545150073,45645.23591676211,1684.2601507265988,30596.638581578518,56931.5110267007,76460.52042984031,12141.409803201397,95635.30862544454,839.3500692330289,316.62245060704845,13195.11592262944,389.0359797814606,41802.402147140514,19263.507069512012,747.818087951304,1535.8631900519042,2790.085070463646,803.8781088470796,29908.894508749334,648.872490598366,2858.35606741023,415175.9779430929,409.7821718132848,12543.725418313918,17394.662595168105,522.1062342859491,12141.409803201397,9740.51689224938,5199.4234913728005,45766.272960899,2567.0739167219367,4831.715916465941,2417.5880406486763,5082.686543693957,3697.7340063828487,13472.111607949182,20652.753180263422,11602.10423834572,12797.394868451114,471.44073265789405,12543.725418313918,1695.6732046475156,932.7370475860574,10980.322568203586,4555.635369302835,117110.65135820676,2094.8448847085992,54758.90192605476,1826.31018819446,421.9954832228255,410.08771833436947,978.2266656454766,12141.409803201397,4796.262256309382,2236.064155834039,135924.7927322855,1041.1204286449113,40221.84276082843,2273.3851256392354,461.10966099955675,2858.35606741023,143848.60557702882,953.6647702716481,648.872490598366,12664.975331868296,45645.23591676211,489.7605752448112,29908.894508749334,8151.559019833033,2427.747778574022,2262.905415567944,23840.068464074022,866.7970691016989,8187.405610699661,26195.686125920434,449.0579108608325,6353.789558047462,12679.428015781728,2715.348806915594,15684.810589735318,12543.725418313918,504.4494927620649,703.1493477716122,4250.960414621229,11896.354947646087,25978.487349635714,15684.810589735318,5875.699548389586,41802.402147140514,347.0090494475603,16909.279272683176,4555.635369302835,15368.26716660901,16108.716384819742,407.0672088952674,16698.569941711827,388.482661364839,993.553834752781,10573.002781125038,971.4284514069652,450.36262704088375,45766.272960899,76574.72476531091,774.5157686911542,1443.732437768414,19176.79696136619,39725.42364104348,24308.715376885695,8706.164457317907,32091.295039989156,11492.974658748008,13508.076892121393,16108.716384819742,1797.2371240684145,5826.8479598607,15545.52362930884,45766.272960899,438.4351253031718,5826.8479598607,22891.92825541111,65413.624508456036,12552.025861937103,12552.025861937103,932.7370475860574,316.0906893952995,12797.394868451114,2860.1717851882872,11807.964454280216,1123.9090100996427,1913.5805743985902,1816.3382332975395,647.864369876902,8323.62833082682,65413.624508456036,5403.795319141717,4218.370391131691,1531.8394329987157,3111.527273406017,4219.676962291559,11223.080944977459,866.7970691016989,13722.736128528055,436.06660273854754,489.0303345318461],\"yaxis\":\"y\",\"type\":\"scatter\"}], {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Actual\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prediction\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"autosize\":false,\"width\":800,\"height\":500,\"title\":{\"text\":\"R2: 0.869\"}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('239a6e0b-8482-49c8-9619-ebde564b91cd'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; }); 4.2.3 Image Preprocessing \u00b6 back to top Image preprocessing is beyond the scope of this session. We cover this topic in General Applications of Neural Networks . For now, know that there is a wealth of considerations for how to handle images, and they all fit within the realm of feature engineering. 4.3 Transformed Features \u00b6 back to top Transformed features, are features that we would like to augment based on their relationship within their own distribution or to other (allegedly) independent data within our training set. e.g. we're not deriving new features based on some empirical knowledge of the data, rather we are changing them due to statistical properties that we can assess based on the data itself. 4.3.1 Skewness \u00b6 back to top Skewed data can lead to imbalances in our model prediction. Why? Skewed values in the distribution will bias the mean. When assigning weights to this input feature, therefore, the model will give preferential treatment to these values. To demonstrate, I'm going to use scipy to create some skewed data. from scipy.stats import skewnorm a = 10 x = np . linspace ( skewnorm . ppf ( 0.01 , a ), skewnorm . ppf ( 0.99 , a ), 100 ) plt . plot ( x , skewnorm . pdf ( x , a ), 'r-' , lw = 5 , alpha = 0.6 , label = 'skewnorm pdf' ) [<matplotlib.lines.Line2D at 0x7fa48d14a130>] We can now generate a random population based on this distribution r = skewnorm . rvs ( a , size = 1000 ) plt . hist ( r ) (array([113., 267., 225., 172., 116., 62., 26., 13., 2., 4.]), array([-0.19733964, 0.15303313, 0.50340589, 0.85377866, 1.20415142, 1.55452419, 1.90489696, 2.25526972, 2.60564249, 2.95601526, 3.30638802]), <BarContainer object of 10 artists>) Unskewed data will return something close to 0 from calling df.skew() . When dealing with actual data, we can use df.skew() to determine whether we should transform our data. x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) x [ 'Skewed Data' ] . skew () 0.9141902067398219 There are a handful of ways to deal with skewed data: log transform square root transform Box-Cox transform Let's try the first two print ( 'square root transformed skew: {:.4f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) print ( 'log transformed skew: {:.4f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . legend () square root transformed skew: 0.0561 log transformed skew: -1.6916 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in sqrt /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log <matplotlib.legend.Legend at 0x7fa49157d340> We see we didn't get much traction with the log transform, and the log transform will not be able to handle 0 values, and so we will sometimes have to code exceptions for those. Box-Cox is often a good route to go, but it has the added restriction that the data has to all be above 0. Let's create a new distribution with this added restriction a = 6 r = skewnorm . rvs ( a , size = 1000 ) r = [ i for i in r if i > 0 ] plt . hist ( r ) (array([220., 277., 182., 127., 66., 39., 17., 5., 4., 2.]), array([2.17150536e-03, 3.88613862e-01, 7.75056219e-01, 1.16149858e+00, 1.54794093e+00, 1.93438329e+00, 2.32082565e+00, 2.70726800e+00, 3.09371036e+00, 3.48015272e+00, 3.86659507e+00]), <BarContainer object of 10 artists>) from scipy import stats x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ], alpha = 0.5 , label = 'box-cox: {:.2f} ' . format ( pd . DataFrame ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ])[ 0 ] . skew ())) ax . legend () <matplotlib.legend.Legend at 0x7fa492058f40> \ud83c\udfcb\ufe0f Exercise 3: Transform data from a gamma distribution \u00b6 Repeat section 4.3.1, this time synthesizing a gamma distribution and transforming it. Which transformation best reduces the skew? Do this for a dataset that does not contain values at or below 0. # Cell for exercise 3 from scipy.stats import gamma <matplotlib.legend.Legend at 0x7fa49111dd30> 4.3.2 Colinearity \u00b6 back to top Colinearity can also affect the performance of your machine learning model. In particular, if features are colinear, it can be easy for your model to overfit to your training dataset. This is often mitigated by regularization. If you're curious you can read more about it on this discussion from StackExchange . We will still explore it explicitly here by calculating the Variance Inflation Factor (VIF) on some hypothetical data. VIF = \\frac{1}{1-R^2} Usually we are concerned about data with a VIF above 10 4.3.2.1 Detecting Colinearity \u00b6 back to top from statsmodels.stats.outliers_influence import variance_inflation_factor Step 1: Make some data # we can throttle the error rate random . seed ( 42 ) # x2 will be sqrt of x1 plus some error def func ( x , err ): return x ** .5 + ( err * random . randint ( - 1 , 1 ) * random . random () * x ) x0 = range ( 100 ) x1 = [ func ( i , .05 ) for i in x0 ] # HIGH degree of colinearity with x0 x2 = [ func ( i , 1 ) for i in x0 ] # MED degree of colinearity with x0 x3 = [ random . randint ( 0 , 100 ) for i in x0 ] # NO degree of colinearity with x0 # take a look fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) ax . plot ( x0 , x1 , label = 'x1' ) ax . plot ( x0 , x2 , label = 'x2' ) ax . plot ( x0 , x3 , label = 'x3' ) ax . legend () <matplotlib.legend.Legend at 0x7fa49121ca90> To calculate the colinearities I'm going to aggregate these x's into a dataframe: colin = pd . DataFrame ([ x0 , x1 , x2 , x3 ]) . T colin . columns = [ 'x0' , 'x1' , 'x2' , 'x3' ] colin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 0 0.0 0.000000 0.000000 29.0 1 1.0 1.013751 0.721523 28.0 2 2.0 1.400260 1.414214 3.0 3 3.0 1.630546 -0.438007 84.0 4 4.0 2.017388 4.304847 24.0 Step 2: Calculate VIF factors # calculate VIF factors vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( colin . values , i ) for i in range ( colin . shape [ 1 ])] vif [ \"features\" ] = colin . columns Step 3: Inspect VIF factors # inspect VIF factors display ( vif ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 12.555415 x0 1 15.823872 x1 2 1.030609 x2 3 3.559468 x3 In this case, we may remove either x0 or x1 from the dataset. 4.3.2.2 Fixing Colinearity \u00b6 back to top It is good to aknowledge where colinearity exists as this will influence the interpretability of your model. In most cases, however, it won't have a heavy influence on the performance of your model. A simple method of dealing with colinearity, is to remove the highest VIF features from your model, iteratively, assessing the performance and determining whether to keep the variable or not. Another method is to create some linear combination of the correlated variables. This is encapsulated in the section on dimensionality reduction. 4.3.3 Normalization \u00b6 back to top Many learning algorithms require zero mean and unit variance to behave optimally. Sklearn preprocessing library contains a very usefull class, StandardScaler for handling this automatically for us. from sklearn.preprocessing import StandardScaler scaler = StandardScaler () normed = scaler . fit_transform ( colin ) colin [[ 'x0' , 'x1' , 'x2' , 'x3' ]] . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'> pd . DataFrame ( normed , columns = [[ 'x0' , 'x1' , 'x2' , 'x3' ]]) . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'> \ud83c\udfcb\ufe0f Exercise 4: Normalization affect on VIF \u00b6 In the above, we saw how to scale and center variables. How does this affect VIF? Calculate the VIF for the scaled-centered data # Code Cell for Exercise 4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 3.286048 x0 1 3.296881 x1 2 1.015805 x2 3 1.035537 x3 4.3.4 Dimensionality Reduction \u00b6 back to top Dimensionality reduction is an awesome way to do feature engineering. It is very commonly used. Because it is also an unsupervised machine learning technique, we will visit this topic in that section. 4.4 Missing Data \u00b6 back to top We will often have missing data in our datasets. How do we deal with this? Let's start by making some data with missing data. We'll use a numpy nan datatype to do this from numpy import nan X = np . array ([[ nan , 0 , 3 ], [ 3 , 7 , 9 ], [ 3 , 5 , 2 ], [ 4 , nan , 6 ], [ 8 , 8 , 1 ]]) y = np . array ([ 14 , 16 , - 1 , 8 , - 5 ]) 4.4.1 Imputation \u00b6 back to top A very common strategy is to impute or fill in the missing data, based on basic statistical descriptions of the feature column (mode, mean, and median) from sklearn.impute import SimpleImputer # strategy = 'mean' will replace nan's with mean value # of the column # others are median and most_frequent (mode) imp = SimpleImputer ( strategy = 'mean' ) X2 = imp . fit_transform ( X ) X2 array([[4.5, 0. , 3. ], [3. , 7. , 9. ], [3. , 5. , 2. ], [4. , 5. , 6. ], [8. , 8. , 1. ]]) 4.4.2 Other Strategies \u00b6 back to top Depending on the severity of missing data, you will sometimes opt to remove the whole column, or perhaps apply some simple learning to fill in the missing data. This is a great article on more advanced strategies for handling missing data. References \u00b6 back to top Box Cox Multicolinearity Missing Data","title":"Feature Engineering"},{"location":"S4_Feature_Engineering/#data-science-foundations-session-4-feature-engineering","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we talked about model pipelines and conveniently began with a suitable set of input data. In the real world, this is hardly ever the case! What is constant is this: at the end of the day, our models need numbers. Not only this, but a suitable set of numbers. What does that mean? The answer to that question is the subject of our session today.","title":"Data Science Foundations  Session 4: Feature Engineering"},{"location":"S4_Feature_Engineering/#40-preparing-environment-and-importing-data","text":"back to top","title":"4.0 Preparing Environment and Importing Data"},{"location":"S4_Feature_Engineering/#401-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats as stats from scipy.stats import gamma from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor import seaborn as sns ; sns . set () from sklearn.datasets import load_iris from sklearn.metrics import mean_squared_error , r2_score","title":"4.0.1 Import Packages"},{"location":"S4_Feature_Engineering/#402-load-dataset","text":"back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) orders = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_orders.csv' ) time_cols = [ i for i in orders . columns if '/' in i ] margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 dfcat = margin . columns [: - 2 ] dfcat Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') orders . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer 1/2020 2/2020 3/2020 4/2020 5/2020 6/2020 7/2020 8/2020 9/2020 10/2020 11/2020 12/2020 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146","title":"4.0.2 Load Dataset"},{"location":"S4_Feature_Engineering/#41-categorical-features","text":"back to top At the end of the day, our algorithms operate on numerical values. How do you get from a series of string values to numerical values? margin [ 'Customer' ] . unique () array(['Slugworth', 'Perk-a-Cola', 'Fickelgruber', 'Zebrabar', \"Dandy's Candies\"], dtype=object) A naive way to do it would be to assign a number to every entry 'Slugworth' = 1 'Perk-a-Cola' = 2 'Dandy's Candies' = 3 but we would inadvertently end up with some weird mathematical relationships between these variables, e.g. Dandy's Candies - Perk-a-Cola = Slugworth (3 - 2 = 1). A work around for this is to think multi-dimensionally we express our categorical values as vectors in a hyperspace where they cannot be expressed in terms of one another, i.e. they are orthogonal 'Slugworth' = [1,0,0] 'Perk-a-Cola' = [0,1,0] 'Dandy's Candies' = [0,0,1] such a scheme, in machine learning vernacular, is termed one-hot encoding.","title":"4.1 Categorical Features"},{"location":"S4_Feature_Engineering/#411-one-hot-encoding","text":"back to top sklearn has a couple useful libraries for one-hot encoding. let's start with the OneHotEncoder class in its preprocessing library from sklearn.preprocessing import OneHotEncoder # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ 'Customer' ] . values . reshape ( - 1 , 1 ) # fit our encoder to this data enc . fit ( X_cat ) OneHotEncoder() After fitting our encoder, we can then use this object to create our training array. # as a reference here's our original data display ( X_cat [: 10 ]) print ( X_cat . shape , end = ' \\n\\n ' ) onehotlabels = enc . transform ( X_cat ) . toarray () print ( onehotlabels . shape , end = ' \\n\\n ' ) # And here is our new data onehotlabels [: 10 ] array([['Slugworth'], ['Slugworth'], ['Perk-a-Cola'], ['Fickelgruber'], ['Fickelgruber'], ['Fickelgruber'], ['Slugworth'], ['Zebrabar'], ['Slugworth'], ['Zebrabar']], dtype=object) (1668, 1) (1668, 5) array([[0., 0., 0., 1., 0.], [0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]]) We have our customer information one-hot encoded, we need to do this for all our variables and concatenate them with our regular numerical variables in our original dataframe. # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ dfcat ] . values # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [ \"KG\" ] print ( X_num . shape ) X = np . concatenate (( onehotlabels , X_num . values . reshape ( - 1 , 1 )), axis = 1 ) X . shape (1668,) (1668, 119) And now we grab our EBITDA (margin) data for prediction y = margin [ \"EBITDA/KG\" ]","title":"4.1.1 One-Hot Encoding"},{"location":"S4_Feature_Engineering/#exercise-1-create-a-simple-linear-model","text":"Using the X and y sets, use train_test_split and LinearRegression to make a baseline model based on what we've learned so far. Assess your model performance visually by plottying y_test vs y_test_pred # Cell for Exercise 1 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression [<matplotlib.lines.Line2D at 0x7fa4b01f90d0>]","title":"\ud83c\udfcb\ufe0f Exercise 1: Create a simple linear model"},{"location":"S4_Feature_Engineering/#question-1","text":"How can we assess the relative feature importance of the features in our model? We could be tempted to inspect the coefficients ( linear.coef_ ) of our model to evaluate the relative feature importance, but in order to do this our features need to be scaled (so that the relative coefficient sizes are meaningful). What other issues might there be (think categorical vs continuous variables).","title":"\ud83d\ude4b Question 1:"},{"location":"S4_Feature_Engineering/#42-derived-features","text":"back to top Can we recall an example of where we've seen this previously? That's right earlier on in our session on model selection and validation we derived some polynomial features to create our polynomial model using the linear regression class in sklearn. We actually see this a lot in engineering, where we will describe log relationships or some other transformation of the original variable. Actually let me see if I can find an example in my handy BSL... concentration profiles in continous stirred tank vs plug flow reactors. Notice the y-axis is log scale. Thanks Bird, Stewart, Lightfoot! Can we think of other examples where we would like to derive features from our input data?","title":"4.2 Derived Features"},{"location":"S4_Feature_Engineering/#421-creating-polynomials","text":"back to top Let's revisit our example from the previous session, right before we introduced Grid Search in sklearn # from Model Selection and Validation, 1.2.1 from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) in the above, we use sklearn's convenient tool, make_pipeline to join together the preprocessing tool PolynomialFeatures and the basic model LinearRegression . Let's take a look at what PolynomialFeatures does to some simple data x = np . arange ( 1 , 11 ) y = x ** 3 print ( x ) print ( y ) [ 1 2 3 4 5 6 7 8 9 10] [ 1 8 27 64 125 216 343 512 729 1000] features = PolynomialFeatures ( degree = 3 ) X2 = features . fit_transform ( x . reshape ( - 1 , 1 )) we see our new feature set contains our original features, plus new features up to the nth-degree polynomial we set when creating the features object from PolynomialFeatures print ( X2 ) [[ 1. 1. 1. 1.] [ 1. 2. 4. 8.] [ 1. 3. 9. 27.] [ 1. 4. 16. 64.] [ 1. 5. 25. 125.] [ 1. 6. 36. 216.] [ 1. 7. 49. 343.] [ 1. 8. 64. 512.] [ 1. 9. 81. 729.] [ 1. 10. 100. 1000.]] model = LinearRegression () . fit ( X2 , y ) yhat = model . predict ( X2 ) plt . scatter ( x , y ) plt . plot ( x , yhat );","title":"4.2.1 Creating Polynomials"},{"location":"S4_Feature_Engineering/#422-dealing-with-time-series","text":"back to top Often, we will be dealing with time series data, whether its data generated by machinery, reactors, or sales and customers. In the following we discuss some simple practices for dealing with time series data.","title":"4.2.2 Dealing with Time Series"},{"location":"S4_Feature_Engineering/#4221-enrichment-fast-fourier-transform","text":"Special thanks to Brian Gerwe for his contribution to this section \ud83d\udc68\u200d\ud83c\udf73 back to top Sometimes we'll want to create a more sophisticated transformation of our input data. As engineers, this can often have to do with some empirical knowledge we understand about our process. When working with equipment and machinery, we will often want to convert a signal from the time to frequency domain. Let's cover how we can do that with numpy! img src What I've drawn here in the following is called a square-wave signal t = np . linspace ( 0 , 1 , 501 ) # FFT should be given an integer number of cycles so we leave out last sample t = t [: - 1 ] f = 5 # linear frequency in Hz w = f * 2 * np . pi # radial frequency h = 4 # height of square wave amp = 4 * h / np . pi s = amp * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) # here is the call to numpy FFT F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ], d = t [ 1 ]) # reorder frequency spectrum and frequency bins with 0 Hz at the center F = np . fft . fftshift ( F ) freq = np . fft . fftshift ( freq ) # scale frequency spectrum to correct amplitude F = F / t . size # amplitudes amps = [ max ( np . sin ( w * t )), max ( np . sin ( w * t * 3 ) / 3 ), max ( np . sin ( w * t * 5 ) / 5 )] fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) ax [ 0 ] . set_xlim ( 0 , 1 ) ax [ 0 ] . set_xlabel ( 'Time (s)' ) # tells us about the amplitude of the component at the # corresponding frequency. Multiplied by two because the # signal power is split between (-) and (+) frequency branches # of FFT, but we're only visualizing the (+) branch magnitude = 2 * np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , 30 ) ax [ 1 ] . set_xlabel ( 'Frequency (Hz)' ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain')","title":"\ud83c\udf52 4.2.2.1 Enrichment: Fast Fourier Transform"},{"location":"S4_Feature_Engineering/#4222-rolling-windows","text":"back to top to see an example of this dataset in action visit this link One powerful technique for dealing with time series data, is to create a rolling window of features based on the historical data. The proper window size can usually be determined by trial and error, or constraints around access to the data itself. In the above gif, we have a window size of 7. What that means is for whatever time step units we are in (that could be minutes, days, months, etc.) we will have 7 of them included in a single instance or observation. This instance or observation is then interpreted by our model and used to assess the target value, typically the quantity in the very next time step after the window (the green bar in the gif). Let's take an example with the orders data tidy_orders = orders . melt ( id_vars = orders . columns [: 6 ], var_name = 'Date' , value_name = 'KG' ) display ( tidy_orders . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 In the next exercise, we are going to attempt to predict an order amount, based on the previous order history. We will scrub all categorical labels and only use historical amounts to inform our models. In effect the data that the model will see will look like the following: fig , ax = plt . subplots ( 3 , 2 , figsize = ( 10 , 20 )) indices = np . argwhere ( ax ) color_dict = { 0 : 'tab:blue' , 1 : 'tab:green' , 2 : 'tab:orange' , 3 : 'tab:red' , 4 : 'tab:pink' , 5 : 'tab:brown' } for index , customer in enumerate ( tidy_orders . Customer . unique ()): orders . loc [ orders . Customer == customer ] . iloc [:, 6 :] . reset_index () . T . plot ( c = color_dict [ index ], legend = False , ax = ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]]) ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]] . set_title ( customer ) What we may notice is that there is very little noise or drift in our order history, but there is certainly some periodicity. The question is can we use a linear model to predict the next order amount based on these history orders? def process_data ( Xy , time_cols = 12 , window = 3 , remove_null = False ): \"\"\" This function splits your time series data into the proper windows Parameters ---------- Xy: array The input data. If there are non-time series columns, assumes they are on the left and time columns are on the right. time_cols: int The number of time columns, default 12 window: int The time window size, default 3 Returns ------- X_: array The independent variables, includes time and non-time series columns with the new window y_: array The dependent variable, selected from the time columns at the end of the window labels: The time series labels, can be used in subsequent plot \"\"\" # separate the non-time series columns X_cat = Xy [:,: - time_cols ] # select the columns to apply the sweeping window X = Xy [:, - time_cols :] X_ = [] y = [] for i in range ( X . shape [ 1 ] - window ): # after attaching the current window to the non-time series # columns, add it to a growing list X_ . append ( np . concatenate (( X_cat , X [:, i : i + window ]), axis = 1 )) # add the next time delta after the window to the list of y # values y . append ( X [:, i + window ]) # X_ is 3D: [number of replicates from sweeping window, # length of input data, # size of new feature with categories and time] # we want to reshape X_ so that the replicates due to the sweeping window is # a part of the same dimension as the instances of the input data X_ = np . array ( X_ ) . reshape ( X . shape [ 0 ] * np . array ( X_ ) . shape [ 0 ], window + X_cat . shape [ 1 ]) y = np . array ( y ) . reshape ( X . shape [ 0 ] * np . array ( y ) . shape [ 0 ],) if remove_null : # remove training data where the target is 0 (may be unfair advantage) X_ = X_ [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] y = y [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] # create labels that show the previous month values used to train the model labels = [] for row in X_ : labels . append ( \"X: {} \" . format ( np . array2string ( row [ - window :] . astype ( float ) . round ()))) return X_ , y , labels","title":"4.2.2.2 Rolling Windows"},{"location":"S4_Feature_Engineering/#exercise-2-optimize-rolling-window-size-for-customer-forecasts","text":"For this exercise, you will use the process_data function below to help you optimize the window size for predicting the order quantity in any given month. You will train a LinearRegression model. create a model using a window size of 3 and predict the order quantity for the month immediately following the window create a model for window sizes 1-11 and report the \\(R^2\\) for each model # Cell for Exercise 2 kg_month_data = orders . values [:, 6 :] # use kg_month_data and the function process_data to create your X, y arrays # then use train_test_split to create train and test portions # USE y_test and y_pred for your actual and true test data # change only window parameter in process_data() print ( \"window R2\" ) for window in range ( 1 , 12 ): ###################### ### YOUR CODE HERE ### ###################### pass window R2 1, 0.97 2, 0.88 3, 0.80 4, 0.96 5, 0.90 6, 0.86 7, 0.61 8, 0.89 9, 0.76 10, 0.74 11, 0.87 #### RUN AFTER EXERCISE 2.2.2.2.1 #### fig = px . scatter ( x = y_test , y = y_pred , labels = { \"y\" : \"Prediction\" , \"x\" : \"Actual\" }) fig . update_layout ( autosize = False , width = 800 , height = 500 , title = 'R2: {:.3f} ' . format ( r2_score ( y_test , y_pred )) ) require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"239a6e0b-8482-49c8-9619-ebde564b91cd\")) { Plotly.newPlot( \"239a6e0b-8482-49c8-9619-ebde564b91cd\", [{\"hovertemplate\":\"Actual=%{x}<br>Prediction=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4528.380503144654,64.01384083044982,15617.489114658929,4528.380503144654,5698.392857142857,124348.89322916669,21939.111111111117,5712.33671988389,64983.42138364781,1483.5,93302.72435897436,1236.25,773.5849056603774,518.4642857142858,3798.928571428572,13671.296296296296,188.7692307692308,4057.832278481013,101149.25,14626.074074074077,12274.009146341465,141456.76315789475,4793.829296424452,74.78571428571429,10861.245674740485,86.24595469255664,19129.33333333333,39307.04113924051,74.78571428571429,46247.0,188.7692307692308,85.32258064516128,90677.41228070176,9306.611111111111,220.55016181229772,466477.57812500006,1158.125,220.55016181229772,1901.1290322580644,19129.33333333333,2637.8993710691825,1868.867924528302,3798.928571428572,2252.261437908497,5222.5,181.5552699228792,5038.387096774193,120.98615916955016,10146.898432174508,85.32258064516128,286.6666666666667,18613.222222222223,818.1891025641025,2499.201741654572,2010.9477124183009,6984.389273356401,42704.25889967639,11715.487421383648,39307.04113924051,440.859375,428.0,96.59546925566345,102.64285714285714,3569.0359477124184,13671.296296296296,773.5849056603774,2010.9477124183009,42704.25889967639,40608.41346153846,83347.75862068967,10796.31640058055,74.78571428571429,80801.72807017544,2540.0,5222.5,131.55526992287918,299024.0885416667,3882.680981595092,895.9082278481012,1180.5379746835442,15617.489114658929,85.32258064516128,6185.59509202454,16749.13157894737,47142.92857142857,3786.966463414634,936.280487804878,15244.632812500002,3272.80163599182,1271.904761904762,47142.92857142857,356.6666666666667,10861.245674740485,108.64285714285714,21438.1875,16918.132716049382,7892.142857142857,1901.1290322580644,2647.5,5038.387096774193,1677.5078125000002,39307.04113924051,51795.97953216374,916.3717948717948,41871.69230769231,12870.766853932584,1271.904761904762,181.5552699228792,211540.7575757576,3798.928571428572,2054.328561690525,3578.592162554427,2854.854368932039,51795.97953216374,11791.8,149.57142857142858,103.3653846153846,119015.43674698794,2499.201741654572,19286.94968553459,2185.3410740203194,19569.69230769231,12317.397660818711,6965.361445783132,86.24595469255664,3071.4691011235955,13867.125,50574.625,14626.074074074077,1005.4545454545454,16918.132716049382,2543.106741573034,10245.87593728698,8608.709677419354,150.3654970760234,3517.9333333333334,14626.074074074077,3177.0,2499.201741654572,345.6428571428572,10736.622807017544,13799.769319492503,12691.209677419354,11.175616835994196,2010.9477124183009,10736.622807017544,71138.51966292135,8104.40251572327,62.168396770472896,211540.7575757576,16569.261006289307,445.54838709677415,16918.132716049382,42.484177215189874,150.3654970760234,11.175616835994196,299024.0885416667,579.0625,10146.898432174508,181.5552699228792,494.57142857142856,411.1303344867359,2355.267295597484,494.57142857142856,79091.23475609756,10146.898432174508,1005.4545454545454,5038.387096774193,191.11111111111111,9304.668674698794,19569.69230769231,8250.491573033707,466477.57812500006,3048.0,1630.1966292134832,64.01384083044982,3569.0359477124184,1901.1290322580644,3272.80163599182,9772.200520833334,2185.3410740203194,28694.0,314.3203883495146,8104.40251572327,18557.572327044025,2231.547169811321,1432.2712418300653,773.5849056603774,40735.10802469136,314.3203883495146,471.91011235955057,42.484177215189874,9194.277108433736,8173.714285714285,837.8787878787879,1604.1437908496732,22.50980392156863,14.663461538461538,663176.1973875181,2854.854368932039,428.0,43.64516129032258,1526.2857142857142,1432.2712418300653,5456.578050443081,22331.935185185182,150.3654970760234,4057.832278481013,1868.867924528302,1630.1966292134832,4723.49129172714,5038.387096774193,19364.70552147239,117.22222222222224,15110.75,4057.832278481013,83347.75862068967,7892.142857142857,19129.33333333333,1968.8904494382025,207.39062500000003,62.168396770472896,3578.592162554427,1677.5078125000002,2499.201741654572,1254.4444444444443,1236.25,3578.592162554427,1992.4528301886792,14626.074074074077,1236.25,6965.361445783132,36.37096774193548,40735.10802469136,20.098039215686278,1432.2712418300653,538.8571428571429,101149.25,93302.72435897436,181.5552699228792,28694.0,2054.328561690525,5872.384615384615,31412.04644412192,5872.384615384615,854.952380952381,31412.04644412192,1253403.0130624091,63142.24137931035,20.098039215686278,3569.0359477124184,16569.261006289307,19286.94968553459,19286.94968553459,837.8787878787879,3665.809768637532,108.64285714285714,56.07911392405064,13671.296296296296,371.2903225806451,4057.832278481013,47142.92857142857,21438.1875,54.833333333333336,26081.56401384083,2540.0,115.76923076923076,16.423076923076923,8608.709677419354,4463.780120481927,8250.491573033707,15110.75,8173.714285714285,157100.37650602407,104499.0512820513,9076.930817610064,2611.25,428.0,14.663461538461538,2694.0,3569.0359477124184,86.24595469255664,678.7183544303797,494.57142857142856,712.4603174603175,663176.1973875181,10245.87593728698,5071.786163522012,1868.867924528302,26081.56401384083,3403.344867358708,4793.829296424452,64983.42138364781,3272.80163599182,5712.33671988389,9194.277108433736,608.4770114942529,42.484177215189874,466477.57812500006,3695.444059976932,3517.9333333333334,345.6428571428572,579.0625,7892.142857142857,5038.387096774193,45601.61516853933,1992.4528301886792,2647.5,5872.384615384615,6965.361445783132,64.01384083044982,45601.61516853933,23123.5,1992.4528301886792,2540.0,9060.337370242214,14.663461538461538,3882.680981595092,36.37096774193548,193984.2734375,2231.547169811321,108.64285714285714,329.7142857142857,117.49826989619376,773.5849056603774,36.37096774193548,8608.709677419354,371.2903225806451,45481.42307692308,10245.87593728698,63142.24137931035,678.7183544303797,11555.9375,4528.380503144654,2499.201741654572,10736.622807017544,5222.5,1901.1290322580644,93302.72435897436,1702.1929824561405,114208.8534107402,343.1394601542417,10245.87593728698,8173.714285714285,140637.14285714287,132.94270833333334,579.0625,663176.1973875181,1180.5379746835442,1236.25,26081.56401384083,854.952380952381,2010.9477124183009,30698.85714285714,11.175616835994196,90677.41228070176,38128.80258899677,663176.1973875181,71138.51966292135,1236.25,31412.04644412192,50574.625,83347.75862068967,12274.009146341465,90677.41228070176,808.2857142857143,11.175616835994196,11555.9375,86.24595469255664,41871.69230769231,19129.33333333333,329.7142857142857,1236.25,2637.8993710691825,579.7777777777778,30698.85714285714,329.7142857142857,2647.5,466477.57812500006,102.64285714285714,12691.209677419354,12317.397660818711,220.55016181229772,19147.454268292684,9304.668674698794,4057.832278481013,47142.92857142857,1630.1966292134832,4463.780120481927,2002.1844660194176,5071.786163522012,7035.866666666667,13504.20634920635,21601.383647798742,10245.87593728698,10861.245674740485,176.36477987421384,12691.209677419354,1432.2712418300653,608.4770114942529,10736.622807017544,3695.444059976932,157100.37650602407,1702.1929824561405,51795.97953216374,2386.449438202247,117.22222222222224,120.98615916955016,777.0363321799308,12274.009146341465,2611.25,2242.446601941748,168764.57142857145,627.2222222222222,40608.41346153846,2002.1844660194176,157.46855345911948,2647.5,119015.43674698794,579.0625,329.7142857142857,13121.345911949686,71138.51966292135,207.39062500000003,30698.85714285714,9060.337370242214,1529.7752808988764,3071.4691011235955,46247.0,538.8571428571429,16347.42857142857,23123.5,132.94270833333334,6984.389273356401,12691.209677419354,2499.201741654572,30221.5,15229.451612903224,191.11111111111111,428.0,3578.592162554427,11715.487421383648,19129.33333333333,30221.5,6432.321799307958,41871.69230769231,42.484177215189874,16918.132716049382,3695.444059976932,13504.20634920635,15617.489114658929,117.49826989619376,16569.261006289307,96.59546925566345,678.7183544303797,10146.898432174508,678.7183544303797,131.55526992287918,47142.92857142857,79091.23475609756,736.1797752808989,1180.5379746835442,29354.53846153846,45481.42307692308,21438.1875,6965.361445783132,31412.04644412192,10146.898432174508,11791.8,15617.489114658929,1677.5078125000002,5456.578050443081,14626.074074074077,94285.85714285714,248.6394601542416,5456.578050443081,26081.56401384083,63142.24137931035,12317.397660818711,12317.397660818711,608.4770114942529,11.175616835994196,10861.245674740485,2566.6935483870966,11555.9375,678.7183544303797,1529.7752808988764,1432.2712418300653,518.4642857142858,8104.40251572327,63142.24137931035,3798.928571428572,4057.832278481013,1526.2857142857142,2854.854368932039,3403.344867358708,9306.611111111111,538.8571428571429,13671.296296296296,117.22222222222224,343.1394601542417],\"xaxis\":\"x\",\"y\":[4785.592155979585,378.9394302913829,15365.600809369838,4785.592155979585,4070.255357333951,134757.3771732384,14409.557558767568,5813.845282617682,61516.49587090381,1510.6755468797405,98738.27282520698,1684.2601507265988,1088.107600326137,647.864369876902,4263.755978756964,14168.403568365671,487.3481032940997,4287.792726026477,51779.415544128475,19934.734830917627,12141.409803201397,87747.52966460225,5818.94510670397,377.43130038813075,12797.394868451114,389.0359797814606,19263.507069512012,40163.79013647013,377.43130038813075,23840.068464074022,487.3481032940997,400.48488833402286,126112.56377290396,11223.080944977459,522.1062342859491,415175.9779430929,894.6750043489748,519.4101103091041,2138.6141864173633,19263.507069512012,2790.085070463646,2196.4290704108694,4263.755978756964,2244.514746383961,2963.0052419712265,602.4723068239653,5217.814575086902,410.08771833436947,16913.133647514336,390.2429375597753,489.6064298303322,16311.253798810561,1109.562627058514,2715.348806915594,2281.9990642021894,6353.789558047462,37073.75373050823,11896.354947646087,38882.42895370776,797.0601491084628,703.1493477716122,388.482661364839,481.8437277075316,3747.013377347842,16794.12337436424,1571.4700357476117,2257.4161561731,37073.75373050823,43146.57407027896,76460.52042984031,5813.845282617682,433.93372568151483,50253.31197444373,2781.8507793892813,2963.0052419712265,450.36262704088375,323624.6828685297,2286.347850033103,959.8167805278175,1463.9293549917084,25867.060321463283,388.50455558540244,3461.34813854915,10658.874464205835,48286.68591679114,4070.54807420052,1208.1895902424544,13863.416557439898,3913.805102706469,1545.439458867743,48286.68591679114,664.1495092797293,12797.394868451114,415.88888264864494,24308.715376885695,17460.789585310762,13878.585909613186,2138.6141864173633,3258.406190332729,5925.265679815453,1797.2371240684145,47713.131047358365,51804.0528093027,1099.5606427530397,40683.104312073505,8508.45844428347,1571.3535403980425,505.4911026847705,213132.54291644722,4263.755978756964,2570.353838719455,3926.506181336219,3317.1418553360736,54758.90192605476,20585.396531692135,377.43130038813075,406.91804655949966,120990.40817472423,4395.853637740449,19387.390319584494,2412.686086825897,19699.926384269966,12552.025861937103,7368.391717428835,390.09029678781303,2262.905415567944,11572.521113313724,56931.5110267007,15545.52362930884,1239.906233381196,17460.789585310762,1926.1552505875645,10673.158532726991,9907.699090223898,454.81639486308137,6355.618165841122,19934.734830917627,2858.35606741023,2834.2664976391197,665.469888483012,11592.823783640883,16177.143075924467,12543.725418313918,317.63569157648635,2257.4161561731,15201.471014315619,45645.23591676211,8323.62833082682,376.8168895165831,213132.54291644722,16698.569941711827,663.3571920970296,20710.099320853376,346.28222029939593,513.9332229141203,323.60536008687086,323624.6828685297,1301.212252741579,16913.133647514336,505.4911026847705,632.0783029798903,778.1760841513764,2635.5625663138067,632.0783029798903,76574.72476531091,11492.974658748008,1239.906233381196,5163.94089137517,494.71513791291954,11527.578799029196,19176.79696136619,8979.131225405608,415175.9779430929,2754.6914615362707,2567.0739167219367,378.9394302913829,3747.013377347842,2138.6141864173633,3913.805102706469,10871.491666793676,2516.669574686992,18752.149151877966,636.9170394143573,8323.62833082682,17785.632844249434,2407.321312452001,1686.4843440391876,1051.2986748894868,49435.50780135217,610.4363857799107,960.0492155948752,346.28222029939593,7141.329503589847,8187.405610699661,1148.288091129662,1686.4843440391876,324.6947768318741,320.7835266802878,639820.6545150073,3317.1418553360736,703.1493477716122,340.3871184326995,1531.8394329987157,1686.4843440391876,9236.305456033022,16619.83993793382,463.3944227377964,4287.792726026477,3364.1619784229933,2019.1544585081797,2715.348806915594,5163.94089137517,10185.63650295469,436.06660273854754,26293.480165383102,4218.370391131691,76460.52042984031,7915.880111460361,25978.487349635714,2375.22637673234,489.7605752448112,376.8168895165831,3756.2286447454576,1797.2371240684145,3060.8574440447824,1384.0395692783663,1535.8631900519042,6162.533552625073,2276.6020512892946,14800.536096877271,1535.8631900519042,7368.391717428835,340.3871184326995,40283.97684677175,325.06940680600604,1695.6732046475156,1028.5053850528614,51779.415544128475,92018.35013115287,505.4911026847705,18752.149151877966,3667.712553203018,5968.182975715722,34939.27854355379,6424.274585977019,992.354894936303,30596.638581578518,639820.6545150073,65413.624508456036,324.82371754886964,3747.013377347842,16698.569941711827,19387.390319584494,19387.390319584494,1148.288091129662,6305.285970941779,492.1628050669814,346.28222029939593,14168.403568365671,663.3571920970296,4218.370391131691,45766.272960899,37175.69967364701,369.63985077669076,22891.92825541111,2754.6914615362707,405.6544524204279,319.54816919005003,8698.931074864986,5689.033000226852,11752.173931839643,26293.480165383102,8187.405610699661,117110.65135820676,90877.76737128492,8855.335527589015,3229.016296504339,703.1493477716122,319.72742324233695,1604.2554168141014,3769.9108375996702,396.30197122916206,959.8167805278175,632.0783029798903,999.9729800621147,731503.1707028296,17075.133945943726,5082.686543693957,2107.5040932279385,22891.92825541111,4219.676962291559,5818.94510670397,61516.49587090381,3617.0754679045876,5813.845282617682,7141.329503589847,932.7370475860574,346.28222029939593,415175.9779430929,4555.635369302835,3885.8140646224183,769.1955699368395,953.6647702716481,8337.819513819635,5163.94089137517,48246.709882894014,2276.6020512892946,2886.664844581823,5968.182975715722,7368.391717428835,378.9394302913829,63573.70285359673,40074.17801737161,2276.6020512892946,3138.4978117622,8151.559019833033,319.72742324233695,2286.347850033103,340.3871184326995,172828.89934304,2407.321312452001,410.08048522816836,747.818087951304,407.0672088952674,1088.107600326137,341.517050774033,8698.931074864986,667.3272676429804,39725.42364104348,10673.158532726991,65413.624508456036,959.8167805278175,13195.11592262944,4785.592155979585,2834.2664976391197,15201.471014315619,2963.0052419712265,2425.884027039221,98738.27282520698,1997.7384398419902,115873.73047883328,489.0303345318461,10673.158532726991,9457.064447629586,137428.5757499219,449.0579108608325,1301.212252741579,1085751.3630379443,1502.4134405828336,1684.2601507265988,22891.92825541111,992.354894936303,2244.514746383961,31550.15438840883,316.0906893952995,90462.3574812228,37318.37237669782,639820.6545150073,45645.23591676211,1684.2601507265988,30596.638581578518,56931.5110267007,76460.52042984031,12141.409803201397,95635.30862544454,839.3500692330289,316.62245060704845,13195.11592262944,389.0359797814606,41802.402147140514,19263.507069512012,747.818087951304,1535.8631900519042,2790.085070463646,803.8781088470796,29908.894508749334,648.872490598366,2858.35606741023,415175.9779430929,409.7821718132848,12543.725418313918,17394.662595168105,522.1062342859491,12141.409803201397,9740.51689224938,5199.4234913728005,45766.272960899,2567.0739167219367,4831.715916465941,2417.5880406486763,5082.686543693957,3697.7340063828487,13472.111607949182,20652.753180263422,11602.10423834572,12797.394868451114,471.44073265789405,12543.725418313918,1695.6732046475156,932.7370475860574,10980.322568203586,4555.635369302835,117110.65135820676,2094.8448847085992,54758.90192605476,1826.31018819446,421.9954832228255,410.08771833436947,978.2266656454766,12141.409803201397,4796.262256309382,2236.064155834039,135924.7927322855,1041.1204286449113,40221.84276082843,2273.3851256392354,461.10966099955675,2858.35606741023,143848.60557702882,953.6647702716481,648.872490598366,12664.975331868296,45645.23591676211,489.7605752448112,29908.894508749334,8151.559019833033,2427.747778574022,2262.905415567944,23840.068464074022,866.7970691016989,8187.405610699661,26195.686125920434,449.0579108608325,6353.789558047462,12679.428015781728,2715.348806915594,15684.810589735318,12543.725418313918,504.4494927620649,703.1493477716122,4250.960414621229,11896.354947646087,25978.487349635714,15684.810589735318,5875.699548389586,41802.402147140514,347.0090494475603,16909.279272683176,4555.635369302835,15368.26716660901,16108.716384819742,407.0672088952674,16698.569941711827,388.482661364839,993.553834752781,10573.002781125038,971.4284514069652,450.36262704088375,45766.272960899,76574.72476531091,774.5157686911542,1443.732437768414,19176.79696136619,39725.42364104348,24308.715376885695,8706.164457317907,32091.295039989156,11492.974658748008,13508.076892121393,16108.716384819742,1797.2371240684145,5826.8479598607,15545.52362930884,45766.272960899,438.4351253031718,5826.8479598607,22891.92825541111,65413.624508456036,12552.025861937103,12552.025861937103,932.7370475860574,316.0906893952995,12797.394868451114,2860.1717851882872,11807.964454280216,1123.9090100996427,1913.5805743985902,1816.3382332975395,647.864369876902,8323.62833082682,65413.624508456036,5403.795319141717,4218.370391131691,1531.8394329987157,3111.527273406017,4219.676962291559,11223.080944977459,866.7970691016989,13722.736128528055,436.06660273854754,489.0303345318461],\"yaxis\":\"y\",\"type\":\"scatter\"}], {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Actual\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prediction\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"autosize\":false,\"width\":800,\"height\":500,\"title\":{\"text\":\"R2: 0.869\"}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('239a6e0b-8482-49c8-9619-ebde564b91cd'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; });","title":"\ud83c\udfcb\ufe0f Exercise 2: Optimize Rolling Window Size for Customer Forecasts"},{"location":"S4_Feature_Engineering/#423-image-preprocessing","text":"back to top Image preprocessing is beyond the scope of this session. We cover this topic in General Applications of Neural Networks . For now, know that there is a wealth of considerations for how to handle images, and they all fit within the realm of feature engineering.","title":"4.2.3 Image Preprocessing"},{"location":"S4_Feature_Engineering/#43-transformed-features","text":"back to top Transformed features, are features that we would like to augment based on their relationship within their own distribution or to other (allegedly) independent data within our training set. e.g. we're not deriving new features based on some empirical knowledge of the data, rather we are changing them due to statistical properties that we can assess based on the data itself.","title":"4.3 Transformed Features"},{"location":"S4_Feature_Engineering/#431-skewness","text":"back to top Skewed data can lead to imbalances in our model prediction. Why? Skewed values in the distribution will bias the mean. When assigning weights to this input feature, therefore, the model will give preferential treatment to these values. To demonstrate, I'm going to use scipy to create some skewed data. from scipy.stats import skewnorm a = 10 x = np . linspace ( skewnorm . ppf ( 0.01 , a ), skewnorm . ppf ( 0.99 , a ), 100 ) plt . plot ( x , skewnorm . pdf ( x , a ), 'r-' , lw = 5 , alpha = 0.6 , label = 'skewnorm pdf' ) [<matplotlib.lines.Line2D at 0x7fa48d14a130>] We can now generate a random population based on this distribution r = skewnorm . rvs ( a , size = 1000 ) plt . hist ( r ) (array([113., 267., 225., 172., 116., 62., 26., 13., 2., 4.]), array([-0.19733964, 0.15303313, 0.50340589, 0.85377866, 1.20415142, 1.55452419, 1.90489696, 2.25526972, 2.60564249, 2.95601526, 3.30638802]), <BarContainer object of 10 artists>) Unskewed data will return something close to 0 from calling df.skew() . When dealing with actual data, we can use df.skew() to determine whether we should transform our data. x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) x [ 'Skewed Data' ] . skew () 0.9141902067398219 There are a handful of ways to deal with skewed data: log transform square root transform Box-Cox transform Let's try the first two print ( 'square root transformed skew: {:.4f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) print ( 'log transformed skew: {:.4f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . legend () square root transformed skew: 0.0561 log transformed skew: -1.6916 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in sqrt /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log <matplotlib.legend.Legend at 0x7fa49157d340> We see we didn't get much traction with the log transform, and the log transform will not be able to handle 0 values, and so we will sometimes have to code exceptions for those. Box-Cox is often a good route to go, but it has the added restriction that the data has to all be above 0. Let's create a new distribution with this added restriction a = 6 r = skewnorm . rvs ( a , size = 1000 ) r = [ i for i in r if i > 0 ] plt . hist ( r ) (array([220., 277., 182., 127., 66., 39., 17., 5., 4., 2.]), array([2.17150536e-03, 3.88613862e-01, 7.75056219e-01, 1.16149858e+00, 1.54794093e+00, 1.93438329e+00, 2.32082565e+00, 2.70726800e+00, 3.09371036e+00, 3.48015272e+00, 3.86659507e+00]), <BarContainer object of 10 artists>) from scipy import stats x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ], alpha = 0.5 , label = 'box-cox: {:.2f} ' . format ( pd . DataFrame ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ])[ 0 ] . skew ())) ax . legend () <matplotlib.legend.Legend at 0x7fa492058f40>","title":"4.3.1 Skewness"},{"location":"S4_Feature_Engineering/#exercise-3-transform-data-from-a-gamma-distribution","text":"Repeat section 4.3.1, this time synthesizing a gamma distribution and transforming it. Which transformation best reduces the skew? Do this for a dataset that does not contain values at or below 0. # Cell for exercise 3 from scipy.stats import gamma <matplotlib.legend.Legend at 0x7fa49111dd30>","title":"\ud83c\udfcb\ufe0f Exercise 3: Transform data from a gamma distribution"},{"location":"S4_Feature_Engineering/#432-colinearity","text":"back to top Colinearity can also affect the performance of your machine learning model. In particular, if features are colinear, it can be easy for your model to overfit to your training dataset. This is often mitigated by regularization. If you're curious you can read more about it on this discussion from StackExchange . We will still explore it explicitly here by calculating the Variance Inflation Factor (VIF) on some hypothetical data. VIF = \\frac{1}{1-R^2} Usually we are concerned about data with a VIF above 10","title":"4.3.2 Colinearity"},{"location":"S4_Feature_Engineering/#4321-detecting-colinearity","text":"back to top from statsmodels.stats.outliers_influence import variance_inflation_factor Step 1: Make some data # we can throttle the error rate random . seed ( 42 ) # x2 will be sqrt of x1 plus some error def func ( x , err ): return x ** .5 + ( err * random . randint ( - 1 , 1 ) * random . random () * x ) x0 = range ( 100 ) x1 = [ func ( i , .05 ) for i in x0 ] # HIGH degree of colinearity with x0 x2 = [ func ( i , 1 ) for i in x0 ] # MED degree of colinearity with x0 x3 = [ random . randint ( 0 , 100 ) for i in x0 ] # NO degree of colinearity with x0 # take a look fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) ax . plot ( x0 , x1 , label = 'x1' ) ax . plot ( x0 , x2 , label = 'x2' ) ax . plot ( x0 , x3 , label = 'x3' ) ax . legend () <matplotlib.legend.Legend at 0x7fa49121ca90> To calculate the colinearities I'm going to aggregate these x's into a dataframe: colin = pd . DataFrame ([ x0 , x1 , x2 , x3 ]) . T colin . columns = [ 'x0' , 'x1' , 'x2' , 'x3' ] colin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 0 0.0 0.000000 0.000000 29.0 1 1.0 1.013751 0.721523 28.0 2 2.0 1.400260 1.414214 3.0 3 3.0 1.630546 -0.438007 84.0 4 4.0 2.017388 4.304847 24.0 Step 2: Calculate VIF factors # calculate VIF factors vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( colin . values , i ) for i in range ( colin . shape [ 1 ])] vif [ \"features\" ] = colin . columns Step 3: Inspect VIF factors # inspect VIF factors display ( vif ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 12.555415 x0 1 15.823872 x1 2 1.030609 x2 3 3.559468 x3 In this case, we may remove either x0 or x1 from the dataset.","title":"4.3.2.1 Detecting Colinearity"},{"location":"S4_Feature_Engineering/#4322-fixing-colinearity","text":"back to top It is good to aknowledge where colinearity exists as this will influence the interpretability of your model. In most cases, however, it won't have a heavy influence on the performance of your model. A simple method of dealing with colinearity, is to remove the highest VIF features from your model, iteratively, assessing the performance and determining whether to keep the variable or not. Another method is to create some linear combination of the correlated variables. This is encapsulated in the section on dimensionality reduction.","title":"4.3.2.2 Fixing Colinearity"},{"location":"S4_Feature_Engineering/#433-normalization","text":"back to top Many learning algorithms require zero mean and unit variance to behave optimally. Sklearn preprocessing library contains a very usefull class, StandardScaler for handling this automatically for us. from sklearn.preprocessing import StandardScaler scaler = StandardScaler () normed = scaler . fit_transform ( colin ) colin [[ 'x0' , 'x1' , 'x2' , 'x3' ]] . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'> pd . DataFrame ( normed , columns = [[ 'x0' , 'x1' , 'x2' , 'x3' ]]) . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'>","title":"4.3.3 Normalization"},{"location":"S4_Feature_Engineering/#exercise-4-normalization-affect-on-vif","text":"In the above, we saw how to scale and center variables. How does this affect VIF? Calculate the VIF for the scaled-centered data # Code Cell for Exercise 4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 3.286048 x0 1 3.296881 x1 2 1.015805 x2 3 1.035537 x3","title":"\ud83c\udfcb\ufe0f Exercise 4: Normalization affect on VIF"},{"location":"S4_Feature_Engineering/#434-dimensionality-reduction","text":"back to top Dimensionality reduction is an awesome way to do feature engineering. It is very commonly used. Because it is also an unsupervised machine learning technique, we will visit this topic in that section.","title":"4.3.4 Dimensionality Reduction"},{"location":"S4_Feature_Engineering/#44-missing-data","text":"back to top We will often have missing data in our datasets. How do we deal with this? Let's start by making some data with missing data. We'll use a numpy nan datatype to do this from numpy import nan X = np . array ([[ nan , 0 , 3 ], [ 3 , 7 , 9 ], [ 3 , 5 , 2 ], [ 4 , nan , 6 ], [ 8 , 8 , 1 ]]) y = np . array ([ 14 , 16 , - 1 , 8 , - 5 ])","title":"4.4 Missing Data"},{"location":"S4_Feature_Engineering/#441-imputation","text":"back to top A very common strategy is to impute or fill in the missing data, based on basic statistical descriptions of the feature column (mode, mean, and median) from sklearn.impute import SimpleImputer # strategy = 'mean' will replace nan's with mean value # of the column # others are median and most_frequent (mode) imp = SimpleImputer ( strategy = 'mean' ) X2 = imp . fit_transform ( X ) X2 array([[4.5, 0. , 3. ], [3. , 7. , 9. ], [3. , 5. , 2. ], [4. , 5. , 6. ], [8. , 8. , 1. ]])","title":"4.4.1 Imputation"},{"location":"S4_Feature_Engineering/#442-other-strategies","text":"back to top Depending on the severity of missing data, you will sometimes opt to remove the whole column, or perhaps apply some simple learning to fill in the missing data. This is a great article on more advanced strategies for handling missing data.","title":"4.4.2 Other Strategies"},{"location":"S4_Feature_Engineering/#references","text":"back to top Box Cox Multicolinearity Missing Data","title":"References"},{"location":"S5_Unsupervised_Learning/","text":"Data Science Foundations Session 5: Unsupervised Learning: Clustering and Dimensionality Reduction \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we began our discussion on feature engineering and ended with a sneak peak into dimensionality reduction. This last topic deserves a whole session on its own because its use case is not limited to feature engineering! It can also be used as a tool for visualization, for noise filtering, and much more. In this session we discuss dimensionality reduction along with other unsupervised learning methods. Up until now, the only learning estimators we've looked at were supervised ones: estimators that predict labels based on training data. Here, however, we are interested in uncovering aspects of the data without reference to any known labels. The usefulness for these learners will become immediately apparent when we revist our wine quality models! 5.0 Preparing Environment and Importing Data \u00b6 back to top 5.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score , silhouette_score , calinski_harabasz_score from sklearn.mixture import GaussianMixture from sklearn.impute import SimpleImputer from scipy.spatial.distance import cdist import seaborn as sns ; sns . set () import copy 5.0.2 Load and Process Dataset \u00b6 back to top wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X_wine = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X_wine [:, 2 :] = scaler . fit_transform ( X_wine [:, 2 :]) wine = pd . DataFrame ( X_wine , columns = cols ) wine [ 'density' ] = y 5.1 Principal Component Analysis \u00b6 back to top Principle Component Analysis or PCA is one of the most wide spread implementations of dimensionality reduction. In PCA, we find the principle components, or linear recombinations of the dimensions of the data, that best explain the variance of the data. 5.1.1 The Covariance Matrix \u00b6 back to top In the cell below, we have plotted acidity and density from our familiar wine dataset. fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) wine . loc [ wine [ 'red' ] == 1 ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> X = wine . loc [ wine [ 'red' ] == 1 ][[ 'fixed acidity' , 'density' ]] . values X [: 5 ] array([[0.14156636, 0.9978 ], [0.45029132, 0.9968 ], [0.45029132, 0.997 ], [3.07445349, 0.998 ], [0.14156636, 0.9978 ]]) The principal components, are actually the eigenvectors of the covariance matrix of the standardized data. We will perform this by hand to get an understanding. First we standardize the data from sklearn.preprocessing import StandardScaler X_std = StandardScaler () . fit_transform ( X ) # note I've already done this in 5.0.2 Then we compute the covariance matrix. There is a nice demonstration of computing covariance on stats quest . The covariance can be expressed as: cov(X,Y) = \\frac{1}{n^2}\\sum\\sum(x_i - x_j)(y_i - y_j) \\;\\;\\;\\;\\;\\sf eq. 1 Every \\((x_i - x_j)(y_i - y_j)\\) is the area described by the rectangle between points \\(i\\) and \\(j\\), and we if we deem to color positive changes as red and negative ones as blue, we get a picture like the following: When we view all the pairwise interactions in aggregate we get a sense of how the areas of rectangles made by each observation influence the covariance: As a side note, the covariance term is the numerator in the pearsons correlation we covered last week: \\rho_{x,y} = \\frac{cov(X,Y)}{\\sigma_x\\sigma_y} \\;\\;\\;\\;\\;\\sf eq. 2 Extrapolating \\(Eq. 1\\) across the entire matrix, \\(X\\) of datapoints: C = \\frac{1}{n-1}(X - \\bar{X})^{T} \\cdot (X - \\bar{X}) \\;\\;\\;\\;\\;\\sf eq. 3 The covariance matrix of our wine data can be obtained from \\(Eq. 3\\): import numpy as np mean_vec = np . mean ( X_std , axis = 0 ) cov_mat = ( X_std - mean_vec ) . T . dot (( X_std - mean_vec )) / ( X_std . shape [ 0 ] - 1 ) print ( 'Covariance matrix \\n %s ' % cov_mat ) Covariance matrix [[1.00062578 0.66847772] [0.66847772 1.00062578]] 5.1.2 How Does the Covariance Matrix Relate to the New Coordinate System \u00b6 We desire a new coordinate system that has no covariance between its dimensions (thus each dimension can be sorted by explained variance to isolate key dimensions (i.e. principal components)) Because the covariance matrix in \\(Eq. 3\\) is a square matrix, we can diagonalize it; the new dimensional space whose covariance matrix is expressed by this diagonolized matrix will have the desired properties explained in point 1 (because everything off the diagonal is zero) The difficult and unintuitive part of PCA is that the vector that produces this transformation to the new coordinate space is given by the eigenvectors of \\(C\\). For those who are interested in investigating further I suggest reading this answer by amoeba and this answer by cardinal . For a more geometric explanation of the principal components checkout the grandparent, spouse, daughter parable These arguments coincide with the Spectral theorem explanation of PCA, and you can read more about it in the links provided above In 5.1.2.1-5.1.2.3 I provide a segue into deriving eigenvectors and eigenvalues, feel free to visit these foundational topics, although they are not necessary to reap the value of PCA. For this particular set of wine data, we will see that the corresponding diagonalized matrix will look like: \\begin{bmatrix} 1.67 & 0 \\\\ 0 & 0.33 \\end{bmatrix} At the end of 5.1.3 we will show that this is also the covariance matrix of our data projected into the new coordinate system! \ud83c\udf2d 5.1.2.1 Enrichment: Deriving the Eigenvectors and Eigenvalues \u00b6 The principal components are found mathematically by determining the eigenvectors of the covariance matrix and sorting them by their egienvalues, i.e. their explained variance. There is a mathematical proof 1 , 2 for why this works, but we will not cover that here. So how do we calculate eigenvalues and their correpsonding eigenvectors? This question is usually posited in the following format: A \\cdot v = \\lambda \\cdot v \\;\\;\\;\\;\\;\\sf eq. 3 In this equation A is an n-by-n matrix (our covariance matrix in this case), v is a non-zero n-by-1 vector and \u03bb is a scalar (which may be either real or complex). Any value of \u03bb for which this equation has a solution is known as an eigenvalue of the matrix A. In other words v, is an eigenvector of A if there exists a scalar value such that \\(A \\cdot v\\) and \\(\\lambda \\cdot v\\) will yield the same result In high school or college, we might've solved for these eigenvalues and eigenvectors by hand using simple, integer-bound matrices of A. Here's an example, for this matrix: \\begin{bmatrix} -6 & 3 \\\\ 4 & 5\\end{bmatrix} an eigenvector is: \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} with a corresponding eigenvalue of 6. Taking the requisite dot products for each side of eq. 3, \\(A v\\) gives us: \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} and \\(\\lambda v\\): 6 \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24\\end{bmatrix} \ud83c\udf2d 5.1.2.2: Find the Eigenvalues \u00b6 The trick that is employed to decompose these equality statements is to multiply the right hand side of eq. 3 by an identity matrix and then subtract this quantity from both sides of the equation. In the case of \\(v\\) being non-zero, this becomes the determinant : | A - \\lambda I | = 0 In the case of our simple example \\begin{vmatrix} \\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\end{vmatrix} = 0 simplifies to \\begin{vmatrix} -6-\\lambda & 3 \\\\ 4 & 5-\\lambda \\end{vmatrix} = 0 writing out the determinant (-6-\\lambda)(5-\\lambda) - 3 x 4 = 0 gives the quadratic equation \\lambda^2 + \\lambda - 42 = 0 and solving for \\(\\lambda\\) \\lambda = -7 \\space or \\space 6 from scipy.optimize import fsolve , leastsq A = np . array ([[ - 6 , 3 ], [ 4 , 5 ]]) I = np . array ([[ 1 , 0 ],[ 0 , 1 ]]) # define the determinant def det ( lamb ): \"\"\" A: the covariance matrix I: the identity matrix \"\"\" return ( A [ 0 , 0 ] - lamb ) * ( A [ 1 , 1 ] - lamb ) - ( A [ 0 , 1 ] * A [ 1 , 0 ]) root = fsolve ( det , [ - 10 , 10 ]) print ( np . isclose ( det ( root ), [ 0.0 , 0.0 ])) print ( root ) [ True True] [-7. 6.] \ud83c\udf2d 5.1.2.3: Find the Eigenvectors \u00b6 We find the eigenvector for each corresponding eigenvalue one at a time \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 6 \\begin{bmatrix} x \\\\ y \\end{bmatrix} multiplying out gives the system of equations -6x + 3y = 6x 4x + 5y = 6y bringing to the left hand side -12x + 3y = 0 4x - 1y = 0 solving for either equation yeilds \\(y = 4x\\) so the eigenvector is \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} def eig ( vec ): \"\"\" A: the covariance matrix lamb: the eigen value \"\"\" return [ A [ 0 , 0 ] * vec [ 0 ] + A [ 0 , 1 ] * vec [ 1 ] - lamb * vec [ 0 ], A [ 1 , 0 ] * vec [ 0 ] + A [ 1 , 1 ] * vec [ 1 ] - lamb * vec [ 1 ]] lamb = round ( root [ 0 ]) vector = fsolve ( eig , [ 1 , - 4 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () lamb = round ( root [ 1 ]) vector = fsolve ( eig , [ 1 , 10 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () -7 [ True True] [-3. 1.] 6 [ True True] [1. 4.] # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( A ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.9486833 -0.24253563] [ 0.31622777 -0.9701425 ]] Eigenvalues [-7. 6.] 5.1.3 Projecting onto the Principal Components \u00b6 To complete our principal component analysis, we need to project our data onto the eigenvectors of the covariance matrix. We can obtain the eigenvectors and corresponding eigenvalues using np or scipy . Here I've completed the task with np : cov_mat = np . cov ( X_std . T ) # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( cov_mat ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.70710678 -0.70710678] [ 0.70710678 -0.70710678]] Eigenvalues [0.33214806 1.6691035 ] And now we are just going to sort our vectors by their eigenvalues to get the proper order of principal components: # Make a list of (eigenvalue, eigenvector) tuples eig_pairs = [( np . abs ( eig_vals [ i ]), eig_vecs [:, i ]) for i in range ( len ( eig_vals ))] # Sort the (eigenvalue, eigenvector) tuples from high to low eig_pairs . sort ( key = lambda x : x [ 0 ], reverse = True ) # Visually confirm that the list is correctly sorted by decreasing eigenvalues print ( 'Eigenvalues in descending order:' ) for i in eig_pairs : print ( i [ 0 ]) Eigenvalues in descending order: 1.669103500110071 0.3321480643454986 eig_pairs [(1.669103500110071, array([-0.70710678, -0.70710678])), (0.3321480643454986, array([-0.70710678, 0.70710678]))] For those of us who are familiar with eigenvectors, we should recognize that they are necessarily orthogonal to one another. This is good to know from the PCA point of view, because we wouldn't want the variance along one vector to be explained by another. Let's plot these vectors along with the standardized data: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 7 )) ax . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . set_aspect ( 'equal' ) ax . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) (-2.138871623907465, 4.356979103463171) We indeed see that these vectors are orthogonal. Continuing on with our task of projecting the data onto our principal components, in order to project our data onto the PCs I'll need to reshape eig_pairs : matrix_w = np . hstack (( eig_pairs [ 0 ][ 1 ] . reshape ( 2 , 1 ), eig_pairs [ 1 ][ 1 ] . reshape ( 2 , 1 ))) print ( 'Matrix W: \\n ' , matrix_w ) Matrix W: [[-0.70710678 -0.70710678] [-0.70710678 0.70710678]] And now taking the dot product: Y = X_std . dot ( matrix_w ) plt . scatter ( Y [:, 0 ], Y [:, 1 ]) <matplotlib.collections.PathCollection at 0x7f681bdf3100> We see that our data is dispersed nicely along these PCs. Finally to tie this in with the point made at the end of 5.1.2, we see that the covariance matrix for the data in this new space is described by the diagonalized matrix of the former dimensional space: mean_vec = np . mean ( Y , axis = 0 ) cov_mat = ( Y - mean_vec ) . T . dot (( Y - mean_vec )) / ( Y . shape [ 0 ] - 1 ) cov_mat array([[1.66910350e+00, 1.76746394e-16], [1.76746394e-16, 3.32148064e-01]]) 5.1.4 Cumulative Explained Variance \u00b6 Often we will need to decide just how many principal components are enough, especially with high dimensional data containing many colinear variables. To assist with this, data scientists will plot the cumulative explained variance. The explained variance is captured by the eigenvalues (this is why we sort by the eigenvalues in the first place). tot = sum ( eig_vals ) var_exp = [( i / tot ) * 100 for i in sorted ( eig_vals , reverse = True )] cum_var_exp = np . cumsum ( var_exp ) with plt . style . context ( 'seaborn-whitegrid' ): plt . figure ( figsize = ( 7 , 4 )) plt . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'individual explained variance' ) plt . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'cumulative explained variance' ) plt . ylabel ( 'Explained variance ratio' ) plt . xlabel ( 'Principal components' ) plt . legend ( loc = 'center right' ) plt . tight_layout () 5.1.5 PCA with Scikit-Learn \u00b6 But we can avoid the fancy footwork and do all this in sklearn! from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) We see that the values we get are the same as for the hand-calculated eigenvalues and vectors print ( pca . components_ ) [[ 0.70710678 0.70710678] [ 0.70710678 -0.70710678]] And the eigenvalues are under pca.explained_variance_ print ( pca . explained_variance_ ) [1.6691035 0.33214806] 5.1.6 PCA as Dimensionality Reduction \u00b6 back to top One obvious use case for PCA, is to drop the dimensions with lowest explained variance. Continuing with our acidity/density example this will look like the following from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . set_aspect ( 'equal' ) ax [ 0 ] . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax [ 0 ] . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) ax [ 0 ] . set_ylabel ( 'Normalized density' ) ax [ 0 ] . set_xlabel ( 'Normalized acidity' ) ax [ 1 ] . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'Individual' ) ax [ 1 ] . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'Cumulative' ) ax [ 1 ] . set_ylabel ( 'Explained variance ratio' ) ax [ 1 ] . set_xlabel ( 'Principal components' ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f68189c5520> We can capture 80% of the explained variance along just the first principal component. What does this projection look like? # we set our components to 1 pca = PCA ( n_components = 1 ) pca . fit ( X_std ) # we then project the data onto the first PC # and then rebroadcast this transformation # back onto the orginal dimensions to see # what this looks like in terms of acidity/density X_pca = pca . inverse_transform ( pca . transform ( X_std )) # original data plt . scatter ( X_std [:, 0 ], X_std [:, 1 ], alpha = 0.2 ) # projected data plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.8 ) <matplotlib.collections.PathCollection at 0x7f681c292ee0> 5.1.7 PCA for visualization \u00b6 back to top For classification tasks, PCA lends itself as a useful method for seeing how the classes separate on the highest variance dimensions of the data without consideration to the classes themselves, i.e. do our classes seperate out according to the other variables in the dataset. With our wine dataset, we see that the wine types fall out nicely along the first two principal components X_std = StandardScaler () . fit_transform ( wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = wine [ 'white' ] . values , edgecolor = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC') \ud83c\udf2d 5.1.8 Enrichment: PCA as Outlier Removal and Noise Filtering \u00b6 back to top In some cases, it can be advantageous to use PCA as a method for outlier removal. There are many caveats to this that we will not discuss here. But for those who are interested, this is a great overview article. As for noise filteration, Vanderplas' DS handbook has a good section on the topic as does Guido/Muller's Intro to ML with Python by the same pusblisher. 5.1.9 PCA for Feature Engineering \u00b6 back to top Finally, PCA is actually a commonly used preprocessing technique for supervised machine learning models. In the next exercise, our goal will be to use PCA to generate a new set of features, to feed into our linear model. \ud83c\udfcb\ufe0f Exercise 1: PCA as Preprocessing for Models \u00b6 Using the wine data, select any number of the first principal components and attemp to predict density for the red wine data. Consider that the maximum number of principal components are: print ( f \"Max principal components: { X . shape [ 1 ] } \" ) # Code Cell for Exercise 1 ################################################################################ ##### CREATE A SKLEARN-PCA OBJECT, FIT AND TRANSFORM TO THE WINE DATA ########## ################################################################################ # as you do this, be sure to remove 'density' from the input features ################################################################################ ############################## UNCOMMENT THE BELOW ############################# ################################################################################ # plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2, c=wine['white'].values, # edgecolor='grey') # plt.xlabel('First PC') # plt.ylabel('Second PC') # plt.show() # model = LinearRegression() # X_train, X_test, y_train, y_test = train_test_split(X_pca, y_wine, train_size=0.8, random_state=42) # model.fit(X_train, y_train) # y_pred = model.predict(X_test) # print(r2_score(y_test, y_pred)) # print(r2_score(y_train, model.predict(X_train))) 0.9634516142421967 0.953295487875815 5.2 K-Means Clustering \u00b6 back to top We now embark on a second class of unsupervised learning techinques: clustering. The K-means algorithm works under two assumptions: * every cluster can be defined by an arithmetic mean or cluster center * each point is closer to one arithmetic center than the other centers Let's turn back to our wine dataset: X_std = StandardScaler () . fit_transform ( X_wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC') It is fairly obvious under these two dimensions that there are two clusters. The K-means algorithm automatically pics this out from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = 2 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); wa-la! Our simple unsupervised algorithm does a pretty good job of differentiating between the red and white wine clusters. 5.2.1 The Algorithm: Expectation-Maximization \u00b6 back to top We won't go too far into the implementation of the underlying algorithm here. In its basic steps, however, it performs the following: Initialize cluster centers (random guess) Then repeat: E-Step: assign points to the nearest center (arithmetic distance) M-step: set the new center point for each cluster according to the mean of it's datapoint members More information on K-means algorithm can be explored here 5.2.2 Limitations \u00b6 back to top A few brief notes on limitations: the global optimum may not be achieved (no guarantee of finding the overall best solution) the number of clusters must be guessed beforehand cluster boundaries are unavoidably linear and the cluster assignments are unavoidably circular can be slow for large datasets cluster assignments are non probabilistic 3 and 5 motivate our next section, Gaussian Mixture Models 5.2.3 Determining K with the Elbow Method \u00b6 The elbow method is a popular technique for determining the value of k . It involves looping through a range of k 's and assessing some goodness of fit metric. Intuitively, we might presume those metrics involve some measure of the distance of datapoints to their cluster centers. We have options: Distortion: distance ( typically Euclidean ) from the cluster centers averaged across the respective clusters. Inertia: the sum of squared distances of samples to their closest cluster center. Silhouette: calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of The Silhouette Coefficient for a sample is (b-a) / max(b-a) best value is 1 worst value is -1 Calinski Harabasz Score or Variance Ratio: the ratio between within-cluster dispersion and between-cluster dispersion should sound familiar to our ANOVA discussion higher is better And there are many other methods of evaluating cluster assignment performance \ud83d\ude4b\u200d\u2640\ufe0f Question 1: Comparing Metrics \u00b6 What is the primary difference between Distortion, Inertia vs Silhouette, Calinksi? distortions = [] inertias = [] silhouette = [] variance = [] for k in range ( 1 , 10 ): kmeans = KMeans ( n_clusters = k ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) labels = kmeans . labels_ distortions . append ( sum ( np . min ( cdist ( X_pca , kmeans . cluster_centers_ , 'euclidean' ), axis = 1 )) / X . shape [ 0 ]) inertias . append ( kmeans . inertia_ ) if k > 1 : silhouette . append ( silhouette_score ( X_pca , labels , metric = 'euclidean' )) variance . append ( calinski_harabasz_score ( X_pca , labels )) We note in the following plots that inertia and distortion asymptotically improve with higher k (because they are unregularized) whereas silhouette and calinski metrics are penalized for inter-cluster relatability fig , [[ ax1 , ax2 ], [ ax3 , ax4 ]] = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) ax1 . plot ( range ( 1 , 10 ), distortions ) ax2 . plot ( range ( 1 , 10 ), inertias ) ax3 . plot ( range ( 2 , 10 ), silhouette ) ax4 . plot ( range ( 2 , 10 ), variance ) [<matplotlib.lines.Line2D at 0x7f67fc5d8bb0>] kmeans = KMeans ( n_clusters = 3 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); 5.3 Gaussian Mixture Models \u00b6 back to top in the simplest case, GMMs can be used in the same way as K-means from sklearn.mixture import GaussianMixture gmm = GaussianMixture ( n_components = 2 ) . fit ( X_pca ) labels = gmm . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , alpha = 0.2 , edgecolor = 'grey' ); But because GMM is a probablistic model, we can visualize the certainty with which we assign these clusters. probs = gmm . predict_proba ( X_pca ) print ( probs [ 5 : 20 ] . round ( 3 )) [[0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.]] # convert probs to 1 dimension probs . max ( 1 ) array([1. , 0.99999994, 0.99999999, ..., 1. , 1. , 0.99999189]) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = probs . max ( 1 ), s = 40 , cmap = 'Blues' , alpha = 0.5 , edgecolor = 'grey' ); 5.3.1 Generalizing E-M for GMMs \u00b6 back to top The algorithm for GMMs is very similar to K-means, but now the EM steps are probablistic 1 2 Initialize cluster centers (random guess) Then repeat: E-Step: assign points their probability of belonging to every cluster M-step: set the new center point for each cluster according to the probabilities of all datapoints 5.3.2 GMMs as a Data Generator \u00b6 back to top One particularly fascinating application of GMMs is to use them to generate new data that is similar to the data on which they are modeled. Let's take an example. We're going to use sklearn make_circles function to create some arbitrary data that has a complex relationship along two different axes. # some helper functions borrowed from Jake Vanderplas with a few minor tweaks # https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html from matplotlib.patches import Ellipse def draw_ellipse ( position , covariance , ax = None , ** kwargs ): \"\"\"Draw an ellipse with a given position and covariance\"\"\" ax = ax or plt . gca () # Convert covariance to principal axes if covariance . shape == ( 2 , 2 ): U , s , Vt = np . linalg . svd ( covariance ) angle = np . degrees ( np . arctan2 ( U [ 1 , 0 ], U [ 0 , 0 ])) width , height = 2 * np . sqrt ( s ) else : angle = 0 width , height = 2 * np . sqrt ( covariance ) # Draw the Ellipse for nsig in range ( 1 , 4 ): ax . add_patch ( Ellipse ( position , nsig * width , nsig * height , angle , ** kwargs )) def plot_gmm ( gmm , X , label = True , ax = None , data_alpha = 1 ): ax = ax or plt . gca () labels = gmm . fit ( X ) . predict ( X ) if label : ax . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , zorder = 2 , alpha = data_alpha ) else : ax . scatter ( X [:, 0 ], X [:, 1 ], s = 40 , zorder = 2 , alpha = data_alpha ) ax . axis ( 'equal' ) w_factor = 0.2 / gmm . weights_ . max () for pos , covar , w in zip ( gmm . means_ , gmm . covariances_ , gmm . weights_ ): draw_ellipse ( pos , covar , alpha = w * w_factor ) from sklearn.datasets import make_circles as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ]); As a side note, as a clustering model, the GMM is not particularly useful: gmm2 = GaussianMixture ( n_components = 2 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm2 , X ) But if we use the GMM as a density estimator on the underlying data, rather than as a clustering algorithm, we find a completely different use case: gmm16 = GaussianMixture ( n_components = 16 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm16 , X , label = False ) Now, with the distributions drawn, we can assemble entirely new data: Xnew = gmm16 . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); 5.3.2.1 Determining the number of components \u00b6 back to top Let's think back to session 1 on model selection. How might we determine the best number of components? A couple analytic approaches that we have not much yet discussed, are the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The important thing to note from each of these is that they penalize added complexity to the models, and we would like to minimize their estimated prediction error. n_components = np . arange ( 1 , 42 ) models = [ GaussianMixture ( n , covariance_type = 'full' , random_state = 42 ) . fit ( X ) for n in n_components ] plt . plot ( n_components , [ m . bic ( X ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( X ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); plt . ylabel ( 'est. prediction error' ) Text(0, 0.5, 'est. prediction error') Both AIC and BIC capture a local minimum on the order of 6 Gaussians. This is where the model captures the overall circular modality of the data. However it totally misses the circumscribed nature of the two circles. Increasing this number to 30-40 gaussians captures this feature of the data. The AIC score reflects this while the BIC score (although captures a local minima in the area) does not define this as a global optimum. gmmNew = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmmNew , X , label = True , data_alpha = 0 ) Xnew = gmmNew . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); \ud83c\udfcb\ufe0f Exercise 2: Determine Number of Components for Circular Moons \u00b6 Repeat the above, this time using sklearn.datasets.make_moons # Code Cell for Exercise 2 from sklearn.datasets import make_moons as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) ################################################################################ ##### FIT GMM MODEL(S) TO 1-42 CLUSTER CENTERS AND RECORD THE AIC/BIC ########## ################################################################################ # uncomment these lines # plt.plot(n_components, [m.bic(X) for m in models], label='BIC') # plt.plot(n_components, [m.aic(X) for m in models], label='AIC') # plt.legend(loc='best') # plt.xlabel('n_components'); # plt.ylabel('est. prediction error') Text(0, 0.5, 'est. prediction error') gmm_moon = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm_moon , X ) Xnew = gmm_moon . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); References \u00b6 PCA \u00b6 Intuitive PCA PCA and Eigenvectors/values GMM \u00b6 GMMs Explained Derive GMM Exercise","title":"Unsupervised Learning"},{"location":"S5_Unsupervised_Learning/#data-science-foundations-session-5-unsupervised-learning-clustering-and-dimensionality-reduction","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we began our discussion on feature engineering and ended with a sneak peak into dimensionality reduction. This last topic deserves a whole session on its own because its use case is not limited to feature engineering! It can also be used as a tool for visualization, for noise filtering, and much more. In this session we discuss dimensionality reduction along with other unsupervised learning methods. Up until now, the only learning estimators we've looked at were supervised ones: estimators that predict labels based on training data. Here, however, we are interested in uncovering aspects of the data without reference to any known labels. The usefulness for these learners will become immediately apparent when we revist our wine quality models!","title":"Data Science Foundations  Session 5: Unsupervised Learning: Clustering and Dimensionality Reduction"},{"location":"S5_Unsupervised_Learning/#50-preparing-environment-and-importing-data","text":"back to top","title":"5.0 Preparing Environment and Importing Data"},{"location":"S5_Unsupervised_Learning/#501-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score , silhouette_score , calinski_harabasz_score from sklearn.mixture import GaussianMixture from sklearn.impute import SimpleImputer from scipy.spatial.distance import cdist import seaborn as sns ; sns . set () import copy","title":"5.0.1 Import Packages"},{"location":"S5_Unsupervised_Learning/#502-load-and-process-dataset","text":"back to top wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X_wine = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X_wine [:, 2 :] = scaler . fit_transform ( X_wine [:, 2 :]) wine = pd . DataFrame ( X_wine , columns = cols ) wine [ 'density' ] = y","title":"5.0.2 Load and Process Dataset"},{"location":"S5_Unsupervised_Learning/#51-principal-component-analysis","text":"back to top Principle Component Analysis or PCA is one of the most wide spread implementations of dimensionality reduction. In PCA, we find the principle components, or linear recombinations of the dimensions of the data, that best explain the variance of the data.","title":"5.1 Principal Component Analysis"},{"location":"S5_Unsupervised_Learning/#511-the-covariance-matrix","text":"back to top In the cell below, we have plotted acidity and density from our familiar wine dataset. fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) wine . loc [ wine [ 'red' ] == 1 ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> X = wine . loc [ wine [ 'red' ] == 1 ][[ 'fixed acidity' , 'density' ]] . values X [: 5 ] array([[0.14156636, 0.9978 ], [0.45029132, 0.9968 ], [0.45029132, 0.997 ], [3.07445349, 0.998 ], [0.14156636, 0.9978 ]]) The principal components, are actually the eigenvectors of the covariance matrix of the standardized data. We will perform this by hand to get an understanding. First we standardize the data from sklearn.preprocessing import StandardScaler X_std = StandardScaler () . fit_transform ( X ) # note I've already done this in 5.0.2 Then we compute the covariance matrix. There is a nice demonstration of computing covariance on stats quest . The covariance can be expressed as: cov(X,Y) = \\frac{1}{n^2}\\sum\\sum(x_i - x_j)(y_i - y_j) \\;\\;\\;\\;\\;\\sf eq. 1 Every \\((x_i - x_j)(y_i - y_j)\\) is the area described by the rectangle between points \\(i\\) and \\(j\\), and we if we deem to color positive changes as red and negative ones as blue, we get a picture like the following: When we view all the pairwise interactions in aggregate we get a sense of how the areas of rectangles made by each observation influence the covariance: As a side note, the covariance term is the numerator in the pearsons correlation we covered last week: \\rho_{x,y} = \\frac{cov(X,Y)}{\\sigma_x\\sigma_y} \\;\\;\\;\\;\\;\\sf eq. 2 Extrapolating \\(Eq. 1\\) across the entire matrix, \\(X\\) of datapoints: C = \\frac{1}{n-1}(X - \\bar{X})^{T} \\cdot (X - \\bar{X}) \\;\\;\\;\\;\\;\\sf eq. 3 The covariance matrix of our wine data can be obtained from \\(Eq. 3\\): import numpy as np mean_vec = np . mean ( X_std , axis = 0 ) cov_mat = ( X_std - mean_vec ) . T . dot (( X_std - mean_vec )) / ( X_std . shape [ 0 ] - 1 ) print ( 'Covariance matrix \\n %s ' % cov_mat ) Covariance matrix [[1.00062578 0.66847772] [0.66847772 1.00062578]]","title":"5.1.1 The Covariance Matrix"},{"location":"S5_Unsupervised_Learning/#512-how-does-the-covariance-matrix-relate-to-the-new-coordinate-system","text":"We desire a new coordinate system that has no covariance between its dimensions (thus each dimension can be sorted by explained variance to isolate key dimensions (i.e. principal components)) Because the covariance matrix in \\(Eq. 3\\) is a square matrix, we can diagonalize it; the new dimensional space whose covariance matrix is expressed by this diagonolized matrix will have the desired properties explained in point 1 (because everything off the diagonal is zero) The difficult and unintuitive part of PCA is that the vector that produces this transformation to the new coordinate space is given by the eigenvectors of \\(C\\). For those who are interested in investigating further I suggest reading this answer by amoeba and this answer by cardinal . For a more geometric explanation of the principal components checkout the grandparent, spouse, daughter parable These arguments coincide with the Spectral theorem explanation of PCA, and you can read more about it in the links provided above In 5.1.2.1-5.1.2.3 I provide a segue into deriving eigenvectors and eigenvalues, feel free to visit these foundational topics, although they are not necessary to reap the value of PCA. For this particular set of wine data, we will see that the corresponding diagonalized matrix will look like: \\begin{bmatrix} 1.67 & 0 \\\\ 0 & 0.33 \\end{bmatrix} At the end of 5.1.3 we will show that this is also the covariance matrix of our data projected into the new coordinate system!","title":"5.1.2 How Does the Covariance Matrix Relate to the New Coordinate System"},{"location":"S5_Unsupervised_Learning/#5121-enrichment-deriving-the-eigenvectors-and-eigenvalues","text":"The principal components are found mathematically by determining the eigenvectors of the covariance matrix and sorting them by their egienvalues, i.e. their explained variance. There is a mathematical proof 1 , 2 for why this works, but we will not cover that here. So how do we calculate eigenvalues and their correpsonding eigenvectors? This question is usually posited in the following format: A \\cdot v = \\lambda \\cdot v \\;\\;\\;\\;\\;\\sf eq. 3 In this equation A is an n-by-n matrix (our covariance matrix in this case), v is a non-zero n-by-1 vector and \u03bb is a scalar (which may be either real or complex). Any value of \u03bb for which this equation has a solution is known as an eigenvalue of the matrix A. In other words v, is an eigenvector of A if there exists a scalar value such that \\(A \\cdot v\\) and \\(\\lambda \\cdot v\\) will yield the same result In high school or college, we might've solved for these eigenvalues and eigenvectors by hand using simple, integer-bound matrices of A. Here's an example, for this matrix: \\begin{bmatrix} -6 & 3 \\\\ 4 & 5\\end{bmatrix} an eigenvector is: \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} with a corresponding eigenvalue of 6. Taking the requisite dot products for each side of eq. 3, \\(A v\\) gives us: \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} and \\(\\lambda v\\): 6 \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24\\end{bmatrix}","title":"\ud83c\udf2d 5.1.2.1 Enrichment: Deriving the Eigenvectors and Eigenvalues"},{"location":"S5_Unsupervised_Learning/#5122-find-the-eigenvalues","text":"The trick that is employed to decompose these equality statements is to multiply the right hand side of eq. 3 by an identity matrix and then subtract this quantity from both sides of the equation. In the case of \\(v\\) being non-zero, this becomes the determinant : | A - \\lambda I | = 0 In the case of our simple example \\begin{vmatrix} \\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\end{vmatrix} = 0 simplifies to \\begin{vmatrix} -6-\\lambda & 3 \\\\ 4 & 5-\\lambda \\end{vmatrix} = 0 writing out the determinant (-6-\\lambda)(5-\\lambda) - 3 x 4 = 0 gives the quadratic equation \\lambda^2 + \\lambda - 42 = 0 and solving for \\(\\lambda\\) \\lambda = -7 \\space or \\space 6 from scipy.optimize import fsolve , leastsq A = np . array ([[ - 6 , 3 ], [ 4 , 5 ]]) I = np . array ([[ 1 , 0 ],[ 0 , 1 ]]) # define the determinant def det ( lamb ): \"\"\" A: the covariance matrix I: the identity matrix \"\"\" return ( A [ 0 , 0 ] - lamb ) * ( A [ 1 , 1 ] - lamb ) - ( A [ 0 , 1 ] * A [ 1 , 0 ]) root = fsolve ( det , [ - 10 , 10 ]) print ( np . isclose ( det ( root ), [ 0.0 , 0.0 ])) print ( root ) [ True True] [-7. 6.]","title":"\ud83c\udf2d  5.1.2.2: Find the Eigenvalues"},{"location":"S5_Unsupervised_Learning/#5123-find-the-eigenvectors","text":"We find the eigenvector for each corresponding eigenvalue one at a time \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 6 \\begin{bmatrix} x \\\\ y \\end{bmatrix} multiplying out gives the system of equations -6x + 3y = 6x 4x + 5y = 6y bringing to the left hand side -12x + 3y = 0 4x - 1y = 0 solving for either equation yeilds \\(y = 4x\\) so the eigenvector is \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} def eig ( vec ): \"\"\" A: the covariance matrix lamb: the eigen value \"\"\" return [ A [ 0 , 0 ] * vec [ 0 ] + A [ 0 , 1 ] * vec [ 1 ] - lamb * vec [ 0 ], A [ 1 , 0 ] * vec [ 0 ] + A [ 1 , 1 ] * vec [ 1 ] - lamb * vec [ 1 ]] lamb = round ( root [ 0 ]) vector = fsolve ( eig , [ 1 , - 4 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () lamb = round ( root [ 1 ]) vector = fsolve ( eig , [ 1 , 10 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () -7 [ True True] [-3. 1.] 6 [ True True] [1. 4.] # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( A ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.9486833 -0.24253563] [ 0.31622777 -0.9701425 ]] Eigenvalues [-7. 6.]","title":"\ud83c\udf2d  5.1.2.3: Find the Eigenvectors"},{"location":"S5_Unsupervised_Learning/#513-projecting-onto-the-principal-components","text":"To complete our principal component analysis, we need to project our data onto the eigenvectors of the covariance matrix. We can obtain the eigenvectors and corresponding eigenvalues using np or scipy . Here I've completed the task with np : cov_mat = np . cov ( X_std . T ) # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( cov_mat ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.70710678 -0.70710678] [ 0.70710678 -0.70710678]] Eigenvalues [0.33214806 1.6691035 ] And now we are just going to sort our vectors by their eigenvalues to get the proper order of principal components: # Make a list of (eigenvalue, eigenvector) tuples eig_pairs = [( np . abs ( eig_vals [ i ]), eig_vecs [:, i ]) for i in range ( len ( eig_vals ))] # Sort the (eigenvalue, eigenvector) tuples from high to low eig_pairs . sort ( key = lambda x : x [ 0 ], reverse = True ) # Visually confirm that the list is correctly sorted by decreasing eigenvalues print ( 'Eigenvalues in descending order:' ) for i in eig_pairs : print ( i [ 0 ]) Eigenvalues in descending order: 1.669103500110071 0.3321480643454986 eig_pairs [(1.669103500110071, array([-0.70710678, -0.70710678])), (0.3321480643454986, array([-0.70710678, 0.70710678]))] For those of us who are familiar with eigenvectors, we should recognize that they are necessarily orthogonal to one another. This is good to know from the PCA point of view, because we wouldn't want the variance along one vector to be explained by another. Let's plot these vectors along with the standardized data: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 7 )) ax . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . set_aspect ( 'equal' ) ax . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) (-2.138871623907465, 4.356979103463171) We indeed see that these vectors are orthogonal. Continuing on with our task of projecting the data onto our principal components, in order to project our data onto the PCs I'll need to reshape eig_pairs : matrix_w = np . hstack (( eig_pairs [ 0 ][ 1 ] . reshape ( 2 , 1 ), eig_pairs [ 1 ][ 1 ] . reshape ( 2 , 1 ))) print ( 'Matrix W: \\n ' , matrix_w ) Matrix W: [[-0.70710678 -0.70710678] [-0.70710678 0.70710678]] And now taking the dot product: Y = X_std . dot ( matrix_w ) plt . scatter ( Y [:, 0 ], Y [:, 1 ]) <matplotlib.collections.PathCollection at 0x7f681bdf3100> We see that our data is dispersed nicely along these PCs. Finally to tie this in with the point made at the end of 5.1.2, we see that the covariance matrix for the data in this new space is described by the diagonalized matrix of the former dimensional space: mean_vec = np . mean ( Y , axis = 0 ) cov_mat = ( Y - mean_vec ) . T . dot (( Y - mean_vec )) / ( Y . shape [ 0 ] - 1 ) cov_mat array([[1.66910350e+00, 1.76746394e-16], [1.76746394e-16, 3.32148064e-01]])","title":"5.1.3 Projecting onto the Principal Components"},{"location":"S5_Unsupervised_Learning/#514-cumulative-explained-variance","text":"Often we will need to decide just how many principal components are enough, especially with high dimensional data containing many colinear variables. To assist with this, data scientists will plot the cumulative explained variance. The explained variance is captured by the eigenvalues (this is why we sort by the eigenvalues in the first place). tot = sum ( eig_vals ) var_exp = [( i / tot ) * 100 for i in sorted ( eig_vals , reverse = True )] cum_var_exp = np . cumsum ( var_exp ) with plt . style . context ( 'seaborn-whitegrid' ): plt . figure ( figsize = ( 7 , 4 )) plt . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'individual explained variance' ) plt . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'cumulative explained variance' ) plt . ylabel ( 'Explained variance ratio' ) plt . xlabel ( 'Principal components' ) plt . legend ( loc = 'center right' ) plt . tight_layout ()","title":"5.1.4 Cumulative Explained Variance"},{"location":"S5_Unsupervised_Learning/#515-pca-with-scikit-learn","text":"But we can avoid the fancy footwork and do all this in sklearn! from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) We see that the values we get are the same as for the hand-calculated eigenvalues and vectors print ( pca . components_ ) [[ 0.70710678 0.70710678] [ 0.70710678 -0.70710678]] And the eigenvalues are under pca.explained_variance_ print ( pca . explained_variance_ ) [1.6691035 0.33214806]","title":"5.1.5 PCA with Scikit-Learn"},{"location":"S5_Unsupervised_Learning/#516-pca-as-dimensionality-reduction","text":"back to top One obvious use case for PCA, is to drop the dimensions with lowest explained variance. Continuing with our acidity/density example this will look like the following from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . set_aspect ( 'equal' ) ax [ 0 ] . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax [ 0 ] . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) ax [ 0 ] . set_ylabel ( 'Normalized density' ) ax [ 0 ] . set_xlabel ( 'Normalized acidity' ) ax [ 1 ] . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'Individual' ) ax [ 1 ] . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'Cumulative' ) ax [ 1 ] . set_ylabel ( 'Explained variance ratio' ) ax [ 1 ] . set_xlabel ( 'Principal components' ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f68189c5520> We can capture 80% of the explained variance along just the first principal component. What does this projection look like? # we set our components to 1 pca = PCA ( n_components = 1 ) pca . fit ( X_std ) # we then project the data onto the first PC # and then rebroadcast this transformation # back onto the orginal dimensions to see # what this looks like in terms of acidity/density X_pca = pca . inverse_transform ( pca . transform ( X_std )) # original data plt . scatter ( X_std [:, 0 ], X_std [:, 1 ], alpha = 0.2 ) # projected data plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.8 ) <matplotlib.collections.PathCollection at 0x7f681c292ee0>","title":"5.1.6 PCA as Dimensionality Reduction"},{"location":"S5_Unsupervised_Learning/#517-pca-for-visualization","text":"back to top For classification tasks, PCA lends itself as a useful method for seeing how the classes separate on the highest variance dimensions of the data without consideration to the classes themselves, i.e. do our classes seperate out according to the other variables in the dataset. With our wine dataset, we see that the wine types fall out nicely along the first two principal components X_std = StandardScaler () . fit_transform ( wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = wine [ 'white' ] . values , edgecolor = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC')","title":"5.1.7 PCA for visualization"},{"location":"S5_Unsupervised_Learning/#518-enrichment-pca-as-outlier-removal-and-noise-filtering","text":"back to top In some cases, it can be advantageous to use PCA as a method for outlier removal. There are many caveats to this that we will not discuss here. But for those who are interested, this is a great overview article. As for noise filteration, Vanderplas' DS handbook has a good section on the topic as does Guido/Muller's Intro to ML with Python by the same pusblisher.","title":"\ud83c\udf2d 5.1.8 Enrichment: PCA as Outlier Removal and Noise Filtering"},{"location":"S5_Unsupervised_Learning/#519-pca-for-feature-engineering","text":"back to top Finally, PCA is actually a commonly used preprocessing technique for supervised machine learning models. In the next exercise, our goal will be to use PCA to generate a new set of features, to feed into our linear model.","title":"5.1.9 PCA for Feature Engineering"},{"location":"S5_Unsupervised_Learning/#exercise-1-pca-as-preprocessing-for-models","text":"Using the wine data, select any number of the first principal components and attemp to predict density for the red wine data. Consider that the maximum number of principal components are: print ( f \"Max principal components: { X . shape [ 1 ] } \" ) # Code Cell for Exercise 1 ################################################################################ ##### CREATE A SKLEARN-PCA OBJECT, FIT AND TRANSFORM TO THE WINE DATA ########## ################################################################################ # as you do this, be sure to remove 'density' from the input features ################################################################################ ############################## UNCOMMENT THE BELOW ############################# ################################################################################ # plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2, c=wine['white'].values, # edgecolor='grey') # plt.xlabel('First PC') # plt.ylabel('Second PC') # plt.show() # model = LinearRegression() # X_train, X_test, y_train, y_test = train_test_split(X_pca, y_wine, train_size=0.8, random_state=42) # model.fit(X_train, y_train) # y_pred = model.predict(X_test) # print(r2_score(y_test, y_pred)) # print(r2_score(y_train, model.predict(X_train))) 0.9634516142421967 0.953295487875815","title":"\ud83c\udfcb\ufe0f Exercise 1: PCA as Preprocessing for Models"},{"location":"S5_Unsupervised_Learning/#52-k-means-clustering","text":"back to top We now embark on a second class of unsupervised learning techinques: clustering. The K-means algorithm works under two assumptions: * every cluster can be defined by an arithmetic mean or cluster center * each point is closer to one arithmetic center than the other centers Let's turn back to our wine dataset: X_std = StandardScaler () . fit_transform ( X_wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC') It is fairly obvious under these two dimensions that there are two clusters. The K-means algorithm automatically pics this out from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = 2 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); wa-la! Our simple unsupervised algorithm does a pretty good job of differentiating between the red and white wine clusters.","title":"5.2 K-Means Clustering"},{"location":"S5_Unsupervised_Learning/#521-the-algorithm-expectation-maximization","text":"back to top We won't go too far into the implementation of the underlying algorithm here. In its basic steps, however, it performs the following: Initialize cluster centers (random guess) Then repeat: E-Step: assign points to the nearest center (arithmetic distance) M-step: set the new center point for each cluster according to the mean of it's datapoint members More information on K-means algorithm can be explored here","title":"5.2.1 The Algorithm: Expectation-Maximization"},{"location":"S5_Unsupervised_Learning/#522-limitations","text":"back to top A few brief notes on limitations: the global optimum may not be achieved (no guarantee of finding the overall best solution) the number of clusters must be guessed beforehand cluster boundaries are unavoidably linear and the cluster assignments are unavoidably circular can be slow for large datasets cluster assignments are non probabilistic 3 and 5 motivate our next section, Gaussian Mixture Models","title":"5.2.2 Limitations"},{"location":"S5_Unsupervised_Learning/#523-determining-k-with-the-elbow-method","text":"The elbow method is a popular technique for determining the value of k . It involves looping through a range of k 's and assessing some goodness of fit metric. Intuitively, we might presume those metrics involve some measure of the distance of datapoints to their cluster centers. We have options: Distortion: distance ( typically Euclidean ) from the cluster centers averaged across the respective clusters. Inertia: the sum of squared distances of samples to their closest cluster center. Silhouette: calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of The Silhouette Coefficient for a sample is (b-a) / max(b-a) best value is 1 worst value is -1 Calinski Harabasz Score or Variance Ratio: the ratio between within-cluster dispersion and between-cluster dispersion should sound familiar to our ANOVA discussion higher is better And there are many other methods of evaluating cluster assignment performance","title":"5.2.3 Determining K with the Elbow Method"},{"location":"S5_Unsupervised_Learning/#question-1-comparing-metrics","text":"What is the primary difference between Distortion, Inertia vs Silhouette, Calinksi? distortions = [] inertias = [] silhouette = [] variance = [] for k in range ( 1 , 10 ): kmeans = KMeans ( n_clusters = k ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) labels = kmeans . labels_ distortions . append ( sum ( np . min ( cdist ( X_pca , kmeans . cluster_centers_ , 'euclidean' ), axis = 1 )) / X . shape [ 0 ]) inertias . append ( kmeans . inertia_ ) if k > 1 : silhouette . append ( silhouette_score ( X_pca , labels , metric = 'euclidean' )) variance . append ( calinski_harabasz_score ( X_pca , labels )) We note in the following plots that inertia and distortion asymptotically improve with higher k (because they are unregularized) whereas silhouette and calinski metrics are penalized for inter-cluster relatability fig , [[ ax1 , ax2 ], [ ax3 , ax4 ]] = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) ax1 . plot ( range ( 1 , 10 ), distortions ) ax2 . plot ( range ( 1 , 10 ), inertias ) ax3 . plot ( range ( 2 , 10 ), silhouette ) ax4 . plot ( range ( 2 , 10 ), variance ) [<matplotlib.lines.Line2D at 0x7f67fc5d8bb0>] kmeans = KMeans ( n_clusters = 3 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 );","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: Comparing Metrics"},{"location":"S5_Unsupervised_Learning/#53-gaussian-mixture-models","text":"back to top in the simplest case, GMMs can be used in the same way as K-means from sklearn.mixture import GaussianMixture gmm = GaussianMixture ( n_components = 2 ) . fit ( X_pca ) labels = gmm . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , alpha = 0.2 , edgecolor = 'grey' ); But because GMM is a probablistic model, we can visualize the certainty with which we assign these clusters. probs = gmm . predict_proba ( X_pca ) print ( probs [ 5 : 20 ] . round ( 3 )) [[0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.]] # convert probs to 1 dimension probs . max ( 1 ) array([1. , 0.99999994, 0.99999999, ..., 1. , 1. , 0.99999189]) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = probs . max ( 1 ), s = 40 , cmap = 'Blues' , alpha = 0.5 , edgecolor = 'grey' );","title":"5.3 Gaussian Mixture Models"},{"location":"S5_Unsupervised_Learning/#531-generalizing-e-m-for-gmms","text":"back to top The algorithm for GMMs is very similar to K-means, but now the EM steps are probablistic 1 2 Initialize cluster centers (random guess) Then repeat: E-Step: assign points their probability of belonging to every cluster M-step: set the new center point for each cluster according to the probabilities of all datapoints","title":"5.3.1 Generalizing E-M for GMMs"},{"location":"S5_Unsupervised_Learning/#532-gmms-as-a-data-generator","text":"back to top One particularly fascinating application of GMMs is to use them to generate new data that is similar to the data on which they are modeled. Let's take an example. We're going to use sklearn make_circles function to create some arbitrary data that has a complex relationship along two different axes. # some helper functions borrowed from Jake Vanderplas with a few minor tweaks # https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html from matplotlib.patches import Ellipse def draw_ellipse ( position , covariance , ax = None , ** kwargs ): \"\"\"Draw an ellipse with a given position and covariance\"\"\" ax = ax or plt . gca () # Convert covariance to principal axes if covariance . shape == ( 2 , 2 ): U , s , Vt = np . linalg . svd ( covariance ) angle = np . degrees ( np . arctan2 ( U [ 1 , 0 ], U [ 0 , 0 ])) width , height = 2 * np . sqrt ( s ) else : angle = 0 width , height = 2 * np . sqrt ( covariance ) # Draw the Ellipse for nsig in range ( 1 , 4 ): ax . add_patch ( Ellipse ( position , nsig * width , nsig * height , angle , ** kwargs )) def plot_gmm ( gmm , X , label = True , ax = None , data_alpha = 1 ): ax = ax or plt . gca () labels = gmm . fit ( X ) . predict ( X ) if label : ax . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , zorder = 2 , alpha = data_alpha ) else : ax . scatter ( X [:, 0 ], X [:, 1 ], s = 40 , zorder = 2 , alpha = data_alpha ) ax . axis ( 'equal' ) w_factor = 0.2 / gmm . weights_ . max () for pos , covar , w in zip ( gmm . means_ , gmm . covariances_ , gmm . weights_ ): draw_ellipse ( pos , covar , alpha = w * w_factor ) from sklearn.datasets import make_circles as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ]); As a side note, as a clustering model, the GMM is not particularly useful: gmm2 = GaussianMixture ( n_components = 2 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm2 , X ) But if we use the GMM as a density estimator on the underlying data, rather than as a clustering algorithm, we find a completely different use case: gmm16 = GaussianMixture ( n_components = 16 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm16 , X , label = False ) Now, with the distributions drawn, we can assemble entirely new data: Xnew = gmm16 . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]);","title":"5.3.2 GMMs as a Data Generator"},{"location":"S5_Unsupervised_Learning/#5321-determining-the-number-of-components","text":"back to top Let's think back to session 1 on model selection. How might we determine the best number of components? A couple analytic approaches that we have not much yet discussed, are the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The important thing to note from each of these is that they penalize added complexity to the models, and we would like to minimize their estimated prediction error. n_components = np . arange ( 1 , 42 ) models = [ GaussianMixture ( n , covariance_type = 'full' , random_state = 42 ) . fit ( X ) for n in n_components ] plt . plot ( n_components , [ m . bic ( X ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( X ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); plt . ylabel ( 'est. prediction error' ) Text(0, 0.5, 'est. prediction error') Both AIC and BIC capture a local minimum on the order of 6 Gaussians. This is where the model captures the overall circular modality of the data. However it totally misses the circumscribed nature of the two circles. Increasing this number to 30-40 gaussians captures this feature of the data. The AIC score reflects this while the BIC score (although captures a local minima in the area) does not define this as a global optimum. gmmNew = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmmNew , X , label = True , data_alpha = 0 ) Xnew = gmmNew . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]);","title":"5.3.2.1 Determining the number of components"},{"location":"S5_Unsupervised_Learning/#exercise-2-determine-number-of-components-for-circular-moons","text":"Repeat the above, this time using sklearn.datasets.make_moons # Code Cell for Exercise 2 from sklearn.datasets import make_moons as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) ################################################################################ ##### FIT GMM MODEL(S) TO 1-42 CLUSTER CENTERS AND RECORD THE AIC/BIC ########## ################################################################################ # uncomment these lines # plt.plot(n_components, [m.bic(X) for m in models], label='BIC') # plt.plot(n_components, [m.aic(X) for m in models], label='AIC') # plt.legend(loc='best') # plt.xlabel('n_components'); # plt.ylabel('est. prediction error') Text(0, 0.5, 'est. prediction error') gmm_moon = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm_moon , X ) Xnew = gmm_moon . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]);","title":"\ud83c\udfcb\ufe0f Exercise 2: Determine Number of Components for Circular Moons"},{"location":"S5_Unsupervised_Learning/#references","text":"","title":"References"},{"location":"S5_Unsupervised_Learning/#pca","text":"Intuitive PCA PCA and Eigenvectors/values","title":"PCA"},{"location":"S5_Unsupervised_Learning/#gmm","text":"GMMs Explained Derive GMM Exercise","title":"GMM"},{"location":"S6_Bagging/","text":"Data Science Foundations Session 6: Bagging Decision Trees and Random Forests \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're going back to the topic of supervised learning models. These models however, belong to a special class of methods called bagging, or bootstrap aggregation. Bagging is an ensemble learning method. In this method, many weak classifiers cast their votes in a general election for the final prediction. The weak learners that random forests are made of, are called decision trees. 6.0 Preparing Environment and Importing Data \u00b6 back to top 6.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score from sklearn.model_selection import train_test_split from sklearn import metrics 6.0.2 Load Dataset \u00b6 back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) print ( margin . shape , end = ' \\n\\n ' ) display ( margin . head ()) (1668, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 We're going to recreate the same operations we employed in Session 4, Feature Engineering: # identify categorical columns cat_cols = margin . columns [: 7 ] # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ cat_cols ] # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [[ 'KG' ]] X_truf = np . concatenate (( onehotlabels , X_num . values ), axis = 1 ) # grab our y data y_truf = margin [ 'EBITDA/KG' ] . values Lastly, to create a classification task, we're going to identify high, med, and low value products: print ( 'bad less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .25 )), end = ' \\n\\n ' ) print ( 'low less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .5 )), end = ' \\n\\n ' ) print ( 'med less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .75 )), end = ' \\n\\n ' ) pd . DataFrame ( margin [ margin . columns [ - 2 ]]) . boxplot ( showfliers = False ) bad less than: 0.12 low less than: 0.22 med less than: 0.35 <AxesSubplot:> margin [ 'profitability' ] = margin [ margin . columns [ - 1 ]] . apply ( lambda x : 'bad' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .25 ) else 'low' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .50 ) else 'med' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .75 ) else 'high' ) margin [ 'profitability' ] . hist () <AxesSubplot:> class_profit = { 'bad' : 0 , 'low' : 1 , 'med' : 2 , 'high' : 3 } y_truf_class = margin [ 'profitability' ] . map ( class_profit ) . values margin [ 'profitability_encoding' ] = y_truf_class margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG profitability profitability_encoding 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 high 3 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 med 2 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 low 1 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 med 2 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 high 3 6.1 Decision Trees \u00b6 back to top In essence, a decision tree is a series of binary questions. Let's begin this discussion by talking about how we make decision trees in sklearn. 6.1.1 Creating a Decision Tree \u00b6 back to top from sklearn import tree X = [[ 0 , 0 ], [ 1 , 1 ]] y = [ 0 , 1 ] clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) After fitting the model we can use the predict method to show the output for a sample clf . predict ([[ 2. , 2. ]]) array([1]) Similar to what we saw with GMMs, we also have access to the probabilities of the outcomes: clf . predict_proba ([[ 2. , 2. ]]) array([[0., 1.]]) Let's now go on to using visual strategies to interpreting trees. 6.1.2 Interpreting a Decision Tree \u00b6 back to top Throughout today, we will discuss many ways to view both a single tree and a random forest of trees. 6.1.2.1 Node & Branch Diagram \u00b6 back to top We can visualize the decision tree: tree . plot_tree ( clf ) [Text(0.5, 0.75, 'X[1] <= 0.5\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'), Text(0.25, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'), Text(0.75, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]')] or, more prettily: import graphviz dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph The gini label, also known as Gini impurity , is a measure of how often a sample passing through the node would be incorrectly labeled if it was randomly assigned a label based on the proportion of all labels passing through the node. So it is a measure of the progress of our tree. Let's take a more complex example from sklearn.datasets import make_classification as gen X , y = gen ( random_state = 42 ) Let's inspect our generated data: print ( X . shape ) print ( y . shape ) y [: 5 ] # a binary classification (100, 20) (100,) array([0, 0, 1, 1, 0]) And now let's train our tree: clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) How do we interpret this graph? dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph Can we confirm the observations in the tree by manually inspecting X and y? y [ X [:, 10 ] < .203 ] array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) We can confirm the gini score of the top left node by hand... scr = [] for j in range ( 1000 ): y_pred = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] y_true = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] scr . append ( mean_squared_error ( y_pred , y_true )) np . mean ( scr ) 0.1091346153846154 Let's take a look at this with our truffle dataset Vary the parameter max_depth what do you notice? Does the term greedy mean anything to you? Do nodes higher in the tree change based on decisions lower in the tree? clf = tree . DecisionTreeClassifier ( max_depth = 1 ) clf . fit ( X_truf , y_truf_class ) DecisionTreeClassifier(max_depth=1) And now lets look at the graph: dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph What is X[4] ??? # It's those tasty sponge cake truffles! enc . get_feature_names_out ()[ 4 ] 'Base Cake_Sponge' This is one great aspect of decision trees, their interpretability . We will perform this analysis again, for now, let's proceed with simpler datasets while exploring the features of decision trees. 6.1.2.1 Decision Boundaries \u00b6 back to top Let's make some random blobs from sklearn.datasets import make_blobs as gen X , y = gen ( random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f850da667c0> Let's call up our Classifier again, this time setting the max_depth to two clf = tree . DecisionTreeClassifier ( max_depth = 2 , random_state = 42 ) clf = clf . fit ( X , y ) # Parameters plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) <matplotlib.collections.PathCollection at 0x7f850cc0f9d0> dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph We can see from the output of this graph, that the tree attempts to create the class boundaries as far from the cluster centers as possible. What happens when these clusters overlap? X , y = gen ( random_state = 42 , cluster_std = 3 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f850cb877f0> Let's go ahead and write our plot into a function def plot_tree ( X , clf ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return plt We see that the boundaries mislabel some points fig = plot_tree ( X , clf ) 6.1.3 Overfitting a Decision Tree \u00b6 back to top Let's increase the max_depth clf = tree . DecisionTreeClassifier ( max_depth = 5 , random_state = 42 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> What we notice is that while the model accurately predicts the training data, we see some spurious labels, noteably the trailing purple bar that extends into the otherwise green region of the data. This is a well known fact about decision trees, that they tend to overfit their training data. In fact, this is a major motivation for why decision trees, a weak classifier, are conveniently packaged into ensembles. We combine the idea of bootstrapping, with decision trees, to come up with an overall better classifier. \ud83c\udfcb\ufe0f Exercise 1: Minimize Overfitting \u00b6 Repeat 6.1.3 with different max_depth settings, also read the docstring and play with any other hyperparameters available to you. What settings do you feel minimize overfitting? The documentation for DecisionTreeClassifier may be helpful # Code Cell for 1 ################################################################################ ##### CHANGE THE HYPERPARAMETERS IN THE CALL TO DECISIONTREECLASSIFIER ######### ################################################################################ clf = tree . DecisionTreeClassifier ( random_state = 42 , max_depth = None , min_samples_split = 3 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = None , max_leaf_nodes = None , min_impurity_decrease = 0.1 , class_weight = None , ccp_alpha = 0.0 ,) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> 6.2 Random Forests and Bagging \u00b6 back to top 6.2.1 What is Bagging? \u00b6 back to top Bagging , or Bootstrap AGGregation is the process of creating subsets of your data and training separate models on them, and using the aggregate votes of the models to make a final prediction. Bootstrapping is a topic in and of itself that we will just touch on here. Without going through the statistical rigor of proof, bootstrapping, or sampling from your observations with replacement, simulates having drawn additional data from the true population. We use this method to create many new datasets that are then used to train separate learners in parallel. This overall approach is called Bagging . A Random Forest is an instance of bagging where the separate learners are decision trees. 6.2.2 Random Forests for Classification \u00b6 back to top from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier tree = DecisionTreeClassifier () bag = BaggingClassifier ( tree , n_estimators = 10 , max_samples = 0.8 , random_state = 1 ) bag . fit ( X , y ) plot_tree ( X , bag ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> In the above, we have bootstrapped by providing each individual tree with 80% of the population data. In practice, Random Forests can achieve even better results by randomizing how the individual classifiers are constructed. In fact there are many unique methods of training individual trees and you can learn more about them here . This randomness is done automatically in sklearn's RandomForestClassifier from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier ( n_estimators = 10 , random_state = 2 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> 6.2.2.1 Interpreting a Random Forest \u00b6 back to top Let's revisit our truffle dataset again, this time with random forests # fit the model clf = RandomForestClassifier ( n_estimators = 100 , min_samples_leaf = 6 ) clf = clf . fit ( X_truf , y_truf_class ) We get a fairly high accuracy when our min_samples_leaf is low and an accuracy that leaves room for improvement when min_samples_leaf is high. This indicates to us the model may be prown to overfitting if we are not careful: accuracy_score ( clf . predict ( X_truf ), y_truf_class ) 0.6127098321342925 We can grab the original feature names with get_feature_names_out() : feats = enc . get_feature_names_out () The feature importances are stored in clf.feature_importances_ . These are calculated from the Mean Decrease in Impurity or MDI also called the Gini Importance . It is the sum of the number of nodes across all trees that include the feature, weighted by the number of samples passing through the node. One downside of estimating feature importance in this way is that it doesn't play well with highly cardinal features (features with many unique values such as mailing addresses, are highly cardinal features) len ( feats ) 118 # grab feature importances imp = clf . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in clf . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ feats , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.109284 0.095848 2 Base Cake_Chiffon 0.049163 0.049299 3 Base Cake_Pound 0.041666 0.03948 4 Base Cake_Butter 0.038501 0.038294 5 Base Cake_Cheese 0.033326 0.037235 I'm going to use plotly to create this chart: px . bar ( feat [: 20 ], x = 'feature' , y = 'importance' , error_y = 'std' , title = 'Feature Importance' ) \ud83d\ude4b\u200d\u2640\ufe0f Question 1: Feature Importance and Cardinality \u00b6 How does feature importance change in the above plot when we change the minimum leaf size from 6 to 1? \ud83d\ude4b\u200d Question 2: Compare to Moods Median \u00b6 We can then go and look at the different EBITDAs when selecting for each of these features. What do you notice as the primary difference between these results and those from Session 2: Inferential Statistics Exercise 1, Part C when we ran Mood's Median test on this same data? feat . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.109284 0.095848 2 Base Cake_Chiffon 0.049163 0.049299 3 Base Cake_Pound 0.041666 0.03948 4 Base Cake_Butter 0.038501 0.038294 5 Base Cake_Cheese 0.033326 0.037235 for feature in feat . iloc [: 10 , 0 ]: group = feature . split ( '_' )[ 0 ] sel = \" \" . join ( feature . split ( '_' )[ 1 :]) pos = margin . loc [( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () neg = margin . loc [ ~ ( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () print ( group + \": \" + sel ) print ( \" \\t with: {:.2f} \" . format ( pos )) print ( \" \\t without: {:.2f} \" . format ( neg )) Base Cake: Sponge with: 0.70 without: 0.20 Base Cake: Chiffon with: 0.13 without: 0.24 Base Cake: Pound with: 0.24 without: 0.20 Base Cake: Butter with: 0.14 without: 0.26 Base Cake: Cheese with: 0.44 without: 0.21 Primary Flavor: Doughnut with: 0.38 without: 0.20 Primary Flavor: Butter Toffee with: 0.46 without: 0.21 Color Group: Olive with: 0.67 without: 0.21 Secondary Flavor: Egg Nog with: 0.23 without: 0.21 Truffle Type: Candy Outer with: 0.20 without: 0.22 6.2.3 Random Forests for Regression \u00b6 back to top from sklearn.ensemble import RandomForestRegressor clf = RandomForestRegressor ( n_estimators = 10 ) Because our labels on our blob data were numerical, we can apply and view the estimator in the same way: clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> I want to revisit a dataset we brought up in Session 2 on feature engineering: t = np . linspace ( 0 , 5 , 200 ) w = 5 h = 4 s = 4 * h / np . pi * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ]) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) # tells us about the amplitude of the component at the # corresponding frequency magnitude = np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , .15 ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain') Let's see if a random forest regression model can capture the wave behavior of the time-series data clf = RandomForestRegressor ( n_estimators = 10 ) clf . fit ( t . reshape ( - 1 , 1 ), s ) RandomForestRegressor(n_estimators=10) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . plot ( t , s ) ax . plot ( t , clf . predict ( t . reshape ( - 1 , 1 ))) [<matplotlib.lines.Line2D at 0x7f850c12adc0>] Nice! without specifying any perdiodicity, the random forest does a good job of embedding this periodicity in the final output. \ud83c\udfcb\ufe0f Exercise 2: Practice with Random Forests \u00b6 With the wine dataset: predict: density create a learning curve of train/test score vs model complexity for your random forest model(s) I have provided the cleaned dataset as well as starter code for training the model and making parity plots Do not change the following 3 cells: wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ 'density' ] = y model = RandomForestRegressor ( n_estimators = 100 , criterion = 'squared_error' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , bootstrap = True , oob_score = False , n_jobs = None , random_state = None , verbose = 0 , warm_start = False , ccp_alpha = 0.0 , max_samples = None ,) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax . set_ylabel ( 'Predicted' ) ax . set_xlabel ( 'Actual' ) ax_ . set_xlabel ( 'Actual' ) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.973') Compare these results with our linear model from Lab 3. Recall that we can quickly grab the names of the paramters in our sklearn model: RandomForestRegressor () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Cell for Exercise 2 <matplotlib.legend.Legend at 0x7f1fe5063f40>","title":"Bagging"},{"location":"S6_Bagging/#data-science-foundations-session-6-bagging-decision-trees-and-random-forests","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're going back to the topic of supervised learning models. These models however, belong to a special class of methods called bagging, or bootstrap aggregation. Bagging is an ensemble learning method. In this method, many weak classifiers cast their votes in a general election for the final prediction. The weak learners that random forests are made of, are called decision trees.","title":"Data Science Foundations  Session 6: Bagging  Decision Trees and Random Forests"},{"location":"S6_Bagging/#60-preparing-environment-and-importing-data","text":"back to top","title":"6.0 Preparing Environment and Importing Data"},{"location":"S6_Bagging/#601-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score from sklearn.model_selection import train_test_split from sklearn import metrics","title":"6.0.1 Import Packages"},{"location":"S6_Bagging/#602-load-dataset","text":"back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) print ( margin . shape , end = ' \\n\\n ' ) display ( margin . head ()) (1668, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 We're going to recreate the same operations we employed in Session 4, Feature Engineering: # identify categorical columns cat_cols = margin . columns [: 7 ] # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ cat_cols ] # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [[ 'KG' ]] X_truf = np . concatenate (( onehotlabels , X_num . values ), axis = 1 ) # grab our y data y_truf = margin [ 'EBITDA/KG' ] . values Lastly, to create a classification task, we're going to identify high, med, and low value products: print ( 'bad less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .25 )), end = ' \\n\\n ' ) print ( 'low less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .5 )), end = ' \\n\\n ' ) print ( 'med less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .75 )), end = ' \\n\\n ' ) pd . DataFrame ( margin [ margin . columns [ - 2 ]]) . boxplot ( showfliers = False ) bad less than: 0.12 low less than: 0.22 med less than: 0.35 <AxesSubplot:> margin [ 'profitability' ] = margin [ margin . columns [ - 1 ]] . apply ( lambda x : 'bad' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .25 ) else 'low' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .50 ) else 'med' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .75 ) else 'high' ) margin [ 'profitability' ] . hist () <AxesSubplot:> class_profit = { 'bad' : 0 , 'low' : 1 , 'med' : 2 , 'high' : 3 } y_truf_class = margin [ 'profitability' ] . map ( class_profit ) . values margin [ 'profitability_encoding' ] = y_truf_class margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG profitability profitability_encoding 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 high 3 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 med 2 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 low 1 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 med 2 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 high 3","title":"6.0.2 Load Dataset"},{"location":"S6_Bagging/#61-decision-trees","text":"back to top In essence, a decision tree is a series of binary questions. Let's begin this discussion by talking about how we make decision trees in sklearn.","title":"6.1 Decision Trees"},{"location":"S6_Bagging/#611-creating-a-decision-tree","text":"back to top from sklearn import tree X = [[ 0 , 0 ], [ 1 , 1 ]] y = [ 0 , 1 ] clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) After fitting the model we can use the predict method to show the output for a sample clf . predict ([[ 2. , 2. ]]) array([1]) Similar to what we saw with GMMs, we also have access to the probabilities of the outcomes: clf . predict_proba ([[ 2. , 2. ]]) array([[0., 1.]]) Let's now go on to using visual strategies to interpreting trees.","title":"6.1.1 Creating a Decision Tree"},{"location":"S6_Bagging/#612-interpreting-a-decision-tree","text":"back to top Throughout today, we will discuss many ways to view both a single tree and a random forest of trees.","title":"6.1.2 Interpreting a Decision Tree"},{"location":"S6_Bagging/#6121-node-branch-diagram","text":"back to top We can visualize the decision tree: tree . plot_tree ( clf ) [Text(0.5, 0.75, 'X[1] <= 0.5\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'), Text(0.25, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'), Text(0.75, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]')] or, more prettily: import graphviz dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph The gini label, also known as Gini impurity , is a measure of how often a sample passing through the node would be incorrectly labeled if it was randomly assigned a label based on the proportion of all labels passing through the node. So it is a measure of the progress of our tree. Let's take a more complex example from sklearn.datasets import make_classification as gen X , y = gen ( random_state = 42 ) Let's inspect our generated data: print ( X . shape ) print ( y . shape ) y [: 5 ] # a binary classification (100, 20) (100,) array([0, 0, 1, 1, 0]) And now let's train our tree: clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) How do we interpret this graph? dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph Can we confirm the observations in the tree by manually inspecting X and y? y [ X [:, 10 ] < .203 ] array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) We can confirm the gini score of the top left node by hand... scr = [] for j in range ( 1000 ): y_pred = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] y_true = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] scr . append ( mean_squared_error ( y_pred , y_true )) np . mean ( scr ) 0.1091346153846154 Let's take a look at this with our truffle dataset Vary the parameter max_depth what do you notice? Does the term greedy mean anything to you? Do nodes higher in the tree change based on decisions lower in the tree? clf = tree . DecisionTreeClassifier ( max_depth = 1 ) clf . fit ( X_truf , y_truf_class ) DecisionTreeClassifier(max_depth=1) And now lets look at the graph: dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph What is X[4] ??? # It's those tasty sponge cake truffles! enc . get_feature_names_out ()[ 4 ] 'Base Cake_Sponge' This is one great aspect of decision trees, their interpretability . We will perform this analysis again, for now, let's proceed with simpler datasets while exploring the features of decision trees.","title":"6.1.2.1 Node &amp; Branch Diagram"},{"location":"S6_Bagging/#6121-decision-boundaries","text":"back to top Let's make some random blobs from sklearn.datasets import make_blobs as gen X , y = gen ( random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f850da667c0> Let's call up our Classifier again, this time setting the max_depth to two clf = tree . DecisionTreeClassifier ( max_depth = 2 , random_state = 42 ) clf = clf . fit ( X , y ) # Parameters plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) <matplotlib.collections.PathCollection at 0x7f850cc0f9d0> dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph We can see from the output of this graph, that the tree attempts to create the class boundaries as far from the cluster centers as possible. What happens when these clusters overlap? X , y = gen ( random_state = 42 , cluster_std = 3 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f850cb877f0> Let's go ahead and write our plot into a function def plot_tree ( X , clf ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return plt We see that the boundaries mislabel some points fig = plot_tree ( X , clf )","title":"6.1.2.1 Decision Boundaries"},{"location":"S6_Bagging/#613-overfitting-a-decision-tree","text":"back to top Let's increase the max_depth clf = tree . DecisionTreeClassifier ( max_depth = 5 , random_state = 42 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> What we notice is that while the model accurately predicts the training data, we see some spurious labels, noteably the trailing purple bar that extends into the otherwise green region of the data. This is a well known fact about decision trees, that they tend to overfit their training data. In fact, this is a major motivation for why decision trees, a weak classifier, are conveniently packaged into ensembles. We combine the idea of bootstrapping, with decision trees, to come up with an overall better classifier.","title":"6.1.3 Overfitting a Decision Tree"},{"location":"S6_Bagging/#exercise-1-minimize-overfitting","text":"Repeat 6.1.3 with different max_depth settings, also read the docstring and play with any other hyperparameters available to you. What settings do you feel minimize overfitting? The documentation for DecisionTreeClassifier may be helpful # Code Cell for 1 ################################################################################ ##### CHANGE THE HYPERPARAMETERS IN THE CALL TO DECISIONTREECLASSIFIER ######### ################################################################################ clf = tree . DecisionTreeClassifier ( random_state = 42 , max_depth = None , min_samples_split = 3 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = None , max_leaf_nodes = None , min_impurity_decrease = 0.1 , class_weight = None , ccp_alpha = 0.0 ,) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'>","title":"\ud83c\udfcb\ufe0f Exercise 1: Minimize Overfitting"},{"location":"S6_Bagging/#62-random-forests-and-bagging","text":"back to top","title":"6.2 Random Forests and Bagging"},{"location":"S6_Bagging/#621-what-is-bagging","text":"back to top Bagging , or Bootstrap AGGregation is the process of creating subsets of your data and training separate models on them, and using the aggregate votes of the models to make a final prediction. Bootstrapping is a topic in and of itself that we will just touch on here. Without going through the statistical rigor of proof, bootstrapping, or sampling from your observations with replacement, simulates having drawn additional data from the true population. We use this method to create many new datasets that are then used to train separate learners in parallel. This overall approach is called Bagging . A Random Forest is an instance of bagging where the separate learners are decision trees.","title":"6.2.1 What is Bagging?"},{"location":"S6_Bagging/#622-random-forests-for-classification","text":"back to top from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier tree = DecisionTreeClassifier () bag = BaggingClassifier ( tree , n_estimators = 10 , max_samples = 0.8 , random_state = 1 ) bag . fit ( X , y ) plot_tree ( X , bag ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> In the above, we have bootstrapped by providing each individual tree with 80% of the population data. In practice, Random Forests can achieve even better results by randomizing how the individual classifiers are constructed. In fact there are many unique methods of training individual trees and you can learn more about them here . This randomness is done automatically in sklearn's RandomForestClassifier from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier ( n_estimators = 10 , random_state = 2 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'>","title":"6.2.2 Random Forests for Classification"},{"location":"S6_Bagging/#6221-interpreting-a-random-forest","text":"back to top Let's revisit our truffle dataset again, this time with random forests # fit the model clf = RandomForestClassifier ( n_estimators = 100 , min_samples_leaf = 6 ) clf = clf . fit ( X_truf , y_truf_class ) We get a fairly high accuracy when our min_samples_leaf is low and an accuracy that leaves room for improvement when min_samples_leaf is high. This indicates to us the model may be prown to overfitting if we are not careful: accuracy_score ( clf . predict ( X_truf ), y_truf_class ) 0.6127098321342925 We can grab the original feature names with get_feature_names_out() : feats = enc . get_feature_names_out () The feature importances are stored in clf.feature_importances_ . These are calculated from the Mean Decrease in Impurity or MDI also called the Gini Importance . It is the sum of the number of nodes across all trees that include the feature, weighted by the number of samples passing through the node. One downside of estimating feature importance in this way is that it doesn't play well with highly cardinal features (features with many unique values such as mailing addresses, are highly cardinal features) len ( feats ) 118 # grab feature importances imp = clf . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in clf . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ feats , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.109284 0.095848 2 Base Cake_Chiffon 0.049163 0.049299 3 Base Cake_Pound 0.041666 0.03948 4 Base Cake_Butter 0.038501 0.038294 5 Base Cake_Cheese 0.033326 0.037235 I'm going to use plotly to create this chart: px . bar ( feat [: 20 ], x = 'feature' , y = 'importance' , error_y = 'std' , title = 'Feature Importance' )","title":"6.2.2.1 Interpreting a Random Forest"},{"location":"S6_Bagging/#question-1-feature-importance-and-cardinality","text":"How does feature importance change in the above plot when we change the minimum leaf size from 6 to 1?","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: Feature Importance and Cardinality"},{"location":"S6_Bagging/#question-2-compare-to-moods-median","text":"We can then go and look at the different EBITDAs when selecting for each of these features. What do you notice as the primary difference between these results and those from Session 2: Inferential Statistics Exercise 1, Part C when we ran Mood's Median test on this same data? feat . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.109284 0.095848 2 Base Cake_Chiffon 0.049163 0.049299 3 Base Cake_Pound 0.041666 0.03948 4 Base Cake_Butter 0.038501 0.038294 5 Base Cake_Cheese 0.033326 0.037235 for feature in feat . iloc [: 10 , 0 ]: group = feature . split ( '_' )[ 0 ] sel = \" \" . join ( feature . split ( '_' )[ 1 :]) pos = margin . loc [( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () neg = margin . loc [ ~ ( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () print ( group + \": \" + sel ) print ( \" \\t with: {:.2f} \" . format ( pos )) print ( \" \\t without: {:.2f} \" . format ( neg )) Base Cake: Sponge with: 0.70 without: 0.20 Base Cake: Chiffon with: 0.13 without: 0.24 Base Cake: Pound with: 0.24 without: 0.20 Base Cake: Butter with: 0.14 without: 0.26 Base Cake: Cheese with: 0.44 without: 0.21 Primary Flavor: Doughnut with: 0.38 without: 0.20 Primary Flavor: Butter Toffee with: 0.46 without: 0.21 Color Group: Olive with: 0.67 without: 0.21 Secondary Flavor: Egg Nog with: 0.23 without: 0.21 Truffle Type: Candy Outer with: 0.20 without: 0.22","title":"\ud83d\ude4b\u200d Question 2: Compare to Moods Median"},{"location":"S6_Bagging/#623-random-forests-for-regression","text":"back to top from sklearn.ensemble import RandomForestRegressor clf = RandomForestRegressor ( n_estimators = 10 ) Because our labels on our blob data were numerical, we can apply and view the estimator in the same way: clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> I want to revisit a dataset we brought up in Session 2 on feature engineering: t = np . linspace ( 0 , 5 , 200 ) w = 5 h = 4 s = 4 * h / np . pi * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ]) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) # tells us about the amplitude of the component at the # corresponding frequency magnitude = np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , .15 ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain') Let's see if a random forest regression model can capture the wave behavior of the time-series data clf = RandomForestRegressor ( n_estimators = 10 ) clf . fit ( t . reshape ( - 1 , 1 ), s ) RandomForestRegressor(n_estimators=10) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . plot ( t , s ) ax . plot ( t , clf . predict ( t . reshape ( - 1 , 1 ))) [<matplotlib.lines.Line2D at 0x7f850c12adc0>] Nice! without specifying any perdiodicity, the random forest does a good job of embedding this periodicity in the final output.","title":"6.2.3 Random Forests for Regression"},{"location":"S6_Bagging/#exercise-2-practice-with-random-forests","text":"With the wine dataset: predict: density create a learning curve of train/test score vs model complexity for your random forest model(s) I have provided the cleaned dataset as well as starter code for training the model and making parity plots Do not change the following 3 cells: wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ 'density' ] = y model = RandomForestRegressor ( n_estimators = 100 , criterion = 'squared_error' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , bootstrap = True , oob_score = False , n_jobs = None , random_state = None , verbose = 0 , warm_start = False , ccp_alpha = 0.0 , max_samples = None ,) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax . set_ylabel ( 'Predicted' ) ax . set_xlabel ( 'Actual' ) ax_ . set_xlabel ( 'Actual' ) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.973') Compare these results with our linear model from Lab 3. Recall that we can quickly grab the names of the paramters in our sklearn model: RandomForestRegressor () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Cell for Exercise 2 <matplotlib.legend.Legend at 0x7f1fe5063f40>","title":"\ud83c\udfcb\ufe0f Exercise 2: Practice with Random Forests"},{"location":"S7_Boosting/","text":"Data Science Foundations Session 7: Boosting \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're continuing on the topic of supervised learning an ensemble learning method called boosting. 7.1 Preparing Environment and Importing Data \u00b6 back to top 7.1.1 Import Packages \u00b6 back to top from sklearn import svm from sklearn.datasets import make_blobs , make_circles from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier , GradientBoostingClassifier import matplotlib.pyplot as plt import numpy as np import plotly.express as px from ipywidgets import interact , FloatSlider , interactive def plot_boundaries ( X , clf , ax = False ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) if ax : cs = ax . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) ax . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return ax else : cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) 7.1.2 Load Dataset \u00b6 back to top For this session, we will use dummy datasets from sklearn. 7.2 Boosting \u00b6 back to top The last supervised learning algorithms we will cover, are the boosting learners. Similar to Bagging, Boosting algorithms leverage the idea of training on variations of the available data, only this time they do so in serial rather than parallel. What do I mean by this? It's a little nuanced, but the idea is straight forward. The first model trains on the dataset, it generates some error. The datapoints creating the greatest amount of error are emphasized in the second round of training, and so on, as the sequence of models proceeds, ever troublesome datapoints receive ever increasing influence. 7.2.1 AdaBoost \u00b6 Back to Top AdaBoost was the first boosting learner of its kind. It's weak learners (the things that are stitched together in serial) are typically stumps or really shallow decision trees. Lets create some data and fit an AdaBoostClassifier to it: X , y = make_circles ( random_state = 42 , noise = .01 ) clf = AdaBoostClassifier ( DecisionTreeClassifier ( max_depth = 3 )) clf . fit ( X , y ) plot_boundaries ( X , clf ) Trying with a different distribution of data: X , y = make_blobs ( random_state = 42 , centers = 2 , cluster_std = 2.5 ) clf = AdaBoostClassifier ( DecisionTreeClassifier ( max_depth = 5 )) clf . fit ( X , y ) plot_boundaries ( X , clf ) And now with make_moons : from sklearn.datasets import make_moons X , y = make_moons ( random_state = 42 , noise = .05 ) clf = AdaBoostClassifier () clf . fit ( X , y ) plot_boundaries ( X , clf ) 7.2.1 Gradient Boosting \u00b6 Back to Top Gradient Boosting builds on the idea of AdaBoost . The term gradient implies that 2 or more derivatives are being taken somewhere. What this is referring to, is while AdaBoost is subject to a predefined loss function, Gradient Boosting can take in any arbitrary (as long as it is differentiable) loss function, to coordinate the training of its weak learners. X , y = make_circles ( random_state = 42 , noise = .01 ) clf = GradientBoostingClassifier ( loss = 'deviance' ) clf . fit ( X , y ) plot_boundaries ( X , clf ) X , y = make_blobs ( random_state = 42 , centers = 2 , cluster_std = 2.5 ) clf = GradientBoostingClassifier () clf . fit ( X , y ) plot_boundaries ( X , clf ) from sklearn.datasets import make_moons X , y = make_moons ( random_state = 42 , noise = .05 ) clf . fit ( X , y ) plot_boundaries ( X , clf ) References \u00b6 back to top Generative vs Discriminative Models","title":"Boosting"},{"location":"S7_Boosting/#data-science-foundations-session-7-boosting","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're continuing on the topic of supervised learning an ensemble learning method called boosting.","title":"Data Science Foundations  Session 7: Boosting"},{"location":"S7_Boosting/#71-preparing-environment-and-importing-data","text":"back to top","title":"7.1 Preparing Environment and Importing Data"},{"location":"S7_Boosting/#711-import-packages","text":"back to top from sklearn import svm from sklearn.datasets import make_blobs , make_circles from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier , GradientBoostingClassifier import matplotlib.pyplot as plt import numpy as np import plotly.express as px from ipywidgets import interact , FloatSlider , interactive def plot_boundaries ( X , clf , ax = False ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) if ax : cs = ax . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) ax . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return ax else : cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 )","title":"7.1.1 Import Packages"},{"location":"S7_Boosting/#712-load-dataset","text":"back to top For this session, we will use dummy datasets from sklearn.","title":"7.1.2 Load Dataset"},{"location":"S7_Boosting/#72-boosting","text":"back to top The last supervised learning algorithms we will cover, are the boosting learners. Similar to Bagging, Boosting algorithms leverage the idea of training on variations of the available data, only this time they do so in serial rather than parallel. What do I mean by this? It's a little nuanced, but the idea is straight forward. The first model trains on the dataset, it generates some error. The datapoints creating the greatest amount of error are emphasized in the second round of training, and so on, as the sequence of models proceeds, ever troublesome datapoints receive ever increasing influence.","title":"7.2 Boosting"},{"location":"S7_Boosting/#721-adaboost","text":"Back to Top AdaBoost was the first boosting learner of its kind. It's weak learners (the things that are stitched together in serial) are typically stumps or really shallow decision trees. Lets create some data and fit an AdaBoostClassifier to it: X , y = make_circles ( random_state = 42 , noise = .01 ) clf = AdaBoostClassifier ( DecisionTreeClassifier ( max_depth = 3 )) clf . fit ( X , y ) plot_boundaries ( X , clf ) Trying with a different distribution of data: X , y = make_blobs ( random_state = 42 , centers = 2 , cluster_std = 2.5 ) clf = AdaBoostClassifier ( DecisionTreeClassifier ( max_depth = 5 )) clf . fit ( X , y ) plot_boundaries ( X , clf ) And now with make_moons : from sklearn.datasets import make_moons X , y = make_moons ( random_state = 42 , noise = .05 ) clf = AdaBoostClassifier () clf . fit ( X , y ) plot_boundaries ( X , clf )","title":"7.2.1 AdaBoost"},{"location":"S7_Boosting/#721-gradient-boosting","text":"Back to Top Gradient Boosting builds on the idea of AdaBoost . The term gradient implies that 2 or more derivatives are being taken somewhere. What this is referring to, is while AdaBoost is subject to a predefined loss function, Gradient Boosting can take in any arbitrary (as long as it is differentiable) loss function, to coordinate the training of its weak learners. X , y = make_circles ( random_state = 42 , noise = .01 ) clf = GradientBoostingClassifier ( loss = 'deviance' ) clf . fit ( X , y ) plot_boundaries ( X , clf ) X , y = make_blobs ( random_state = 42 , centers = 2 , cluster_std = 2.5 ) clf = GradientBoostingClassifier () clf . fit ( X , y ) plot_boundaries ( X , clf ) from sklearn.datasets import make_moons X , y = make_moons ( random_state = 42 , noise = .05 ) clf . fit ( X , y ) plot_boundaries ( X , clf )","title":"7.2.1 Gradient Boosting"},{"location":"S7_Boosting/#references","text":"back to top Generative vs Discriminative Models","title":"References"},{"location":"introduction/","text":"Data Science Foundations \u00b6 Cut through the deluge of data science material by focusing on the essentials. This course uses illustrations, code-based examples and case studies to demonstrate essential data science topics and the practical application of existing machine learning frameworks. By the end of the course, you will have trained and validated machine learning algorithms to make continuous-value as well as discrete-value predictions from data sources relevant in business and engineering. You will also be able to make statistically sound, data-driven decisions in business from sales and production data. The breakdown for this course is as follows: Data Topics Bias-variance tradeoff; regression: linear, logistic, and multivariate; regularization: L1 and L2; inferential statistics: moods median, t-tests, f-tests, ANOVA; descriptive statistics: mean, median, mode, kurtosis, skew; beyond regression coefficients: tree-based and resampling methods; unsupervised learning: clustering and dimensionality reduction Software Topics Unit Tests Sessions S1: Regression and Analysis S2: Inferential Statistics S3: Model Selection and Validation S4: Feature Engineering S5: Unsupervised Learning: Clustering and Dimensionality Reduction S6: Bagging: Decision Trees and Random Forests S7: Boosting: AdaBoost and XGBoost Labs L1: Descriptive Statistics Data Hunt L2: Inferential Statistics Data Hunt L3: Feature Engineering L4: Supervised Learners L5: Writing Unit Tests Project P1: Statistical Analysis of Tic-Tac-Toe Games P2: Heuristical Tic-Tac-Toe Agents P3: 1-Step Look Ahead Agents P4: N-Step Look Ahead Agents Extras X1: Thinking Data Reading JVDP chapter 5","title":"Introduction"},{"location":"introduction/#data-science-foundations","text":"Cut through the deluge of data science material by focusing on the essentials. This course uses illustrations, code-based examples and case studies to demonstrate essential data science topics and the practical application of existing machine learning frameworks. By the end of the course, you will have trained and validated machine learning algorithms to make continuous-value as well as discrete-value predictions from data sources relevant in business and engineering. You will also be able to make statistically sound, data-driven decisions in business from sales and production data. The breakdown for this course is as follows: Data Topics Bias-variance tradeoff; regression: linear, logistic, and multivariate; regularization: L1 and L2; inferential statistics: moods median, t-tests, f-tests, ANOVA; descriptive statistics: mean, median, mode, kurtosis, skew; beyond regression coefficients: tree-based and resampling methods; unsupervised learning: clustering and dimensionality reduction Software Topics Unit Tests Sessions S1: Regression and Analysis S2: Inferential Statistics S3: Model Selection and Validation S4: Feature Engineering S5: Unsupervised Learning: Clustering and Dimensionality Reduction S6: Bagging: Decision Trees and Random Forests S7: Boosting: AdaBoost and XGBoost Labs L1: Descriptive Statistics Data Hunt L2: Inferential Statistics Data Hunt L3: Feature Engineering L4: Supervised Learners L5: Writing Unit Tests Project P1: Statistical Analysis of Tic-Tac-Toe Games P2: Heuristical Tic-Tac-Toe Agents P3: 1-Step Look Ahead Agents P4: N-Step Look Ahead Agents Extras X1: Thinking Data Reading JVDP chapter 5","title":"Data Science Foundations"},{"location":"extras/Probability/","text":"Measuring Uncertainty \u00b6 One strategy is to just count occurance of outcomes The probability of either of two mutually exclusive events occuring is the sum of their probabilities P(A \\space or \\space B) = P(A \\cup B) = P(A) + P(B) The probability of two mutually exclusive events occuring together is the product of their probabilities P(A \\space and \\space B) = P(A \\cap B) = P(A) * P(B) For non-mutually exclusive events: P (A \\cup B) = P(A) + P(B) - P(A \\cap B) Q1 \u00b6 In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that their sum will be at most 9 tot_outcomes = 6 ** 2 sum_less_than_9 = 6 + 6 + 6 + 5 + 4 + 3 sum_less_than_9 / tot_outcomes 0.8333333333333334 Q2 \u00b6 In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that the values rolled by each die will be different and the two dice have a sum of 6. # only 5 outcomes will sum to 6 # one of those has equal numbers # so there are 4/36 chances or 1/9 probability Q3 \u00b6 There are 3 urns labeled X, Y, and Z. Urn X contains 4 red balls and 3 black balls. Urn Y contains 5 red balls and 4 black balls. Urn Z contains 4 red balls and 4 black balls. One ball is drawn from each of the 3 urns. What is the probability that, of the 3 balls drawn, 2 are red and 1 is black? # multiply and sum probabilities # RRB 4/7 * 5/9 * 4/8 # RBR 4/7 * 4/9 * 4/8 # BRR 3/7 * 5/9 * 4/8 ( 4 / 7 * 5 / 9 * 1 / 2 ) + \\ ( 4 / 7 * 4 / 9 * 1 / 2 ) + \\ ( 3 / 7 * 5 / 9 * 1 / 2 ) 0.40476190476190477 Conditional Probability \u00b6 The flagship expression here is Bayes Rule or Bayesian Inference: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\). Q1 \u00b6 Suppose a family has 2 children, one of which is a boy. What is the probability that both children are boys? # child1 child2 # a boy; a girl # a girl; a boy # a boy; a boy Q2 \u00b6 You draw 2 cards from a standard 52-card deck without replacing them. What is the probability that both cards are of the same suit? # suites # 13 13 13 13 # hearts ( 13 - 1 ) / 51 #12/51 # spades 4/17 # clubs 4/17 # diamonds 4/17 # (4*4)/(4*17) # multiplying out still yields # 12/51 0.9411764705882353 Q3 \u00b6 If the probability of student A passing an exam is 2/7 and the probability of student B failing the exam is 3/7, then find the probability that at least 1 of the 2 students will pass the exam # P(A) = 2/7 # P(B) = 4/7 # All outcomes - sum to 1 # A pass B pass 2/7 * 4/7 # A fail B fail 5/7 * 3/7 # A pass B fail 2/7 * 3/7 # A fail B pass 5/7 * 4/7 # all outcomes ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 5 / 7 ) * ( 3 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) # outcomes we care about ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) 34 / 49 0.6938775510204082 Q4 \u00b6 Historical data shows that it has only rained 5 days per year in some desert region (assuming a 365 day year). A meteorologist predicts that it will rain today. When it actually rains, the meteorologist correctly predicts rain 90% of the time. When it doesn't rain, the meteorologist incorrectly predicts rain 10% of the time. Find the probability that it will rain today. # P(A|B) = probability that it will rain today given that the meteorologist has predicted it will rain # P(B|A) = probability that the meteoroligist will say it will rain when it rains; 90% # P(A) = probability that it will rain; 5/365 # P(B) = probability that meteoroligist will say it will rain # what is P(B) then? # P(B) = (5/365*.90) + ((365-5)/365*.1) P_B = ( 5 / 365 * .90 ) + (( 365 - 5 ) / 365 * .1 ) P_A = 5 / 365 P_BA = 0.9 P_AB = P_BA * P_A / P_B print ( f \"P(B|A): { P_BA } \" ) print ( f \"P(B): { P_B } \" ) print ( f \"P(A): { P_A } \" ) print ( f \"P(A|B): { P_AB } \" ) P(B|A): 0.9 P(B): 0.11095890410958904 P(A): 0.0136986301369863 P(A|B): 0.1111111111111111 Binomial Probabilities \u00b6 Operates on PMF (Probability Mass Functions) for discrete values answer key B(K;n,p) = \\binom{n}{k} \\times p^k \\times (1 - p)^{n-k} We can calculate the total number of outcomes we care about from a total number of trials using the binomial coefficient (this field of study is called combinatorics): \\binom{n}{k} = \\frac{n!}{k! \\times (n - k)!} This allows us to calculate the probability of an event: B(K;n,p) = \\binom{n}{k} \\times P(desired \\space outcome) def fact ( x ): \"\"\" return the factorial of a number using recursion \"\"\" if x == 1 or x == 0 : return 1 else : return fact ( x - 1 ) * x def n_choose_k ( n , k ): \"\"\" Returns the number of outcomes we care about of all possible outcomes \"\"\" return fact ( n ) / ( fact ( k ) * fact ( n - k )) def binom ( n , k , p ): \"\"\" Returns the probability of an event occuring K times in a total number of n trials having a probability of p \"\"\" return n_choose_k ( n , k ) * p ** k * ( 1 - p ) ** ( n - k ) def k_or_more ( n , k , p ): \"\"\" we can solve the K or more problem recursively \"\"\" if k == n : return binom ( n , k , p ) else : return k_or_more ( n , k + 1 , p ) + binom ( n , k , p ) Q1 \u00b6 When you're searching for a new job, it's always helpful to have more than one offer on the table so you can use it in negotiations. If you have \u2155 probability of receiving a job offer when you interview, and you interview iwth seven companies in a month, what is the probability you'll have at least two competing offers by the end of that month? p = 1 / 5 n = 7 k = 2 offers1 = k_or_more ( n , k , p ) print ( offers1 ) 0.4232832000000002 Q2 \u00b6 You get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you're tired. You really don't want to go on this many interviews unless you are at least twice as likely to get a least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7? p = 1 / 10 n = 25 k = 2 offers2 = k_or_more ( n , k , p ) print ( offers2 ) print ( offers2 / offers1 ) 0.7287940935386341 1.7217647512082543 The ratio of boys to girls for babies born in Russia is 1.09:1. If there is 1 child born per birth, what proportion of Russian families with exactly 6 children will have at least 3 boys? br , gr = 1.09 , 1 p = br / ( br + gr ) n = 6 k = 3 k_or_more ( n , k , p ) 0.6957033161509107 The Beta Distribution \u00b6 Operates on PDF (Probability Density Function) for continuous values Think: Probability of probabilities Beta(\\rho; \\alpha, \\beta) = \\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)} where \\(\\rho\\) is the probability of an event. This corresponds to the different hypotheses for the possible probabilities that could be generating our observed data; \\(\\alpha\\) represents how many times we observe an event we care about such as winning a coin toss; \\(\\beta\\) represents how many times the event we care about didn't happen, such as losing a coin toss. The total number of trials is \\(\\alpha + \\beta\\) (contrast this with \\(n\\) and \\(k\\) in the binomial distribution). The beta (lowercase) distribution: \\int_0^1{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}} Putting this all together. The probability that an event occurs in a specific range: Beta(\\rho; \\alpha, \\beta) = \\int_{lower \\space bound}^{upper \\space bound}{\\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)}} Q1 \u00b6 You want to use the beta distribution to determine whether or not a coin you have is a fair coin - meaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time? from scipy.stats import beta _alpha = 4 _beta = 6 model = beta ( _alpha , _beta ) model . pdf ( 0.6 ) 1.1147673600000005","title":"Probability"},{"location":"extras/Probability/#measuring-uncertainty","text":"One strategy is to just count occurance of outcomes The probability of either of two mutually exclusive events occuring is the sum of their probabilities P(A \\space or \\space B) = P(A \\cup B) = P(A) + P(B) The probability of two mutually exclusive events occuring together is the product of their probabilities P(A \\space and \\space B) = P(A \\cap B) = P(A) * P(B) For non-mutually exclusive events: P (A \\cup B) = P(A) + P(B) - P(A \\cap B)","title":"Measuring Uncertainty"},{"location":"extras/Probability/#q1","text":"In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that their sum will be at most 9 tot_outcomes = 6 ** 2 sum_less_than_9 = 6 + 6 + 6 + 5 + 4 + 3 sum_less_than_9 / tot_outcomes 0.8333333333333334","title":"Q1"},{"location":"extras/Probability/#q2","text":"In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that the values rolled by each die will be different and the two dice have a sum of 6. # only 5 outcomes will sum to 6 # one of those has equal numbers # so there are 4/36 chances or 1/9 probability","title":"Q2"},{"location":"extras/Probability/#q3","text":"There are 3 urns labeled X, Y, and Z. Urn X contains 4 red balls and 3 black balls. Urn Y contains 5 red balls and 4 black balls. Urn Z contains 4 red balls and 4 black balls. One ball is drawn from each of the 3 urns. What is the probability that, of the 3 balls drawn, 2 are red and 1 is black? # multiply and sum probabilities # RRB 4/7 * 5/9 * 4/8 # RBR 4/7 * 4/9 * 4/8 # BRR 3/7 * 5/9 * 4/8 ( 4 / 7 * 5 / 9 * 1 / 2 ) + \\ ( 4 / 7 * 4 / 9 * 1 / 2 ) + \\ ( 3 / 7 * 5 / 9 * 1 / 2 ) 0.40476190476190477","title":"Q3"},{"location":"extras/Probability/#conditional-probability","text":"The flagship expression here is Bayes Rule or Bayesian Inference: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\).","title":"Conditional Probability"},{"location":"extras/Probability/#q1_1","text":"Suppose a family has 2 children, one of which is a boy. What is the probability that both children are boys? # child1 child2 # a boy; a girl # a girl; a boy # a boy; a boy","title":"Q1"},{"location":"extras/Probability/#q2_1","text":"You draw 2 cards from a standard 52-card deck without replacing them. What is the probability that both cards are of the same suit? # suites # 13 13 13 13 # hearts ( 13 - 1 ) / 51 #12/51 # spades 4/17 # clubs 4/17 # diamonds 4/17 # (4*4)/(4*17) # multiplying out still yields # 12/51 0.9411764705882353","title":"Q2"},{"location":"extras/Probability/#q3_1","text":"If the probability of student A passing an exam is 2/7 and the probability of student B failing the exam is 3/7, then find the probability that at least 1 of the 2 students will pass the exam # P(A) = 2/7 # P(B) = 4/7 # All outcomes - sum to 1 # A pass B pass 2/7 * 4/7 # A fail B fail 5/7 * 3/7 # A pass B fail 2/7 * 3/7 # A fail B pass 5/7 * 4/7 # all outcomes ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 5 / 7 ) * ( 3 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) # outcomes we care about ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) 34 / 49 0.6938775510204082","title":"Q3"},{"location":"extras/Probability/#q4","text":"Historical data shows that it has only rained 5 days per year in some desert region (assuming a 365 day year). A meteorologist predicts that it will rain today. When it actually rains, the meteorologist correctly predicts rain 90% of the time. When it doesn't rain, the meteorologist incorrectly predicts rain 10% of the time. Find the probability that it will rain today. # P(A|B) = probability that it will rain today given that the meteorologist has predicted it will rain # P(B|A) = probability that the meteoroligist will say it will rain when it rains; 90% # P(A) = probability that it will rain; 5/365 # P(B) = probability that meteoroligist will say it will rain # what is P(B) then? # P(B) = (5/365*.90) + ((365-5)/365*.1) P_B = ( 5 / 365 * .90 ) + (( 365 - 5 ) / 365 * .1 ) P_A = 5 / 365 P_BA = 0.9 P_AB = P_BA * P_A / P_B print ( f \"P(B|A): { P_BA } \" ) print ( f \"P(B): { P_B } \" ) print ( f \"P(A): { P_A } \" ) print ( f \"P(A|B): { P_AB } \" ) P(B|A): 0.9 P(B): 0.11095890410958904 P(A): 0.0136986301369863 P(A|B): 0.1111111111111111","title":"Q4"},{"location":"extras/Probability/#binomial-probabilities","text":"Operates on PMF (Probability Mass Functions) for discrete values answer key B(K;n,p) = \\binom{n}{k} \\times p^k \\times (1 - p)^{n-k} We can calculate the total number of outcomes we care about from a total number of trials using the binomial coefficient (this field of study is called combinatorics): \\binom{n}{k} = \\frac{n!}{k! \\times (n - k)!} This allows us to calculate the probability of an event: B(K;n,p) = \\binom{n}{k} \\times P(desired \\space outcome) def fact ( x ): \"\"\" return the factorial of a number using recursion \"\"\" if x == 1 or x == 0 : return 1 else : return fact ( x - 1 ) * x def n_choose_k ( n , k ): \"\"\" Returns the number of outcomes we care about of all possible outcomes \"\"\" return fact ( n ) / ( fact ( k ) * fact ( n - k )) def binom ( n , k , p ): \"\"\" Returns the probability of an event occuring K times in a total number of n trials having a probability of p \"\"\" return n_choose_k ( n , k ) * p ** k * ( 1 - p ) ** ( n - k ) def k_or_more ( n , k , p ): \"\"\" we can solve the K or more problem recursively \"\"\" if k == n : return binom ( n , k , p ) else : return k_or_more ( n , k + 1 , p ) + binom ( n , k , p )","title":"Binomial Probabilities"},{"location":"extras/Probability/#q1_2","text":"When you're searching for a new job, it's always helpful to have more than one offer on the table so you can use it in negotiations. If you have \u2155 probability of receiving a job offer when you interview, and you interview iwth seven companies in a month, what is the probability you'll have at least two competing offers by the end of that month? p = 1 / 5 n = 7 k = 2 offers1 = k_or_more ( n , k , p ) print ( offers1 ) 0.4232832000000002","title":"Q1"},{"location":"extras/Probability/#q2_2","text":"You get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you're tired. You really don't want to go on this many interviews unless you are at least twice as likely to get a least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7? p = 1 / 10 n = 25 k = 2 offers2 = k_or_more ( n , k , p ) print ( offers2 ) print ( offers2 / offers1 ) 0.7287940935386341 1.7217647512082543 The ratio of boys to girls for babies born in Russia is 1.09:1. If there is 1 child born per birth, what proportion of Russian families with exactly 6 children will have at least 3 boys? br , gr = 1.09 , 1 p = br / ( br + gr ) n = 6 k = 3 k_or_more ( n , k , p ) 0.6957033161509107","title":"Q2"},{"location":"extras/Probability/#the-beta-distribution","text":"Operates on PDF (Probability Density Function) for continuous values Think: Probability of probabilities Beta(\\rho; \\alpha, \\beta) = \\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)} where \\(\\rho\\) is the probability of an event. This corresponds to the different hypotheses for the possible probabilities that could be generating our observed data; \\(\\alpha\\) represents how many times we observe an event we care about such as winning a coin toss; \\(\\beta\\) represents how many times the event we care about didn't happen, such as losing a coin toss. The total number of trials is \\(\\alpha + \\beta\\) (contrast this with \\(n\\) and \\(k\\) in the binomial distribution). The beta (lowercase) distribution: \\int_0^1{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}} Putting this all together. The probability that an event occurs in a specific range: Beta(\\rho; \\alpha, \\beta) = \\int_{lower \\space bound}^{upper \\space bound}{\\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)}}","title":"The Beta Distribution"},{"location":"extras/Probability/#q1_3","text":"You want to use the beta distribution to determine whether or not a coin you have is a fair coin - meaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time? from scipy.stats import beta _alpha = 4 _beta = 6 model = beta ( _alpha , _beta ) model . pdf ( 0.6 ) 1.1147673600000005","title":"Q1"},{"location":"extras/X1_Thinking_Data/","text":"Data Science Foundations Extras 1: Thinking Data \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to take our newfound knowledge from the course, and practice how we can leverage data to build predictive models. We'll start with a feature engineering problem on some dummy data. This will get us thinking creatively about problem solving. We will then pivot over to an Airbnb dataset . After performing some general, exploratory data analysis, we will solve the following business case: Airbnb is interested in using historical list prices from their airbnb hosts, to make pricing suggestions to new hosts. How can we use this existing datset to assist with this price listing suggestion? Prepare Environment and Import Data \u00b6 # basic packages import pandas as pd import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score from sklearn.model_selection import GridSearchCV , cross_val_score Warm Up \u00b6 Add aditional feature(s) to X to predict y with a linear classifier (e.g. logistic regression) from sklearn.datasets import make_circles X , y = make_circles ( random_state = 42 , noise = .01 ) relabel = dict ( zip ([ 0 , 1 , 2 , 3 ],[ 0 , 1 , 0 , 1 ])) y = np . vectorize ( relabel . get )( y ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f20ec66d280> and now predict # consider using # LogisticRegression() # r2_score Exploratory Data Analysis \u00b6 which columns are numerical, string; which contain nans/nulls; what is the correlation between features airbnb = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/datasets/main/datasets/airbnb/AB_NYC_2019.csv\" ) # recall these attributes # shape dtypes # and these methods # head() tail() isnull() sum() nunique() copy() select_dtypes() describe() drop() what visualizations would be useful to make? plt . figure ( figsize = ( 10 , 6 )) sns . scatterplot ( x = airbnb . longitude , y = airbnb . latitude , hue = airbnb . neighbourhood_group ) <AxesSubplot:xlabel='longitude', ylabel='latitude'> Feature Engineering \u00b6 Say we want to predict price , using an ML model. How would you build your features? Based on the number of null values, what would you do with the last_review and reviews_per_month column? X = airbnb . copy () How will we deal with the categorical features? # Recall # OneHotEncoder() X_cat = X . select_dtypes ( include = 'object' ) display ( X_cat . head ()) print ( X_cat . nunique ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name host_name neighbourhood_group neighbourhood room_type last_review 0 Clean & quiet apt home by the park John Brooklyn Kensington Private room 2018-10-19 1 Skylit Midtown Castle Jennifer Manhattan Midtown Entire home/apt 2019-05-21 2 THE VILLAGE OF HARLEM....NEW YORK ! Elisabeth Manhattan Harlem Private room NaN 3 Cozy Entire Floor of Brownstone LisaRoxanne Brooklyn Clinton Hill Entire home/apt 2019-07-05 4 Entire Apt: Spacious Studio/Loft by central park Laura Manhattan East Harlem Entire home/apt 2018-11-19 name 47905 host_name 11452 neighbourhood_group 5 neighbourhood 221 room_type 3 last_review 1764 dtype: int64 X_num = X . select_dtypes ( exclude = 'object' ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 149 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 225 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 150 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 89 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 80 10 9 0.10 1 0 Feature Transformation \u00b6 What features do you think will cause the most problems if untransformed? Scale and Center all but the target variable, price # Recall # StandardScaler() df.pop() Model Baseline \u00b6 # Recall # X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42) # LinearRegression() # r2_score() try a model that captures non-linear relationships # Recall # RandomForestRegressor() both of these results from the LinearRegression and RandomForest models should indicate something to you (think back to Model Selection and Validation ) Additional Strategies \u00b6 After this first pass, what are some additional strategies to consider for improving the model?","title":"Thinking Data"},{"location":"extras/X1_Thinking_Data/#data-science-foundations-extras-1-thinking-data","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to take our newfound knowledge from the course, and practice how we can leverage data to build predictive models. We'll start with a feature engineering problem on some dummy data. This will get us thinking creatively about problem solving. We will then pivot over to an Airbnb dataset . After performing some general, exploratory data analysis, we will solve the following business case: Airbnb is interested in using historical list prices from their airbnb hosts, to make pricing suggestions to new hosts. How can we use this existing datset to assist with this price listing suggestion?","title":"Data Science Foundations  Extras 1: Thinking Data"},{"location":"extras/X1_Thinking_Data/#prepare-environment-and-import-data","text":"# basic packages import pandas as pd import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score from sklearn.model_selection import GridSearchCV , cross_val_score","title":"Prepare Environment and Import Data"},{"location":"extras/X1_Thinking_Data/#warm-up","text":"Add aditional feature(s) to X to predict y with a linear classifier (e.g. logistic regression) from sklearn.datasets import make_circles X , y = make_circles ( random_state = 42 , noise = .01 ) relabel = dict ( zip ([ 0 , 1 , 2 , 3 ],[ 0 , 1 , 0 , 1 ])) y = np . vectorize ( relabel . get )( y ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f20ec66d280> and now predict # consider using # LogisticRegression() # r2_score","title":"Warm Up"},{"location":"extras/X1_Thinking_Data/#exploratory-data-analysis","text":"which columns are numerical, string; which contain nans/nulls; what is the correlation between features airbnb = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/datasets/main/datasets/airbnb/AB_NYC_2019.csv\" ) # recall these attributes # shape dtypes # and these methods # head() tail() isnull() sum() nunique() copy() select_dtypes() describe() drop() what visualizations would be useful to make? plt . figure ( figsize = ( 10 , 6 )) sns . scatterplot ( x = airbnb . longitude , y = airbnb . latitude , hue = airbnb . neighbourhood_group ) <AxesSubplot:xlabel='longitude', ylabel='latitude'>","title":"Exploratory Data Analysis"},{"location":"extras/X1_Thinking_Data/#feature-engineering","text":"Say we want to predict price , using an ML model. How would you build your features? Based on the number of null values, what would you do with the last_review and reviews_per_month column? X = airbnb . copy () How will we deal with the categorical features? # Recall # OneHotEncoder() X_cat = X . select_dtypes ( include = 'object' ) display ( X_cat . head ()) print ( X_cat . nunique ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name host_name neighbourhood_group neighbourhood room_type last_review 0 Clean & quiet apt home by the park John Brooklyn Kensington Private room 2018-10-19 1 Skylit Midtown Castle Jennifer Manhattan Midtown Entire home/apt 2019-05-21 2 THE VILLAGE OF HARLEM....NEW YORK ! Elisabeth Manhattan Harlem Private room NaN 3 Cozy Entire Floor of Brownstone LisaRoxanne Brooklyn Clinton Hill Entire home/apt 2019-07-05 4 Entire Apt: Spacious Studio/Loft by central park Laura Manhattan East Harlem Entire home/apt 2018-11-19 name 47905 host_name 11452 neighbourhood_group 5 neighbourhood 221 room_type 3 last_review 1764 dtype: int64 X_num = X . select_dtypes ( exclude = 'object' ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 149 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 225 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 150 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 89 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 80 10 9 0.10 1 0","title":"Feature Engineering"},{"location":"extras/X1_Thinking_Data/#feature-transformation","text":"What features do you think will cause the most problems if untransformed? Scale and Center all but the target variable, price # Recall # StandardScaler() df.pop()","title":"Feature Transformation"},{"location":"extras/X1_Thinking_Data/#model-baseline","text":"# Recall # X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42) # LinearRegression() # r2_score() try a model that captures non-linear relationships # Recall # RandomForestRegressor() both of these results from the LinearRegression and RandomForest models should indicate something to you (think back to Model Selection and Validation )","title":"Model Baseline"},{"location":"extras/X1_Thinking_Data/#additional-strategies","text":"After this first pass, what are some additional strategies to consider for improving the model?","title":"Additional Strategies"},{"location":"extras/X2_Probability/","text":"Measuring Uncertainty \u00b6 One strategy is to just count occurance of outcomes The probability of either of two mutually exclusive events occuring is the sum of their probabilities P(A \\space or \\space B) = P(A \\cup B) = P(A) + P(B) The probability of two mutually exclusive events occuring together is the product of their probabilities P(A \\space and \\space B) = P(A \\cap B) = P(A) * P(B) For non-mutually exclusive events: P (A \\cup B) = P(A) + P(B) - P(A \\cap B) Q1 \u00b6 In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that their sum will be at most 9 tot_outcomes = 6 ** 2 sum_less_than_9 = 6 + 6 + 6 + 5 + 4 + 3 sum_less_than_9 / tot_outcomes 0.8333333333333334 Q2 \u00b6 In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that the values rolled by each die will be different and the two dice have a sum of 6. # only 5 outcomes will sum to 6 # one of those has equal numbers # so there are 4/36 chances or 1/9 probability Q3 \u00b6 There are 3 urns labeled X, Y, and Z. Urn X contains 4 red balls and 3 black balls. Urn Y contains 5 red balls and 4 black balls. Urn Z contains 4 red balls and 4 black balls. One ball is drawn from each of the 3 urns. What is the probability that, of the 3 balls drawn, 2 are red and 1 is black? # multiply and sum probabilities # RRB 4/7 * 5/9 * 4/8 # RBR 4/7 * 4/9 * 4/8 # BRR 3/7 * 5/9 * 4/8 ( 4 / 7 * 5 / 9 * 1 / 2 ) + \\ ( 4 / 7 * 4 / 9 * 1 / 2 ) + \\ ( 3 / 7 * 5 / 9 * 1 / 2 ) 0.40476190476190477 Conditional Probability \u00b6 The flagship expression here is Bayes Rule or Bayesian Inference: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\). Q1 \u00b6 Suppose a family has 2 children, one of which is a boy. What is the probability that both children are boys? # child1 child2 # a boy; a girl # a girl; a boy # a boy; a boy Q2 \u00b6 You draw 2 cards from a standard 52-card deck without replacing them. What is the probability that both cards are of the same suit? # suites # 13 13 13 13 # hearts ( 13 - 1 ) / 51 #12/51 # spades 4/17 # clubs 4/17 # diamonds 4/17 # (4*4)/(4*17) # multiplying out still yields # 12/51 0.9411764705882353 Q3 \u00b6 If the probability of student A passing an exam is 2/7 and the probability of student B failing the exam is 3/7, then find the probability that at least 1 of the 2 students will pass the exam # P(A) = 2/7 # P(B) = 4/7 # All outcomes - sum to 1 # A pass B pass 2/7 * 4/7 # A fail B fail 5/7 * 3/7 # A pass B fail 2/7 * 3/7 # A fail B pass 5/7 * 4/7 # all outcomes ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 5 / 7 ) * ( 3 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) # outcomes we care about ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) 34 / 49 0.6938775510204082 Q4 \u00b6 Historical data shows that it has only rained 5 days per year in some desert region (assuming a 365 day year). A meteorologist predicts that it will rain today. When it actually rains, the meteorologist correctly predicts rain 90% of the time. When it doesn't rain, the meteorologist incorrectly predicts rain 10% of the time. Find the probability that it will rain today. # P(A|B) = probability that it will rain today given that the meteorologist has predicted it will rain # P(B|A) = probability that the meteoroligist will say it will rain when it rains; 90% # P(A) = probability that it will rain; 5/365 # P(B) = probability that meteoroligist will say it will rain # what is P(B) then? # P(B) = (5/365*.90) + ((365-5)/365*.1) P_B = ( 5 / 365 * .90 ) + (( 365 - 5 ) / 365 * .1 ) P_A = 5 / 365 P_BA = 0.9 P_AB = P_BA * P_A / P_B print ( f \"P(B|A): { P_BA } \" ) print ( f \"P(B): { P_B } \" ) print ( f \"P(A): { P_A } \" ) print ( f \"P(A|B): { P_AB } \" ) P(B|A): 0.9 P(B): 0.11095890410958904 P(A): 0.0136986301369863 P(A|B): 0.1111111111111111 Binomial Probabilities \u00b6 Operates on PMF (Probability Mass Functions) for discrete values answer key B(K;n,p) = \\binom{n}{k} \\times p^k \\times (1 - p)^{n-k} We can calculate the total number of outcomes we care about from a total number of trials using the binomial coefficient (this field of study is called combinatorics): \\binom{n}{k} = \\frac{n!}{k! \\times (n - k)!} This allows us to calculate the probability of an event: B(K;n,p) = \\binom{n}{k} \\times P(desired \\space outcome) def fact ( x ): \"\"\" return the factorial of a number using recursion \"\"\" if x == 1 or x == 0 : return 1 else : return fact ( x - 1 ) * x def n_choose_k ( n , k ): \"\"\" Returns the number of outcomes we care about of all possible outcomes \"\"\" return fact ( n ) / ( fact ( k ) * fact ( n - k )) def binom ( n , k , p ): \"\"\" Returns the probability of an event occuring K times in a total number of n trials having a probability of p \"\"\" return n_choose_k ( n , k ) * p ** k * ( 1 - p ) ** ( n - k ) def k_or_more ( n , k , p ): \"\"\" we can solve the K or more problem recursively \"\"\" if k == n : return binom ( n , k , p ) else : return k_or_more ( n , k + 1 , p ) + binom ( n , k , p ) Q1 \u00b6 When you're searching for a new job, it's always helpful to have more than one offer on the table so you can use it in negotiations. If you have \u2155 probability of receiving a job offer when you interview, and you interview iwth seven companies in a month, what is the probability you'll have at least two competing offers by the end of that month? p = 1 / 5 n = 7 k = 2 offers1 = k_or_more ( n , k , p ) print ( offers1 ) 0.4232832000000002 Q2 \u00b6 You get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you're tired. You really don't want to go on this many interviews unless you are at least twice as likely to get a least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7? p = 1 / 10 n = 25 k = 2 offers2 = k_or_more ( n , k , p ) print ( offers2 ) print ( offers2 / offers1 ) 0.7287940935386341 1.7217647512082543 The ratio of boys to girls for babies born in Russia is 1.09:1. If there is 1 child born per birth, what proportion of Russian families with exactly 6 children will have at least 3 boys? br , gr = 1.09 , 1 p = br / ( br + gr ) n = 6 k = 3 k_or_more ( n , k , p ) 0.6957033161509107 The Beta Distribution \u00b6 Operates on PDF (Probability Density Function) for continuous values Think: Probability of probabilities Beta(\\rho; \\alpha, \\beta) = \\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)} where \\(\\rho\\) is the probability of an event. This corresponds to the different hypotheses for the possible probabilities that could be generating our observed data; \\(\\alpha\\) represents how many times we observe an event we care about such as winning a coin toss; \\(\\beta\\) represents how many times the event we care about didn't happen, such as losing a coin toss. The total number of trials is \\(\\alpha + \\beta\\) (contrast this with \\(n\\) and \\(k\\) in the binomial distribution). The beta (lowercase) distribution: \\int_0^1{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}} Putting this all together. The probability that an event occurs in a specific range: Beta(\\rho; \\alpha, \\beta) = \\int_{lower \\space bound}^{upper \\space bound}{\\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)}} Q1 \u00b6 You want to use the beta distribution to determine whether or not a coin you have is a fair coin - meaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time? from scipy.stats import beta _alpha = 4 _beta = 6 model = beta ( _alpha , _beta ) model . pdf ( 0.6 ) 1.1147673600000005 Poisson Random Variables \u00b6 What happens when we can't estimate \\(p\\) for a binomial probability? Enter the poisson experiemnt A Poisson experiment is a statistical experiment that has the following properties: The outcome of each trial is either success or failure. The average number of successes that occurs in a specified region is known. The probability that a success will occur is proportional to the size of the region. The probability that a success will occur in an extremely small region is virtually zero. The Poisson distribution is: P(k,\\lambda) = \\frac{\\lambda^k \\exp^{-\\lambda}}{k!} where \\(\\lambda\\) is the average number of successes that occur in a specified region, \\(k\\) is the actual number of successes that occur in a specified region, \\(P(K,\\lambda)\\) is the Poisson probability, which is the probability of getting exactly successes when the average number of successes is \\(\\lambda\\). from math import exp def poisson ( k , lamb ): return ( lamb ** k * exp ( - lamb )) / ( fact ( k )) Q1 \u00b6 A random variable, \\(X\\), follows Poisson distribution with mean of 2.5. Find the probability with which the random variable \\(X\\) is equal to 5. # P(k=5,lambda=2.5) poisson ( k = 5 , lamb = 2.5 ) 0.06680094289054264","title":"X2 Probability"},{"location":"extras/X2_Probability/#measuring-uncertainty","text":"One strategy is to just count occurance of outcomes The probability of either of two mutually exclusive events occuring is the sum of their probabilities P(A \\space or \\space B) = P(A \\cup B) = P(A) + P(B) The probability of two mutually exclusive events occuring together is the product of their probabilities P(A \\space and \\space B) = P(A \\cap B) = P(A) * P(B) For non-mutually exclusive events: P (A \\cup B) = P(A) + P(B) - P(A \\cap B)","title":"Measuring Uncertainty"},{"location":"extras/X2_Probability/#q1","text":"In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that their sum will be at most 9 tot_outcomes = 6 ** 2 sum_less_than_9 = 6 + 6 + 6 + 5 + 4 + 3 sum_less_than_9 / tot_outcomes 0.8333333333333334","title":"Q1"},{"location":"extras/X2_Probability/#q2","text":"In a single toss of 2 fair (evenly-weighted) six-sided dice, find the probability that the values rolled by each die will be different and the two dice have a sum of 6. # only 5 outcomes will sum to 6 # one of those has equal numbers # so there are 4/36 chances or 1/9 probability","title":"Q2"},{"location":"extras/X2_Probability/#q3","text":"There are 3 urns labeled X, Y, and Z. Urn X contains 4 red balls and 3 black balls. Urn Y contains 5 red balls and 4 black balls. Urn Z contains 4 red balls and 4 black balls. One ball is drawn from each of the 3 urns. What is the probability that, of the 3 balls drawn, 2 are red and 1 is black? # multiply and sum probabilities # RRB 4/7 * 5/9 * 4/8 # RBR 4/7 * 4/9 * 4/8 # BRR 3/7 * 5/9 * 4/8 ( 4 / 7 * 5 / 9 * 1 / 2 ) + \\ ( 4 / 7 * 4 / 9 * 1 / 2 ) + \\ ( 3 / 7 * 5 / 9 * 1 / 2 ) 0.40476190476190477","title":"Q3"},{"location":"extras/X2_Probability/#conditional-probability","text":"The flagship expression here is Bayes Rule or Bayesian Inference: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\).","title":"Conditional Probability"},{"location":"extras/X2_Probability/#q1_1","text":"Suppose a family has 2 children, one of which is a boy. What is the probability that both children are boys? # child1 child2 # a boy; a girl # a girl; a boy # a boy; a boy","title":"Q1"},{"location":"extras/X2_Probability/#q2_1","text":"You draw 2 cards from a standard 52-card deck without replacing them. What is the probability that both cards are of the same suit? # suites # 13 13 13 13 # hearts ( 13 - 1 ) / 51 #12/51 # spades 4/17 # clubs 4/17 # diamonds 4/17 # (4*4)/(4*17) # multiplying out still yields # 12/51 0.9411764705882353","title":"Q2"},{"location":"extras/X2_Probability/#q3_1","text":"If the probability of student A passing an exam is 2/7 and the probability of student B failing the exam is 3/7, then find the probability that at least 1 of the 2 students will pass the exam # P(A) = 2/7 # P(B) = 4/7 # All outcomes - sum to 1 # A pass B pass 2/7 * 4/7 # A fail B fail 5/7 * 3/7 # A pass B fail 2/7 * 3/7 # A fail B pass 5/7 * 4/7 # all outcomes ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 5 / 7 ) * ( 3 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) # outcomes we care about ( 2 / 7 ) * ( 4 / 7 ) + \\ ( 2 / 7 ) * ( 3 / 7 ) + \\ ( 5 / 7 ) * ( 4 / 7 ) 34 / 49 0.6938775510204082","title":"Q3"},{"location":"extras/X2_Probability/#q4","text":"Historical data shows that it has only rained 5 days per year in some desert region (assuming a 365 day year). A meteorologist predicts that it will rain today. When it actually rains, the meteorologist correctly predicts rain 90% of the time. When it doesn't rain, the meteorologist incorrectly predicts rain 10% of the time. Find the probability that it will rain today. # P(A|B) = probability that it will rain today given that the meteorologist has predicted it will rain # P(B|A) = probability that the meteoroligist will say it will rain when it rains; 90% # P(A) = probability that it will rain; 5/365 # P(B) = probability that meteoroligist will say it will rain # what is P(B) then? # P(B) = (5/365*.90) + ((365-5)/365*.1) P_B = ( 5 / 365 * .90 ) + (( 365 - 5 ) / 365 * .1 ) P_A = 5 / 365 P_BA = 0.9 P_AB = P_BA * P_A / P_B print ( f \"P(B|A): { P_BA } \" ) print ( f \"P(B): { P_B } \" ) print ( f \"P(A): { P_A } \" ) print ( f \"P(A|B): { P_AB } \" ) P(B|A): 0.9 P(B): 0.11095890410958904 P(A): 0.0136986301369863 P(A|B): 0.1111111111111111","title":"Q4"},{"location":"extras/X2_Probability/#binomial-probabilities","text":"Operates on PMF (Probability Mass Functions) for discrete values answer key B(K;n,p) = \\binom{n}{k} \\times p^k \\times (1 - p)^{n-k} We can calculate the total number of outcomes we care about from a total number of trials using the binomial coefficient (this field of study is called combinatorics): \\binom{n}{k} = \\frac{n!}{k! \\times (n - k)!} This allows us to calculate the probability of an event: B(K;n,p) = \\binom{n}{k} \\times P(desired \\space outcome) def fact ( x ): \"\"\" return the factorial of a number using recursion \"\"\" if x == 1 or x == 0 : return 1 else : return fact ( x - 1 ) * x def n_choose_k ( n , k ): \"\"\" Returns the number of outcomes we care about of all possible outcomes \"\"\" return fact ( n ) / ( fact ( k ) * fact ( n - k )) def binom ( n , k , p ): \"\"\" Returns the probability of an event occuring K times in a total number of n trials having a probability of p \"\"\" return n_choose_k ( n , k ) * p ** k * ( 1 - p ) ** ( n - k ) def k_or_more ( n , k , p ): \"\"\" we can solve the K or more problem recursively \"\"\" if k == n : return binom ( n , k , p ) else : return k_or_more ( n , k + 1 , p ) + binom ( n , k , p )","title":"Binomial Probabilities"},{"location":"extras/X2_Probability/#q1_2","text":"When you're searching for a new job, it's always helpful to have more than one offer on the table so you can use it in negotiations. If you have \u2155 probability of receiving a job offer when you interview, and you interview iwth seven companies in a month, what is the probability you'll have at least two competing offers by the end of that month? p = 1 / 5 n = 7 k = 2 offers1 = k_or_more ( n , k , p ) print ( offers1 ) 0.4232832000000002","title":"Q1"},{"location":"extras/X2_Probability/#q2_2","text":"You get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you're tired. You really don't want to go on this many interviews unless you are at least twice as likely to get a least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7? p = 1 / 10 n = 25 k = 2 offers2 = k_or_more ( n , k , p ) print ( offers2 ) print ( offers2 / offers1 ) 0.7287940935386341 1.7217647512082543 The ratio of boys to girls for babies born in Russia is 1.09:1. If there is 1 child born per birth, what proportion of Russian families with exactly 6 children will have at least 3 boys? br , gr = 1.09 , 1 p = br / ( br + gr ) n = 6 k = 3 k_or_more ( n , k , p ) 0.6957033161509107","title":"Q2"},{"location":"extras/X2_Probability/#the-beta-distribution","text":"Operates on PDF (Probability Density Function) for continuous values Think: Probability of probabilities Beta(\\rho; \\alpha, \\beta) = \\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)} where \\(\\rho\\) is the probability of an event. This corresponds to the different hypotheses for the possible probabilities that could be generating our observed data; \\(\\alpha\\) represents how many times we observe an event we care about such as winning a coin toss; \\(\\beta\\) represents how many times the event we care about didn't happen, such as losing a coin toss. The total number of trials is \\(\\alpha + \\beta\\) (contrast this with \\(n\\) and \\(k\\) in the binomial distribution). The beta (lowercase) distribution: \\int_0^1{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}} Putting this all together. The probability that an event occurs in a specific range: Beta(\\rho; \\alpha, \\beta) = \\int_{lower \\space bound}^{upper \\space bound}{\\frac{\\rho^{\\alpha - 1} \\times (1-\\rho)^{\\beta - 1}}{beta(\\alpha, \\beta)}}","title":"The Beta Distribution"},{"location":"extras/X2_Probability/#q1_3","text":"You want to use the beta distribution to determine whether or not a coin you have is a fair coin - meaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time? from scipy.stats import beta _alpha = 4 _beta = 6 model = beta ( _alpha , _beta ) model . pdf ( 0.6 ) 1.1147673600000005","title":"Q1"},{"location":"extras/X2_Probability/#poisson-random-variables","text":"What happens when we can't estimate \\(p\\) for a binomial probability? Enter the poisson experiemnt A Poisson experiment is a statistical experiment that has the following properties: The outcome of each trial is either success or failure. The average number of successes that occurs in a specified region is known. The probability that a success will occur is proportional to the size of the region. The probability that a success will occur in an extremely small region is virtually zero. The Poisson distribution is: P(k,\\lambda) = \\frac{\\lambda^k \\exp^{-\\lambda}}{k!} where \\(\\lambda\\) is the average number of successes that occur in a specified region, \\(k\\) is the actual number of successes that occur in a specified region, \\(P(K,\\lambda)\\) is the Poisson probability, which is the probability of getting exactly successes when the average number of successes is \\(\\lambda\\). from math import exp def poisson ( k , lamb ): return ( lamb ** k * exp ( - lamb )) / ( fact ( k ))","title":"Poisson Random Variables"},{"location":"extras/X2_Probability/#q1_4","text":"A random variable, \\(X\\), follows Poisson distribution with mean of 2.5. Find the probability with which the random variable \\(X\\) is equal to 5. # P(k=5,lambda=2.5) poisson ( k = 5 , lamb = 2.5 ) 0.06680094289054264","title":"Q1"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/","text":"Data Science Foundations Lab 1: Data Hunt I \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's a data hunt. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from ipywidgets import interact df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_explorers/main/assets/imdb_movies.csv\" ) # converting years to numbers for easy conditionals df [ 'year' ] = pd . to_numeric ( df [ 'year' ], errors = 'coerce' ) df . shape /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) (85855, 22) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics 0 tt0000009 Miss Jerry Miss Jerry 1894.0 1894-10-09 Romance 45 USA None Alexander Black ... Blanche Bayliss, William Courtenay, Chauncey D... The adventures of a female reporter in the 1890s. 5.9 154 NaN NaN NaN NaN 1.0 2.0 1 tt0000574 The Story of the Kelly Gang The Story of the Kelly Gang 1906.0 1906-12-26 Biography, Crime, Drama 70 Australia None Charles Tait ... Elizabeth Tait, John Tait, Norman Campbell, Be... True story of notorious Australian outlaw Ned ... 6.1 589 $ 2250 NaN NaN NaN 7.0 7.0 2 tt0001892 Den sorte dr\u00f8m Den sorte dr\u00f8m 1911.0 1911-08-19 Drama 53 Germany, Denmark NaN Urban Gad ... Asta Nielsen, Valdemar Psilander, Gunnar Helse... Two men of high rank are both wooing the beaut... 5.8 188 NaN NaN NaN NaN 5.0 2.0 3 rows \u00d7 22 columns \ud83c\udfa5 L1 Q1 What american director has the highest mean avg_vote? \u00b6 director Daniel Keith, Snorri Sturluson 9.3 Anthony Bawn 9.3 Derek Ahonen 9.2 Raghav Peri 9.1 James Marlowe 8.8 ... Waleed Bedour 1.2 Fred Ashman 1.1 Aeneas Middleton 1.1 Steven A. Sandt 1.1 Francis Hamada 1.1 Name: avg_vote, Length: 12463, dtype: float64 \ud83c\udfa5 L1 Q2 What american director with more than 5 movies, has the highest mean avg_vote? \u00b6 director Quentin Tarantino 7.811111 Charles Chaplin 7.764286 David Fincher 7.625000 Billy Wilder 7.580952 Martin Scorsese 7.544444 ... Barry Mahon 2.728571 Dennis Devine 2.657143 Bill Zebub 2.483333 Mark Polonia 2.462500 Christopher Forbes 2.000000 Name: avg_vote, Length: 859, dtype: float64 \ud83c\udfa5 L1 Q3 What director has the largest variance in avg_vote? \u00b6 director Deniz Denizciler 4.030509 Rudi Lagemann 3.747666 Emilio Ruiz Barrachina 3.676955 Krishna Ghattamaneni 3.676955 Milos Avramovic 3.606245 ... \u00dcmit Degirmenci NaN \u00dcmit El\u00e7i NaN \u00dcmit K\u00f6reken NaN \u00deorsteinn Gunnar Bjarnason NaN \u00de\u00f3rhildur \u00deorleifsd\u00f3ttir NaN Name: avg_vote, Length: 34733, dtype: float64 \ud83c\udfa5 L1 Q4 What director with more than 10 movies has the largest variance in avg_vote? \u00b6 director Harry Baweja 1.869954 Shaji Kailas 1.854502 Zdenek Troska 1.775984 Adam Rifkin 1.711251 Ram Gopal Varma 1.687850 ... Ford Beebe 0.224343 Ray Nazarro 0.210311 Jean Gr\u00e9millon 0.196946 Louis Feuillade 0.156428 Tsutomu Shibayama 0.126121 Name: avg_vote, Length: 1135, dtype: float64 \ud83c\udfa5 L1 Q5 What american directors with more than 5 movies have the largest variance in avg_vote? \u00b6 director Martin Brest 2.033716 David Winters 1.926049 Adam Rifkin 1.711251 Gus Trikonis 1.661271 Jerry Jameson 1.646107 ... Edward Killy 0.155265 Willis Goldbeck 0.139443 Richard T. Heffron 0.136626 Bill Plympton 0.136626 Nate Watt 0.129099 Name: avg_vote, Length: 859, dtype: float64 \ud83c\udfa5 L1 Q6 Where does M. Night Shyamalan fall on this rank scale? \u00b6 (He's number 36/859) what happens when you only include directors who, on average (based on mean), have made most their movies after 1990 and have produced 10 or more movies? (Shyamalan rises to 3/83) \ud83c\udfa5 L1 Q7 How many movies were made each year in US from 2000-2020 \u00b6 year 2000.0 363 2001.0 386 2002.0 360 2003.0 339 2004.0 362 2005.0 453 2006.0 590 2007.0 574 2008.0 592 2009.0 656 2010.0 611 2011.0 652 2012.0 738 2013.0 820 2014.0 807 2015.0 800 2016.0 869 2017.0 905 2018.0 886 2019.0 700 2020.0 276 Name: title, dtype: int64 \ud83c\udfa5 L1 Q8 Visualize The Results of Q7! \u00b6 <matplotlib.axes._subplots.AxesSubplot at 0x7fea042dc890> \ud83c\udfa5 L1 Q9 For single country movies, how many movies were made each year in each country from 2000-2020, only include countries that made more than 1000 movies in that timeframe \u00b6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year title 0 Canada 2000.0 39 1 Canada 2001.0 51 2 Canada 2002.0 49 3 Canada 2003.0 38 4 Canada 2004.0 52 \ud83c\udfa5 L1 Q10 Visualize the results from Q9! \u00b6","title":"Descriptive Statistics Data Hunt"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#data-science-foundations-lab-1-data-hunt-i","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's a data hunt. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from ipywidgets import interact df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_explorers/main/assets/imdb_movies.csv\" ) # converting years to numbers for easy conditionals df [ 'year' ] = pd . to_numeric ( df [ 'year' ], errors = 'coerce' ) df . shape /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) (85855, 22) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics 0 tt0000009 Miss Jerry Miss Jerry 1894.0 1894-10-09 Romance 45 USA None Alexander Black ... Blanche Bayliss, William Courtenay, Chauncey D... The adventures of a female reporter in the 1890s. 5.9 154 NaN NaN NaN NaN 1.0 2.0 1 tt0000574 The Story of the Kelly Gang The Story of the Kelly Gang 1906.0 1906-12-26 Biography, Crime, Drama 70 Australia None Charles Tait ... Elizabeth Tait, John Tait, Norman Campbell, Be... True story of notorious Australian outlaw Ned ... 6.1 589 $ 2250 NaN NaN NaN 7.0 7.0 2 tt0001892 Den sorte dr\u00f8m Den sorte dr\u00f8m 1911.0 1911-08-19 Drama 53 Germany, Denmark NaN Urban Gad ... Asta Nielsen, Valdemar Psilander, Gunnar Helse... Two men of high rank are both wooing the beaut... 5.8 188 NaN NaN NaN NaN 5.0 2.0 3 rows \u00d7 22 columns","title":"Data Science Foundations  Lab 1: Data Hunt I"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q1-what-american-director-has-the-highest-mean-avg_vote","text":"director Daniel Keith, Snorri Sturluson 9.3 Anthony Bawn 9.3 Derek Ahonen 9.2 Raghav Peri 9.1 James Marlowe 8.8 ... Waleed Bedour 1.2 Fred Ashman 1.1 Aeneas Middleton 1.1 Steven A. Sandt 1.1 Francis Hamada 1.1 Name: avg_vote, Length: 12463, dtype: float64","title":"\ud83c\udfa5 L1 Q1 What american director has the highest mean  avg_vote?"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q2-what-american-director-with-more-than-5-movies-has-the-highest-mean-avg_vote","text":"director Quentin Tarantino 7.811111 Charles Chaplin 7.764286 David Fincher 7.625000 Billy Wilder 7.580952 Martin Scorsese 7.544444 ... Barry Mahon 2.728571 Dennis Devine 2.657143 Bill Zebub 2.483333 Mark Polonia 2.462500 Christopher Forbes 2.000000 Name: avg_vote, Length: 859, dtype: float64","title":"\ud83c\udfa5 L1 Q2 What american director with more than 5 movies, has the highest mean avg_vote?"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q3-what-director-has-the-largest-variance-in-avg_vote","text":"director Deniz Denizciler 4.030509 Rudi Lagemann 3.747666 Emilio Ruiz Barrachina 3.676955 Krishna Ghattamaneni 3.676955 Milos Avramovic 3.606245 ... \u00dcmit Degirmenci NaN \u00dcmit El\u00e7i NaN \u00dcmit K\u00f6reken NaN \u00deorsteinn Gunnar Bjarnason NaN \u00de\u00f3rhildur \u00deorleifsd\u00f3ttir NaN Name: avg_vote, Length: 34733, dtype: float64","title":"\ud83c\udfa5 L1 Q3 What director has the largest variance in avg_vote?"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q4-what-director-with-more-than-10-movies-has-the-largest-variance-in-avg_vote","text":"director Harry Baweja 1.869954 Shaji Kailas 1.854502 Zdenek Troska 1.775984 Adam Rifkin 1.711251 Ram Gopal Varma 1.687850 ... Ford Beebe 0.224343 Ray Nazarro 0.210311 Jean Gr\u00e9millon 0.196946 Louis Feuillade 0.156428 Tsutomu Shibayama 0.126121 Name: avg_vote, Length: 1135, dtype: float64","title":"\ud83c\udfa5 L1 Q4 What director with more than 10 movies has the largest variance in avg_vote?"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q5-what-american-directors-with-more-than-5-movies-have-the-largest-variance-in-avg_vote","text":"director Martin Brest 2.033716 David Winters 1.926049 Adam Rifkin 1.711251 Gus Trikonis 1.661271 Jerry Jameson 1.646107 ... Edward Killy 0.155265 Willis Goldbeck 0.139443 Richard T. Heffron 0.136626 Bill Plympton 0.136626 Nate Watt 0.129099 Name: avg_vote, Length: 859, dtype: float64","title":"\ud83c\udfa5 L1 Q5 What american directors with more than 5 movies have the largest variance in avg_vote?"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q6-where-does-m-night-shyamalan-fall-on-this-rank-scale","text":"(He's number 36/859) what happens when you only include directors who, on average (based on mean), have made most their movies after 1990 and have produced 10 or more movies? (Shyamalan rises to 3/83)","title":"\ud83c\udfa5 L1 Q6 Where does M. Night Shyamalan fall on this rank scale?"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q7-how-many-movies-were-made-each-year-in-us-from-2000-2020","text":"year 2000.0 363 2001.0 386 2002.0 360 2003.0 339 2004.0 362 2005.0 453 2006.0 590 2007.0 574 2008.0 592 2009.0 656 2010.0 611 2011.0 652 2012.0 738 2013.0 820 2014.0 807 2015.0 800 2016.0 869 2017.0 905 2018.0 886 2019.0 700 2020.0 276 Name: title, dtype: int64","title":"\ud83c\udfa5 L1 Q7 How many movies were made each year in US from 2000-2020"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q8-visualize-the-results-of-q7","text":"<matplotlib.axes._subplots.AxesSubplot at 0x7fea042dc890>","title":"\ud83c\udfa5 L1 Q8 Visualize The Results of Q7!"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q9-for-single-country-movies-how-many-movies-were-made-each-year-in-each-country-from-2000-2020-only-include-countries-that-made-more-than-1000-movies-in-that-timeframe","text":".dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year title 0 Canada 2000.0 39 1 Canada 2001.0 51 2 Canada 2002.0 49 3 Canada 2003.0 38 4 Canada 2004.0 52","title":"\ud83c\udfa5 L1 Q9 For single country movies, how many movies were made each year in each country from 2000-2020, only include countries that made more than 1000 movies in that timeframe"},{"location":"labs/L1_Descriptive_Statistics_Data_Hunt/#l1-q10-visualize-the-results-from-q9","text":"","title":"\ud83c\udfa5 L1 Q10 Visualize the results from Q9!"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/","text":"Data Science Foundations Lab 2: Data Hunt II \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's the data hunt part TWO. Preparing Environment and Importing Data \u00b6 Import Packages \u00b6 ! pip install - U plotly Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1) Collecting plotly Downloading plotly-5.1.0-py2.py3-none-any.whl (20.6 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.6 MB 1.3 MB/s \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0) Collecting tenacity>=6.2.0 Downloading tenacity-8.0.1-py3-none-any.whl (24 kB) Installing collected packages: tenacity, plotly Attempting uninstall: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-5.1.0 tenacity-8.0.1 # our standard libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns from ipywidgets import interact # our stats libraries import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy # our scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score Import and Clean Data \u00b6 df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_fundamentals/main/assets/truffle_rates.csv\" ) df = df . loc [ df [ 'rate' ] > 0 ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group rate 0 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.167097 1 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.153827 2 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.100299 3 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.333008 4 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.078108 df . shape (9210, 6) Exploratory Data Analysis \u00b6 \ud83c\udf6b L2 Q1 Finding Influential Features \u00b6 Which of the five features (base_cake, truffle_type, primary_flavor, secondary_flavor, color_group) of the truffles is most influential on production rate? Back your answer with both a visualization of the distributions (boxplot, kernel denisty estimate, histogram, violin plot) and a statistical test (moods median, ANOVA, t-test) Be sure: everything is labeled (can you improve your labels with additional descriptive statistical information e.g. indicate mean, std, etc.) you meet the assumptions of your statistical test \ud83c\udf6b L2 Q1.1 Visualization \u00b6 Use any number of visualizations. Here is an example to get you started: # Example: a KDE of the truffle_type and base_cake columns fig , ax = plt . subplots ( 2 , 1 , figsize = ( 12 , 12 )) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'truffle_type' ], fill = True , ax = ax [ 0 ]) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'base_cake' ], fill = True , ax = ax [ 1 ]) <matplotlib.axes._subplots.AxesSubplot at 0x7f549eea03d0> /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 9 missing from current font. /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 9 missing from current font. \ud83c\udf6b L2 Q1.2 Statistical Analysis \u00b6 What statistical tests can you perform to evaluate your hypothesis from the visualizations (maybe you think one particular feature is significant). Here's an ANOVA on the truffle_type column to get you started: model = ols ( 'rate ~ C( {} )' . format ( 'truffle_type' ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Is this P value significant? What is the null hypothesis? How do we check the assumptions of ANOVA? base_cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(base_cake) 331.373550 5.0 2349.684756 0.0 Residual 259.606073 9204.0 NaN NaN Shapiro: 0.9281061887741089 0.0 Bartlett: 619.3727153356931 1.3175663824168166e-131 truffle_type /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Shapiro: 0.9645588994026184 1.3704698981096711e-42 /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Bartlett: 533.0206680979852 1.8031528902362296e-116 primary_flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(primary_flavor) 159.105452 47.0 71.815842 0.0 Residual 431.874171 9162.0 NaN NaN /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Shapiro: 0.9738250970840454 6.485387538059916e-38 Bartlett: 1609.0029005171464 1.848613457353585e-306 secondary_flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(secondary_flavor) 115.773877 28.0 79.884192 0.0 Residual 475.205747 9181.0 NaN NaN Shapiro: 0.9717048406600952 4.3392384038527993e-39 Bartlett: 1224.4882890761903 3.5546073028894766e-240 color_group /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(color_group) 33.878491 11.0 50.849974 1.873235e-109 Residual 557.101132 9198.0 NaN NaN Shapiro: 0.9598756432533264 1.401298464324817e-44 Bartlett: 298.6432027161358 1.6917844519244488e-57 /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. \ud83c\udf6b L2 Q2 Finding Best and Worst Groups \u00b6 \ud83c\udf6b L2 Q2.1 Compare Every Group to the Whole \u00b6 Of the primary flavors (feature), what 5 flavors (groups) would you recommend Truffletopia discontinue? Iterate through every level (i.e. pound, cheese, sponge cakes) of every category (i.e. base cake, primary flavor, secondary flavor) and use moods median testing to compare the group distribution to the grand median rate. (98, 10) After you've computed a moods median test on every group, filter any data above a significance level of 0.05 (76, 10) Return the groups with the lowest median performance (your table need not look exactly like the one I've created) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 primary_flavor Coconut 56.8675 4.66198e-14 0.310345 0.139998 0.0856284 100 2.64572e-29 [[12, 4593], [88, 4517]] 1 secondary_flavor Wild Cherry Cream 56.8675 4.66198e-14 0.310345 0.139998 0.0856284 100 2.64572e-29 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.5563 4.30253e-15 0.310345 0.129178 0.0928782 85 2.05798e-28 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.3203 7.84617e-13 0.310345 0.145727 0.0957584 91 1.11719e-28 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.5452 4.14269e-11 0.310345 0.148964 0.10588 70 2.59384e-20 [[7, 4598], [63, 4542]] 5 secondary_flavor Mixed Berry 164.099 1.43951e-37 0.310345 0.153713 0.115202 261 6.73636e-75 [[28, 4577], [233, 4372]] 6 secondary_flavor Peppermint 66.0235 4.45582e-16 0.310345 0.129107 0.12201 86 7.6449e-37 [[5, 4600], [81, 4524]] 7 base_cake Butter 696.649 1.60093e-153 0.310345 0.15951 0.136231 905 0 [[75, 4530], [830, 3775]] 8 secondary_flavor Rum 69.5192 7.56747e-17 0.310345 0.157568 0.139834 93 4.42643e-42 [[6, 4599], [87, 4518]] 9 secondary_flavor Cucumber 175.061 5.80604e-40 0.310345 0.170015 0.14097 288 4.33234e-79 [[33, 4572], [255, 4350]] 10 primary_flavor Gingersnap 131.114 2.33844e-30 0.310345 0.159268 0.143347 192 1.01371e-69 [[17, 4588], [175, 4430]] 11 primary_flavor Cherry Cream Spice 66.3302 3.81371e-16 0.310345 0.175751 0.146272 100 1.03408e-36 [[9, 4596], [91, 4514]] 12 primary_flavor Orange Brandy 97.0624 6.71776e-23 0.310345 0.185908 0.157804 186 1.86398e-49 [[26, 4579], [160, 4445]] 13 primary_flavor Irish Cream 87.5008 8.42448e-21 0.310345 0.184505 0.176935 151 6.30631e-52 [[18, 4587], [133, 4472]] 14 base_cake Chiffon 908.383 1.47733e-199 0.310345 0.208286 0.177773 1821 0 [[334, 4271], [1487, 3118]] 15 primary_flavor Ginger Lime 40.1257 2.38138e-10 0.310345 0.225157 0.181094 100 2.63308e-18 [[18, 4587], [82, 4523]] 16 primary_flavor Doughnut 98.4088 3.40338e-23 0.310345 0.234113 0.189888 300 1.03666e-40 [[65, 4540], [235, 4370]] 17 primary_flavor Butter Milk 28.3983 9.87498e-08 0.310345 0.237502 0.190708 100 1.96333e-20 [[23, 4582], [77, 4528]] 18 primary_flavor Pecan 40.8441 1.64868e-10 0.310345 0.197561 0.192372 89 2.86564e-25 [[14, 4591], [75, 4530]] 19 secondary_flavor Dill Pickle 69.8101 6.52964e-17 0.310345 0.228289 0.19916 241 6.39241e-33 [[56, 4549], [185, 4420]] We would want to cut the following primary flavors. Check to see that you get a similar answer. rip wild cherry cream. ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] \ud83c\udf6b L2 Q2.2 Beyond Statistical Testing: Using Reasoning \u00b6 Let's look at the total profile of the products associated with the five worst primary flavors. Given the number of different products made with any of these flavors, would you alter your answer at all? # 1. filter df for only bottom five flavors # 2. groupby all columns besides rate # 3. describe the rate column. # by doing this we can evaluate just how much sampling variety we have for the # worst performing flavors. bottom_five = [ 'Coconut' , 'Pink Lemonade' , 'Chocolate' , 'Wild Cherry Cream' , 'Gingersnap' ] df . loc [ df [ 'primary_flavor' ] . isin ( bottom_five )] . groupby ( list ( df . columns [: - 1 ]))[ 'rate' ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max base_cake truffle_type primary_flavor secondary_flavor color_group Butter Jelly Filled Pink Lemonade Butter Rum Rose 85.0 0.129178 0.137326 0.000061 0.032887 0.092878 0.171350 0.860045 Chiffon Candy Outer Wild Cherry Cream Rock and Rye Olive 17.0 0.094287 0.059273 0.010464 0.053976 0.077098 0.120494 0.229933 Chocolate Outer Gingersnap Dill Pickle Burgundy 59.0 0.133272 0.080414 0.021099 0.069133 0.137972 0.172066 0.401387 Jelly Filled Chocolate Tutti Frutti Burgundy 91.0 0.145727 0.135230 0.000033 0.044847 0.095758 0.185891 0.586570 Pound Candy Outer Coconut Wild Cherry Cream Taupe 100.0 0.139998 0.147723 0.000705 0.036004 0.085628 0.187318 0.775210 Chocolate Outer Gingersnap Rock and Rye Black 67.0 0.156160 0.110666 0.002846 0.074615 0.139572 0.241114 0.551898 Jelly Filled Gingersnap Kiwi Taupe 66.0 0.185662 0.132272 0.000014 0.086377 0.166340 0.247397 0.593016 Wild Cherry Cream Mango Taupe 53.0 0.166502 0.160090 0.001412 0.056970 0.108918 0.207306 0.787224 \ud83c\udf6b L2 Q2.3 The Jelly Filled Conundrum \u00b6 Your boss notices the Jelly filled truffles are being produced much faster than the candy outer truffles and suggests expanding into this product line. What is your response? Use the visualization tool below to help you think about this problem, then create any visualizations or analyses of your own. sunburst charts def sun ( path = [[ 'base_cake' , 'truffle_type' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ], [ 'truffle_type' , 'base_cake' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ]]): fig = px . sunburst ( df , path = path , color = 'rate' , color_continuous_scale = 'viridis' , ) fig . update_layout ( margin = dict ( l = 20 , r = 20 , t = 20 , b = 20 ), height = 650 ) fig . show () interact ( sun ) interactive(children=(Dropdown(description='path', options=(['base_cake', 'truffle_type', 'primary_flavor', 's\u2026 <function __main__.sun>","title":"Inferential Statistics Data Hunt"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#data-science-foundations-lab-2-data-hunt-ii","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's the data hunt part TWO.","title":"Data Science Foundations  Lab 2: Data Hunt II"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#preparing-environment-and-importing-data","text":"","title":"Preparing Environment and Importing Data"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#import-packages","text":"! pip install - U plotly Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1) Collecting plotly Downloading plotly-5.1.0-py2.py3-none-any.whl (20.6 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.6 MB 1.3 MB/s \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0) Collecting tenacity>=6.2.0 Downloading tenacity-8.0.1-py3-none-any.whl (24 kB) Installing collected packages: tenacity, plotly Attempting uninstall: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-5.1.0 tenacity-8.0.1 # our standard libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns from ipywidgets import interact # our stats libraries import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy # our scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score","title":"Import Packages"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#import-and-clean-data","text":"df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_fundamentals/main/assets/truffle_rates.csv\" ) df = df . loc [ df [ 'rate' ] > 0 ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group rate 0 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.167097 1 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.153827 2 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.100299 3 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.333008 4 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.078108 df . shape (9210, 6)","title":"Import and Clean Data"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#exploratory-data-analysis","text":"","title":"Exploratory Data Analysis"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q1-finding-influential-features","text":"Which of the five features (base_cake, truffle_type, primary_flavor, secondary_flavor, color_group) of the truffles is most influential on production rate? Back your answer with both a visualization of the distributions (boxplot, kernel denisty estimate, histogram, violin plot) and a statistical test (moods median, ANOVA, t-test) Be sure: everything is labeled (can you improve your labels with additional descriptive statistical information e.g. indicate mean, std, etc.) you meet the assumptions of your statistical test","title":"\ud83c\udf6b L2 Q1 Finding Influential Features"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q11-visualization","text":"Use any number of visualizations. Here is an example to get you started: # Example: a KDE of the truffle_type and base_cake columns fig , ax = plt . subplots ( 2 , 1 , figsize = ( 12 , 12 )) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'truffle_type' ], fill = True , ax = ax [ 0 ]) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'base_cake' ], fill = True , ax = ax [ 1 ]) <matplotlib.axes._subplots.AxesSubplot at 0x7f549eea03d0> /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 9 missing from current font. /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 9 missing from current font.","title":"\ud83c\udf6b L2 Q1.1 Visualization"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q12-statistical-analysis","text":"What statistical tests can you perform to evaluate your hypothesis from the visualizations (maybe you think one particular feature is significant). Here's an ANOVA on the truffle_type column to get you started: model = ols ( 'rate ~ C( {} )' . format ( 'truffle_type' ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Is this P value significant? What is the null hypothesis? How do we check the assumptions of ANOVA? base_cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(base_cake) 331.373550 5.0 2349.684756 0.0 Residual 259.606073 9204.0 NaN NaN Shapiro: 0.9281061887741089 0.0 Bartlett: 619.3727153356931 1.3175663824168166e-131 truffle_type /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Shapiro: 0.9645588994026184 1.3704698981096711e-42 /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Bartlett: 533.0206680979852 1.8031528902362296e-116 primary_flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(primary_flavor) 159.105452 47.0 71.815842 0.0 Residual 431.874171 9162.0 NaN NaN /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. Shapiro: 0.9738250970840454 6.485387538059916e-38 Bartlett: 1609.0029005171464 1.848613457353585e-306 secondary_flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(secondary_flavor) 115.773877 28.0 79.884192 0.0 Residual 475.205747 9181.0 NaN NaN Shapiro: 0.9717048406600952 4.3392384038527993e-39 Bartlett: 1224.4882890761903 3.5546073028894766e-240 color_group /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(color_group) 33.878491 11.0 50.849974 1.873235e-109 Residual 557.101132 9198.0 NaN NaN Shapiro: 0.9598756432533264 1.401298464324817e-44 Bartlett: 298.6432027161358 1.6917844519244488e-57 /usr/local/lib/python3.7/dist-packages/scipy/stats/morestats.py:1676: UserWarning: p-value may not be accurate for N > 5000.","title":"\ud83c\udf6b L2 Q1.2 Statistical Analysis"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q2-finding-best-and-worst-groups","text":"","title":"\ud83c\udf6b L2 Q2 Finding Best and Worst Groups"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q21-compare-every-group-to-the-whole","text":"Of the primary flavors (feature), what 5 flavors (groups) would you recommend Truffletopia discontinue? Iterate through every level (i.e. pound, cheese, sponge cakes) of every category (i.e. base cake, primary flavor, secondary flavor) and use moods median testing to compare the group distribution to the grand median rate. (98, 10) After you've computed a moods median test on every group, filter any data above a significance level of 0.05 (76, 10) Return the groups with the lowest median performance (your table need not look exactly like the one I've created) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 primary_flavor Coconut 56.8675 4.66198e-14 0.310345 0.139998 0.0856284 100 2.64572e-29 [[12, 4593], [88, 4517]] 1 secondary_flavor Wild Cherry Cream 56.8675 4.66198e-14 0.310345 0.139998 0.0856284 100 2.64572e-29 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.5563 4.30253e-15 0.310345 0.129178 0.0928782 85 2.05798e-28 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.3203 7.84617e-13 0.310345 0.145727 0.0957584 91 1.11719e-28 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.5452 4.14269e-11 0.310345 0.148964 0.10588 70 2.59384e-20 [[7, 4598], [63, 4542]] 5 secondary_flavor Mixed Berry 164.099 1.43951e-37 0.310345 0.153713 0.115202 261 6.73636e-75 [[28, 4577], [233, 4372]] 6 secondary_flavor Peppermint 66.0235 4.45582e-16 0.310345 0.129107 0.12201 86 7.6449e-37 [[5, 4600], [81, 4524]] 7 base_cake Butter 696.649 1.60093e-153 0.310345 0.15951 0.136231 905 0 [[75, 4530], [830, 3775]] 8 secondary_flavor Rum 69.5192 7.56747e-17 0.310345 0.157568 0.139834 93 4.42643e-42 [[6, 4599], [87, 4518]] 9 secondary_flavor Cucumber 175.061 5.80604e-40 0.310345 0.170015 0.14097 288 4.33234e-79 [[33, 4572], [255, 4350]] 10 primary_flavor Gingersnap 131.114 2.33844e-30 0.310345 0.159268 0.143347 192 1.01371e-69 [[17, 4588], [175, 4430]] 11 primary_flavor Cherry Cream Spice 66.3302 3.81371e-16 0.310345 0.175751 0.146272 100 1.03408e-36 [[9, 4596], [91, 4514]] 12 primary_flavor Orange Brandy 97.0624 6.71776e-23 0.310345 0.185908 0.157804 186 1.86398e-49 [[26, 4579], [160, 4445]] 13 primary_flavor Irish Cream 87.5008 8.42448e-21 0.310345 0.184505 0.176935 151 6.30631e-52 [[18, 4587], [133, 4472]] 14 base_cake Chiffon 908.383 1.47733e-199 0.310345 0.208286 0.177773 1821 0 [[334, 4271], [1487, 3118]] 15 primary_flavor Ginger Lime 40.1257 2.38138e-10 0.310345 0.225157 0.181094 100 2.63308e-18 [[18, 4587], [82, 4523]] 16 primary_flavor Doughnut 98.4088 3.40338e-23 0.310345 0.234113 0.189888 300 1.03666e-40 [[65, 4540], [235, 4370]] 17 primary_flavor Butter Milk 28.3983 9.87498e-08 0.310345 0.237502 0.190708 100 1.96333e-20 [[23, 4582], [77, 4528]] 18 primary_flavor Pecan 40.8441 1.64868e-10 0.310345 0.197561 0.192372 89 2.86564e-25 [[14, 4591], [75, 4530]] 19 secondary_flavor Dill Pickle 69.8101 6.52964e-17 0.310345 0.228289 0.19916 241 6.39241e-33 [[56, 4549], [185, 4420]] We would want to cut the following primary flavors. Check to see that you get a similar answer. rip wild cherry cream. ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap']","title":"\ud83c\udf6b L2 Q2.1 Compare Every Group to the Whole"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q22-beyond-statistical-testing-using-reasoning","text":"Let's look at the total profile of the products associated with the five worst primary flavors. Given the number of different products made with any of these flavors, would you alter your answer at all? # 1. filter df for only bottom five flavors # 2. groupby all columns besides rate # 3. describe the rate column. # by doing this we can evaluate just how much sampling variety we have for the # worst performing flavors. bottom_five = [ 'Coconut' , 'Pink Lemonade' , 'Chocolate' , 'Wild Cherry Cream' , 'Gingersnap' ] df . loc [ df [ 'primary_flavor' ] . isin ( bottom_five )] . groupby ( list ( df . columns [: - 1 ]))[ 'rate' ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max base_cake truffle_type primary_flavor secondary_flavor color_group Butter Jelly Filled Pink Lemonade Butter Rum Rose 85.0 0.129178 0.137326 0.000061 0.032887 0.092878 0.171350 0.860045 Chiffon Candy Outer Wild Cherry Cream Rock and Rye Olive 17.0 0.094287 0.059273 0.010464 0.053976 0.077098 0.120494 0.229933 Chocolate Outer Gingersnap Dill Pickle Burgundy 59.0 0.133272 0.080414 0.021099 0.069133 0.137972 0.172066 0.401387 Jelly Filled Chocolate Tutti Frutti Burgundy 91.0 0.145727 0.135230 0.000033 0.044847 0.095758 0.185891 0.586570 Pound Candy Outer Coconut Wild Cherry Cream Taupe 100.0 0.139998 0.147723 0.000705 0.036004 0.085628 0.187318 0.775210 Chocolate Outer Gingersnap Rock and Rye Black 67.0 0.156160 0.110666 0.002846 0.074615 0.139572 0.241114 0.551898 Jelly Filled Gingersnap Kiwi Taupe 66.0 0.185662 0.132272 0.000014 0.086377 0.166340 0.247397 0.593016 Wild Cherry Cream Mango Taupe 53.0 0.166502 0.160090 0.001412 0.056970 0.108918 0.207306 0.787224","title":"\ud83c\udf6b L2 Q2.2 Beyond Statistical Testing: Using Reasoning"},{"location":"labs/L2_Inferential_Statistics_Data_Hunt/#l2-q23-the-jelly-filled-conundrum","text":"Your boss notices the Jelly filled truffles are being produced much faster than the candy outer truffles and suggests expanding into this product line. What is your response? Use the visualization tool below to help you think about this problem, then create any visualizations or analyses of your own. sunburst charts def sun ( path = [[ 'base_cake' , 'truffle_type' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ], [ 'truffle_type' , 'base_cake' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ]]): fig = px . sunburst ( df , path = path , color = 'rate' , color_continuous_scale = 'viridis' , ) fig . update_layout ( margin = dict ( l = 20 , r = 20 , t = 20 , b = 20 ), height = 650 ) fig . show () interact ( sun ) interactive(children=(Dropdown(description='path', options=(['base_cake', 'truffle_type', 'primary_flavor', 's\u2026 <function __main__.sun>","title":"\ud83c\udf6b L2 Q2.3 The Jelly Filled Conundrum"},{"location":"labs/L3_Feature_Engineering/","text":"Data Science Foundations Lab 3: Practice with Feature Engineering and Pipelines \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines and feature engineering. We will use the wine dataset. import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn.model_selection import train_test_split from statsmodels.stats.outliers_influence import variance_inflation_factor wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) \ud83c\udf47 L3 Q1: Feature Derivation \u00b6 Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 One-Hot encode categorical variables in the wine dataset # Code Cell for L1 Q1 display ( wine . head ()) print ( wine . shape ) str_cols = [ 'type' ] enc = OneHotEncoder () imp = SimpleImputer () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 (6497, 13) (6497,) (6497, 13) \ud83c\udf7e L3 Q2: Feature Transformation \u00b6 Use StandardScaler on the input data and evaluate how this affects VIF, kurtosis, and skew You should ignore the one-hot encoded column(s) for this section # Non-one hot encoded columns cols = list ( wine . columns ) cols . remove ( 'density' ) cols . remove ( 'type' ) # Code Cell for L1 Q2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 41.790949 5.070143 1.724131 volatile acidity 9.482732 2.834263 1.496433 citric acid 9.344218 2.404077 0.473142 residual sugar 3.336944 4.360399 1.435221 chlorides 5.398369 50.911457 5.400680 free sulfur dioxide 8.529778 7.906238 1.220066 total sulfur dioxide 13.448130 -0.371664 -0.001177 pH 149.003349 0.374743 0.387234 sulphates 18.402953 8.667071 1.799021 alcohol 114.836088 -0.531687 0.565718 quality 63.455488 0.232322 0.189623 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 1.781336 5.070143 1.724131 volatile acidity 1.808525 2.834263 1.496433 citric acid 1.606484 2.404077 0.473142 residual sugar 1.533403 4.360399 1.435221 chlorides 1.564413 50.911457 5.400680 free sulfur dioxide 2.156598 7.906238 1.220066 total sulfur dioxide 2.872586 -0.371664 -0.001177 pH 1.413100 0.374743 0.387234 sulphates 1.364157 8.667071 1.799021 alcohol 1.696986 -0.531687 0.565718 quality 1.408210 0.232322 0.189623 \ud83c\udf77 L3 Q3: Modeling \u00b6 Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using logistic regression: dependent variable: wine quality If you are using linear regression: dependent variable: wine density # Code Cell for L1 Q3 model = LinearRegression () X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) Text(0.5, 1.0, 'Test, R2: 0.963')","title":"Practice with Feature Engineering"},{"location":"labs/L3_Feature_Engineering/#data-science-foundations-lab-3-practice-with-feature-engineering-and-pipelines","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines and feature engineering. We will use the wine dataset. import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn.model_selection import train_test_split from statsmodels.stats.outliers_influence import variance_inflation_factor wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" )","title":"Data Science Foundations  Lab 3: Practice with Feature Engineering and Pipelines"},{"location":"labs/L3_Feature_Engineering/#l3-q1-feature-derivation","text":"Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 One-Hot encode categorical variables in the wine dataset # Code Cell for L1 Q1 display ( wine . head ()) print ( wine . shape ) str_cols = [ 'type' ] enc = OneHotEncoder () imp = SimpleImputer () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 (6497, 13) (6497,) (6497, 13)","title":"\ud83c\udf47 L3 Q1: Feature Derivation"},{"location":"labs/L3_Feature_Engineering/#l3-q2-feature-transformation","text":"Use StandardScaler on the input data and evaluate how this affects VIF, kurtosis, and skew You should ignore the one-hot encoded column(s) for this section # Non-one hot encoded columns cols = list ( wine . columns ) cols . remove ( 'density' ) cols . remove ( 'type' ) # Code Cell for L1 Q2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 41.790949 5.070143 1.724131 volatile acidity 9.482732 2.834263 1.496433 citric acid 9.344218 2.404077 0.473142 residual sugar 3.336944 4.360399 1.435221 chlorides 5.398369 50.911457 5.400680 free sulfur dioxide 8.529778 7.906238 1.220066 total sulfur dioxide 13.448130 -0.371664 -0.001177 pH 149.003349 0.374743 0.387234 sulphates 18.402953 8.667071 1.799021 alcohol 114.836088 -0.531687 0.565718 quality 63.455488 0.232322 0.189623 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 1.781336 5.070143 1.724131 volatile acidity 1.808525 2.834263 1.496433 citric acid 1.606484 2.404077 0.473142 residual sugar 1.533403 4.360399 1.435221 chlorides 1.564413 50.911457 5.400680 free sulfur dioxide 2.156598 7.906238 1.220066 total sulfur dioxide 2.872586 -0.371664 -0.001177 pH 1.413100 0.374743 0.387234 sulphates 1.364157 8.667071 1.799021 alcohol 1.696986 -0.531687 0.565718 quality 1.408210 0.232322 0.189623","title":"\ud83c\udf7e L3 Q2: Feature Transformation"},{"location":"labs/L3_Feature_Engineering/#l3-q3-modeling","text":"Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using logistic regression: dependent variable: wine quality If you are using linear regression: dependent variable: wine density # Code Cell for L1 Q3 model = LinearRegression () X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) Text(0.5, 1.0, 'Test, R2: 0.963')","title":"\ud83c\udf77 L3 Q3: Modeling"},{"location":"labs/L4_Supervised_Learners/","text":"Data Science Foundations Lab 4: Practice with Supervised Learners \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines, feature engineering, and applying learning algorithms. Now that we have covered supervised learning methods, and we've covered Grid Search, we will use these tools to do a sophisticated, search of hyperparameter optimization. import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression , LinearRegression import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.model_selection import train_test_split from sklearn import metrics wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'quality' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ target ] = y to compare, here is our results performing classification on this set of data with just logistic regression: model = LogisticRegression ( max_iter = 1e4 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.60 0.07 0.12 46 5 0.58 0.61 0.59 420 6 0.52 0.68 0.59 579 7 0.44 0.19 0.26 221 8 0.00 0.00 0.00 32 accuracy 0.54 1300 macro avg 0.36 0.26 0.26 1300 weighted avg 0.51 0.54 0.50 1300 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) <AxesSubplot:> \ud83c\udfce\ufe0f L4 Q1: \u00b6 Evaluate the performance of a Random Forest on classifying wine quality # Code Cell for L1 Q2 \ud83d\udd2c L4 Q2: \u00b6 Do a grid search to optimize your Random Forest model, use whatever hyperparameters you would like RandomForestClassifier () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Code Cell for L1 Q3 from sklearn.model_selection import GridSearchCV param_grid = { 'bootstrap' : [ True , False ]} grid = GridSearchCV ( RandomForestClassifier (), param_grid , cv = 7 )","title":"Practice with Supervised Learners"},{"location":"labs/L4_Supervised_Learners/#data-science-foundations-lab-4-practice-with-supervised-learners","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines, feature engineering, and applying learning algorithms. Now that we have covered supervised learning methods, and we've covered Grid Search, we will use these tools to do a sophisticated, search of hyperparameter optimization. import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression , LinearRegression import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.model_selection import train_test_split from sklearn import metrics wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'quality' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ target ] = y to compare, here is our results performing classification on this set of data with just logistic regression: model = LogisticRegression ( max_iter = 1e4 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.60 0.07 0.12 46 5 0.58 0.61 0.59 420 6 0.52 0.68 0.59 579 7 0.44 0.19 0.26 221 8 0.00 0.00 0.00 32 accuracy 0.54 1300 macro avg 0.36 0.26 0.26 1300 weighted avg 0.51 0.54 0.50 1300 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) <AxesSubplot:>","title":"Data Science Foundations  Lab 4: Practice with Supervised Learners"},{"location":"labs/L4_Supervised_Learners/#l4-q1","text":"Evaluate the performance of a Random Forest on classifying wine quality # Code Cell for L1 Q2","title":"\ud83c\udfce\ufe0f L4 Q1:"},{"location":"labs/L4_Supervised_Learners/#l4-q2","text":"Do a grid search to optimize your Random Forest model, use whatever hyperparameters you would like RandomForestClassifier () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Code Cell for L1 Q3 from sklearn.model_selection import GridSearchCV param_grid = { 'bootstrap' : [ True , False ]} grid = GridSearchCV ( RandomForestClassifier (), param_grid , cv = 7 )","title":"\ud83d\udd2c L4 Q2:"},{"location":"labs/L5_Writing_Unit_Tests/","text":"Data Science Foundations Lab 5: Writing Unit Tests \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab, we will try our hand at writing unit tests Import Libraries \u00b6 import random import numpy as np from contextlib import contextmanager import sys , os @contextmanager def suppress_stdout (): with open ( os . devnull , \"w\" ) as devnull : old_stdout = sys . stdout sys . stdout = devnull try : yield finally : sys . stdout = old_stdout Types of Tests \u00b6 There are two main types of tests we want to distinguish: Unit test : an automatic test to test the internal workings of a class or function. It should be a stand-alone test which is not related to other resources. Integration test : an automatic test that is done on an environment, it tests the coordination of different classes and functions as well as with the running environment. This usually precedes sending code to a QA team. To this I will add: Acid test : extremely rigorous tests that push beyond the intended use cases for your classes/functions. Written when you, like me, cannot afford QA employees to actually test your code. (word origin: gold acid tests in the 1850s , acid tests in the 70's ) EDIT : you could also call this a corner, or an edge case In this lab we will focus on unit tests . Unit Tests \u00b6 Each unit test should test the smallest portion of your code possible, i.e. a single method or function. Any random number generators should be seeded so that they run the exact same way every time. Unit tests should not rely on any local files or the local environment. Why bother with Unit Tests when we have Integration tests? A major challenge with integration testing is when an integration test fails. It\u2019s very hard to diagnose a system issue without being able to isolate which part of the system is failing. Here comes the unit test to the rescue. Let's take a simple example. If I wanted to test that the sume of two numbers is correct assert sum ([ 2 , 5 ]) == 7 , \"should be 7\" Nothing is sent to the print out because the condition is satisfied. If we run, however: assert sum([2, 4]) == 7, \"should be 7\" we get an error message: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-3-d5724b127818> in <module>() ----> 1 assert sum([2, 4]) == 7, \"should be 7\" AssertionError: should be 7 To make this a Unit Test, you will want to wrap it in a function def test_sum (): assert sum ([ 1 , 2 , 3 ]) == 6 , \"Should be 6\" test_sum () print ( \"Everything passed\" ) Everything passed And if we include a test that does not pass: def test_sum(): assert sum([3, 3]) == 6, \"Should be 6\" def test_my_broken_func(): assert sum([1, 2]) == 5, \"Should be 5\" test_sum() test_my_broken_func() print(\"Everything passed\") Here our test fails, because the sum of 1 and 2 is 3 and not 5. We get a traceback that tells us the source of the error: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-13-8a552fbf52bd> in <module>() 6 7 test_sum() ----> 8 test_my_broken_func() 9 print(\"Everything passed\") <ipython-input-13-8a552fbf52bd> in test_my_broken_func() 3 4 def test_my_broken_func(): ----> 5 assert sum([1, 2]) == 5, \"Should be 5\" 6 7 test_sum() AssertionError: Should be 5 Before sending us on our merry way to practice writing unit tests, we will want to ask, what do I want to write a test about? Here, we've been testing sum(). There are many behaviors in sum() we could check, such as: Does it sum a list of whole numbers (integers)? Can it sum a tuple or set? Can it sum a list of floats? What happens if one of the numbers is negative? etc.. In the end, what you test is up to you, and depends on your intended use cases. As a general rule of thumb, your unit test should test what is relevant. The only caveat to that, is that many continuous integration services (like TravisCI ) will benchmark you based on the percentage of lines of code you have that are covered by your unit tests (ex: 85% coverage ). \u270d\ud83c\udffd L5 Q1 Write a Unit Test \u00b6 Remember our Pokeball discussion in Python Foundations ? We'll return to that here. This time writing unit tests for our classes. Sometimes when writing unit tests, it can be more complicated than checking the return value of a function. Think back on our pokemon example: class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None If I wanted to write a unit test for the release method, I couldn't directly check for the output of a function. I'll have to check for a side effect , in this case, the change of an attribute belonging to a pokeball object; that is the change to the attribute contains . class Pokeball : def __init__ ( self , contains = None , type_name = \"poke ball\" ): self . contains = contains self . type_name = type_name self . catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch ( self , pokemon ): if self . contains == None : if random . random () < self . catch_rate : self . contains = pokemon print ( f \" { pokemon } captured!\" ) else : print ( f \" { pokemon } escaped!\" ) pass else : print ( \"pokeball is not empty!\" ) def release ( self ): if self . contains == None : print ( \"Pokeball is already empty\" ) else : print ( self . contains , \"has been released\" ) self . contains = None In the following cell, finish the code to test the functionality of the release method: def test_release (): ball = Pokeball () ball . contains = 'Pikachu' ball . release () # turn the pseudo code below into an assert statement ### YOUR CODE HERE ### # assert <object.attribute> == <something>, \"some erroneous message\" test_release () Pikachu has been released \u26f9\ufe0f L5 Q2 Write a Unit Test for the Catch Rate \u00b6 First, we will check that the succcessful catch is operating correctly. Remember that we depend on random.random and condition our success on whether that random value is less than the catch_rate of the pokeball: if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon so to test whether the successful catch is working we will seed our random number generator with a value that returns less than the catch_rate of the pokeball and then write our assert statement: def test_successful_catch (): # choose a random seed such that # we know the catch call should succeed ### YOUR CODE BELOW ### # random.seed(<your number here>) ball = Pokeball () ball . catch ( 'Psyduck' ) # Someone's fave pokemon (bless 'em) ### YOUR CODE BELOW ### # <object.attribute> == <something>, \"ball did not catch as expected\" NICE. Now we will do the same thing again, this time testing for an unsuccessful catch. SO in order to do this, we need to choose a random seed that will cause our catch to fail: def test_unsuccessful_catch (): # choose a random seed such that # we know the catch call should FAIL ### YOUR CODE BELOW ### # random.seed(<your number here>) ball = Pokeball () ball . catch ( 'Psyduck' ) ### YOUR CODE BELOW ### # <object.attribute> == <something>, \"ball did not fail as expected\" When you are finished test your functions below test_unsuccessful_catch () Psyduck captured! test_successful_catch () Psyduck captured! \u2696\ufe0f L5 Q3 Write a Unit Test that Checks Whether the Overall Catch Rate is 50/50 \u00b6 For this one, we're going to take those same ideas around seeding the random number generator. However, here we'd like to run the catch function multiple times to check whether it is truly creating a 50/50 catch rate situation. Here's a pseudo code outline: seed the random number generator for 100 iterations: create a pokeball try to catch something log whether it was successful check that for the 100 attempts the success was approximately 50/50 note: you can use my suppress stdout() function to suppress the print statements from ball.catch ex: with suppress_stdout(): print(\"HELLO OUT THERE!\") quick segway : what is the actual behavior of random.seed() ? Does it produce the same number every time we call random.random() now? Check for yourself: random . seed ( 42 ) [ random . random () for i in range ( 5 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124] We see that it still produces random numbers with each call to random.random . However, those numbers are the same with every execution of the cell. What happens when we do this: [ random . random () for i in range ( 5 )] [0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] The numbers are different. BUT: random . seed ( 42 ) [ random . random () for i in range ( 10 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124, 0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] We see them here in the bottom half of the list again. So, random.seed() is seeding the random number generator such that it will produce the same sequence of random numbers every time from the given seed. This will reset whenever random.seed() is set again. This behavior is useful because it allows us to continue using random number generation in our code, (for testing, creating examples and demos, etc.) but it will be reproducable each time. End Segway # 1. seed the random number generator # 2. for 100 iterations: # * create a pokeball # * try to catch something # * log whether it was successful # 3. check that for the 100 attempts the success was approximately 50/50 def test_catch_rate (): ### YOUR CODE HERE ### ### END YOUR CODE ### # assert np.abs(np.mean(results) - 0.5) < 0.1, \"catch rate not 50/50\" pass test_catch_rate () Test Runners \u00b6 When we start to create many tests like this, it can be cumbersome to run them all at once and log which ones fail. To handle our unit tests we use what are called test runners . We won't dedicate time to any single one here but the three most common are: unittest nose2 pytest unittest is built into python. I don't like it because you have to follow a strict class/method structure when writing the tests. nose2 is popular with many useful features and is generally good for high volumes of tests. My favorite is pytest, it's flexible and has an ecosystem of plugins for extensibility.","title":"Practice with Writing Unit Tests"},{"location":"labs/L5_Writing_Unit_Tests/#data-science-foundations-lab-5-writing-unit-tests","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab, we will try our hand at writing unit tests","title":"Data Science Foundations  Lab 5: Writing Unit Tests"},{"location":"labs/L5_Writing_Unit_Tests/#import-libraries","text":"import random import numpy as np from contextlib import contextmanager import sys , os @contextmanager def suppress_stdout (): with open ( os . devnull , \"w\" ) as devnull : old_stdout = sys . stdout sys . stdout = devnull try : yield finally : sys . stdout = old_stdout","title":"Import Libraries"},{"location":"labs/L5_Writing_Unit_Tests/#types-of-tests","text":"There are two main types of tests we want to distinguish: Unit test : an automatic test to test the internal workings of a class or function. It should be a stand-alone test which is not related to other resources. Integration test : an automatic test that is done on an environment, it tests the coordination of different classes and functions as well as with the running environment. This usually precedes sending code to a QA team. To this I will add: Acid test : extremely rigorous tests that push beyond the intended use cases for your classes/functions. Written when you, like me, cannot afford QA employees to actually test your code. (word origin: gold acid tests in the 1850s , acid tests in the 70's ) EDIT : you could also call this a corner, or an edge case In this lab we will focus on unit tests .","title":"Types of Tests"},{"location":"labs/L5_Writing_Unit_Tests/#unit-tests","text":"Each unit test should test the smallest portion of your code possible, i.e. a single method or function. Any random number generators should be seeded so that they run the exact same way every time. Unit tests should not rely on any local files or the local environment. Why bother with Unit Tests when we have Integration tests? A major challenge with integration testing is when an integration test fails. It\u2019s very hard to diagnose a system issue without being able to isolate which part of the system is failing. Here comes the unit test to the rescue. Let's take a simple example. If I wanted to test that the sume of two numbers is correct assert sum ([ 2 , 5 ]) == 7 , \"should be 7\" Nothing is sent to the print out because the condition is satisfied. If we run, however: assert sum([2, 4]) == 7, \"should be 7\" we get an error message: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-3-d5724b127818> in <module>() ----> 1 assert sum([2, 4]) == 7, \"should be 7\" AssertionError: should be 7 To make this a Unit Test, you will want to wrap it in a function def test_sum (): assert sum ([ 1 , 2 , 3 ]) == 6 , \"Should be 6\" test_sum () print ( \"Everything passed\" ) Everything passed And if we include a test that does not pass: def test_sum(): assert sum([3, 3]) == 6, \"Should be 6\" def test_my_broken_func(): assert sum([1, 2]) == 5, \"Should be 5\" test_sum() test_my_broken_func() print(\"Everything passed\") Here our test fails, because the sum of 1 and 2 is 3 and not 5. We get a traceback that tells us the source of the error: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-13-8a552fbf52bd> in <module>() 6 7 test_sum() ----> 8 test_my_broken_func() 9 print(\"Everything passed\") <ipython-input-13-8a552fbf52bd> in test_my_broken_func() 3 4 def test_my_broken_func(): ----> 5 assert sum([1, 2]) == 5, \"Should be 5\" 6 7 test_sum() AssertionError: Should be 5 Before sending us on our merry way to practice writing unit tests, we will want to ask, what do I want to write a test about? Here, we've been testing sum(). There are many behaviors in sum() we could check, such as: Does it sum a list of whole numbers (integers)? Can it sum a tuple or set? Can it sum a list of floats? What happens if one of the numbers is negative? etc.. In the end, what you test is up to you, and depends on your intended use cases. As a general rule of thumb, your unit test should test what is relevant. The only caveat to that, is that many continuous integration services (like TravisCI ) will benchmark you based on the percentage of lines of code you have that are covered by your unit tests (ex: 85% coverage ).","title":"Unit Tests"},{"location":"labs/L5_Writing_Unit_Tests/#l5-q1-write-a-unit-test","text":"Remember our Pokeball discussion in Python Foundations ? We'll return to that here. This time writing unit tests for our classes. Sometimes when writing unit tests, it can be more complicated than checking the return value of a function. Think back on our pokemon example: class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None If I wanted to write a unit test for the release method, I couldn't directly check for the output of a function. I'll have to check for a side effect , in this case, the change of an attribute belonging to a pokeball object; that is the change to the attribute contains . class Pokeball : def __init__ ( self , contains = None , type_name = \"poke ball\" ): self . contains = contains self . type_name = type_name self . catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch ( self , pokemon ): if self . contains == None : if random . random () < self . catch_rate : self . contains = pokemon print ( f \" { pokemon } captured!\" ) else : print ( f \" { pokemon } escaped!\" ) pass else : print ( \"pokeball is not empty!\" ) def release ( self ): if self . contains == None : print ( \"Pokeball is already empty\" ) else : print ( self . contains , \"has been released\" ) self . contains = None In the following cell, finish the code to test the functionality of the release method: def test_release (): ball = Pokeball () ball . contains = 'Pikachu' ball . release () # turn the pseudo code below into an assert statement ### YOUR CODE HERE ### # assert <object.attribute> == <something>, \"some erroneous message\" test_release () Pikachu has been released","title":"\u270d\ud83c\udffd L5 Q1 Write a Unit Test"},{"location":"labs/L5_Writing_Unit_Tests/#l5-q2-write-a-unit-test-for-the-catch-rate","text":"First, we will check that the succcessful catch is operating correctly. Remember that we depend on random.random and condition our success on whether that random value is less than the catch_rate of the pokeball: if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon so to test whether the successful catch is working we will seed our random number generator with a value that returns less than the catch_rate of the pokeball and then write our assert statement: def test_successful_catch (): # choose a random seed such that # we know the catch call should succeed ### YOUR CODE BELOW ### # random.seed(<your number here>) ball = Pokeball () ball . catch ( 'Psyduck' ) # Someone's fave pokemon (bless 'em) ### YOUR CODE BELOW ### # <object.attribute> == <something>, \"ball did not catch as expected\" NICE. Now we will do the same thing again, this time testing for an unsuccessful catch. SO in order to do this, we need to choose a random seed that will cause our catch to fail: def test_unsuccessful_catch (): # choose a random seed such that # we know the catch call should FAIL ### YOUR CODE BELOW ### # random.seed(<your number here>) ball = Pokeball () ball . catch ( 'Psyduck' ) ### YOUR CODE BELOW ### # <object.attribute> == <something>, \"ball did not fail as expected\" When you are finished test your functions below test_unsuccessful_catch () Psyduck captured! test_successful_catch () Psyduck captured!","title":"\u26f9\ufe0f L5 Q2 Write a Unit Test for the Catch Rate"},{"location":"labs/L5_Writing_Unit_Tests/#l5-q3-write-a-unit-test-that-checks-whether-the-overall-catch-rate-is-5050","text":"For this one, we're going to take those same ideas around seeding the random number generator. However, here we'd like to run the catch function multiple times to check whether it is truly creating a 50/50 catch rate situation. Here's a pseudo code outline: seed the random number generator for 100 iterations: create a pokeball try to catch something log whether it was successful check that for the 100 attempts the success was approximately 50/50 note: you can use my suppress stdout() function to suppress the print statements from ball.catch ex: with suppress_stdout(): print(\"HELLO OUT THERE!\") quick segway : what is the actual behavior of random.seed() ? Does it produce the same number every time we call random.random() now? Check for yourself: random . seed ( 42 ) [ random . random () for i in range ( 5 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124] We see that it still produces random numbers with each call to random.random . However, those numbers are the same with every execution of the cell. What happens when we do this: [ random . random () for i in range ( 5 )] [0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] The numbers are different. BUT: random . seed ( 42 ) [ random . random () for i in range ( 10 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124, 0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] We see them here in the bottom half of the list again. So, random.seed() is seeding the random number generator such that it will produce the same sequence of random numbers every time from the given seed. This will reset whenever random.seed() is set again. This behavior is useful because it allows us to continue using random number generation in our code, (for testing, creating examples and demos, etc.) but it will be reproducable each time. End Segway # 1. seed the random number generator # 2. for 100 iterations: # * create a pokeball # * try to catch something # * log whether it was successful # 3. check that for the 100 attempts the success was approximately 50/50 def test_catch_rate (): ### YOUR CODE HERE ### ### END YOUR CODE ### # assert np.abs(np.mean(results) - 0.5) < 0.1, \"catch rate not 50/50\" pass test_catch_rate ()","title":"\u2696\ufe0f L5 Q3 Write a Unit Test that Checks Whether the Overall Catch Rate is 50/50"},{"location":"labs/L5_Writing_Unit_Tests/#test-runners","text":"When we start to create many tests like this, it can be cumbersome to run them all at once and log which ones fail. To handle our unit tests we use what are called test runners . We won't dedicate time to any single one here but the three most common are: unittest nose2 pytest unittest is built into python. I don't like it because you have to follow a strict class/method structure when writing the tests. nose2 is popular with many useful features and is generally good for high volumes of tests. My favorite is pytest, it's flexible and has an ecosystem of plugins for extensibility.","title":"Test Runners"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/","text":"Data Science Foundations Project Part 1: Statistical Analysis \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to perform statistical analysis on data generated from our tictactoe program! 1.0 Preparing Environment and Importing Data \u00b6 back to top 1.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 1.0.2 Load Dataset \u00b6 back to top data = {} for i in range ( 1000 ): game = GameEngine () game . setup_game () board = game . play_game () data [ 'game {} ' . format ( i )] = { 'board' : board . board , 'winner' : board . winner , 'starting player' : board . start_player } | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| |O| | | | | |O|X|X| |O| |O| |X| | | |O|X|X| 'O' Won! |O| |O| |X|O| | |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O| | | |X|X| 'O' Won! |X| |O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X| | | |O|X| | | |O|O| |X| |X| |O|X| | | |O|O| |X|O|X| |O|X| | | |O|O| |X|O|X| |O|X|X| | |O|O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | |X|X| | | |O| | |O|O| | |X|X| |X| |O| | |O|O| | |X|X| 'O' Won! |X| |O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | |O| | | |X|X| |O|X| | | |O| | | |X|X| |O|X| | | |O|O| | |X|X| |O|X|X| | |O|O| | |X|X| 'O' Won! |O|X|X| |O|O|O| | |X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | |O| | |O| |X| |X| | | | |O| | |O|X|X| |X| | | | |O|O| |O|X|X| |X| |X| | |O|O| |O|X|X| 'O' Won! |X| |X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | | | | |X| |X| |O| |O|O| | | | |X| |X|X|O| |O|O| | | | |X| 'O' Won! |X|X|O| |O|O| | |O| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X|X| |X| | | |O| |O| | |X|X| |X|O| | |O| |O| | |X|X| |X|O|X| |O| |O| 'O' Won! |O|X|X| |X|O|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | |O|O| | | |X| |X| |X| | |O|O| | | |X| |X|O|X| | |O|O| | | |X| |X|O|X| |X|O|O| | | |X| 'O' Won! |X|O|X| |X|O|O| | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| |X| | |O| | |O| |X| |O| |X| | |O| | 'X' Won! |O| |X| |O| |X| | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X|O| | | |O| |X|X| | 'X' Won! |O|X|O| | |X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| |X| | | | | | | | |O|X| |X|O| | | | | | | |O|X| 'X' Won! |X|O| | | |X| | | |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | |X|O| | | |X|O| | |X|O| |X|O| | | |X|O| |X|X|O| |X|O| | 'O' Won! | |X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X|O| | | |O|X| | | | | |X|O| | | |O|X| | | |X| |X|O|O| | |O|X| | | |X| |X|O|O| | |O|X| | |X|X| 'O' Won! |X|O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| |X| | | |X| | |O|O| |X| |X| |O| |X| |X|O|O| |X| |X| |O| |X| 'O' Won! |X|O|O| |X|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| |X|X| | | | |O| | |X|O| |X|X| | |O| |O| | |X|O| |X|X| | |O| |O| |X|X|O| 'O' Won! |X|X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | |X|O| | | | | |O| |X| | |X|O| |O| | | |O| |X| |X|X|O| |O| | | |O| |X| 'O' Won! |X|X|O| |O|O| | |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | |X| | | |O| |X| |O| | | |X|O| | |O| |X| |O| | | |X|O| | |O| |X| |O|X| | |X|O| | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | |X| | | | |O|O| |X|X| | |X|O| | | |O|O| 'X' Won! |X|X| | |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| | | | |X|O| |X| | | |O|X| | | |X|O| |X| | | |O|X| | |O|X|O| |X| | | |O|X|X| |O|X|O| |X|O| | |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| |X| |O| | |O|X| |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| | | |X| |O|X|O| |O|X|X| | | |X| |O|X|O| 'O' Won! |O|X|X| |O| |X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X|X| | |O|O| | |O| | | |X|X| | |O|O| | |O| | | |X|X| | |O|O|X| |O|O| | |X|X| | |O|O|X| |O|O|X| |X|X| | |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | |X| |X|O| | | |O| | | | |X| |X|O| | |X|O| | | | |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O|X| |O| |X| |X|O| | |X|O|X| |O| |X| |X|O|O| 'X' Won! |X|O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | |X| | | |O| |O| | |X| | |X|O| | |O| |O| | |X| | |X|O|X| |O| |O| 'O' Won! |O|X| | |X|O|X| |O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| 'O' Won! | |X|O| | |O| | |O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O| |X| |X|X| | |O| |O| |O| |X| |X|X| | 'X' Won! |O| |O| |O| |X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X| | | | |X| |O| |X| |O|X| | | | |X| |O|O|X| 'X' Won! |O|X|X| | | |X| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O|X| | | | | | | |X| |O|O|X| | |O| | | | |X| |O|O|X| | |O| | |X| |X| 'O' Won! |O|O|X| | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| |O| |X| | | | | | |O|X| |O| |X| |X| | | | |O|X| |O| |X| |X| | | |O|O|X| 'X' Won! |O| |X| |X| |X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | | |X|X| |O| | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| | | |X|O|O| |O|X|X| |O| |X| |X|O|O| 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O|O|X| | | | | | |X|X| |O|O|X| | | | | | |X|X| |O|O|X| | |O| | | |X|X| |O|O|X| |X|O| | | |X|X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| | |O|X| | |X|O| |O| |X| 'X' Won! |X|O|X| | |X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | | |O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| | |X|X| |X|O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | |X| |X| |O| |X| |O|O| | |X|O|X| |O|X|X| |O|O| | |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| 'O' Won! |X|O| | |X| |X| |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | |X| |O| | |X| | | |O|O| |X| |O| | |X| | | |O|O| |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X|O| | | | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| |X| | |O|O| |X|X| | |O| |X| 'X' Won! | |O|O| |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X|O| |O|X| | |X| | | | |X|O| |O|X| | |X| | | | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| 'X' Won! |X|X|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X|O| | | | |O| | | |X| |X|O|X| | | |O| | | |X| |X|O|X| |O| |O| | | |X| 'X' Won! |X|O|X| |O|X|O| | | |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | | |X| | |X|X| | |O|O| | | |X| |O|X|X| | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X|O| | |O|X| | | |X| | |X|O| | |O|X| |O| |X| | |X|O| | |O|X| 'X' Won! |O| |X| | |X|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| | |O|O|X| | |X|X| | |O| | 'X' Won! |O|O|X| | |X|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| | | |O| |O|O|X| | | |X| |X| |O| |O|O|X| | | |X| |X|O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | |X|X| | | | | |O| | | |O|X|X| | | | | |O| | | |O|X|X| |X| | | |O| | | |O|X|X| |X| | | |O| |O| |O|X|X| |X| |X| |O| |O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | |O| |X| | | |X| |X|O| | |O| |X| | | |X| |X|O|O| |O| |X| | | |X| 'X' Won! |X|O|O| |O|X|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | |X|X| |O| |O| | | | | |O|X|X| |O| |O| | |X| | |O|X|X| |O| |O| 'O' Won! | |X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| |X| | |X| | 'O' Won! |O| | | |O| |X| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | | | | | | |X|X| |O|O|X| | | |O| | |X|X| |O|O|X| | |X|O| | |X|X| |O|O|X| | |X|O| |O|X|X| |O|O|X| 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O|O| | | | |X| 'X' Won! | | |X| |O|O|X| | | |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | | |O|X| |X| | | |O| |O| | |O|X| |X| | | |O| |O| | |O|X| |X| |X| |O| |O| | |O|X| |X|O|X| |O| |O| |X|O|X| |X|O|X| 'O' Won! |O|O|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O| |O| | |X|X| | | | | |O| |O| | |X|X| | | |O| |O| |O| | |X|X| |X| |O| |O| |O| | |X|X| |X|O|O| |O| |O| 'X' Won! |X|X|X| |X|O|O| |O| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| |X| |X| | |X| | | |O|O| |X|O|X| | |X| | | |O|O| |X|O|X| |X|X| | | |O|O| |X|O|X| |X|X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | |X|X| |O| | | | | |O| | |X|X| |O| | | 'X' Won! | | |O| |X|X|X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| | |X|X| |O|O|X| | |O|O| | |X|X| 'X' Won! |O|O|X| | |O|O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | | |X|X| |O| |O| | | | | |O|X|X| |O| |O| |X| | | |O|X|X| 'O' Won! |O|O|O| |X| | | |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| | | |O| | |X| | | |X|O| |O| |O| | |X| | 'X' Won! | |X|O| |O|X|O| | |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | |X| | | |O|X| |O|X|O| | |X| | | |O|X| |O|X|O| | |X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | | | | |X| |X| |O|O| | | | | | |X|O|X| |O|O| | | | |X| |X|O|X| 'O' Won! |O|O| | | |O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | |X|O| | | |X|X| | |O| | |X|O| | | |X|X| | |O|O| |X|O|X| | |X|X| | |O|O| |X|O|X| |O|X|X| | |O|O| 'X' Won! |X|O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| |O|O| | | |X| | | | |X| |O|O| | |X|X| | | |O|X| |O|O| | |X|X| | |X|O|X| |O|O| | |X|X| | |X|O|X| |O|O| | |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| | | | | |X|X|O| |X|O|O| | | | | |X|X|O| |X|O|O| | |X| | |X|X|O| 'O' Won! |X|O|O| | |X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X|X| | |X|O| |O|O| | | |X|X| |X|X|O| |O|O| | | |X|X| 'O' Won! |X|X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | | |X| | | |O|O| |X| | | | |X| | | |O|O| |X| |X| |O|X| | | |O|O| |X| |X| |O|X| | |X|O|O| |X| |X| |O|X| | |X|O|O| |X|O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | |X| |X| |O|O| | | | | | 'O' Won! |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X|X| | |O| | |O|X|O| | |X|X| 'X' Won! | |O| | |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| |X| | |O|O| | |X|O| |X| |X| 'X' Won! | |O|O| | |X|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O|O| | |O|X| | |X| | | |O|O| |X|O|X| | |X| | | |O|O| |X|O|X| | |X|O| | |O|O| |X|O|X| |X|X|O| 'O' Won! |O|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | |X| | | |X| | |O|O| | 'O' Won! | |X| | | |X| | |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | | | | |O|O| |X| |X| |X| | | | |O|O| 'O' Won! |X| |X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| |X| | | | |O|O| |O|X|X| |X| | | | |O|O| |O|X|X| |X|X| | | |O|O| |O|X|X| |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O|O| |X|O| | 'X' Won! |X| | | |X|O|O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | 'X' Won! |X| | | |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | |O|X| |X|X| | | |O|O| | |O|X| 'O' Won! |X|X| | |O|O|O| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | | |X| | 'O' Won! |O|O|O| |X| | | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | | |O|O| | |X|X| |X| | | | |O|O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O| | |X|O|O| | |X|X| |X|O|O| |X|O|O| 'X' Won! |X|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| | | |O| |O|X|X| | | |O| | |X|O| |O|X|X| | | |O| | |X|O| |O|X|X| |O| |O| 'X' Won! | |X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X|X| | | |O|O| | | |O| |X|X| | | |O|O| | |X|O| |X|X| | 'O' Won! | |O|O| | |X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| |X| |O| |O| |X|O| | |X| |X| |O|X|O| |X|O| | |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| |X| | | | |X|O| |X|O|O| |X| | | | |X|O| 'X' Won! |X|O|O| |X| | | |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | | | |X|X|O| 'O' Won! |O| | | | |O| | |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| | | |O| | | | | |X|O|X| | | |O| |X| | | |X|O|X| | | |O| |X| |O| 'X' Won! |X|O|X| |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| 'O' Won! |O| |X| | |O| | | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| |O| 'X' Won! | | |X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O| | | |X| | |O|X|X| 'O' Won! |O|O|O| | |X| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| | |O| | |X|X| | |O| |X| | |O|O| |X|X| | 'X' Won! |O| |X| | |O|O| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| | | |O| |X|O|X| | | |X| | | |O| |X|O|X| |O| |X| | | |O| |X|O|X| |O|X|X| | | |O| |X|O|X| |O|X|X| |O| |O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | |O| | | |X| |O|X|O| | |X|O| | | |X| |O|X|O| 'O' Won! | |X|O| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | |X|O| | | | | | |O|X| |O|X|O| | | | | |X|O|X| |O|X|O| 'O' Won! | | |O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| | | |O| |X|O|X| | | |X| | | |O| |X|O|X| | | |X| |O| |O| |X|O|X| |X| |X| |O| |O| |X|O|X| |X|O|X| |O| |O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| |X| |X| |O| |X| 'O' Won! | |O|O| |X|O|X| |O| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| | | |O| |X| | | |X|X|O| | |O|O| |X| | | |X|X|O| | |O|O| |X| |X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | | | |O| |X| |X|X|O| | | |O| |O| |X| |X|X|O| |X| |O| |O| |X| |X|X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X|O|X| | | | | |O|X| | |X|O|X| | | |O| |O|X| | |X|O|X| | |X|O| |O|X| | |X|O|X| |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | | | |O| |X| | |X|O| | | | | |O|X|X| 'O' Won! | |X|O| | |O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| |X| | | | |O| | |O| |X| |X| | | |O|O| | |O| |X| |X| | | |O|O|X| |O| |X| |X| |O| |O|O|X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | | |X| | |O| | |X| |O| | | |X| | |O|O| |X| |O| |X| |X| | |O|O| |X|O|O| |X| |X| | |O|O| 'X' Won! |X|O|O| |X|X|X| | |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | |X| | | |X| |O| |O|X|O| |X| | | |X| |O| |O|X|O| |X| | | |X|X|O| 'O' Won! |O|X|O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O| |O| | |X| | | | |X| |O| |O| | |X|X| | | |X| |O| |O| |O|X|X| | | |X| |O| |O| |O|X|X| | |X|X| 'O' Won! |O|O|O| |O|X|X| | |X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | | | |X| | |X|O| |O|O| | | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| |O| |X| | |X|O| |O|O|X| |O|X|X| | |X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O| | | | |X|O| | | |X| |O| | | |X|X|O| | | |X| |O|O| | |X|X|O| |X| |X| |O|O| | |X|X|O| 'O' Won! |X| |X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| 'X' Won! |O|O|X| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | |X| | |O| | |X|O| | | | |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O|O| 'X' Won! |X|X|X| |O|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| |O| | | |X|O| | | |O|X| |O| | | |X|O|X| | |O|X| |O| | | |X|O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| |O| | |X| | | |O| | |X| |O| | |X| | |X|O| | |X| |O| |O|X| | |X|O| | 'X' Won! |X| |O| |O|X| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | |X| |X| |O|O| | | | |O| |X| |X| |O|O| | 'X' Won! | | |O| |X|X|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| |X|X| | |O| | | | |O|X| |X|X|O| |O| | | | |O|X| 'X' Won! |X|X|O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X| | | |X| |X| 'O' Won! |O|O|O| |X| | | |X| |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O| | |X|O| | | |X|X| | |O|O| |X|O| | 'X' Won! |X|X|X| | |O|O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| |O| | | | | |X| |O| 'X' Won! |X| |O| |X| | | |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| | |O| | |X|O|O| | |X|X| |X|O| | |X|O|O| |O|X|X| |X|O| | 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X| | | |O| | 'X' Won! |O| |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |X| | | |O| |O|X| | | | |X| | |X|O| |O|X| | | | |X| | |X|O| |O|X| | | |O|X| 'X' Won! |X|X|O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | | | |X| |X|O|O| | |X| | | |O|X| |X|O|O| | |X| | | |O|X| |X|O|O| | |X|X| |O|O|X| |X|O|O| | |X|X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | |X|O| | | |X| |O| | | | |X|O| | | |X| |O|O| | | |X|O| |X| |X| |O|O| | 'O' Won! | |X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| |O| | | | |X|X| | |X|O| |O| | | | |X|X| |O|X|O| |O| | | 'X' Won! |X|X|X| |O|X|O| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X|X| |O| | | | |O| | | |X|X| |O|O| | | |O| | | |X|X| |O|O|X| | |O| | |O|X|X| |O|O|X| 'X' Won! | |O|X| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | |X| | | |X| |O| |O|X|O| |X| | | |X| |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X| |X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| 'O' Won! |O|X| | | |O| | | |X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | | |O| |X|X| | |X|O| | | |O|O| |X|X| | 'X' Won! |X|O| | |X|O|O| |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | |O|X|X| | |O|X| |O|O| | |O|X|X| |X|O|X| 'O' Won! |O|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O|X| | | | | |O| |X| | |O|X| | |O| | |O|X|X| | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | 'X' Won! |O|X|X| | |O|X| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| |O| | | | | |X|O|X| |O|X|O| 'O' Won! |O| | | |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | |O|X| | | | | |X| |O| |X|O|X| | | | | |X| |O| |X|O|X| | | | | |X|O|O| |X|O|X| | | |X| |X|O|O| |X|O|X| |O| |X| |X|O|O| 'X' Won! |X|O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| |X| 'O' Won! | | | | |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| |X| |X| |O| | | | |O|X| |X|O|X| |O| | | | |O|X| 'X' Won! |X|O|X| |O|X| | | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| |X| |O| | | |O|X| | |X| |X| |O| |O| |O|X|X| |X| |X| |O| |O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | |X| | |X|O| | | |O| | | |X| | |X|O| | | |O|O| | |X| | |X|O| | |X|O|O| | |X| | |X|O|O| |X|O|O| 'X' Won! |X|X| | |X|O|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O| |O| 'X' Won! |O| | | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O|X|O| | | |X| | | |O| |O|X|O| | | |X| | |X|O| |O|X|O| | |O|X| | |X|O| |O|X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O| | 'O' Won! |X|X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X|O| | | |X| | | | | |O|X|O| | |X|X| | | | | |O|X|O| | |X|X| | | |O| |O|X|O| | |X|X| |X| |O| |O|X|O| |O|X|X| |X| |O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| |O| | |O|X| | |O|X| |X| |O| | |O|X| |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X| | | |O|O| 'X' Won! | |O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | |X| |O| | | |X| |O|O| | |X| |O| | | |X| |O|O| | |X|X|O| 'O' Won! | | |X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | |X| |O| | | | | |O|X| | 'O' Won! |X| |O| | |O| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O|X| |O| | | |X| | | | |O|X| |O| |O| |X|X| | | |O|X| |O| |O| 'O' Won! |X|X| | | |O|X| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | |X| |X| |O|X| | |O| | | |X| |X| |O|X| | |O| |O| |X| |X| |O|X| | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | |O|O| | | |X| | | |X|X| |O|O| | | |X| | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | |X|X| | | | | |O| |O| | |X|X| |O| | | |O| |O| | |X|X| |O|X| | |O| |O| | |X|X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | | | |X| |X| |O| | |O| | | |O|X| |X| |O| | |O| | |X|O|X| 'O' Won! |X|O|O| | |O| | |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O|X| | | |X| |X| |O| |O|O|X| | | |X| |X|O|O| |O|O|X| |X| |X| |X|O|O| 'O' Won! |O|O|X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X| | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | | |X|O| |O| |O| |X|X| | |X|X|O| 'O' Won! |O| |O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O|X| | | | | |O|X| | |O|O|X| | |X| | |O|X| | |O|O|X| | |X| | |O|X|O| |O|O|X| | |X|X| |O|X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X| |O| |X|O|O| | | |X| |X| |O| |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| |O| | | | | |X| 'X' Won! |X|O|O| |O|X| | | | |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | |O| | |X|X|O| |X| | | | |O| | |X|X|O| |X|O| | | |O| | 'X' Won! |X|X|O| |X|O| | |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | |X|X| | | |O| |O| |O| | |X|X| | | |O| |O|X|O| | |X|X| | |O|O| |O|X|O| | |X|X| 'X' Won! | |O|O| |O|X|O| |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | |O| | | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O|X| |X| | | |O| | | |X|O|X| |X|O| | |O| | | |X|O|X| |X|O|X| |O| | | |X|O|X| |X|O|X| |O| |O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| |O|O| | 'X' Won! |X| | | | |X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X|O| |X|O| | | |X|O| | |X|O| |X|O| | 'O' Won! | |X|O| | |X|O| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | |O| |O| | |X|X| |O| | | 'X' Won! |O| |O| |X|X|X| |O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| |X| |X| | | | | | |O|O| |X| |X| | |O| | |X|O|O| |X| |X| | |O| | |X|O|O| |X| |X| |O|O| | |X|O|O| |X| |X| |O|O|X| 'O' Won! |X|O|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| |O|O|X| |X| | | |O|X|O| |O|O|X| |X| |X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | | | |X|O|O| |X|O| | | | | | 'X' Won! |X|O|O| |X|O| | |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| |O| | | | | |X| |X| |O| |O|X| | | |O|X| |X| |O| |O|X| | | |O|X| |X|X|O| |O|X| | |O|O|X| |X|X|O| |O|X| | It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X|O| | |O| | | | |X|X| |X|O| | |O| | | |O|X|X| |X|O| | 'X' Won! |O| |X| |O|X|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | |O|O| |X| | | |X| |O| 'X' Won! |X|O|O| |X| | | |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | | |X| | | |O|X| |O| | | |O|X| | | |O|X| |O| |X| |O|X| | | |O|X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | | |O| | |X|X| |O|O| | |X| |O| | |X|X| |O|O| | 'O' Won! |X| |O| | |X|X| |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | |O|O| | |X| |X| | | | | |O|O|X| |X| |X| | | | | |O|O|X| |X|O|X| | | | | 'X' Won! |O|O|X| |X|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| |X| 'O' Won! |O| | | |O|X| | |O| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | | |O| |X|O| | |O|X|X| | | |O| |X|O| | |O|X|X| |O| |O| |X|O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | |O| | | |X| | |X|O|X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| |O|O| | |O| |X| | |X|X| |O|O| | |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | |O| | |O| |X| |X|X| | | |O| | |O| |X| |X|X| | | |O|O| |O| |X| |X|X| | |X|O|O| |O| |X| 'O' Won! |X|X|O| |X|O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X|O| |X| | | | |X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| |X| |O|X|O| | |X|O| |X|O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| | | |X| 'O' Won! |O|X|X| |O|O|O| | | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X|O| | |O| | | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O| |O|X| | | |X|X| | |O|O| |O|X| | 'O' Won! | |X|X| |O|O|O| |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | | | | | | |X|O| |O| | | |X| | | | |X|O| |O| | | |X| |O| | |X|O| |O| | | |X|X|O| 'O' Won! | |X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| | |X|X|O| | | |O| | |X| | 'O' Won! |X|X|O| | | |O| | |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| |O| | | | |X| | | |O|X| |O| | | | |X|X| | |O|X| |O| |O| | |X|X| | |O|X| |O|X|O| | |X|X| |O|O|X| |O|X|O| | |X|X| 'X' Won! |O|O|X| |O|X|O| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | | |X|O| 'O' Won! |O|X|X| | |O| | | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O|O| | |O|X| | | | |X| |O|O| | |O|X| | |X| |X| 'O' Won! |O|O|O| |O|X| | |X| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | | |X|X| | |O|O| |O| | | | |X|X| | |O|O| |O| |X| | |X|X| | |O|O| |O|O|X| | |X|X| 'X' Won! | |O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | |X|O| | | |O| |X| | | | |X|O| | |O|O| |X| | | | |X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X|X| | 'O' Won! |O|X|O| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | |X|O| |O| | | |X| | | | |X|O| |O|O| | 'X' Won! |X| | | | |X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | |O|X| |X| | | |O| |O| | |O|X| |X| | | |O|X|O| | |O|X| |X|O| | |O|X|O| |X|O|X| |X|O| | |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | 'X' Won! |X| | | |X|O| | |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | |O| | |X|O| | | |X|X| | |O| | |X|O| | | |X|X| | |O| | |X|O|O| 'X' Won! |X|X|X| | |O| | |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | |X| |O| | | |O|X| | | |O|X| |O| | | |O|X|X| | |O|X| |O| |O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | |X| | | | |O| | |X|O| |O|X| | | | |O| | |X|O| |O|X|X| | | |O| |O|X|O| |O|X|X| 'X' Won! |X| |O| |O|X|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X|O|O| |X| | | 'X' Won! |X|O| | |X|O|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X| | |O|O| | | | |X| |X|X| | |O|O| | | | |X| |X|X|O| |O|O| | | | |X| |X|X|O| |O|O| | | |X|X| 'O' Won! |X|X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | | |O| | | |X|X| |O|O| | 'X' Won! | |O| | |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | |O|X| |X| | | | |O|O| |X|O|X| |X| | | | |O|O| 'O' Won! |X|O|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | |X| |X|O|X| | | |O| 'O' Won! |O| |X| |X|O|X| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| |X| |O| | | |X| | | |O| |X| |O| |O| |X| | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| |O| |X|X| | 'X' Won! |O|X|O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |X| | |O| | |X|X|O| | |O|X| 'X' Won! |X|O| | |X|X|O| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | | 'O' Won! |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X|O| | |X| | | |O|X|O| |X|O| | |X| | | |O|X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X| |X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X|X| | |X|O|O| | | |O| |X|X| | 'X' Won! |X|O|O| | | |O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X|X| | | |O|O| |O| | | |X|X| | |X|O|O| |O| | | |X|X| | |X|O|O| |O|O| | |X|X| | 'X' Won! |X|O|O| |O|O| | |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | |X|X| | |O|O| | | |O| | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O|O| |X|X| | |O|O|X| |X|O|O| |X|X| | 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | |O|X| | | | | |O|X| | |X|O|X| | | | | |O|X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O|X| | |O|X| | |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | |O|X| |O| |X| | | | | | |O|X| |O|X|X| | | | | | |O|X| |O|X|X| | | |O| | |O|X| |O|X|X| | |X|O| |O|O|X| |O|X|X| | |X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| | |X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | |X| | |O|O| | | | |X| 'O' Won! | |X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X| | | |X|O| | | |O| |O|X| | | |X|O| | | |O| |O|X|X| 'O' Won! | |X|O| | |O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | |O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O|X| | | |X| | |O| |X| |O|X|O| | |X|X| |O| |X| |O|X|O| 'O' Won! |O|X|X| |O| |X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | |X|O| |O| | | | |X| | |O|X|O| |O| | | | |X| | 'X' Won! |O|X|O| |O|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| 'O' Won! |O| | | |X|O|X| | | |O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | | | |O|X| 'X' Won! |X| |O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X|X| | |X|O| | |O|O| | |X|X| | |X|O| |X|O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | |X|O| | | | | |X|O| | | |X|O| | | |O| |X|O| | | |X|O| | |X|O| |X|O| | | |X|O| |O|X|O| |X|O| | | |X|O| |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |X| | | |O|O|X| | | | | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| |X| |O| | | | | |X| |O| |X|O|O| | | | | |X| |O| 'X' Won! |X|O|O| |X| | | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O|O| | | | | | |X| |X| |O|O| | |X| | | |X| |X| |O|O| | |X| |O| |X| |X| |O|O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | |X| | | |X| |O| | |O| | |X|O| | |X| |O| | |O| | |X|O|X| |X| |O| |O|O| | |X|O|X| |X|X|O| |O|O| | |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O|X|O| 'O' Won! |O| | | |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | | |X| | |X| |O| |O|X| | | |X|O| |X| |O| |O|X| | |X|X|O| |X|O|O| |O|X| | |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X|X|O| 'O' Won! |O|O|O| | | |X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | | |X|X|O| | | | | |O| | | |X|X|O| | |O| | |O| | | |X|X|O| |X|O| | |O| | | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| |O| |O| | | | | | |X|X| |O|X|O| | | | | | |X|X| |O|X|O| | | |O| 'X' Won! | |X|X| |O|X|O| | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | | |O| |X| |O| |X|X| | | | |O| 'O' Won! |X| |O| |X|X|O| | | |O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | |X| |X| |X| |O| |O|O| | |X| |X| |X|X|O| |O|O| | |X| |X| |X|X|O| |O|O| | |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | | | | | |O| |X|X|O| | | | | |X| |O| |X|X|O| |O| | | |X| |O| |X|X|O| |O| |X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | | | | |O|X|O| | |X|X| | | | | |O|X|O| |O|X|X| | | |X| |O|X|O| |O|X|X| 'O' Won! |O| |X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| 'X' Won! | |O|X| | | |X| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X|O| | | |O| | |O|X| |X|X|O| | | |O| |O|O|X| |X|X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | |O| 'X' Won! | | |O| |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| |X| |O|O| | | |O|X| |X| |X| 'X' Won! |O|O| | | |O|X| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| | |O|X| |O|X| | | | |X| | |O|X| |O|X|O| | |X|X| | |O|X| |O|X|O| 'O' Won! |O|X|X| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | |X| |X|O| | | | |O| | | |X| |X|O| | |O| |O| | | |X| |X|O|X| |O| |O| 'O' Won! |O| |X| |X|O|X| |O| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | |O| |X| | | 'X' Won! | |O|X| | |X|O| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| 'X' Won! |X| | | | |X| | |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X|X| | | | | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O|X| | |O|O|X| |X|X| | |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O|O| | |X| | |X|O| | | |O|O| | |X| | |X|O|X| 'O' Won! |O|O|O| | |X| | |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | | |O|O| |X| |X| | | | | |X|O|O| |X|O|X| | | | | |X|O|O| 'X' Won! |X|O|X| |X| | | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | |X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O|X| |O|X| | |O|O|X| | |O|X| |O|X| | |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X| | | |X|O| |O| | | | |X| | |X|X|O| |O| | | | |X|O| |X|X|O| |O| | | | |X|O| |X|X|O| |O| |X| | |X|O| |X|X|O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | | | |X| |O|X|O| | | | | |X| |X| |O|X|O| | | | | |X| |X| |O|X|O| |O| | | 'X' Won! |X|X|X| |O|X|O| |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X|O| | |X| | | | |O| 'X' Won! |O|X|O| | |X| | | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X| | | | |O|O| |X| |X| |X| | | | |O|O| |X|O|X| |X| | | | |O|O| |X|O|X| |X| |X| 'O' Won! |O|O|O| |X|O|X| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | |X|X| |O|O| | |O| |X| | |X|X| 'X' Won! |O|O|X| |O| |X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| | | |O| | | |O|X|O| |X| | | |O|X| | |O|X|O| |X| | | |O|X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | |X| | | |O| | |X| |O| | |X| | |O|O| | |X| |O| | |X| | |O|O|X| |X| |O| 'O' Won! |O|X| | |O|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | |X| | |O| |X| |O|O| | | |X| | |O| |X| |O|O| | | |X| | |O|X|X| 'O' Won! |O|O| | |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| |X| |X| | | | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X|O|X| | |O|O| |O| |X| |X|O|X| | |O|O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| |X| |O| |X| | | |O|O|X| |X|O|O| |X| | | |O|O|X| 'X' Won! |X|O|O| |X|X| | |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | | |O|X| |O|X|X| |O| | | | |O|X| |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| | | |X|X| |O| |O| | |O| | | |X|X| |O|X|O| | |O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| |O|X|O| It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| |X|O| | | | | | |X| |O| |X|O|O| | | | | |X| |O| 'X' Won! |X|O|O| |X| | | |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | |O| 'X' Won! |X| | | |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | |O|O| | |X|X| | |X| | | |O|O| | |X|X| | |X| |O| |O|O| | |X|X| | |X| |O| |O|O|X| |X|X| | |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| 'O' Won! |O| |X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O| |X|X| | 'O' Won! | |X| | |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O|O| | |X| | |X|O| | |X|O|O| 'O' Won! |O|X| | |X|O| | |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | |X| | | | |O|X| |O|O| | |X| | | |X|O|X| |O|O| | |X| | | 'O' Won! |X|O|X| |O|O|O| |X| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | |O| |O|X|X| | | |X| | | |O| |O|X|X| | | |X| | |O|O| |O|X|X| | |X|X| | |O|O| 'O' Won! |O|X|X| | |X|X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| | | | | |X|X|O| | |O|X| | | | | 'O' Won! |X|X|O| | |O|X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | | | |O| |X| | |X|O| | | |X| |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |X|X| 'O' Won! |O| |X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | | |X| |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| |X|O|X| |X| | | 'O' Won! |O|X|O| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O|O| | |X| |X| | |X|O| |O|O| | 'O' Won! |X| |X| | |X|O| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | |X| | |O| |O| |O|X| | | |X| | |O| |O| |O|X| | | |X|X| 'O' Won! |O| |O| |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| |X| |X|O| | | | | | |O|O|X| |X|O| | |X| | | |O|O|X| 'O' Won! |X|O| | |X|O| | |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X| | |X|O|O| | | |X| | |X| | |X|O|O| |O| |X| | |X| | |X|O|O| |O| |X| |X|X| | |X|O|O| |O|O|X| |X|X| | 'X' Won! |X|O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | | |O| |O|X| | | | |X| | |X|O| |O|X| | | | |X| | |X|O| |O|X|O| | | |X| 'X' Won! |X|X|O| |O|X|O| | | |X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | |O| | |X| | | |O|O|X| | |O|X| |X| | | |O|O|X| |O|O|X| 'X' Won! |X| |X| |O|O|X| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| | | | | | |O|O| |O|X|X| |X| | | | |O|O| 'O' Won! |O|X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | | |X| | |X|O|O| |O|X|O| | |X| | |X|O|O| |O|X|O| | |X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O| |O| |X| |X| | |O| | |O|X|O| |X| |X| | |O|O| |O|X|O| |X| |X| 'X' Won! |X|O|O| |O|X|O| |X| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| | | | | | |X|X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| |X| | |X|X| |X|O|O| |O| |X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X|O| | | |X| | |O|X|O| |X|O| | |O|X| | |O|X|O| |X|O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | |O|O|X| | |X| | | |O| | 'X' Won! |O|O|X| | |X| | |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O|X| |X| | | | |O|X| |O|O|X| |X| | | 'X' Won! | |O|X| |O|O|X| |X| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| | |O| | 'X' Won! |O| |X| |O| |X| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | |X|O| |O|X|O| | | |X| | |X|O| |O|X|O| | | |X| |X|X|O| |O|X|O| |O| |X| |X|X|O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | |O|X| | |X|X| |O| | | | |O|X| | |X|X| |O| | | |O|O|X| 'X' Won! |X|X|X| |O| | | |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | |O| |X| | | | |X|O| | | |O| |X| |X| | |X|O| | |O|O| |X| |X| 'X' Won! | |X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| | | |O| |O|O| | | |X|X| | | |O| |O|O|X| | |X|X| |O| |O| |O|O|X| | |X|X| |O|X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | |O| | |O| | | | |X| |X| |O| | |O| | | | |X| |X|O|O| | |O| | | | |X| |X|O|O| | |O|X| | | |X| |X|O|O| |O|O|X| | | |X| |X|O|O| |O|O|X| | |X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X|O| | |X| |O| 'X' Won! |X| | | |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | |O| |O| |O|X|X| |X| | | |O| |O| |O|X|X| |X| |O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | 'O' Won! | | | | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | |O| | | |X| |O| |X|X| | |O| | | |X| |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X|X|O| |X|X|O| |O|O| | |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | | | | | |O| |X| | |X| | |O| | | |O| |X| | |X| | |O| |X| |O| |X| | |X|O| |O| |X| |O| |X| |X|X|O| |O| |X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | | | | | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| | |X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O|X| | | |X| | |O| |X| |O|X| | | |X| | |O|O|X| |O|X| | |X|X| | |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | |O|O| | |X| | |X|O|X| | |O|O| | |X| | |X|O|X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | |O|X| | |X| |O| | | |X| |O|X| | |X| |O| | | |X| |O|X|O| |X|X|O| | | |X| |O|X|O| |X|X|O| |O| |X| |O|X|O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O| | | |X| | 'O' Won! |O|X|X| |O|O|O| | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X| |O| |X|X|O| |O|O| | |X|X|O| |X|X|O| 'O' Won! |O|O|O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | | |O|O| |X| |X| |O| | | |X|O|O| |X| |X| |O|O| | |X|O|O| |X| |X| |O|O|X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O|X| | |X|X| | | | | |O|O|X| |O|X|X| |X| | | |O|O|X| |O|X|X| |X|O| | |O|O|X| |O|X|X| 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| | |O| | |O|O| | | |X|X| | |O| | |O|O|X| |O|X|X| | |O| | |O|O|X| |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | 'X' Won! |X| | | | |X|O| | |O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| |X| |X| |X| |O| 'O' Won! |O|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O|O| | | |X| |X|X| | | |O|O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| |X|O|O| | | |X| 'O' Won! |X|X|O| |X|O|O| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | | |O|X| |X|X| | |O|O| | | |O|X| 'O' Won! |X|X| | |O|O|O| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| |O| |X| |X| |X|O| | |O| |O| |X| |X| |X|O|O| |O| |O| |X| |X| 'X' Won! |X|O|O| |O|X|O| |X| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| |O|X| | | |O|X| | | |X| |O|X| | | |O|X| | | |X| |O|X|O| |X|O|X| | | |X| |O|X|O| |X|O|X| | |O|X| |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | |X|O| |O|X| | |X|O| | | |X|O| |O|X| | |X|O| | |X|X|O| |O|X| | |X|O|O| |X|X|O| |O|X| | 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| |X|O| | |X|O|O| | |X|X| |X|O|O| |X|O|O| 'X' Won! |X|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| |O| |O| | | |X| | | |X| |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |X|X| |O|X|O| |O|O|X| | |X|X| |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X| |O| | | | | |X|O| | |X|O|O| | | | | |X|O| | |X|O|O| | | | | |X|O|X| |X|O|O| |O| | | |X|O|X| |X|O|O| |O| |X| |X|O|X| 'O' Won! |X|O|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| |O|X| | | | |X| | | |O| |O|X|X| | |O|X| | | |O| |O|X|X| |X|O|X| | | |O| |O|X|X| |X|O|X| | |O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| | | | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| 'O' Won! |X| | | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| | |X|X| | | |O| | |O|O| | 'X' Won! |X|X|X| | |O| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| |X| |X|O| | | | | | |O| |X| |X|O|O| | |X| | |O| |X| |X|O|O| | |X|O| |O| |X| |X|O|O| |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | |O| |X| | | |X|O| | | | |O| |X| |X| |X|O| | |O| |O| |X| |X| |X|O|X| |O| |O| |X| |X| 'O' Won! |X|O|X| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X|O| | | |O| |O|O|X| 'X' Won! |X|X|O| | |X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | 'O' Won! |X| |O| | | |O| |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | |O| | | |X|X| |O|O|X| | |O| | | |X|X| |O|O|X| |O|O| | | |X|X| 'X' Won! |O|O|X| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| | |O|O| | |X| | | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | |X| | |O|O| | | | |X| | |X|O| |O|O| | | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| | |O|X| |X|X|O| |O|O|X| | |O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | |X| | |O| |X| | |O|O| | |X| | |O| |X| 'X' Won! |X|O|O| | |X| | |O| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | |O| | | |X| |O| |X| | | |O| |X| |X| |O| |X| | | |O|O|X| |X| |O| |X| | | |O|O|X| |X| |O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | | | |X|O|O| |X|X| | |O| | | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X| | 'O' Won! |O|X| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | |X|O| |X| | | |O|O| | | |X|O| |X| | | |O|O| | |X|X|O| |X|O| | |O|O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | |X| | |O|X| | |O| | | | |X|O| |O|X| | |O| |X| | |X|O| 'O' Won! |O|X| | |O| |X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| |O| |O| |X| | | | | |X|X|O| |O| |X| | | | | |X|X|O| |O| |X| | |O| | |X|X|O| |O| |X| |X|O| | |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | |O| |O| | | |X| | |X| | |O|X|O| | | |X| |O|X| | |O|X|O| |X| |X| |O|X| | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X|O| |O| | | |X| | | |X|X|O| |O| | | |X| | | |X|X|O| |O|O| | |X|X| | |X|X|O| |O|O| | |X|X|O| |X|X|O| |O|O| | 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| 'O' Won! | |O| | | |O|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O|O|X| 'X' Won! |X| | | | |X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| |O| | | | |X| | | | |O| |O| |X| | |X| | | | |O| |O| |X| | |X| | | |O|O| |O| |X| |X|X| | | |O|O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| | |O|O| | | | | |X|X|O| |X|O|O| | | | | 'O' Won! |X|X|O| |X|O|O| | | |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O|X| |O|X|O| |X| |X| |O|O|X| 'X' Won! |O|X|O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O| |X| |X| | | |O|X| | |O| |X| |X| |O| |O|X| | |O| |X| |X| |O| |O|X|X| |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | | | |O| |X|O|O| |X|X| | | |O|O| |X|O|O| 'X' Won! |X|X| | |X|O|O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X| | | |O| |O| |X|X| | |X| | | |O| |O| 'O' Won! |X|X| | |X| | | |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O| | | |X|X|O| | |X| | |O|O| | |X|X|O| | |X| | |O|O| | |X|X|O| | |X|X| |O|O| | |X|X|O| |O|X|X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | | |X|X| |O|X| | |O| | | | |X|X| |O|X|O| |O| | | | |X|X| |O|X|O| |O| |X| 'O' Won! |O|X|X| |O|X|O| |O| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | |X| | | | |O|O| |X|X|O| |X| | | | |O|O| 'X' Won! |X|X|O| |X| | | |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| |X| | |O| | |O|X|X| |O| |X| | |O|O| |O|X|X| |O|X|X| | |O|O| 'O' Won! |O|X|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| 'X' Won! |O| | | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | | | 'O' Won! |O| |X| |O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | |X| |X| | | 'O' Won! |O|O|O| | | |X| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | |X|X| | | | |O| | | |O| |X|X| | | | |O| | |X|O| |X|X| | | | |O| | |X|O| |X|X| | | |O|O| | |X|O| |X|X| | |X|O|O| |O|X|O| |X|X| | |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| |X| | |O|O| |X|X| | |O| |X| | |O|O| |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | |O| |X| |O|X| | | | |X| |O| |X| |O|X| | | | |X| |O|O|X| |O|X| | 'X' Won! | | |X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| |O| | | | |X|O| | | |X| |O|X| | | |X|O| | | |X| |O|X| | |O|X|O| 'X' Won! | |X|X| |O|X| | |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| | 'X' Won! |O|O| | |X|O| | |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | |O| | | |O| 'X' Won! |X|X|X| | | |O| | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| | |O|X| | |X| | | 'X' Won! |O|O|X| |O|X| | |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O|X| | | | |O| |X| |O| |O|X| | | | |O| |X|X|O| |O|X| | | | |O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X|X| | |O|O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O|X| 'O' Won! |X|O| | |X|O| | | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| | |O|O| |X| | | |X|O|X| | |O|O| |X| | | |X|O|X| | |O|O| |X| |O| |X|O|X| | |O|O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| | | |O| |X| | |O|O| |X| | | |O| |X| |X|O|O| |X|O| | |O| |X| |X|O|O| |X|O| | |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| | | | | |X| |O|X|O| |X| | | |O| |X| |O|X|O| |X|X| | |O| |X| |O|X|O| |X|X| | |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | | | |O|X|O| | | |X| |X| | | |O|X|O| |O| |X| |X| | | |O|X|O| |O| |X| |X| |X| |O|X|O| 'O' Won! |O| |X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | | |O| | |X| | |X|O| | | |X|O| | |X|O| |X|O| | | |X|O| |X|X|O| |X|O| | | |X|O| 'O' Won! |X|X|O| |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | |X| | | | |O| |O|X|X| | |X| | |O| |O| |O|X|X| 'X' Won! | |X| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| 'X' Won! |X| |X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| |X| 'O' Won! | | |O| |X|O| | |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| | |X| | | | |O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X|X| | |O|O| | |X|O|X| 'O' Won! |X|X| | |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | |X|O|X| |O| | | |O| | | |X|O|X| |O| | | |O| | | |X|O|X| |O| |X| |O| | | |X|O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O| |X| | |X| | |X|O|O| |O| |X| |O|X| | |X|O|O| |O|X|X| |O|X| | It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X|O| | | | | |X|X|O| |O|X|O| | | | | 'O' Won! |X|X|O| |O|X|O| | | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | | |O|X| | | | | |X|O|X| 'O' Won! | |O|X| | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | |X|X|O| |O| |O| | |X| | |X|X|O| 'X' Won! |O|X|O| | |X| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| |X|O| | | | |O| | |X|X| |X|O|O| | |X|O| | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O|X|O| | | | | | |X|O| |O|X|O| | | | | |X|X|O| |O|X|O| | | | | 'O' Won! |X|X|O| |O|X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | |X|O|X| | |O| | 'O' Won! |O|X| | |X|O|X| | |O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | | | |X|O|X| | |O|O| | | | | |X|O|X| | |O|O| | |X| | |X|O|X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'O' Won! |O|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| | | |O|X|O| |X|X| | 'X' Won! |O| |X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | |O|X| |X| | | |O| | | |X|O|X| |X| | | |O| | | |X|O|X| |X|O| | |O|X| | |X|O|X| |X|O| | 'O' Won! |O|X| | |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | |X|O|X| | | |O| |O| | | |X|O|X| | | |O| |O|X| | |X|O|X| |O| |O| |O|X| | |X|O|X| |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| 'X' Won! |X|O| | |O|X| | | | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O|O| | |X| | |X| | | |X|O|O| | |X| | |X| | | |X|O|O| |O|X| | |X| | | 'X' Won! |X|O|O| |O|X| | |X| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | | |X| | | |O| |O|X| | |O| |X| | | |O| |O|X|X| |O| |X| | |O|O| |O|X|X| |O| |X| | |O|O| |O|X|X| |O|X|X| 'O' Won! |O|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| |X| |O| |O| | | |X| |X| |X| |O| |O|O| | |X| |X| 'X' Won! |X| |O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | |X|O| |O| | | |O| |X| | |X|O| |O| |X| |O| |X| | |X|O| 'O' Won! |O| |X| |O| |X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | |X| | | | | |X|O|O| | | |X| | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X| |X| | |O|O| |X|O|O| 'X' Won! |X| |X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X|O|O| | | |X| | |X| | |X|O|O| | | |X| |O|X| | |X|O|O| | | |X| |O|X| | |X|O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O|X| |X| | | | |O|X| |O|O|X| 'X' Won! |X| |X| | |O|X| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | | | |X| | | |X| |O|O| | | |O|X| | | |X| |O|O| | |X|O|X| |O| |X| |O|O| | |X|O|X| 'X' Won! |O| |X| |O|O|X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X|O|O| | | |X| |O| |X| |X|O|O| | | |X| |O| |X| |X|O|O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| | |O|O| | 'X' Won! |X| | | |O|X| | |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X| | | | | |O| |O|O|X| |X| | | |X| |O| 'O' Won! |O|O|X| |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | 'O' Won! | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O| |X| |O|X| | |X|O| | |O|O|X| |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | |O|X| | |O|O| | | |X| | |O|X|X| |O|O| | | |X|O| |O|X|X| |O|O| | |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | |X| |O|X| | | |O|O| | | |X| |O|X| | | |O|O| |X| |X| |O|X|O| | |O|O| |X| |X| 'X' Won! |O|X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| |X| | |X| | | |O| | |O|O|X| |X|X| | | |O| | |O|O|X| |X|X| | |O|O| | |O|O|X| 'X' Won! |X|X|X| |O|O| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X| | | | |O|O| |X| |X| |X| | | | |O|O| |X|O|X| |X| |X| | |O|O| |X|O|X| 'O' Won! |X|O|X| | |O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| 'O' Won! |X| | | |X| | | |O|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | |O| | | |O|X| | |X|X| | |O| | | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | |O|O| |X|O|X| 'O' Won! | |X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X|O| | |X|O|O| | |X| | |X|O| | |X|O|O| | |X|O| |X|O| | 'X' Won! |X|O|O| | |X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| |X| | | |O| |O| 'O' Won! | |X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | |O| | |X| | | |X|O|O| | |O|X| |X| | | |X|O|O| | |O|X| |X| |O| |X|O|O| 'X' Won! | |O|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| | | | |O|X| |X|O|X| |O| | | | |O|X| |X|O|X| |O| | | |O|O|X| 'X' Won! |X|O|X| |O|X| | |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| |X| | | |O| |O| | |X|O| |X| | | |O| |O| | |X|O| |X| |X| |O| |O| |O|X|O| |X| |X| 'X' Won! |O| |O| |O|X|O| |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X|X| |O| | | |X| | | |O|X|X| |O| | | |X|O| | 'X' Won! |O|X|X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| |X| |X| | |O| | | |O|O| |X| |X| |X|O| | | |O|O| |X| |X| 'O' Won! |X|O| | | |O|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| |O| | | | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| |X| |X|X| | |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O| | | |X|O| |X|O|O| |X|O| | 'X' Won! |X|X|O| |X|O|O| |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O|X| | | | | | |O| | 'O' Won! |X|O|X| | |O| | | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| 'O' Won! | | |X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| |X| |O| | | | | | |X|O| |X|O|O| 'X' Won! | | |X| | |X|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | |X| |O| 'O' Won! |O|X| | | |O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X|X| | | | |O| |O|O| | |X|X| | | | |O| |O|O| | |X|X| | |X| |O| 'O' Won! |O|O|O| |X|X| | |X| |O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O| | |O| |O| |X| |X| | |O| | |O|X|O| |X| |X| | |O|O| |O|X|O| 'X' Won! |X|X|X| | |O|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| |X|X|O| | |O| | | |O|X| |X|X|O| | |O| | 'X' Won! | |O|X| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | | |O| |O|X|O| | | |X| |X| |O| |O|X|O| |O| |X| |X| |O| |O|X|O| |O|X|X| |X| |O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X|O| |O|X|X| | | | | | |X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | |X| |O|X|O| 'O' Won! |O|X|X| |O| |X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| |O| | | |O|X|X| |X| |O| |O| | | |O|X|X| |X| |O| |O|O| | |O|X|X| |X|X|O| |O|O| | 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X|X| |O| |O| | |O|X| | |X|X| 'X' Won! |O| |O| | |O|X| |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |O| 'X' Won! | |X| | |O|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X| |X| |X| |O|O| | |O| |X| |X| |X| |O|O| | |O|X|X| |X| |X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | |O| |X| |X| | |X|O| | | |O| |X|O|X| | |X|O| | | |O| |X|O|X| | |X|O| | |X|O| |X|O|X| |O|X|O| | |X|O| 'X' Won! |X|O|X| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X|X| | |O|O| |O| |X| | |X|X| | |O|O| |O|X|X| | |X|X| | |O|O| 'O' Won! |O|X|X| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X|O| | | | | |X| |O| |X|X|O| | | | | |X| |O| 'O' Won! |X|X|O| | | |O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | |X| | | |X| |O| |O|X| | |X| | | |X|O|O| |O|X| | |X|X| | |X|O|O| |O|X| | |X|X|O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| | | | |X|O| |O|O| | |X| |X| | |X|O| |O|O| | |X| |X| |O|X|O| |O|O| | 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| |X| |O|O| | 'O' Won! | | | | |X| |X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| | |X| | | |X| | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | |X|X| | |X|O|O| |O|X| | |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | | |X| |O| | | |X|O| | | |O|X| |O|X| | |X|O| | | |O|X| |O|X|O| |X|O| | | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | |O| |O| 'X' Won! | |X| | | |X| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| |O| | | | |X| | | |O|X| |O| | | | |X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O| | | |O|X|X| |X|O|X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | | |X| | | |O| |O|X|X| |O| |X| | | |O| |O|X|X| |O|X|X| | | |O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | |O| |O| | |X|X| |O| | | |O| |O| | |X|X| |O|X| | 'O' Won! |O| |O| |O|X|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| |O| |O| | | | | |X| 'X' Won! |X| |O| |O|X| | | | |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X| | | |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| | | |O| | | |O| |X|O|X| |X| |O| | | |O| |X|O|X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'O' Won! |O|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| |X| |O| | | |X| |O| |O| |X| |O| |X| |X| |O| |O| |X| |O|O|X| |X| |O| |O| |X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |O| | |O|X| | |O| | |X| |O| | |O|X| |X|O| | |X| |O| |O|O|X| |X|O| | |X|X|O| |O|O|X| |X|O| | It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O|X| | |O| | | |X|X| | |O|X| | |O|O| | |X|X| | |O|X| |X|O|O| | |X|X| |O|O|X| |X|O|O| | |X|X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| |O| | | | | |X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X| | |X|O|X| |O|X|O| |O|X| | It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | |X|X| | |O|O| | | |O| | |X|X| | |O|O| | | |O|X| |X|X| | 'O' Won! |O|O| | | |O|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X| | | | | | |O|X|X| 'O' Won! |O|X| | |O| | | |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | | | | | |X|O|O| |O|X| | | | | | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | |O|X| | 'X' Won! |X|O|O| |O|X| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| |O|X| | | | | | 'X' Won! | |O|X| |O|X| | |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| | |X| | |O| |X| |O|O|X| | |X| | |O| |X| |O|O|X| |X|X| | |O| |X| |O|O|X| |X|X| | |O|O|X| 'X' Won! |O|O|X| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X|X| | |O| | | |X| |O| |X|X|O| |O| | | |X| |O| |X|X|O| |O| |X| |X| |O| |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | |X|O| |O| |X| | | |O| | |X|O| |O| |X| 'X' Won! |X| |O| | |X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | |X| | | |X|X|O| | |O| | |X| | | |X|X|O| | |O| | |X|O| | |X|X|O| | |O|X| |X|O| | |X|X|O| |O|O|X| |X|O| | It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | | |O| | | |X| | |O|X|X| | |O| | | |X| | 'O' Won! |O|X|X| | |O| | | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| | | |X|O| | 'X' Won! |X| |O| |X| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | 'O' Won! | |X| | |O|O|O| | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| |X| | | |O| | |X| | |O|O|X| 'X' Won! |X| |O| | |X| | |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | |O|O| | 'X' Won! | | | | |X|X|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | | | | |O| | | |O|X|X| | | | | |O|X| | |O|X|X| | | | | |O|X|O| 'X' Won! |O|X|X| | |X| | |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X|O| | 'X' Won! |X|O|O| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | | |O| | | | |X| |X|O| | | |O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O| | 'O' Won! |X|O| | |O|O|X| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| | |O|O| | | | | |X| |X| 'X' Won! | |O|O| | | | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| |X| | |O| | |X| |O| |O| |X| | |O| | |X|X|O| |O| |X| | |O| | |X|X|O| |O| |X| |O|O| | |X|X|O| |O| |X| |O|O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | | | |O|X|O| |X| | | | |X| | |O|X|O| |X| | | | |X| | |O|X|O| |X| |O| |X|X| | |O|X|O| |X| |O| |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | |O| | |X| |O| | |O|X| | |O|X| |X| |O| |O|O|X| | |O|X| |X|X|O| |O|O|X| | |O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X|X| | | | |O| |O| | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O|O| | |X|X| | |O|X|O| |O|O|X| |X|X| | |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O|X|O| 'O' Won! |O|X| | |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | 'X' Won! | | |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| | | | | |X|O| | | |O|X| |O| | | |X|O| | |X|O|X| |O| | | 'O' Won! |X|O| | |X|O|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X|X| | | |O|O| 'O' Won! | | |X| |X|X| | |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | | |X| |O| | | |X|O|O| | | |X| |O|X| | |X|O|O| | | |X| |O|X| | |X|O|O| | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |O| |X|X| | | | |O| | | |O| |X|X| | | | |O| | |O|O| |X|X| | | |X|O| | |O|O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X|O|X| | | |O| |X|O|O| 'X' Won! |X|O|X| | |X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| | |O|X| |O| |O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| | |X| | |O| | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X|X| |O| | | |X|O|X| |O|X|X| |O| |O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | | |O|X| | |X|X| | |O| | | |O|X| | |X|X| |O|O| | |X|O|X| | |X|X| |O|O| | 'O' Won! |X|O|X| | |X|X| |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | | | | |X|X| |O|O| | | | | | |O|X|X| |O|O| | | | |X| |O|X|X| 'O' Won! |O|O| | |O| |X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| | | | |O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X|X| | |O|O| | |X|X|O| |X|X|O| 'X' Won! |O|O|X| |X|X|O| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | | | 'O' Won! |O|X|X| | |O| | | | |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| |X|X| | | |O|O| |X| |O| |X|X| | | |O|O| 'O' Won! |X| |O| |X|X|O| | |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | |O| | |X|X|O| | |X| | | |O| | |X|X|O| | |X|O| | |O| | 'X' Won! |X|X|O| | |X|O| | |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| |X|O| | |X|X| | | |O|O| |X|O|O| |X|X| | 'X' Won! | |O|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| |X|X| | |O| |O| 'X' Won! | | |O| |X|X|X| |O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| |X|X| | 'O' Won! | | |O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X|X| |X| | | |O| |O| |O|X|X| 'X' Won! |X| | | |O|X|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | | |O|O| | |X|X| | | | | | |O|O|X| |X|X| | |O| | | |O|O|X| 'X' Won! |X|X|X| |O| | | |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | |O| |X| |O| |X|O| | 'X' Won! |X| |O| |X| |O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | |O| | |O| | | |X| |X| |X|O| | |O| | | |X| |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| |X| |X| |X| |X|O|O| |O|O|X| 'X' Won! |X|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| | | |X|O| |X| | | |O|O|X| | |X|O| |X| |O| |O|O|X| | |X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X| |X| | |O|O| | | |X| |X| |X| | |O|O| | |O|X| 'X' Won! |X|X|X| | |O|O| | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X|O| |X|O| | |O|X| | |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X| | |O|X| |O| | | |O| |X| |X|O|X| |O| | | |O|O|X| |X|O|X| |O| | | |O|O|X| |X|O|X| |O|X| | 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | |X| | | | |X|O| |O| | | |X| | | |O|X|O| |O| | | 'X' Won! |X| | | |O|X|O| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | | |X|O| |X| | | |O| | | |O|X|O| |X| | | |O| | | |O|X|O| |X|X| | |O|O| | |O|X|O| |X|X| | 'X' Won! |O|O| | |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | |X| |O|X| | |O|O| | | | |X| |O|X|X| |O|O| | | | |X| 'O' Won! |O|X|X| |O|O| | |O| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | | | | | |O|O|X| |O|X| | | | |X| |O|O|X| |O|X| | | |O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | |X| | | | |X| | 'O' Won! |O|O|O| |X| | | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X|O| |O|X| | | | | | |X|X|O| |O|X| | | | | | |X|X|O| |O|X| | | | |O| |X|X|O| |O|X|X| | | |O| |X|X|O| |O|X|X| |O| |O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| |O| | | | | |X| |X| |O|X|O| | | | | |X| |X| |O|X|O| | | |O| 'X' Won! |X| |X| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O|X|O| |O| | | |X| |X| |O|X|O| |O|O| | 'X' Won! |X|X|X| |O|X|O| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| | |O| | 'X' Won! |X|O|O| | |X|X| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X|O| | |X|O| | | | | |O|X|O| | |X|O| | | |X| |O|X|O| | |X|O| |O| |X| 'X' Won! |O|X|O| | |X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| |X| |O|X|O| | |X|O| |X|O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | | |O| | | |X| | |X|O| | | |O| | |O|X| | |X|O| | | |O| | |O|X| | |X|O|X| | |O| | |O|X|O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|O| | |O| | |O|X|X| | |X|O| | |O|O| |O|X|X| | |X|O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X| | |X|X| | | | |O| |O|X| | |X|X| | | |O|O| |O|X| | |X|X| | |X|O|O| |O|X| | |X|X|O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X| |O| |O|O| | |X|O|X| |X| |O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X|O|X| | |O| | | | | | |X|O|X| |X|O| | | | | | 'O' Won! |X|O|X| |X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| | | |X| |O| | |O|O| 'X' Won! |X| | | |X| |O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| |O| |O| | | | | |X| |X|X|O| |O| | | | | |X| |X|X|O| |O| |O| |X| |X| |X|X|O| |O| |O| 'O' Won! |X| |X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X| | |X| |X| |O| | | |O|X|O| |X| |X| |O|X| | |O|X|O| |X| |X| |O|X| | |O|X|O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |X| |X| | | | |O| | |O| |X| |X| | | | |O|X| |O| |X| |X| | | | |O|X| |O| |X| |X| |O| |X|O|X| |O| |X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O| | | |X| | |O|X|X| |O|O| | | |X| | |O|X|X| |O|O| | | |X|X| 'O' Won! |O|X|X| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | |X|O|O| |X| |O| | | |X| |X|O|O| |X| |O| | | |X| |X|O|O| |X|O|O| | |X|X| |X|O|O| |X|O|O| 'O' Won! |O|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | | | |X|O|O| |X|X| | |O| | | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| |X| | |O| | | |O|X| |O| |X| | |O| | |X|O|X| |O| |X| |O|O| | |X|O|X| |O| |X| 'X' Won! |O|O|X| |X|O|X| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | |X| | |O|O| | | | |X| |O|X| | |O|O| | | | |X| |O|X| | |O|O|X| | | |X| |O|X| | |O|O|X| | |O|X| |O|X| | |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | | |O|X|O| |O|X| | | | |X| |O|X|O| |O|X| | 'O' Won! |O| |X| |O|X|O| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | |O|O| | |X|O| |X| |X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| | |O|O| 'O' Won! |X|X|O| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | |X|X| | | |O|X| | |O| | |X|X|O| | |O|X| | |O|X| |X|X|O| | |O|X| | |O|X| |X|X|O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| | | | | |O|X|X| | |O|O| | | | | |O|X|X| | |O|O| |X| | | |O|X|X| | |O|O| |X|O| | |O|X|X| | |O|O| |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | |X| | | |X| | |O| |O| | |X|O| | |X| | |O| |O| |X|X|O| 'O' Won! | |X| | |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X|X| | 'O' Won! |X| | | |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | | | |O| |X|O|O| |X|X| | |O| |O| |X|O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X|O| |O| | | | | |X| |X|X|O| 'O' Won! |O| | | | |O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X|X| | | | | |X|O|O| |O|X|X| | | |X| |X|O|O| |O|X|X| |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X|X| | |O|O| | | |X| | |X|X| | |O|O| | |O|X| | |X|X| |X|O|O| | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | |X| | |O|X| | | |O| 'O' Won! |O| |X| | |O|X| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |O|X| | | |X| | |O|O| | |O|X|X| 'O' Won! | |X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | |O| | |X| |O| | | |X| |X|O| | |X| |O| | | |X| |X|O| | |X|O|O| | | |X| |X|O|X| |X|O|O| | | |X| |X|O|X| |X|O|O| |O| |X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | | | |O| | |X|X| |O|O| | | |X|O| | |X|X| |O|O| | |O|X|O| | |X|X| |O|O| | |O|X|O| | |X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O| | | |O|X|X| |X|O|X| |O| | | |O|X|X| |X|O|X| |O|O| | 'X' Won! |O|X|X| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | | |O|X| |X| | | |O| | | | |O|X| |X| | | |O|O| | | |O|X| |X| |X| |O|O| | 'O' Won! | |O|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O| |X| | |X|X| |O| | | |O|O|X| | |X|X| 'X' Won! |O| |X| |O|O|X| | |X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X| | |X| | 'O' Won! |O| | | |O| |X| |O|X| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| |O|O| | |O|X|X| | |X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| |O| | | |X| | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| | |O| | 'X' Won! |X|O|O| | |X|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| | | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| |X|O| | |X| |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'X' Won! |X|O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O| |X| | |X| | |O| |X| 'O' Won! |O| |X| |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | | |O| |O|X|X| | |O| | | |X|O| |O|X|X| | |O| | |O|X|O| |O|X|X| 'X' Won! |X|O| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| | | |X|X|O| |O| |X| |O|O| | |X|X|O| |O| |X| |O|O| | |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O| |X| |O|X| | |X|O|O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| | | |X| | |O|X| | | |O|O| | |X| | |O|X| | |X|O|O| | |X| | |O|X|O| |X|O|O| | |X| | |O|X|O| |X|O|O| | |X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| |O| | | |X| |O| |X| |X| |O| | |O|X| |O| |X| |X| |O| | |O|X| |O| |X| |X|X|O| |O|O|X| |O| |X| |X|X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| 'O' Won! |X|X|O| | | |O| | | |O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | |O|X| | |X|O| |X|O| | | |O|X| | |X|O| |X|O|O| | |O|X| |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| |O| | | |X|X| | | |O|X| |O| | | |X|X| | | |O|X| |O| |O| |X|X| | |X|O|X| |O| |O| 'O' Won! |X|X| | |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | |X|O| | |X| | |X|O| | |O|X|O| | |X| | |X|O| | |O|X|O| |X|X| | |X|O| | |O|X|O| |X|X| | |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| |X|O| | |X| | | |O|O|X| |X|O| | |X| | | |O|O|X| |X|O|X| |X| | | 'O' Won! |O|O|X| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O|X| | |X| | 'O' Won! | | |O| | |O|X| |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O|X| 'O' Won! | |O| | | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | |X| | | |X| |O|O| | 'O' Won! | | |X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O|X| | |O| | | |X|X| |O|O|X| | |O| | | |X|X| 'X' Won! |O|O|X| | |O|X| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| |O|X|O| | | | | |X| |X| |O|X|O| | | | | |X|O|X| |O|X|O| | | |X| |X|O|X| |O|X|O| 'O' Won! |O| |X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X| |X| |O|X|X| 'O' Won! |O| |O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | |X| | | | |X| |O| |O| | |X|O| | | |X| |O| |O| |X|X|O| | | |X| |O| |O| |X|X|O| | |O|X| |O|X|O| |X|X|O| | |O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | |O| |X| |O| | |X|X| | | |O| |X|O|O| | |X|X| | | |O| 'X' Won! |X|O|O| |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | |X| | | |X|O|O| | |X| | |X| | | |X|O|O| |O|X| | |X| | | |X|O|O| |O|X|X| |X|O| | |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| | | |X|X| |O|O| | |X|O| | | |X|X| |O|O|X| |X|O| | | |X|X| |O|O|X| |X|O|O| | |X|X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | |X| |X| | |O|O| | |O| | |X| |X| |X|O|O| | |O| | |X| |X| 'O' Won! |X|O|O| | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | |X| | |X| | | |O| |O| | |X| | |X| |O| |O| |O| | |X| | |X| |O| |O|X|O| 'O' Won! | |X|O| |X| |O| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| | | |X| |X| | |X|O| 'O' Won! |O| | | |X|O|X| | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| |X| | |O| | |O|O|X| |X| |X| 'X' Won! | |O| | |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| |O| | | | | |X| |X|X|O| |O| | | |O| |X| |X|X|O| |O| | | |O|X|X| |X|X|O| |O| | | |O|X|X| |X|X|O| |O| |O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| | |O| | |X|O|X| | | |O| |X|O| | |X|O|X| | | |O| |X|O|O| |X|O|X| 'X' Won! |X| |O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X|O| | |O| | | |X| |X| |X|O| | |O| | | |X|O|X| |X|O| | |O| | | |X|O|X| |X|O|X| |O| | | 'O' Won! |X|O|X| |X|O|X| |O|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| | |X| | |X|O|O| | | |O| |X|X| | |X|O|O| 'O' Won! | | |O| |X|X|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| | | |O| | |X| | |O|O|X| | | |O| |X|X| | |O|O|X| |O| |O| |X|X| | 'X' Won! |O|O|X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| |X| | | | | |X| |O| |O| 'O' Won! |X| | | | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | | | |X| |O|O| | |O|X| | | |X|X| |O|O| | |O|X| | | |X|X| |O|O| | |O|X|O| 'X' Won! |X|X|X| |O|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | |O| | |X|O| | |O|X| |X| |O| | |X|O| | |O|X| |X| |O| |O|X|O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X|O| | | | | |O|O| | |X|X|O| | |X| | |O|O| | |X|X|O| |O|X| | |O|O| | 'X' Won! |X|X|O| |O|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O| |O| |X|O| | | |X| | |O|X|O| |X|O|O| | |X| | |O|X|O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| 'X' Won! | | |X| |O|X| | |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| |X| | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O|X|X| | | |O| |X|X|O| |O|X|X| |O| |O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | 'X' Won! |X| |O| |O|X| | | | |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | |O|X| | |X| |O| |O|X| | |O|X|X| |X| |O| |O|X| | |O|X|X| |X|O|O| |O|X| | It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| |O| |O| |X| |X| |X| |O| |O| 'O' Won! |O| |X| |X|O|X| |O| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | |X| | | | | |O| 'X' Won! |X|O| | |X| | | |X| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| |X| | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X| | |O| |O| |X|X| | |O|X|X| |O| |O| 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| |X| | |O| | | |X| | |O| |X| |O|O| | | |X| | |O| |X| |O|O|X| | |X| | |O|O|X| |O|O|X| |X|X| | |O|O|X| |O|O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | | | | |X| | | |O|X|O| | |O| | |X| | | |O|X|O| | |O| | |X|X| | |O|X|O| |O|O| | |X|X| | 'X' Won! |O|X|O| |O|O| | |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |O| | |X| | |O|X|O| | | |O| |X|X| | 'O' Won! |O|X|O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| |O| | |O|X| |X|X| | |O| |O| | |O|X| |X|X|O| |O| |O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X|O| |X| | | |O| | | | |X|O| |X|X| | |O| | | | |X|O| |X|X| | |O| |O| 'X' Won! | |X|O| |X|X| | |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| | |X|X| |O| |O| | | |O| | |X|X| |O|X|O| | | |O| |O|X|X| |O|X|O| 'X' Won! | |X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | |O|O|X| | |X| | | | | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O| |X| |O|O|X| |X|X| | |O|O|X| |O|O|X| |X|X| | 'X' Won! |O|O|X| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | |X| |X| | |X|O| |O|O| | |X| |X| | |X|O| |O|O| | |X| |X| |X|X|O| 'O' Won! |O|O| | |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | | |O| | |X|O|X| |O|X| | | |O|O| |X|O|X| |O|X|X| | |O|O| |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | |X| |O| |X| | |X|O| | | |X| |O| |X| |O|X|O| | | |X| |O|X|X| |O|X|O| 'O' Won! |O| |X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O| |O|X|O| | | | | | |X|O| |O|X|O| |X| | | 'O' Won! | |X|O| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | 'O' Won! |O| |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X| | | |O|X| |X| |O| | |X|O| | |O|X| |X| |O| | |X|O| | |O|X| |X|X|O| 'O' Won! |O|X|O| | |O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X|X| | | |O|O| | | |O| |X|X| | | |O|O| |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| |O| | | |X|X|O| | | |X| |O| | | 'O' Won! |X|X|O| | |O|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O| | | |X| | |X|O|O| | |O| | | |X|X| |X|O|O| | |O| | |O|X|X| |X|O|O| | |O| | |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | |O|X| |O|X| | | |X| | | |O|X| |O|X| | | |X| | |O|O|X| |O|X|X| | |X| | |O|O|X| |O|X|X| | |X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | | | |O| |X|O|O| |X| |X| | |O|O| |X|O|O| |X| |X| 'X' Won! |X|O|O| |X|O|O| |X| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O|O| | |X| |X| |O| | | |O|O|X| |X| |X| |O|O| | |O|O|X| |X| |X| 'X' Won! |O|O|X| |O|O|X| |X| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X|O|O| | |X|X| | | | | |X|O|O| |O|X|X| | | | | |X|O|O| 'X' Won! |O|X|X| | |X| | |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | |O| | | | | |X|X|O| | | |O| |X| | | |X|X|O| |O| |O| |X| | | |X|X|O| |O|X|O| |X| | | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | |X|O| |O|O| | |X| | | | |X|O| |O|O|X| |X| | | | |X|O| 'O' Won! |O|O|X| |X|O| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | |X|O| | | |X|O| | | | | |X|O|O| | |X|O| | | | | |X|O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X|O| | 'X' Won! |X|O|O| | |X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | | |X| | |X| | |O| |O| | | |X| | |X|O| |O| |O| | |X|X| | |X|O| 'O' Won! |O|O|O| | |X|X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X|O|O| 'X' Won! | |O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | |O| | |X|O| | |X|X| | | |O| | |X|O| | |X|X| | | |O| | |X|O|O| |X|X| | | |O|X| |X|O|O| |X|X| | |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | |O| | |X|X|O| | | |X| | |O| | |X|X|O| | | |X| |O|O| | |X|X|O| |X| |X| |O|O| | 'O' Won! |X|X|O| |X| |X| |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| | | |X|O| |X| |O| | |X| | 'O' Won! | |X|O| |X| |O| | |X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| |X| |O| | | | | |X| |O| |X| |O|O| | | | |X| |O|X|X| |O|O| | 'O' Won! |O| |X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | |X|X| |O| | | |O| | | 'O' Won! |O|X|X| |O| | | |O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | | |X|X| | |O| | |O| | | |O|X|X| | |O|X| |O| | | |O|X|X| | |O|X| |O| |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| | |X|X| | |O| | | |X|O| | |X|X|O| |O| | | |X|O| | |X|X|O| |O|X| | |X|O| | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | |O| | | |O|X|X| | |O| | |O|X| | |O|X|X| | |O| | |O|X|O| |O|X|X| 'X' Won! |X|O| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| |X|O| | | | |O| |X|X|O| |X|O| | | | |O| |X|X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'X' Won! |X|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| |O| |O|X| | |X|X| | |O| |O| |O|X|X| |X|X| | |O| |O| |O|X|X| |X|X|O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X| |X| | |O| | |X| |O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'X' Won! |X|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O|O| | | |X| | |X|O| | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | |X| | |O| | | |X|O| | |O|X| | |O| | | |X|O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| | |O|O| | | | | |X|X|O| |X|O|O| | | | | |X|X|O| |X|O|O| | |O| | |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | |X| | | |O|O| | | |X| | |X| |O| |O|O| | | |X| | |X|X|O| |O|O| | | |X| | |X|X|O| |O|O| | | |X|O| |X|X|O| |O|O|X| | |X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | | |O| | | |X| |X| |O|X| | |O|O| | |X| |X| |O|X| | |O|O| | |X| |X| |O|X|X| 'O' Won! |O|O|O| |X| |X| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | |O|X| |X| |O| | |X| | | |O|X| |X| |O| | |X| | | |O|X| |X|O|O| | |X| | |X|O|X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| | |O|O| | | |X| |X|O|X| | |O|O| | |X|X| |X|O|X| | |O|O| |O|X|X| |X|O|X| 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | 'O' Won! |O|X| | |O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O|X| | | |O| | |X| |X| |O|X| | | |O|O| |X| |X| 'X' Won! |O|X| | | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| 'O' Won! |X| | | | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| 'O' Won! |O| |X| |O| | | |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | | |X| |X|O| | | | |O| | | |X| |X|O| | |X| |O| | | |X| |X|O| | |X| |O| | |O|X| |X|O|X| |X| |O| | |O|X| |X|O|X| |X| |O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | |O| | |X| |X| | |O| | | |O|X| |X| |X| | |O| | | |O|X| |X| |X| |O|O| | 'X' Won! | |O|X| |X| |X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X|X| | |O| |O| | |X| | |X|X|O| |O| |O| 'X' Won! | |X| | |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X|X| | |O|O| | | |O| | |X|X| |X|O|O| | | |O| | |X|X| |X|O|O| | | |O| |O|X|X| |X|O|O| |X| |O| |O|X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O|O| |X|X|O| 'X' Won! |X|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O|X| | | |O| |X|X| | | |O|X| | |O|O| |X|X| | |X|O|X| | |O|O| |X|X| | 'O' Won! |X|O|X| |O|O|O| |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X|X| | | | | | |X|O|O| |X|X| | | | |O| 'X' Won! |X|O|O| |X|X| | |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |X|X| | |O| |O| | |X|O| |X|X| | |O| |O| |X|X|O| |X|X| | 'O' Won! |O| |O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | |X| | | |X| |O| |O|O| | |X| | | |X|X|O| |O|O| | |X| | | 'O' Won! |X|X|O| |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O| | | 'X' Won! |O|O|X| |X|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| |O| | | | | |X| |O|X|O| |O|X| | | | |X| |O|X|O| 'O' Won! |O|X| | |O| |X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |O| | | | | |X|O| | |X| |O| | | | | |X|O| | |X| |O| | | |O| 'X' Won! |X|O| | |X| |O| |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| | | | |O|X| | |X|O| |X| |O| | |O|X| | |X|O| |X| |O| | |O|X| |X|X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | | |X| | | | | |O|X|O| | |O|X| | | | | |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | |O| |X|X|O| | | | | | | |O| |X|X|O| | | |X| | | |O| |X|X|O| | |O|X| 'X' Won! |X| |O| |X|X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | |O|X| |X| | | | |O|O| | |O|X| |X| |X| | |O|O| 'O' Won! | |O|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| |O| | | | |X| | |X| |O| |O| |X| |O|X| | |X| |O| |O| |X| |O|X|X| |X| |O| |O| |X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | |O| | | |O| |X| |X|X| | |O| | | |O| |X| |X|X|O| |O| | | |O| |X| |X|X|O| |O| | | |O|X|X| 'O' Won! |X|X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X|X|O| |O| |O| | | |X| |X|X|O| |O| |O| | |O|X| 'X' Won! |X|X|O| |O|X|O| | |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | |O| |X| |O| | |O|X| | |X|O| 'O' Won! |X| |O| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | |X|X| | | | |O| | | | | |X|X| | | |O|O| | | |X| |X|X| | | |O|O| |O| |X| |X|X| | | |O|O| |O|X|X| |X|X| | | |O|O| 'O' Won! |O|X|X| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| | |O| | | |X| | |X|O|O| | |O|X| | |X|O| |X|O|O| | |O|X| | |X|O| |X|O|O| |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X|X|O| |O| | | |X|O| | |X|X|O| |O| | | 'X' Won! |X|O| | |X|X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X|O| |X| | | | |O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X| | | |O|O|X| |X|X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | | | | | |O|O|X| | |X|O| | | | | |O|O|X| |X|X|O| 'O' Won! |O| | | |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X|X| | |O|O| | | | | |O|X|X| |X|O|O| | | | | |O|X|X| 'O' Won! |X|O|O| | |O| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| 'X' Won! | |O|X| | |O|X| | | |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | |O| |O| |X| |O| |X| | | |O| |O| |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O| |O| |X| |X| | |O| | |O|X|O| |X| |X| | |O| | |O|X|O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| |X| | |O| | |O|X|X| |O| |X| | |O|O| |O|X|X| |O| |X| |X|O|O| 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X|X|O| | | | | |O|O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X|O| | |O|O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| |O|X|X| | |X| | | | |O| |O|X|X| | |X|O| | | |O| |O|X|X| | |X|O| |X| |O| |O|X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | |X|O| | | |X| | | |O| |O|X|O| | | |X| | | |O| |O|X|O| | | |X| |X| |O| 'O' Won! |O|X|O| | |O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | | | | | |X|O| |X|O| | | | | | |O|X|O| |X|O| | | | |X| |O|X|O| |X|O| | |O| |X| |O|X|O| |X|O|X| |O| |X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X| |X| |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X| |X|O|X| | | |O| |O| |X| |X|O|X| | | |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| |O| | | |X| | | | |X|O| |O| | | |X| |X| | |X|O| |O|O| | |X| |X| | |X|O| 'X' Won! |O|O| | |X|X|X| | |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| | |O| |O| | |X|X| | |O|X| |O| |O| | |X|X| |O|O|X| |O| |O| | |X|X| |O|O|X| |O|X|O| 'O' Won! |O|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X|X| | |O| | | |O|O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O| | | |O|O| |O|X|X| |X|O|X| | |O|O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| |X| |O| | | | |O|X| |O| |X| 'X' Won! |O| |X| | |O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X|O| | |O| | 'X' Won! |X|O|X| |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| 'O' Won! | | |X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| 'O' Won! |O| | | |X|O|X| | | |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | |X|O|X| |O|X| | |O|X|O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X| | | |O|X| |X| |O| | |X| | | |O|X| |X|O|O| | |X| | |X|O|X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | |X| |O| |X| |O| | |O| | 'X' Won! |X| |O| |X| |O| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| | |O| | |X|X| | 'O' Won! |O| |X| | |O| | |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X|X| | |O|O| | |O| | | |X|X| | 'X' Won! |O|O| | |O| | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | |O| | | |X|X| |X| |O| | |O| | | |X|X| |X| |O| | |O|O| | |X|X| |X|X|O| | |O|O| | |X|X| 'O' Won! |X|X|O| | |O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |X|X| | |O| | | | |X|O| |X|X|O| |O| | | | |X|O| |X|X|O| |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| | |O| | |O|X| | |X| |X| |O|O| | |O|X| | |X| |X| 'X' Won! |O|O|X| |O|X| | |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O|X|X| | |O| | |X| | | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | | | |X|O| | |X| |O| | | | | |X|O| | |X| |O| |O| | | |X|O| | |X| |O| |O| |X| |X|O|O| |X| |O| |O| |X| |X|O|O| |X| |O| |O|X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |X| | |O|X| |O|O| | |X| |X| 'X' Won! | |O|X| |O|O|X| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | |X| | | | |X|X| |O|O| | |X|O| | | |X|X| |O|O| | 'X' Won! |X|O| | | |X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |X| | |O|X| | |O|O| |X| |X| | |O|X| |X|O|O| |X| |X| | |O|X| 'O' Won! |X|O|O| |X|O|X| | |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O|X| | |O|O|X| |X|X|O| |O|X| | It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | |X|O| | | | |X| | |O| | |X|O| | | | |X| | |O|O| |X|O| | | | |X| |X|O|O| |X|O| | |O| |X| |X|O|O| |X|O|X| |O| |X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| |O| |O| | |X| | | |X|X| |O| |O| | |X| | | |X|X| |O| |O| | |X|O| 'X' Won! |X|X|X| |O| |O| | |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| |O| | | | |O| | | | |X| |O| | | |X|O| | | | |X| |O|O| | |X|O| | | | |X| |O|O| | |X|O| | |X| |X| 'O' Won! |O|O|O| |X|O| | |X| |X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | |X|O| |X| |O| | |O| | | |X|O| |X| |O| | |O|X| | |X|O| |X| |O| |O|O|X| | |X|O| |X|X|O| |O|O|X| | |X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| | |O|O| |X| | | |X|O|X| | |O|O| |X| | | 'O' Won! |X|O|X| | |O|O| |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | 'O' Won! |X| |O| |X| |O| | | |O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| |X| |X|O| | |O| | | |X|O|X| |X|O|X| |O| | | |X|O|X| |X|O|X| |O| |O| |X|O|X| 'X' Won! |X|O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X|O| | |X|O|O| | |X| | |X|O| | |X|O|O| |O|X| | |X|O| | |X|O|O| |O|X|X| |X|O| | It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X|O|X| |O| | | | | |X| |X|O|X| |O| | | |O| |X| |X|O|X| |O|X| | |O| |X| |X|O|X| |O|X|O| |O| |X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | |X| | | | |O|O| | |X| | 'O' Won! |X| | | |O|O|O| | |X| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | |X| |X|O|O| | | |O| | | |X| |X|O|O| | | |O| | |X|X| |X|O|O| | |O|O| | |X|X| |X|O|O| |X|O|O| | |X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | |X| | |O|X| | | |O| |O| |X| | |O|X| |X| |O| |O| |X| | |O|X| |X| |O| |O|O|X| |X|O|X| |X| |O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| |O| |X| |X| | |O|O| |X| |O| |X|O|X| | |O|O| |X| |O| 'X' Won! |X|O|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | | |X| |O| |O| | |O|X| | | |X| |O| |O| |X|O|X| | |O|X| |O| |O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X| |O| |X| |X|O|O| | | |X| |O| |X| |X|O|O| |O| |X| |O| |X| 'X' Won! |X|O|O| |O|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X| | | |O|O| | |O|X| | |X| | | |O|O| |X|O|X| | |X|O| | |O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| | |O| | | |X| | |O| |X| | |O| | | |X|O| |O| |X| | |O| | | |X|O| |O|X|X| | |O| | |O|X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X|X| 'O' Won! |O|O|O| | |X| | | |X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| |X| |O| |O| | | |X|O|X| |X| |O| |O| | | |X|O|X| |X|X|O| |O| | | |X|O|X| |X|X|O| |O|O| | 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| | | |O| | | |X| |O| |X| |X| |O| |O| |X| |O| |X| |X| |O| 'X' Won! |O| |X| |O|X|X| |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X| | | |X|X| | |O|O| |O|X| | | |X|X| 'X' Won! |X|O|O| |O|X| | | |X|X| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | |O| | | |O|X|X| |X| | | |O| | | |O|X|X| |X| |O| |O| | | |O|X|X| |X| |O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | 'X' Won! |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | |O| | | |X| |O| |X| |X| |O| | | |X| |O| |X| |X| |O| | |O|X| |O|X|X| |X| |O| | |O|X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| |O|X| | | |O|X| | |X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O| |X| |O|O| | |X|X| | |O| |X| |O|O|X| |X|X| | |O| |X| |O|O|X| |X|X| | |O|O|X| 'X' Won! |O|O|X| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| | | | | |X| | |O|O| |X| | | | |X|X| | |O|O| |X|O| | | |X|X| | |O|O| |X|O| | | |X|X| |X|O|O| |X|O|O| | |X|X| |X|O|O| 'X' Won! |X|O|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X|O| |X| | | |X|O| | | |X|O| |X| | | |X|O| | | |X|O| |X|O| | 'X' Won! |X|O| | | |X|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | |O|X| |O|X|O| | | |X| | |O|X| |O|X|O| | |O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X| |O| | | |O| | |O|X| |X| |O| | | |O| |X|O|X| |X|O|O| | | |O| |X|O|X| 'X' Won! |X|O|O| |X| |O| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| | |X|O| | |O|X|X| |X|O| | |X|O| | 'O' Won! |O|X|X| |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| |X| | | | |O| | |O| |X| |X| | | | |O| | |O|O|X| |X| | | |X|O| | |O|O|X| 'O' Won! |X| |O| |X|O| | |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | |O|X|O| | | | | 'X' Won! | |X| | |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X|O|O| | | |X| | | | | |X|O|O| | |X|X| | | |O| |X|O|O| | |X|X| |X| |O| |X|O|O| | |X|X| 'O' Won! |X| |O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | | | |X| | | | |X|O| | |O| | 'X' Won! |X| | | | |X|O| | |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| |O| |X| | | | | | |X|O| |O| |X| | | | | |X|X|O| 'O' Won! |O| |X| | |O| | |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | |O| | | |X|X| |X| |O| | |O| | |O|X|X| |X| |O| | |O| | |O|X|X| |X|X|O| | |O| | |O|X|X| |X|X|O| | |O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| |X| |X| | |O| | | |O|O| |X| |X| | |O| | 'X' Won! | |O|O| |X|X|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| | | |O| |O| |X|X| | |O| |X| |O| |O| |X|X| | |O|O|X| |O| |O| |X|X| | |O|O|X| |O|X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| |O| | | | | |X| |X| |O|X|O| | | | | |X| |X| |O|X|O| |O| | | 'X' Won! |X|X|X| |O|X|O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X| | | |X|O| |O| | | |O|X|X| | |X|O| |O|O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| |X| | | |X| | | |O| |O|O|X| |X| |X| | | |O| |O|O|X| |X| |X| |O| |O| |O|O|X| 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| |O| | | 'X' Won! | |O| | |X|X|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| |X| | | | | | |O|O| |X|O|X| | | |X| | |O|O| |X|O|X| 'O' Won! | |O|X| | |O|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| | | |X| |X| |O| |O| |X| | | |X|O|X| |O| |O| |X|X| | |X|O|X| |O| |O| 'O' Won! |X|X| | |X|O|X| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O| | | |X| |O|X|O| | | |O| | | |X| |O|X|O| | | |O| | |X|X| |O|X|O| | | |O| |O|X|X| |O|X|O| |X| |O| |O|X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O| |X| |X| |O| | | |X| |O|O|X| |X| |O| | | |X| |O|O|X| |X|X|O| | | |X| |O|O|X| |X|X|O| | |O|X| 'X' Won! |O|O|X| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X|O|O| | |X| | | | |X| |X|O|O| | |X|O| | | |X| |X|O|O| |X|X|O| | | |X| |X|O|O| |X|X|O| | |O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | | |X| | |X| |O| |O|X|O| | |X|X| |X| |O| |O|X|O| |O|X|X| |X| |O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | |X|X| | |O| |O| | | | | |X|X| | |O| |O| | | |O| |X|X| | |O| |O| |X| |O| |X|X| | |O| |O| |X|O|O| 'X' Won! |X|X|X| |O| |O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| | | |X|O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | |O|O| | | |O| |X| |X| |X|O|O| | |O|O| |X| |X| |X|O|O| 'X' Won! | |O|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | |X|O|O| | |X|O| | | | | |X|O|O| |X|X|O| | | | | |X|O|O| |X|X|O| | |O| | |X|O|O| |X|X|O| | |O|X| |X|O|O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| | | |O|O|X| |O| | | |X| |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| |O| 'X' Won! |X|X|X| |O|O|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X|X| 'O' Won! |O| |X| |O| | | |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|O| | | | | |O|X|X| |X|O|O| | | | | 'O' Won! |O|X|X| |X|O|O| | | |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X|O| |X| | | |O|X| | 'X' Won! |O|X|O| |X|X| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | |O|X| |O|X| | | | | | |X|O|X| |O|X| | | | | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X| | |O|X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | |O| |X|X| | | | |O| |X| |O| |X|X| | | | |O| 'O' Won! |X| |O| |X|X|O| | | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | | |X| |X|O| | |O| | | |X| |X| |X|O| | |O| | | |X| |X| |X|O| | |O| |O| |X| |X| |X|O|X| |O| |O| |X|O|X| |X|O|X| |O| |O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | | |O| | |X| | |O|X| | |O| |O| | |X| | |O|X| | |O| |O| |X|X| | |O|X| | |O| |O| |X|X|O| |O|X| | |O| |O| |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |O| |O|X|X| | | | | | | |O| 'O' Won! |O|X|X| | |O| | | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | | | | |X|X| 'O' Won! |O|O|O| | | | | | |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| | |O| | |X| | | |O| |X| | |O| | |X|O| | |O| |X| | |O|X| |X|O| | |O| |X| 'O' Won! | |O|X| |X|O| | |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | |X|O| |O| |X| |X| | | |O|X|O| |O| |X| |X|X| | |O|X|O| |O|O|X| |X|X| | |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | |O| |X| | | |X| |O|O| | |O| |X| |X| |X| 'O' Won! |O|O|O| |O| |X| |X| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| |O| |X| 'X' Won! | |O|X| |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O|O| |X| | | | |X| | | |O|O| |X|X| | | |X| | 'O' Won! |O|O|O| |X|X| | | |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | |O|X| | | |O|X| | | | | |O|X|X| | |O|X| | | | | |O|X|X| |O|O|X| 'X' Won! | | |X| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | |X| |X| | |O|O| | | | | |X| |X| | |O|O| | |O| | |X| |X| |X|O|O| | |O| | |X| |X| |X|O|O| | |O|O| 'X' Won! |X| |X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O| |O| |X| |O| | |X| | |O| |O| |X|X|O| |O|X| | |O| |O| |X|X|O| |O|X|X| |O| |O| |X|X|O| 'O' Won! |O|X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | |O| | |X|O| |X| | | | | |O| |X|X|O| |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X|X| | 'O' Won! |O|O|O| |X|X|O| |X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | | |O| |X| |X| |O|O|X| | | |O| |X|O|X| |O|O|X| | | |O| |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X|O| | |X|O| |X|O| | | |X|O| 'O' Won! | |X|O| |X|O|O| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X| |O| | | | | |X|O|O| |X|X|O| | | | | |X|O|O| |X|X|O| | |O| | 'X' Won! |X|O|O| |X|X|O| |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| | |O| | 'O' Won! | |O|X| |X|O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| |O| | |O| | | | |X| |X|X|O| | |O| | | | |X| |X|X|O| |O|O| | | | |X| |X|X|O| |O|O| | |X| |X| 'O' Won! |X|X|O| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X|X| |X| | | | | |O| |O|X|X| |X| | | | |O|O| |O|X|X| |X| |X| | |O|O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | |O| |X| | | 'X' Won! |X| |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |O|X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | | |X|X| |O|X| | |O| |O| | |X|X| |O|X| | 'X' Won! |O|X|O| | |X|X| |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | | | |O| |O| | |X|X| | |X| | |O| |O| 'O' Won! | |X|X| | |X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| |X|O|O| |O| |X| |X| |X| |X|O|O| |O|O|X| |X| |X| 'X' Won! |X|O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | |O| |O| |X| |X|X| | | | |O| |O| |X| |X|X|O| | | |O| 'X' Won! |O| |X| |X|X|O| |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | | |O|X| | | |O| |X| | | | |O|X| | |X|O| |X|O| | | |O|X| | |X|O| |X|O| | | |O|X| |X|X|O| |X|O|O| | |O|X| |X|X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O|X| | |X|O| |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | |O| | | |X|X| |X| |O| | |O| | | |X|X| |X|O|O| | |O| | | |X|X| |X|O|O| | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X|X| | | |O| |X| |O| |O|X|X| | | |O| |X|X|O| |O|X|X| | |O|O| |X|X|O| |O|X|X| 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X| |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |X|X| |O|X|O| | |O|X| |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | | |O| | |O|X| | |X| | | |X|O| | |O|X| | |X|O| | |X|O| |X|O|X| | |X|O| | |X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X|O|X| |X| | | |O|X|O| |X|O|X| |X| | | 'O' Won! |O|X|O| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| |O| | |X|X| |X|O| | |O| |O| 'O' Won! |O|X|X| |X|O| | |O| |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | |O| | |X|O| | |X|X| | | |O| | |X|O| | |X|X| | | |O| |O|X|O| 'X' Won! | |X|X| | |X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | |X| |O| |X| | | | |O|O| |X| |O| |X| |X| | |O|O| |X| |O| 'O' Won! |X| |X| |O|O|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | |O| | |O| | |X| |X| | |X|O| | |O| | |X| |X| | |X|O| | |O|O| |X| |X| | |X|O| |X|O|O| |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| |O|X| | | | |O| |X|O|X| |O|X| | | | |O| |X|O|X| |O|X| | |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | |X| |X| | | |O| | |O| | |X| |X| | | |O| | |O|O| 'X' Won! |X|X|X| | | |O| | |O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X|X| | | | |O| |O| | | |X|X| | | | |O| |O| |X| |X|X| | | |O|O| |O| |X| |X|X| | 'X' Won! | |O|O| |O| |X| |X|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O|O| | | | | |X| |X| |X|O|O| | | | | |X|O|X| |X|O|O| | | | | |X|O|X| |X|O|O| | | |X| |X|O|X| |X|O|O| |O| |X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| |X|O| | | | | | |O| |X| |X|O| | |O| | | |O| |X| |X|O| | |O| | | |O|X|X| |X|O|O| |O| | | |O|X|X| 'X' Won! |X|O|O| |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | | | |X|O|X| | | |O| | | |O| |X|O|X| | | |O| | | |O| |X|O|X| |X| |O| | |O|O| |X|O|X| |X| |O| 'X' Won! |X|O|O| |X|O|X| |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| | |X| | | |O| | |O|X|X| | |X| | |O|O| | |O|X|X| | |X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | | |O|O| |X| |X| | | | | |X|O|O| |X| |X| |O| | | |X|O|O| |X| |X| |O|X| | |X|O|O| |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | |X| | |X| | |O|O| | | | |X| | |X|O| |O|O|X| | | |X| | |X|O| 'O' Won! |O|O|X| | |O|X| | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| |O| | | | | |X| |X|O|X| |O| | | | | |X| |X|O|X| |O| |O| | | |X| |X|O|X| |O|X|O| | |O|X| |X|O|X| |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | |O| | 'X' Won! |X| | | |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| | |O|O|X| | | | | |X|X| | |O|O|X| | | | | |X|X| | |O|O|X| | |O| | 'X' Won! |X|X|X| |O|O|X| | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | | |O|O| | |X|X| |O| | | | |O|O| | |X|X| |O|X| | | |O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| |O|X| | It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X| | | |O|O| 'X' Won! | |O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O|X| | |X| |X| |O| |O| |O|X| | |X|O|X| |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O|X| | |O|O| |X| |X| | |O|X| 'X' Won! | |O|O| |X|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O| | |O| |X| |X| | | |X|O| | |O| |X| |X| | | |X|O| | |O|O|X| |X| |X| |X|O| | |O|O|X| |X| |X| |X|O|O| |O|O|X| 'X' Won! |X|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | | | 'O' Won! |O| |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| | |O|X| |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X|O| | |X| | | | |O| |O|X|O| |X|X| | |O| |O| |O|X|O| |X|X| | 'X' Won! |O|X|O| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| |O|X| | | |O| | | |X|X| |O|X| | | |O|O| | |X|X| |O|X| | | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O|O|X| | | |X| | | |O| |O|O|X| |X| |X| | | |O| |O|O|X| |X| |X| | |O|O| 'X' Won! |O|O|X| |X|X|X| | |O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O| | |X|X| |O| | | |X| |O| | |X|X| |O|O| | |X|X|O| | |X|X| |O|O| | 'O' Won! |X|X|O| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|O| |X| | | | |X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| 'X' Won! |X| |X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X|X| |O| | | |O|O| | | |X|X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O| |O| |O|O|X| | |X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | |X|O|X| | | | | | |O| | 'O' Won! |X|O|X| | |O| | | |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | |X| | | | |O| |X|O| | | |X| | |O| |O| |X|O| | | |X| | |O|X|O| |X|O| | | |X|O| |O|X|O| |X|O| | |X|X|O| |O|X|O| |X|O| | 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| | | |X| | 'O' Won! |O| | | |X|O| | | |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | |O|O| | | |X|O| | |X| | |O|O|X| | |X|O| |O|X| | |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X| | | |O|X| |O|O|X| | |X| | | |O|X| 'X' Won! |O|O|X| | |X|X| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| |X| | |O| | |X|O| | |O| |X| | |O| | |X|O| | |O|X|X| | |O| | |X|O|O| |O|X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O|O|X| |X| |X| |O| | | |O|O|X| |X| |X| |O| |O| |O|O|X| 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| |X| |O| | | |X| |O| |O| |X| |O| |X| |X| |O| |O| |X| |O|O|X| |X| |O| |O| |X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | |O| | |X|X| | | |O|X| | |O| | |X|X| | |O|O|X| | |O| | |X|X| | 'X' Won! |O|O|X| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |O| |O| | |X|X| |X|O| | |O| |O| 'O' Won! |O|X|X| |X|O| | |O| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X| | |X|X| | | | |O| |O|X| | |X|X|O| | | |O| |O|X|X| |X|X|O| |O| |O| |O|X|X| |X|X|O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | | | | | |O|X|O| |O|X|X| | | | | |O|X|O| 'O' Won! |O|X|X| | |O| | |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | |X|X| | | |O| |O| | | | |X|X| | | |O| |O| |O| | |X|X| | |X|O| |O| |O| 'O' Won! | |X|X| | |X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| | | |X|X| |X| | | |O|O| | | |X|X| |X|O| | 'X' Won! |O|O| | |X|X|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O|X| | | |O| | |X| | |X|O|X| |O| |O| | |X| | |X|O|X| |O| |O| | |X|X| 'O' Won! |X|O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O|X|O| | |X| | | | |O| |O|X|O| | |X| | | | |O| |O|X|O| | |X|X| |O| |O| |O|X|O| | |X|X| 'X' Won! |O| |O| |O|X|O| |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| | | | | |O| |O|X|O| |X|X| | | | |O| 'O' Won! |O|X|O| |X|X|O| | | |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O|X| | | |O| | |X|X| | |O|X|O| | |O| | |X|X| | 'X' Won! |O|X|O| | |O| | |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | |X| | | | |X| | |O|O| |O|X| | | | |X| | |O|O| |O|X|X| | | |X| | |O|O| |O|X|X| | |O|X| 'X' Won! |X|O|O| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O|O| | | |X| | | | | |X|O|O| | |X|X| | | | | |X|O|O| |O|X|X| | |X| | |X|O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| | |X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O|O| |O| | | |X|X| | |X|O|O| |O| | | |X|X| | |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | |O|X| | |X| | |O|O|X| |X|O|X| | |X| | 'O' Won! |O|O|X| |X|O|X| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | |X|X| | | | | 'O' Won! |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | | |X| |X|O| | | |O|O| | | |X| |X|O| | |X|O|O| | | |X| |X|O| | |X|O|O| |O| |X| |X|O| | |X|O|O| |O|X|X| |X|O| | It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| |X| |X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | | |X| | | |X| |O|X|O| |O| |X| | | |X| |O|X|O| |O|X|X| | | |X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | |O|X| | |O|O| |X| |X| | |O|X| 'O' Won! | |O|O| |X|O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| | | |O| |O| | |X|O| |X| | | |O|X|O| | |X|O| 'O' Won! |X| |O| |O|X|O| | |X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | | |O|X|O| |O|X| | 'X' Won! | |X| | |O|X|O| |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| |O|X| | | |O|X| | |O|X| |O|X| | | |O|X| | |O|X| |O|X|O| | |O|X| |X|O|X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | |X|O|X| | |O| | 'O' Won! | |O| | |X|O|X| | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X|O| | | | | |X|O| | | |X|O| |X| | | |X|O| | | |X|O| |X|O| | |X|O| | 'X' Won! |X|X|O| |X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | |O| |O| | | |X|X| | 'X' Won! | | |O| |O| | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X| |O| |O|O| | |X| | | |X|X|O| |O|O| | |X| |O| |X|X|O| |O|O| | 'X' Won! |X| |O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | | |O| | |X| |X| |O|X| | | |O| | |X| |X| |O|X| | |O|O| | 'X' Won! |X|X|X| |O|X| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O| | |O|X|O| | |X|X| | |O| | |O|X|O| | |X|X| | |O|O| |O|X|O| | |X|X| |X|O|O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | | | |X| | 'O' Won! |O|X| | |O| | | |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | |X|O|X| |O| |O| | |X| | |X|O|X| |O| |O| | |X|X| |X|O|X| |O| |O| |O|X|X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | | | | | |X|X|O| 'O' Won! |O|X| | | |O| | |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | |X|O| |X| | | | | |O| |O|X|O| |X| | | | |X|O| |O|X|O| |X| | | |O|X|O| |O|X|O| |X| | | |O|X|O| |O|X|O| |X| |X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | |X| | | | | | |X|O|O| | |X| | | | |O| |X|O|O| | |X| | | |X|O| |X|O|O| |O|X| | | |X|O| |X|O|O| 'X' Won! |O|X|X| | |X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| |O|X| | |X| | | | |O|O| |O|X|X| 'O' Won! |X| |O| | |O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| | |O|X| |X|O| | | | |X| | |O|X| |X|O| | |O| |X| | |O|X| |X|O| | |O|X|X| |O|O|X| |X|O| | |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| | | | | |X| |X|O|X| |O|O| | | | |X| 'X' Won! |X|O|X| |O|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | | |O|X| | | |X| |O|O| | |X|O|X| | | |X| |O|O| | 'O' Won! |X|O|X| | |O|X| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O|O| 'X' Won! | | |X| | |X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| |O| |X|X| | | |O| | |O|X|O| |X|X| | | |O| | |O|X|O| |X|X|O| | |O| | |O|X|O| |X|X|O| |X|O| | 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O|X| |O| | | | | |X| |O|O|X| |O| | | | | |X| |O|O|X| |O|X| | | | |X| 'O' Won! |O|O|X| |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | |X|X| | | |O| |O| |O| | |X|X| | | |O| |O|X|O| | |X|X| | |O|O| |O|X|O| | |X|X| 'X' Won! |X|O|O| |O|X|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X|O| | |X| | | |O| |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O|X| 'O' Won! |X|O| | |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X|O| | |X| |O| | |O| | |X|O| | |X|X|O| | |O| | |X|O|O| |X|X|O| | |O| | 'X' Won! |X|O|O| |X|X|O| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| |O| | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O|O| | | | |X| | |X|O| |O|O|X| |O| |X| | |X|O| |O|O|X| |O|X|X| | |X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X| | | |X|X| | |O| |O| |X|O| | |X|X| | 'X' Won! |O| |O| |X|O| | |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| | | | | |X|O| | |O| |X| | | | | |X|O| | |O|O|X| | | | | |X|O| | |O|O|X| | |X| | |X|O| | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | | |O| | |X|X|O| |O|X| | | |O| | |X|X|O| |O|X|O| | |O| | 'X' Won! |X|X|O| |O|X|O| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| | | | | |O|X| | |X|X|O| | | | | |O|X| | |X|X|O| | |O| | |O|X|X| |X|X|O| | |O| | |O|X|X| |X|X|O| |O|O| | It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| | | |O| |X|O| | | |O|X| |X| |O| |X|O| | |O|O|X| |X| |O| |X|O| | |O|O|X| |X| |O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X| |O| | |X| | |O|O| | |X| |O| |X|X| | |O|O| | 'O' Won! |X| |O| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | |O| | | |X|X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | |X| |X| |O| | | |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| |X| 'O' Won! |X| |X| |O|O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O| |X| | | |O| | |X|O| |O| |X| | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O|O|X| | |X|O| |X|X|O| |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | |O| | |O|X|X| |X| | | | |O| | |O|X|X| |X| | | | |O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X|O| | |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |X| |O| | |X| | |X|O|O| |X| |O| | |X|X| |X|O|O| |X| |O| 'O' Won! |O|X|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | | | |X| |O|O|X| |X| | | |O| |X| 'X' Won! |O|O|X| |X| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | |O|X| |X| | | |O| |O| |X|O|X| |X| | | |O| |O| |X|O|X| |X| |O| |O| |O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | |O| | |X| |X| |O| |O| 'X' Won! | |O| | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | | | |X| |O| | |X|O| | |X| | |X| |O| | |X|O| |O|X| | |X| |O| | |X|O| |O|X|X| |X| |O| | |X|O| 'O' Won! |O|X|X| |X|O|O| | |X|O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| |O| | | |O| |O| |X| |X| |O| |X| |O| |O| |X| 'O' Won! |X| |O| |X|O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | |O| | | | |X| |X| |O| | |O|X| | | |X| |X| |O| | |O|X| | |O|X| |X| |O| |X|O|X| | |O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| |O| |X| |O|X| | | |X|O| |O| |X| |O|X| | 'O' Won! | |X|O| |O|O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O|O| | |X| |X| | |O| | |O|O| | |X| |X| |X|O| | |O|O| | |X| |X| |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | | |X| |O|O| | 'X' Won! |O| |X| | | |X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | |O| | | |X|X| | |O|O| | |O| |X| |X|X| | |O|O| | 'O' Won! |O| |X| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | |O|X| |X| | | |O|X| | | |O|X| |X| | | |O|X| | | |O|X| |X|O| | |O|X| | |X|O|X| |X|O| | 'O' Won! |O|X| | |X|O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | | | | |X| | | |O|X|O| | | | | |X|O| | |O|X|O| | | | | |X|O|X| |O|X|O| | |O| | |X|O|X| |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| |X| | | | | |O|O| | |X| |X| | | | | |O|O|X| |X| |X| |O| | | |O|O|X| |X| |X| 'X' Won! |O| | | |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| |X| | | | |O| | |O| |X| |X| | | |O|O| | |O| |X| |X| |X| |O|O| | |O| |X| |X|O|X| |O|O| | |O| |X| 'X' Won! |X|O|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X| |X| | |O| | |O|X|O| 'X' Won! |X|X|X| | |O| | |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O|X| |X| |O| |O| | | | |O|X| 'X' Won! |X| |O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| | | |X|O| |O|X| | | |O| | | |X|O| |O|X| | | |O|X| | |X|O| |O|X| | |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | |X| |X|O| | | |O| | 'O' Won! | |O|X| |X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X|O| | | | |O| | | |X| |X|O| | | | |O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O|X| | |O|O| | |X|X| |X|O|X| | |O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| | |O|O| 'X' Won! |X| | | |X| |O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| | |X|O| |X| |X| | | |O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X|O|X| 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | |X|O| |O| | | | | |X| |O|X|O| |O| | | | | |X| |O|X|O| |O|X| | | | |X| |O|X|O| |O|X|O| | | |X| |O|X|O| |O|X|O| |X| |X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| | |X| |O| | |X|X| |O|O| | |X| |O| | |X|X| 'X' Won! |O|O| | |X| |O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| |O| |O| | | | | |X|X|O| |O| |O| | |X| | |X|X|O| |O| |O| |O|X| | 'X' Won! |X|X|O| |O|X|O| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | |O| | |X|O| | | | |X| | |O|X| |X|O| | | | |X| |O|O|X| |X|O| | | | |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O|O| |X| |X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | | | |O|X| | | |O|X| | |X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X|X| |O|X| | 'O' Won! |O|O|X| |O|X|X| |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| | | |O| |X| |X| | | |O| | |X|O| |X| |X| | |O|O| | |X|O| 'X' Won! |X|X|X| | |O|O| | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X|O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | |X|O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| | |X|O| 'X' Won! |X|O|X| |O|X|O| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X| |O| |X| | | |O|O| | |X| |O| |X| |X| |O|O| | |X| |O| |X|O|X| |O|O|X| |X| |O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| |O| |X| |X| | | |O| |O| |O| |X| |X| |X| |O| |O| |O|O|X| |X| |X| |O|X|O| |O|O|X| |X| |X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O|X| | | |X|O| |O| |X| |O|X| | | |X|O| |O| |X| |O|X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X| | |X|X| |O| |O| | |O|X| | |X|X| 'X' Won! |O| |O| | |O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |O| | |O| | |X|X|O| | |X|O| | |O| | 'O' Won! |X|X|O| | |X|O| | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | | |O|X| | | |O| |X| |X| | |O|X| | | |O| |X|O|X| | |O|X| | |X|O| |X|O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X|X| | |O| | | |O|O| | |X|X| | |O| | | 'X' Won! |O|O| | |X|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O| |X|X| | | |X| | | |O|O| |X|X| | | |X|O| |X|O|O| |X|X| | | |X|O| 'O' Won! |X|O|O| |X|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O|O| | |X| | | |X|O| | |O|O| | |X|X| | |X|O| | 'O' Won! |O|O|O| |X|X| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| |X| | | |X| |O| |O| 'O' Won! |X| |X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | 'X' Won! |O|X|O| | |X|O| |X|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X|O| |X|O| | | | |O| |X|X|O| |X|O| | |O| |O| |X|X|O| |X|O| | |O|X|O| |X|X|O| 'O' Won! |X|O|O| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| |X|X|O| | |X|X| 'O' Won! |O|O|O| |X|X|O| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | 'X' Won! | |X| | |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X|X| | |O|O| | | | |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X|O|O| |X|X| | |O|O| | 'X' Won! |X|O|O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| |X| | |O| | |X|X|O| |O| |X| | |O|X| |X|X|O| |O| |X| | |O|X| |X|X|O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O|X| |X| | | |O| | | | |O|X| |X| | | |O|O| | |X|O|X| |X| | | |O|O| | 'O' Won! |X|O|X| |X|O| | |O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O|O| | | | | |O|X|X| | |O|O| | | |X| |O|X|X| 'O' Won! | |O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |X| |O|O|X| | | | | | | |X| 'X' Won! |O|O|X| | | |X| | | |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| |X| | | |O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| |X| |X| |O| |X| | |O|O| |X|O|X| |O|X|X| | |O|O| |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X|O| | |X|X| |O| | | | |X|O| | |X|X| |O| | | |O|X|O| | |X|X| |O| |X| |O|X|O| | |X|X| |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | |O|O| |X| | | |X| | | 'X' Won! |X|O|O| |X| | | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| | | | |O|X| |O|O| | |X|X| | | |O|X| |O|O| | |X|X|O| | |O|X| |O|O| | |X|X|O| |X|O|X| 'O' Won! |O|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| |X| | | | |X| | |O|X|O| |X| | | |O|X| | |O|X|O| |X| |X| |O|X| | 'O' Won! |O|X|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | |X| |O| | |X|X| | |O| | |X| |O| |O|X|X| | |O| | |X| |O| |O|X|X| |X|O| | |X|O|O| |O|X|X| |X|O| | 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | | | | | 'X' Won! |O|X|O| | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| |X|X| | | |O|O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| | |O|O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |X| |O|O| | |X| |X| |O| |X| 'X' Won! |O|O| | |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | |O| | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| |X|O| | |X|O|X| |O| |O| |X|O| | |X|O|X| |O|X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| | | | | |O| |X|O|X| |O| | | | |X|O| |X|O|X| |O| | | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O|O|X| | | |O| | |X| | |O|O|X| | | |O| |X|X| | |O|O|X| | | |O| |X|X|O| |O|O|X| |X| |O| |X|X|O| 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| |X| |O| | | |X|X| | |O| |X| |O| |O| |X|X| | |O|X|X| |O| |O| 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | | |O|O| |X| | | |O|X| | |X|O|O| |X| | | 'O' Won! |O|X| | |X|O|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | |X|O| |X| | | | |O|O| | |X|O| |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X|O| | 'X' Won! |X|O|O| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | 'O' Won! |X|O| | |X|O| | | |O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| |X| | | |X|O|X| | | |O| |X| | | |X|O|X| | |O|O| 'X' Won! |X| | | |X|O|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | | |X|O| | |O| | |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X|O| | |O|X|O| | |O|X| |X|O|X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|X| | |O| | |X| | | |O|X|X| | |O|O| |X| | | |O|X|X| |X|O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| |X|O| | | | |O| | |X|X| |X|O| | | | |O| |O|X|X| 'X' Won! |X|O| | | |X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | |O|X|X| | |X| | | |O| | |O|X|X| 'O' Won! | |X|O| | |O| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| |O| | | |X|O| | 'X' Won! | | |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | |O| |X| | |O|X| | | | | |O|X|X| | |O|X| | | | | |O|X|X| |O|O|X| 'X' Won! |X| | | |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X| | |O|X|O| | | |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O| | | |X| |X| | |X|O| |O| |O| |X| |X| 'X' Won! | |X|O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | |O|X| | | | | |O|X| | | |O|X| | | | | |O|X|X| | |O|X| | |O| | |O|X|X| | |O|X| |X|O| | |O|X|X| |O|O|X| |X|O| | |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| |O| | | | | | |X|X| |O| |O| | | |X| 'O' Won! | |X|X| |O|O|O| | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| |X| | |O|X| |X|O| | |O| |X| | |O|X| |X|O| | |O|X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O| | |X| |O| |X| |X| | |O| | |X|O|O| 'X' Won! |X|X|X| | |O| | |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| |O| 'X' Won! |O| |X| |O|X| | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | |O| |X| |O|X| | | | | | |O|X|X| |O|X| | | | | | |O|X|X| |O|X| | | |O| | |O|X|X| |O|X|X| | |O| | |O|X|X| |O|X|X| | |O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | | | | |O|X|O| | |X|X| |O| | | |O|X|O| | |X|X| |O| |X| |O|X|O| | |X|X| |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | 'O' Won! |O| | | |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| |O| |X| |X| | | | | |O|X|O| |X| |X| |O| | | |O|X|O| |X| |X| 'X' Won! |O| |X| |O|X|O| |X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | |O| |O| |O|O|X| 'X' Won! |X|X|X| |O| |O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O| |O| |X| |X| |O| |X| |O| |O| |X| |X| |O|O|X| |O| |O| |X| |X| 'X' Won! |O|O|X| |O|X|O| |X| |X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| 'O' Won! |O|X| | | |O|X| | | |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O|O|X| | | | | |X| |X| |O|O|X| | | | | |X| |X| |O|O|X| | | |O| |X| |X| |O|O|X| |X| |O| |X| |X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| | | |X| |O| | |O|O| 'X' Won! |X| | | |X| |O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X|X| |O| |O| | |X| | | |X|X| |O| |O| |O|X| | 'X' Won! |X|X|X| |O| |O| |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| |X| 'O' Won! | | | | |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| |O| |X| | | | | | |X|O| |O| |X| |X| | | | |X|O| |O| |X| |X|O| | |X|X|O| |O| |X| |X|O| | |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | |O|X| |X| | | | | |O| | |O|X| |X|X| | | | |O| | |O|X| |X|X| | | |O|O| 'X' Won! | |O|X| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| |O| |X| |O|X| | | | |O| |O|X|X| |O|X| | | | |O| |O|X|X| |O|X|O| 'X' Won! | |X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | 'X' Won! | |X| | |O|X| | |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| |X| | | | |O|O| | |X|O| |X| | | | |O|O| | |X|O| |X| |X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'O' Won! |O|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | |O| |X| | | |O|X| | |X| |O| |X| | | |O|X| | |X| |O| |X| |O| |O|X| | |X|X|O| |X| |O| |O|X| | 'O' Won! |X|X|O| |X| |O| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | |X|X| | | | |O| |O|X| | |X|X|O| | | |O| |O|X| | |X|X|O| | | |O| |O|X|X| |X|X|O| |O| |O| |O|X|X| 'X' Won! |X|X|O| |O|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | |X| | | | | | |X|O|O| | |X| | | |O| | |X|O|O| | |X| | |X|O| | |X|O|O| |O|X| | |X|O| | 'X' Won! |X|O|O| |O|X| | |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | |O| |X| | |O|X| | |O| | |O| |X| |X|O|X| |O|O| | |O| |X| |X|O|X| 'X' Won! |O|O|X| |O| |X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | | | | |O|X|O| | |O|X| | | | | |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | |X| | | | |X| | |O| | | |X| | | | |X| | |O|O| | |X| |X| | |X| | |O|O| | |X|O|X| | |X| | |O|O| | |X|O|X| | |X|X| |O|O| | 'O' Won! |X|O|X| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | | |O|X| | | |X|O| |X| | | |O|X| | | |X|O| |X| | | |O|X|O| 'X' Won! | |X|O| |X|X| | |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | | |X| | |O|X|X| |O| | | | |X| | |O|X|X| |O| |O| | |X|X| |O|X|X| |O| |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| |O|X| | | | |O| 'X' Won! | | |X| |O|X| | |X| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | |X| | |O| |X| |O|O| | | |X| | |O| |X| |O|O| | |X|X| | |O|O|X| |O|O| | |X|X| | 'X' Won! |O|O|X| |O|O| | |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X| | | |O| | |X| |X| |O|X| | | |O|O| |X| |X| |O|X| | |X|O|O| |X| |X| |O|X| | |X|O|O| |X|O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X|X| | |O| | | | |O|O| |X|X| | |O| | | |X|O|O| |X|X| | 'O' Won! |O| | | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| | | |O|O| | | |O|X| |X| | | |O|O|X| 'O' Won! | |O|X| |X|O| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | |X|X| | |O| |O| | | | | |X|X| | 'X' Won! |O| |O| | | | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| | | |X| |O| |O|X| | |O| | | |X| |O| |O|X| | |O|X| | |X| |O| |O|X|O| |O|X| | |X| |O| 'X' Won! |O|X|O| |O|X| | |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| |O|O| | | |X| | | |X|X| |O|O| | | |X| | | |X|X| |O|O| | | |X|O| | |X|X| |O|O| | |X|X|O| 'O' Won! |O|X|X| |O|O| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O| | |O| |O| |X|X| | | |O| | |O|X|O| |X|X| | |O|O| | |O|X|O| 'X' Won! |X|X|X| |O|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| |O| |X| |O|X| | | | |O| |O|X|X| |O|X| | | |O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| |O|X| | It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| | | | |O|X| |X| |X| |O|O| | | |O|X| |X| |X| |O|O| | |X|O|X| |X| |X| 'O' Won! |O|O| | |X|O|X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O| | | |X|O| | |O|X| | |O| |X| |X|O| | |O|X|O| |O| |X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X|O| | |X| | |O| | | |O|X|O| |X|X| | |O| |O| |O|X|O| |X|X| | 'X' Won! |O| |O| |O|X|O| |X|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | |X| | |O|O| |X|X| | |O| |X| | |O|O| |X|X| | |O| |X| |X|O|O| |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | |X| | |O| |O| |X| |X| | |X|O| |O| |O| |X| |X| |X|X|O| |O| |O| |X| |X| 'O' Won! |X|X|O| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | |O|X| | | | | |X| |O| |X|O|X| | | | | |X| |O| |X|O|X| | |O| | |X| |O| 'X' Won! |X|O|X| |X|O| | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| |X| | | |X| |X|O| | |O|O|X| | |X|X| |X|O| | |O|O|X| | |X|X| |X|O|O| |O|O|X| 'X' Won! |X|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| |O| | | |O|O|X| 'X' Won! | | |X| |O| |X| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| | | | | | 'O' Won! |O|X| | |X|O| | | | |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | |O| | |O| | | 'X' Won! |X|X|X| | |O| | |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | | | |O| |X|O|O| |X| | | | |X|O| |X|O|O| |X|O| | | |X|O| |X|O|O| |X|O|X| | |X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | 'O' Won! | |X| | |O|O|O| | |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| |X|O| | 'O' Won! |O|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| | | | | | |X|O| |O| |X| | | | | | |X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | | | 'X' Won! |O|X|O| |O|X|X| | |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | |O|X| | |O| |X| | | | | |O|X| | |O| |X| |X| | | |O|X| | |O| |X| |X| |O| |O|X| | |O|X|X| |X| |O| |O|X| | |O|X|X| |X|O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | | |O|O| | 'X' Won! |X|X|X| | | | | |O|O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | |O| |X| |X| | | |O| | |X|O| |X| |X| | |O|O| | |X|O| |X| |X| | |O|O| |X|X|O| |X| |X| 'O' Won! |O|O|O| |X|X|O| |X| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | |X| |O|O| | |X|X| | | |O|X| |O|O| | |X|X| | |X|O|X| |O|O| | 'O' Won! |X|X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | |O| | |X| |X| | | |O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O| | |X|O|X| |X|X|O| 'O' Won! |O|O| | |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X| |O| | |X| | | |O| | |X| |O| |O|X| | | |O| | 'X' Won! |X| |O| |O|X| | | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |O| | |O| | | |X|X| | | |O| |O|O| | | |X|X| | | |O| |O|O| | | |X|X| | |X|O| |O|O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | | |O|O| |O|X|X| |X| | | | |O|O| 'O' Won! |O|X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | | | |O| |X|O|O| |X|X| | |O| |O| |X|O|O| |X|X| | |O|X|O| |X|O|O| |X|X| | 'O' Won! |O|X|O| |X|O|O| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X|O| |O| |X| | | |X| | |X|O| |O| |X| |O| |X| | |X|O| |O| |X| |O| |X| |X|X|O| 'O' Won! |O| |X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | | | | |X|X|O| | |X|O| |O| | | |X|X|O| |X|X|O| |O| | | |X|X|O| |X|X|O| |O|O| | |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| | | | |X|O| | |X| | |O| | | | |X|O| |O|X| | |O| | | | |X|O| |O|X| | |O| |X| 'O' Won! |O|X|O| |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | | | | |O| |X| 'X' Won! | |O|X| | | |X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X|O| | | |O| | |X|X| | |X|O| | | |O| | |X|X| | |X|O|O| 'X' Won! | |O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | |X| | |O| |O| |X|O|X| | |X| | |O|X|O| |X|O|X| | |X| | 'O' Won! |O|X|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | | |X|X| 'O' Won! |O| | | |O| | | |O|X|X| 1.1 Clean Data \u00b6 We will first need to organize the data into a parsable format. Q1 \u00b6 What is the object data and what does it contain? what are the keys of data? what are the keys of each game? # inspect data below by grabbing the first key in data # what are the three different keys within each game? data [ 'game 0' ] {'board': {1: 'X', 2: ' ', 3: 'O', 4: 'X', 5: 'X', 6: 'O', 7: 'O', 8: 'X', 9: 'O'}, 'starting player': 'X', 'winner': 'O'} Q2 \u00b6 Using those keys, iterate through every game in data and append the board, the winner, and the starting player to separate lists. Call these lists: boards, winners, and starters boards = [] winners = [] starters = [] for game in data : # YOUR CODE HERE Q3 \u00b6 Make a dataframe out of the list boards and call it df . Make a series out of the list winners . Make a series out of the list starters . Make a new column of df called \"Winner\" and set it equal to the pandas Series of the winners. Make a new column of df called \"Starter\" and set it equal to the pandas Series of the starters. # YOUR CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 Winner Starter 0 X O O O X O O 1 O O X O X X O X O O O 2 O X X X X O O X X 3 X X O O O X X O O Stalemate O 4 O O X O O X X X X O 1.2 Inferential Analysis \u00b6 We're going to use Bayes Rule or Bayesian Inference to make a probability of winning based on positions of the board. The formula is: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\). The example we will use is the following: what is the probability of 'O' being the winner, given that they've played the center piece. \\(B\\) = 'O' played the center piece \\(A\\) = 'O' won the game So what is probability? We will define it in terms of frequencies. So if we are for instance asking what is the probability of player 'O' being in the center piece, it would be defined as: P(B) = \\frac{|O_c|} {|O_c| + |X_c| + |empty|} Where the pipes, | | , or cardinality represent the count of the indicated observation or set. In this case \\(O_c\\) (O being in the center) and \\(X_c\\) (X being in the center). Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc_Xc_empty 1000 # example of assessing the probability of B, O playing the center piece player = 'O' Oc = ( df [ 5 ] == player ) . value_counts () Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc / Oc_Xc_empty False 0.577 True 0.423 Name: 5, dtype: float64 # we can also clean this up and replace the denominator with the whole # observation space (which is just the total number of games, df.shape[0]). # example of assesing probabiliy of A ( df [ 'Winner' ] == 'O' ) . value_counts () / df . shape [ 0 ] False 0.571 True 0.429 Name: Winner, dtype: float64 The \\(P(B|A) * P(A)\\) is the intersection of \\(B\\) and \\(A\\). The intersection is defined as the two events occuring together. Continuing with the example, the probablity of 'O' playing the center piece AND 'O' being the winner is the number of times these observations occured together divided by the whole observation space : # in this view, the total times A and B occured together is 247 player = 'O' df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] 247 # the total observation space is 1000 (1000 games) df . shape [ 0 ] 1000 And so we get: P(B|A) * P(A) = \\frac{247} {1000} = 0.247 In code: df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] / df . shape [ 0 ] 0.247 1.2.1 Behavioral Analysis of the Winner \u00b6 Q4 \u00b6 define the 3 different board piece types and label them middle , side , and corner . Middle should be an int and the other two should be lists. # define the 3 different board piece types # middle = # side = # corner = 1.2.1.1 What is the probability of winning after playing the middle piece? \u00b6 Q5: For player X \u00b6 # A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'X' # define the intersection of A AND B, A_B # A_B = # define prob B # B = # return A_B over B (The prob B given A) A_B / B 0.5732758620689655 Q6 For player O \u00b6 # A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'O' # define the intersection of A AND B, A_B # A_B = # define prob B # B = # return A_B over B (The prob B given A) A_B / B 0.5839243498817968 1.2.1.2 What is the probability of winning after playing a side piece? \u00b6 Q7 For player O \u00b6 # A intersect B: O played side and O won / tot games # B: O played side / tot games player = 'O' A_B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.4158609451385117 Q8 For player X \u00b6 # A intersect B: X played side and X won / tot games # B: X played side / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.38845460012026456 1.2.1.3 What is the probability of winning after playing a corner piece? \u00b6 Q9 For player O \u00b6 # A intersect B: O played corner and O won / tot games # B: O played corner / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.4779116465863454 Q10 For player X \u00b6 # A intersect B: X played corner and X won / tot games # B: X played corner / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.47386964180857316 Are these results surprising to you? Why? This resource may be illustrative. 1.3 Improving the Analysis \u00b6 In this analysis, we only tracked what moves were made, not the order they were made in. It really limited our assessment! How might we change our recording of the games to track order of moves as well? Do we need to track all the moves or just the first and the winner?","title":"Statistical Analysis of TicTacToe"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#data-science-foundations-project-part-1-statistical-analysis","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to perform statistical analysis on data generated from our tictactoe program!","title":"Data Science Foundations  Project Part 1: Statistical Analysis"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#10-preparing-environment-and-importing-data","text":"back to top","title":"1.0 Preparing Environment and Importing Data"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#101-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"1.0.1 Import Packages"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#102-load-dataset","text":"back to top data = {} for i in range ( 1000 ): game = GameEngine () game . setup_game () board = game . play_game () data [ 'game {} ' . format ( i )] = { 'board' : board . board , 'winner' : board . winner , 'starting player' : board . start_player } | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| |O| | | | | |O|X|X| |O| |O| |X| | | |O|X|X| 'O' Won! |O| |O| |X|O| | |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O| | | |X|X| 'O' Won! |X| |O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X| | | |O|X| | | |O|O| |X| |X| |O|X| | | |O|O| |X|O|X| |O|X| | | |O|O| |X|O|X| |O|X|X| | |O|O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | |X|X| | | |O| | |O|O| | |X|X| |X| |O| | |O|O| | |X|X| 'O' Won! |X| |O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | |O| | | |X|X| |O|X| | | |O| | | |X|X| |O|X| | | |O|O| | |X|X| |O|X|X| | |O|O| | |X|X| 'O' Won! |O|X|X| |O|O|O| | |X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | |O| | |O| |X| |X| | | | |O| | |O|X|X| |X| | | | |O|O| |O|X|X| |X| |X| | |O|O| |O|X|X| 'O' Won! |X| |X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | | | | |X| |X| |O| |O|O| | | | |X| |X|X|O| |O|O| | | | |X| 'O' Won! |X|X|O| |O|O| | |O| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X|X| |X| | | |O| |O| | |X|X| |X|O| | |O| |O| | |X|X| |X|O|X| |O| |O| 'O' Won! |O|X|X| |X|O|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | |O|O| | | |X| |X| |X| | |O|O| | | |X| |X|O|X| | |O|O| | | |X| |X|O|X| |X|O|O| | | |X| 'O' Won! |X|O|X| |X|O|O| | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| |X| | |O| | |O| |X| |O| |X| | |O| | 'X' Won! |O| |X| |O| |X| | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X|O| | | |O| |X|X| | 'X' Won! |O|X|O| | |X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| |X| | | | | | | | |O|X| |X|O| | | | | | | |O|X| 'X' Won! |X|O| | | |X| | | |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | |X|O| | | |X|O| | |X|O| |X|O| | | |X|O| |X|X|O| |X|O| | 'O' Won! | |X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X|O| | | |O|X| | | | | |X|O| | | |O|X| | | |X| |X|O|O| | |O|X| | | |X| |X|O|O| | |O|X| | |X|X| 'O' Won! |X|O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| |X| | | |X| | |O|O| |X| |X| |O| |X| |X|O|O| |X| |X| |O| |X| 'O' Won! |X|O|O| |X|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| |X|X| | | | |O| | |X|O| |X|X| | |O| |O| | |X|O| |X|X| | |O| |O| |X|X|O| 'O' Won! |X|X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | |X|O| | | | | |O| |X| | |X|O| |O| | | |O| |X| |X|X|O| |O| | | |O| |X| 'O' Won! |X|X|O| |O|O| | |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | |X| | | |O| |X| |O| | | |X|O| | |O| |X| |O| | | |X|O| | |O| |X| |O|X| | |X|O| | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | |X| | | | |O|O| |X|X| | |X|O| | | |O|O| 'X' Won! |X|X| | |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| | | | |X|O| |X| | | |O|X| | | |X|O| |X| | | |O|X| | |O|X|O| |X| | | |O|X|X| |O|X|O| |X|O| | |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| |X| |O| | |O|X| |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| | | |X| |O|X|O| |O|X|X| | | |X| |O|X|O| 'O' Won! |O|X|X| |O| |X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X|X| | |O|O| | |O| | | |X|X| | |O|O| | |O| | | |X|X| | |O|O|X| |O|O| | |X|X| | |O|O|X| |O|O|X| |X|X| | |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | |X| |X|O| | | |O| | | | |X| |X|O| | |X|O| | | | |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O|X| |O| |X| |X|O| | |X|O|X| |O| |X| |X|O|O| 'X' Won! |X|O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | |X| | | |O| |O| | |X| | |X|O| | |O| |O| | |X| | |X|O|X| |O| |O| 'O' Won! |O|X| | |X|O|X| |O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| 'O' Won! | |X|O| | |O| | |O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O| |X| |X|X| | |O| |O| |O| |X| |X|X| | 'X' Won! |O| |O| |O| |X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X| | | | |X| |O| |X| |O|X| | | | |X| |O|O|X| 'X' Won! |O|X|X| | | |X| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O|X| | | | | | | |X| |O|O|X| | |O| | | | |X| |O|O|X| | |O| | |X| |X| 'O' Won! |O|O|X| | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| |O| |X| | | | | | |O|X| |O| |X| |X| | | | |O|X| |O| |X| |X| | | |O|O|X| 'X' Won! |O| |X| |X| |X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | | |X|X| |O| | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| | | |X|O|O| |O|X|X| |O| |X| |X|O|O| 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O|O|X| | | | | | |X|X| |O|O|X| | | | | | |X|X| |O|O|X| | |O| | | |X|X| |O|O|X| |X|O| | | |X|X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| | |O|X| | |X|O| |O| |X| 'X' Won! |X|O|X| | |X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | | |O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| | |X|X| |X|O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | |X| |X| |O| |X| |O|O| | |X|O|X| |O|X|X| |O|O| | |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| 'O' Won! |X|O| | |X| |X| |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | |X| |O| | |X| | | |O|O| |X| |O| | |X| | | |O|O| |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X|O| | | | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| |X| | |O|O| |X|X| | |O| |X| 'X' Won! | |O|O| |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X|O| |O|X| | |X| | | | |X|O| |O|X| | |X| | | | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| 'X' Won! |X|X|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X|O| | | | |O| | | |X| |X|O|X| | | |O| | | |X| |X|O|X| |O| |O| | | |X| 'X' Won! |X|O|X| |O|X|O| | | |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | | |X| | |X|X| | |O|O| | | |X| |O|X|X| | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X|O| | |O|X| | | |X| | |X|O| | |O|X| |O| |X| | |X|O| | |O|X| 'X' Won! |O| |X| | |X|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| | |O|O|X| | |X|X| | |O| | 'X' Won! |O|O|X| | |X|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| | | |O| |O|O|X| | | |X| |X| |O| |O|O|X| | | |X| |X|O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | |X|X| | | | | |O| | | |O|X|X| | | | | |O| | | |O|X|X| |X| | | |O| | | |O|X|X| |X| | | |O| |O| |O|X|X| |X| |X| |O| |O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | |O| |X| | | |X| |X|O| | |O| |X| | | |X| |X|O|O| |O| |X| | | |X| 'X' Won! |X|O|O| |O|X|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | |X|X| |O| |O| | | | | |O|X|X| |O| |O| | |X| | |O|X|X| |O| |O| 'O' Won! | |X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| |X| | |X| | 'O' Won! |O| | | |O| |X| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | | | | | | |X|X| |O|O|X| | | |O| | |X|X| |O|O|X| | |X|O| | |X|X| |O|O|X| | |X|O| |O|X|X| |O|O|X| 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O|O| | | | |X| 'X' Won! | | |X| |O|O|X| | | |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | | |O|X| |X| | | |O| |O| | |O|X| |X| | | |O| |O| | |O|X| |X| |X| |O| |O| | |O|X| |X|O|X| |O| |O| |X|O|X| |X|O|X| 'O' Won! |O|O|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O| |O| | |X|X| | | | | |O| |O| | |X|X| | | |O| |O| |O| | |X|X| |X| |O| |O| |O| | |X|X| |X|O|O| |O| |O| 'X' Won! |X|X|X| |X|O|O| |O| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| |X| |X| | |X| | | |O|O| |X|O|X| | |X| | | |O|O| |X|O|X| |X|X| | | |O|O| |X|O|X| |X|X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | |X|X| |O| | | | | |O| | |X|X| |O| | | 'X' Won! | | |O| |X|X|X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| | |X|X| |O|O|X| | |O|O| | |X|X| 'X' Won! |O|O|X| | |O|O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | | |X|X| |O| |O| | | | | |O|X|X| |O| |O| |X| | | |O|X|X| 'O' Won! |O|O|O| |X| | | |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| | | |O| | |X| | | |X|O| |O| |O| | |X| | 'X' Won! | |X|O| |O|X|O| | |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | |X| | | |O|X| |O|X|O| | |X| | | |O|X| |O|X|O| | |X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | | | | |X| |X| |O|O| | | | | | |X|O|X| |O|O| | | | |X| |X|O|X| 'O' Won! |O|O| | | |O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | |X|O| | | |X|X| | |O| | |X|O| | | |X|X| | |O|O| |X|O|X| | |X|X| | |O|O| |X|O|X| |O|X|X| | |O|O| 'X' Won! |X|O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| |O|O| | | |X| | | | |X| |O|O| | |X|X| | | |O|X| |O|O| | |X|X| | |X|O|X| |O|O| | |X|X| | |X|O|X| |O|O| | |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| | | | | |X|X|O| |X|O|O| | | | | |X|X|O| |X|O|O| | |X| | |X|X|O| 'O' Won! |X|O|O| | |X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X|X| | |X|O| |O|O| | | |X|X| |X|X|O| |O|O| | | |X|X| 'O' Won! |X|X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | | |X| | | |O|O| |X| | | | |X| | | |O|O| |X| |X| |O|X| | | |O|O| |X| |X| |O|X| | |X|O|O| |X| |X| |O|X| | |X|O|O| |X|O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | |X| |X| |O|O| | | | | | 'O' Won! |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X|X| | |O| | |O|X|O| | |X|X| 'X' Won! | |O| | |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| |X| | |O|O| | |X|O| |X| |X| 'X' Won! | |O|O| | |X|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O|O| | |O|X| | |X| | | |O|O| |X|O|X| | |X| | | |O|O| |X|O|X| | |X|O| | |O|O| |X|O|X| |X|X|O| 'O' Won! |O|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | |X| | | |X| | |O|O| | 'O' Won! | |X| | | |X| | |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | | | | |O|O| |X| |X| |X| | | | |O|O| 'O' Won! |X| |X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| |X| | | | |O|O| |O|X|X| |X| | | | |O|O| |O|X|X| |X|X| | | |O|O| |O|X|X| |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O|O| |X|O| | 'X' Won! |X| | | |X|O|O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | 'X' Won! |X| | | |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | |O|X| |X|X| | | |O|O| | |O|X| 'O' Won! |X|X| | |O|O|O| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | | |X| | 'O' Won! |O|O|O| |X| | | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | | |O|O| | |X|X| |X| | | | |O|O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O| | |X|O|O| | |X|X| |X|O|O| |X|O|O| 'X' Won! |X|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| | | |O| |O|X|X| | | |O| | |X|O| |O|X|X| | | |O| | |X|O| |O|X|X| |O| |O| 'X' Won! | |X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X|X| | | |O|O| | | |O| |X|X| | | |O|O| | |X|O| |X|X| | 'O' Won! | |O|O| | |X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| |X| |O| |O| |X|O| | |X| |X| |O|X|O| |X|O| | |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| |X| | | | |X|O| |X|O|O| |X| | | | |X|O| 'X' Won! |X|O|O| |X| | | |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | | | |X|X|O| 'O' Won! |O| | | | |O| | |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| | | |O| | | | | |X|O|X| | | |O| |X| | | |X|O|X| | | |O| |X| |O| 'X' Won! |X|O|X| |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| 'O' Won! |O| |X| | |O| | | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| |O| 'X' Won! | | |X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O| | | |X| | |O|X|X| 'O' Won! |O|O|O| | |X| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| | |O| | |X|X| | |O| |X| | |O|O| |X|X| | 'X' Won! |O| |X| | |O|O| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| | | |O| |X|O|X| | | |X| | | |O| |X|O|X| |O| |X| | | |O| |X|O|X| |O|X|X| | | |O| |X|O|X| |O|X|X| |O| |O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | |O| | | |X| |O|X|O| | |X|O| | | |X| |O|X|O| 'O' Won! | |X|O| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | |X|O| | | | | | |O|X| |O|X|O| | | | | |X|O|X| |O|X|O| 'O' Won! | | |O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| | | |O| |X|O|X| | | |X| | | |O| |X|O|X| | | |X| |O| |O| |X|O|X| |X| |X| |O| |O| |X|O|X| |X|O|X| |O| |O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| |X| |X| |O| |X| 'O' Won! | |O|O| |X|O|X| |O| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| | | |O| |X| | | |X|X|O| | |O|O| |X| | | |X|X|O| | |O|O| |X| |X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | | | |O| |X| |X|X|O| | | |O| |O| |X| |X|X|O| |X| |O| |O| |X| |X|X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X|O|X| | | | | |O|X| | |X|O|X| | | |O| |O|X| | |X|O|X| | |X|O| |O|X| | |X|O|X| |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | | | |O| |X| | |X|O| | | | | |O|X|X| 'O' Won! | |X|O| | |O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| |X| | | | |O| | |O| |X| |X| | | |O|O| | |O| |X| |X| | | |O|O|X| |O| |X| |X| |O| |O|O|X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | | |X| | |O| | |X| |O| | | |X| | |O|O| |X| |O| |X| |X| | |O|O| |X|O|O| |X| |X| | |O|O| 'X' Won! |X|O|O| |X|X|X| | |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | |X| | | |X| |O| |O|X|O| |X| | | |X| |O| |O|X|O| |X| | | |X|X|O| 'O' Won! |O|X|O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O| |O| | |X| | | | |X| |O| |O| | |X|X| | | |X| |O| |O| |O|X|X| | | |X| |O| |O| |O|X|X| | |X|X| 'O' Won! |O|O|O| |O|X|X| | |X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | | | |X| | |X|O| |O|O| | | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| |O| |X| | |X|O| |O|O|X| |O|X|X| | |X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O| | | | |X|O| | | |X| |O| | | |X|X|O| | | |X| |O|O| | |X|X|O| |X| |X| |O|O| | |X|X|O| 'O' Won! |X| |X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| 'X' Won! |O|O|X| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | |X| | |O| | |X|O| | | | |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O|O| 'X' Won! |X|X|X| |O|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| |O| | | |X|O| | | |O|X| |O| | | |X|O|X| | |O|X| |O| | | |X|O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| |O| | |X| | | |O| | |X| |O| | |X| | |X|O| | |X| |O| |O|X| | |X|O| | 'X' Won! |X| |O| |O|X| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | |X| |X| |O|O| | | | |O| |X| |X| |O|O| | 'X' Won! | | |O| |X|X|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| |X|X| | |O| | | | |O|X| |X|X|O| |O| | | | |O|X| 'X' Won! |X|X|O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X| | | |X| |X| 'O' Won! |O|O|O| |X| | | |X| |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O| | |X|O| | | |X|X| | |O|O| |X|O| | 'X' Won! |X|X|X| | |O|O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| |O| | | | | |X| |O| 'X' Won! |X| |O| |X| | | |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| | |O| | |X|O|O| | |X|X| |X|O| | |X|O|O| |O|X|X| |X|O| | 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X| | | |O| | 'X' Won! |O| |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |X| | | |O| |O|X| | | | |X| | |X|O| |O|X| | | | |X| | |X|O| |O|X| | | |O|X| 'X' Won! |X|X|O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | | | |X| |X|O|O| | |X| | | |O|X| |X|O|O| | |X| | | |O|X| |X|O|O| | |X|X| |O|O|X| |X|O|O| | |X|X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | |X|O| | | |X| |O| | | | |X|O| | | |X| |O|O| | | |X|O| |X| |X| |O|O| | 'O' Won! | |X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| |O| | | | |X|X| | |X|O| |O| | | | |X|X| |O|X|O| |O| | | 'X' Won! |X|X|X| |O|X|O| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X|X| |O| | | | |O| | | |X|X| |O|O| | | |O| | | |X|X| |O|O|X| | |O| | |O|X|X| |O|O|X| 'X' Won! | |O|X| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | |X| | | |X| |O| |O|X|O| |X| | | |X| |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X| |X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| 'O' Won! |O|X| | | |O| | | |X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | | |O| |X|X| | |X|O| | | |O|O| |X|X| | 'X' Won! |X|O| | |X|O|O| |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | |O|X|X| | |O|X| |O|O| | |O|X|X| |X|O|X| 'O' Won! |O|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O|X| | | | | |O| |X| | |O|X| | |O| | |O|X|X| | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | 'X' Won! |O|X|X| | |O|X| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| |O| | | | | |X|O|X| |O|X|O| 'O' Won! |O| | | |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | |O|X| | | | | |X| |O| |X|O|X| | | | | |X| |O| |X|O|X| | | | | |X|O|O| |X|O|X| | | |X| |X|O|O| |X|O|X| |O| |X| |X|O|O| 'X' Won! |X|O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| |X| 'O' Won! | | | | |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| |X| |X| |O| | | | |O|X| |X|O|X| |O| | | | |O|X| 'X' Won! |X|O|X| |O|X| | | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| |X| |O| | | |O|X| | |X| |X| |O| |O| |O|X|X| |X| |X| |O| |O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | |X| | |X|O| | | |O| | | |X| | |X|O| | | |O|O| | |X| | |X|O| | |X|O|O| | |X| | |X|O|O| |X|O|O| 'X' Won! |X|X| | |X|O|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O| |O| 'X' Won! |O| | | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O|X|O| | | |X| | | |O| |O|X|O| | | |X| | |X|O| |O|X|O| | |O|X| | |X|O| |O|X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O| | 'O' Won! |X|X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X|O| | | |X| | | | | |O|X|O| | |X|X| | | | | |O|X|O| | |X|X| | | |O| |O|X|O| | |X|X| |X| |O| |O|X|O| |O|X|X| |X| |O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| |O| | |O|X| | |O|X| |X| |O| | |O|X| |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X| | | |O|O| 'X' Won! | |O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | |X| |O| | | |X| |O|O| | |X| |O| | | |X| |O|O| | |X|X|O| 'O' Won! | | |X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | |X| |O| | | | | |O|X| | 'O' Won! |X| |O| | |O| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O|X| |O| | | |X| | | | |O|X| |O| |O| |X|X| | | |O|X| |O| |O| 'O' Won! |X|X| | | |O|X| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | |X| |X| |O|X| | |O| | | |X| |X| |O|X| | |O| |O| |X| |X| |O|X| | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | |O|O| | | |X| | | |X|X| |O|O| | | |X| | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | |X|X| | | | | |O| |O| | |X|X| |O| | | |O| |O| | |X|X| |O|X| | |O| |O| | |X|X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | | | |X| |X| |O| | |O| | | |O|X| |X| |O| | |O| | |X|O|X| 'O' Won! |X|O|O| | |O| | |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O|X| | | |X| |X| |O| |O|O|X| | | |X| |X|O|O| |O|O|X| |X| |X| |X|O|O| 'O' Won! |O|O|X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X| | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | | |X|O| |O| |O| |X|X| | |X|X|O| 'O' Won! |O| |O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O|X| | | | | |O|X| | |O|O|X| | |X| | |O|X| | |O|O|X| | |X| | |O|X|O| |O|O|X| | |X|X| |O|X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X| |O| |X|O|O| | | |X| |X| |O| |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| |O| | | | | |X| 'X' Won! |X|O|O| |O|X| | | | |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | |O| | |X|X|O| |X| | | | |O| | |X|X|O| |X|O| | | |O| | 'X' Won! |X|X|O| |X|O| | |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | |X|X| | | |O| |O| |O| | |X|X| | | |O| |O|X|O| | |X|X| | |O|O| |O|X|O| | |X|X| 'X' Won! | |O|O| |O|X|O| |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | |O| | | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O|X| |X| | | |O| | | |X|O|X| |X|O| | |O| | | |X|O|X| |X|O|X| |O| | | |X|O|X| |X|O|X| |O| |O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| |O|O| | 'X' Won! |X| | | | |X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X|O| |X|O| | | |X|O| | |X|O| |X|O| | 'O' Won! | |X|O| | |X|O| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | |O| |O| | |X|X| |O| | | 'X' Won! |O| |O| |X|X|X| |O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| |X| |X| | | | | | |O|O| |X| |X| | |O| | |X|O|O| |X| |X| | |O| | |X|O|O| |X| |X| |O|O| | |X|O|O| |X| |X| |O|O|X| 'O' Won! |X|O|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| |O|O|X| |X| | | |O|X|O| |O|O|X| |X| |X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | | | |X|O|O| |X|O| | | | | | 'X' Won! |X|O|O| |X|O| | |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| |O| | | | | |X| |X| |O| |O|X| | | |O|X| |X| |O| |O|X| | | |O|X| |X|X|O| |O|X| | |O|O|X| |X|X|O| |O|X| | It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X|O| | |O| | | | |X|X| |X|O| | |O| | | |O|X|X| |X|O| | 'X' Won! |O| |X| |O|X|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | |O|O| |X| | | |X| |O| 'X' Won! |X|O|O| |X| | | |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | | |X| | | |O|X| |O| | | |O|X| | | |O|X| |O| |X| |O|X| | | |O|X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | | |O| | |X|X| |O|O| | |X| |O| | |X|X| |O|O| | 'O' Won! |X| |O| | |X|X| |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | |O|O| | |X| |X| | | | | |O|O|X| |X| |X| | | | | |O|O|X| |X|O|X| | | | | 'X' Won! |O|O|X| |X|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| |X| 'O' Won! |O| | | |O|X| | |O| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | | |O| |X|O| | |O|X|X| | | |O| |X|O| | |O|X|X| |O| |O| |X|O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | |O| | | |X| | |X|O|X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| |O|O| | |O| |X| | |X|X| |O|O| | |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | |O| | |O| |X| |X|X| | | |O| | |O| |X| |X|X| | | |O|O| |O| |X| |X|X| | |X|O|O| |O| |X| 'O' Won! |X|X|O| |X|O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X|O| |X| | | | |X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| |X| |O|X|O| | |X|O| |X|O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| | | |X| 'O' Won! |O|X|X| |O|O|O| | | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X|O| | |O| | | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O| |O|X| | | |X|X| | |O|O| |O|X| | 'O' Won! | |X|X| |O|O|O| |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | | | | | | |X|O| |O| | | |X| | | | |X|O| |O| | | |X| |O| | |X|O| |O| | | |X|X|O| 'O' Won! | |X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| | |X|X|O| | | |O| | |X| | 'O' Won! |X|X|O| | | |O| | |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| |O| | | | |X| | | |O|X| |O| | | | |X|X| | |O|X| |O| |O| | |X|X| | |O|X| |O|X|O| | |X|X| |O|O|X| |O|X|O| | |X|X| 'X' Won! |O|O|X| |O|X|O| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | | |X|O| 'O' Won! |O|X|X| | |O| | | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O|O| | |O|X| | | | |X| |O|O| | |O|X| | |X| |X| 'O' Won! |O|O|O| |O|X| | |X| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | | |X|X| | |O|O| |O| | | | |X|X| | |O|O| |O| |X| | |X|X| | |O|O| |O|O|X| | |X|X| 'X' Won! | |O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | |X|O| | | |O| |X| | | | |X|O| | |O|O| |X| | | | |X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X|X| | 'O' Won! |O|X|O| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | |X|O| |O| | | |X| | | | |X|O| |O|O| | 'X' Won! |X| | | | |X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | |O|X| |X| | | |O| |O| | |O|X| |X| | | |O|X|O| | |O|X| |X|O| | |O|X|O| |X|O|X| |X|O| | |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | 'X' Won! |X| | | |X|O| | |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | |O| | |X|O| | | |X|X| | |O| | |X|O| | | |X|X| | |O| | |X|O|O| 'X' Won! |X|X|X| | |O| | |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | |X| |O| | | |O|X| | | |O|X| |O| | | |O|X|X| | |O|X| |O| |O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | |X| | | | |O| | |X|O| |O|X| | | | |O| | |X|O| |O|X|X| | | |O| |O|X|O| |O|X|X| 'X' Won! |X| |O| |O|X|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X|O|O| |X| | | 'X' Won! |X|O| | |X|O|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X| | |O|O| | | | |X| |X|X| | |O|O| | | | |X| |X|X|O| |O|O| | | | |X| |X|X|O| |O|O| | | |X|X| 'O' Won! |X|X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | | |O| | | |X|X| |O|O| | 'X' Won! | |O| | |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | |O|X| |X| | | | |O|O| |X|O|X| |X| | | | |O|O| 'O' Won! |X|O|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | |X| |X|O|X| | | |O| 'O' Won! |O| |X| |X|O|X| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| |X| |O| | | |X| | | |O| |X| |O| |O| |X| | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| |O| |X|X| | 'X' Won! |O|X|O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |X| | |O| | |X|X|O| | |O|X| 'X' Won! |X|O| | |X|X|O| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | | 'O' Won! |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X|O| | |X| | | |O|X|O| |X|O| | |X| | | |O|X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X| |X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X|X| | |X|O|O| | | |O| |X|X| | 'X' Won! |X|O|O| | | |O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X|X| | | |O|O| |O| | | |X|X| | |X|O|O| |O| | | |X|X| | |X|O|O| |O|O| | |X|X| | 'X' Won! |X|O|O| |O|O| | |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | |X|X| | |O|O| | | |O| | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O|O| |X|X| | |O|O|X| |X|O|O| |X|X| | 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | |O|X| | | | | |O|X| | |X|O|X| | | | | |O|X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O|X| | |O|X| | |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | |O|X| |O| |X| | | | | | |O|X| |O|X|X| | | | | | |O|X| |O|X|X| | | |O| | |O|X| |O|X|X| | |X|O| |O|O|X| |O|X|X| | |X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| | |X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | |X| | |O|O| | | | |X| 'O' Won! | |X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X| | | |X|O| | | |O| |O|X| | | |X|O| | | |O| |O|X|X| 'O' Won! | |X|O| | |O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | |O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| | | | |X|O| |O|X|X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O|X| | | |X| | |O| |X| |O|X|O| | |X|X| |O| |X| |O|X|O| 'O' Won! |O|X|X| |O| |X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | |X|O| |O| | | | |X| | |O|X|O| |O| | | | |X| | 'X' Won! |O|X|O| |O|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| 'O' Won! |O| | | |X|O|X| | | |O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | | | |O|X| 'X' Won! |X| |O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X|X| | |X|O| | |O|O| | |X|X| | |X|O| |X|O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | |X|O| | | | | |X|O| | | |X|O| | | |O| |X|O| | | |X|O| | |X|O| |X|O| | | |X|O| |O|X|O| |X|O| | | |X|O| |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |X| | | |O|O|X| | | | | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| |X| |O| | | | | |X| |O| |X|O|O| | | | | |X| |O| 'X' Won! |X|O|O| |X| | | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O|O| | | | | | |X| |X| |O|O| | |X| | | |X| |X| |O|O| | |X| |O| |X| |X| |O|O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | |X| | | |X| |O| | |O| | |X|O| | |X| |O| | |O| | |X|O|X| |X| |O| |O|O| | |X|O|X| |X|X|O| |O|O| | |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O|X|O| 'O' Won! |O| | | |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | | |X| | |X| |O| |O|X| | | |X|O| |X| |O| |O|X| | |X|X|O| |X|O|O| |O|X| | |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X|X|O| 'O' Won! |O|O|O| | | |X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | | |X|X|O| | | | | |O| | | |X|X|O| | |O| | |O| | | |X|X|O| |X|O| | |O| | | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| |O| |O| | | | | | |X|X| |O|X|O| | | | | | |X|X| |O|X|O| | | |O| 'X' Won! | |X|X| |O|X|O| | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | | |O| |X| |O| |X|X| | | | |O| 'O' Won! |X| |O| |X|X|O| | | |O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | |X| |X| |X| |O| |O|O| | |X| |X| |X|X|O| |O|O| | |X| |X| |X|X|O| |O|O| | |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | | | | | |O| |X|X|O| | | | | |X| |O| |X|X|O| |O| | | |X| |O| |X|X|O| |O| |X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | | | | |O|X|O| | |X|X| | | | | |O|X|O| |O|X|X| | | |X| |O|X|O| |O|X|X| 'O' Won! |O| |X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| 'X' Won! | |O|X| | | |X| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X|O| | | |O| | |O|X| |X|X|O| | | |O| |O|O|X| |X|X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | |O| 'X' Won! | | |O| |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| |X| |O|O| | | |O|X| |X| |X| 'X' Won! |O|O| | | |O|X| |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| | |O|X| |O|X| | | | |X| | |O|X| |O|X|O| | |X|X| | |O|X| |O|X|O| 'O' Won! |O|X|X| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | |X| |X|O| | | | |O| | | |X| |X|O| | |O| |O| | | |X| |X|O|X| |O| |O| 'O' Won! |O| |X| |X|O|X| |O| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | |O| |X| | | 'X' Won! | |O|X| | |X|O| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| 'X' Won! |X| | | | |X| | |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X|X| | | | | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O|X| | |O|O|X| |X|X| | |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O|O| | |X| | |X|O| | | |O|O| | |X| | |X|O|X| 'O' Won! |O|O|O| | |X| | |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | | |O|O| |X| |X| | | | | |X|O|O| |X|O|X| | | | | |X|O|O| 'X' Won! |X|O|X| |X| | | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | |X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O|X| |O|X| | |O|O|X| | |O|X| |O|X| | |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X| | | |X|O| |O| | | | |X| | |X|X|O| |O| | | | |X|O| |X|X|O| |O| | | | |X|O| |X|X|O| |O| |X| | |X|O| |X|X|O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | | | |X| |O|X|O| | | | | |X| |X| |O|X|O| | | | | |X| |X| |O|X|O| |O| | | 'X' Won! |X|X|X| |O|X|O| |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X|O| | |X| | | | |O| 'X' Won! |O|X|O| | |X| | | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X| | | | |O|O| |X| |X| |X| | | | |O|O| |X|O|X| |X| | | | |O|O| |X|O|X| |X| |X| 'O' Won! |O|O|O| |X|O|X| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | |X|X| |O|O| | |O| |X| | |X|X| 'X' Won! |O|O|X| |O| |X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| | | |O| | | |O|X|O| |X| | | |O|X| | |O|X|O| |X| | | |O|X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | |X| | | |O| | |X| |O| | |X| | |O|O| | |X| |O| | |X| | |O|O|X| |X| |O| 'O' Won! |O|X| | |O|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | |X| | |O| |X| |O|O| | | |X| | |O| |X| |O|O| | | |X| | |O|X|X| 'O' Won! |O|O| | |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| |X| |X| | | | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X|O|X| | |O|O| |O| |X| |X|O|X| | |O|O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| |X| |O| |X| | | |O|O|X| |X|O|O| |X| | | |O|O|X| 'X' Won! |X|O|O| |X|X| | |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | | |O|X| |O|X|X| |O| | | | |O|X| |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| | | |X|X| |O| |O| | |O| | | |X|X| |O|X|O| | |O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| |O|X|O| It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| |X|O| | | | | | |X| |O| |X|O|O| | | | | |X| |O| 'X' Won! |X|O|O| |X| | | |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | |O| 'X' Won! |X| | | |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | |O|O| | |X|X| | |X| | | |O|O| | |X|X| | |X| |O| |O|O| | |X|X| | |X| |O| |O|O|X| |X|X| | |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| 'O' Won! |O| |X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O| |X|X| | 'O' Won! | |X| | |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O|O| | |X| | |X|O| | |X|O|O| 'O' Won! |O|X| | |X|O| | |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | |X| | | | |O|X| |O|O| | |X| | | |X|O|X| |O|O| | |X| | | 'O' Won! |X|O|X| |O|O|O| |X| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | |O| |O|X|X| | | |X| | | |O| |O|X|X| | | |X| | |O|O| |O|X|X| | |X|X| | |O|O| 'O' Won! |O|X|X| | |X|X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| | | | | |X|X|O| | |O|X| | | | | 'O' Won! |X|X|O| | |O|X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | | | |O| |X| | |X|O| | | |X| |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |X|X| 'O' Won! |O| |X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | | |X| |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| |X|O|X| |X| | | 'O' Won! |O|X|O| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O|O| | |X| |X| | |X|O| |O|O| | 'O' Won! |X| |X| | |X|O| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | |X| | |O| |O| |O|X| | | |X| | |O| |O| |O|X| | | |X|X| 'O' Won! |O| |O| |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| |X| |X|O| | | | | | |O|O|X| |X|O| | |X| | | |O|O|X| 'O' Won! |X|O| | |X|O| | |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X| | |X|O|O| | | |X| | |X| | |X|O|O| |O| |X| | |X| | |X|O|O| |O| |X| |X|X| | |X|O|O| |O|O|X| |X|X| | 'X' Won! |X|O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | | |O| |O|X| | | | |X| | |X|O| |O|X| | | | |X| | |X|O| |O|X|O| | | |X| 'X' Won! |X|X|O| |O|X|O| | | |X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | |O| | |X| | | |O|O|X| | |O|X| |X| | | |O|O|X| |O|O|X| 'X' Won! |X| |X| |O|O|X| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| | | | | | |O|O| |O|X|X| |X| | | | |O|O| 'O' Won! |O|X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | | |X| | |X|O|O| |O|X|O| | |X| | |X|O|O| |O|X|O| | |X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O| |O| |X| |X| | |O| | |O|X|O| |X| |X| | |O|O| |O|X|O| |X| |X| 'X' Won! |X|O|O| |O|X|O| |X| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| | | | | | |X|X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| |X| | |X|X| |X|O|O| |O| |X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X|O| | | |X| | |O|X|O| |X|O| | |O|X| | |O|X|O| |X|O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | |O|O|X| | |X| | | |O| | 'X' Won! |O|O|X| | |X| | |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O|X| |X| | | | |O|X| |O|O|X| |X| | | 'X' Won! | |O|X| |O|O|X| |X| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| | |O| | 'X' Won! |O| |X| |O| |X| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | |X|O| |O|X|O| | | |X| | |X|O| |O|X|O| | | |X| |X|X|O| |O|X|O| |O| |X| |X|X|O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | |O|X| | |X|X| |O| | | | |O|X| | |X|X| |O| | | |O|O|X| 'X' Won! |X|X|X| |O| | | |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | |O| |X| | | | |X|O| | | |O| |X| |X| | |X|O| | |O|O| |X| |X| 'X' Won! | |X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| | | |O| |O|O| | | |X|X| | | |O| |O|O|X| | |X|X| |O| |O| |O|O|X| | |X|X| |O|X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | |O| | |O| | | | |X| |X| |O| | |O| | | | |X| |X|O|O| | |O| | | | |X| |X|O|O| | |O|X| | | |X| |X|O|O| |O|O|X| | | |X| |X|O|O| |O|O|X| | |X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X|O| | |X| |O| 'X' Won! |X| | | |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | |O| |O| |O|X|X| |X| | | |O| |O| |O|X|X| |X| |O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | 'O' Won! | | | | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | |O| | | |X| |O| |X|X| | |O| | | |X| |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X|X|O| |X|X|O| |O|O| | |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | | | | | |O| |X| | |X| | |O| | | |O| |X| | |X| | |O| |X| |O| |X| | |X|O| |O| |X| |O| |X| |X|X|O| |O| |X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | | | | | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| | |X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O|X| | | |X| | |O| |X| |O|X| | | |X| | |O|O|X| |O|X| | |X|X| | |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | |O|O| | |X| | |X|O|X| | |O|O| | |X| | |X|O|X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | |O|X| | |X| |O| | | |X| |O|X| | |X| |O| | | |X| |O|X|O| |X|X|O| | | |X| |O|X|O| |X|X|O| |O| |X| |O|X|O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O| | | |X| | 'O' Won! |O|X|X| |O|O|O| | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X| |O| |X|X|O| |O|O| | |X|X|O| |X|X|O| 'O' Won! |O|O|O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | | |O|O| |X| |X| |O| | | |X|O|O| |X| |X| |O|O| | |X|O|O| |X| |X| |O|O|X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O|X| | |X|X| | | | | |O|O|X| |O|X|X| |X| | | |O|O|X| |O|X|X| |X|O| | |O|O|X| |O|X|X| 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| | |O| | |O|O| | | |X|X| | |O| | |O|O|X| |O|X|X| | |O| | |O|O|X| |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | 'X' Won! |X| | | | |X|O| | |O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| |X| |X| |X| |O| 'O' Won! |O|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O|O| | | |X| |X|X| | | |O|O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| |X|O|O| | | |X| 'O' Won! |X|X|O| |X|O|O| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | | |O|X| |X|X| | |O|O| | | |O|X| 'O' Won! |X|X| | |O|O|O| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| |O| |X| |X| |X|O| | |O| |O| |X| |X| |X|O|O| |O| |O| |X| |X| 'X' Won! |X|O|O| |O|X|O| |X| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| |O|X| | | |O|X| | | |X| |O|X| | | |O|X| | | |X| |O|X|O| |X|O|X| | | |X| |O|X|O| |X|O|X| | |O|X| |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | |X|O| |O|X| | |X|O| | | |X|O| |O|X| | |X|O| | |X|X|O| |O|X| | |X|O|O| |X|X|O| |O|X| | 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| |X|O| | |X|O|O| | |X|X| |X|O|O| |X|O|O| 'X' Won! |X|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| |O| |O| | | |X| | | |X| |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |X|X| |O|X|O| |O|O|X| | |X|X| |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X| |O| | | | | |X|O| | |X|O|O| | | | | |X|O| | |X|O|O| | | | | |X|O|X| |X|O|O| |O| | | |X|O|X| |X|O|O| |O| |X| |X|O|X| 'O' Won! |X|O|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| |O|X| | | | |X| | | |O| |O|X|X| | |O|X| | | |O| |O|X|X| |X|O|X| | | |O| |O|X|X| |X|O|X| | |O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| | | | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| 'O' Won! |X| | | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| | |X|X| | | |O| | |O|O| | 'X' Won! |X|X|X| | |O| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| |X| |X|O| | | | | | |O| |X| |X|O|O| | |X| | |O| |X| |X|O|O| | |X|O| |O| |X| |X|O|O| |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | |O| |X| | | |X|O| | | | |O| |X| |X| |X|O| | |O| |O| |X| |X| |X|O|X| |O| |O| |X| |X| 'O' Won! |X|O|X| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X|O| | | |O| |O|O|X| 'X' Won! |X|X|O| | |X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | 'O' Won! |X| |O| | | |O| |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | |O| | | |X|X| |O|O|X| | |O| | | |X|X| |O|O|X| |O|O| | | |X|X| 'X' Won! |O|O|X| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| | |O|O| | |X| | | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | |X| | |O|O| | | | |X| | |X|O| |O|O| | | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| | |O|X| |X|X|O| |O|O|X| | |O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | |X| | |O| |X| | |O|O| | |X| | |O| |X| 'X' Won! |X|O|O| | |X| | |O| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | |O| | | |X| |O| |X| | | |O| |X| |X| |O| |X| | | |O|O|X| |X| |O| |X| | | |O|O|X| |X| |O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | | | |X|O|O| |X|X| | |O| | | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X| | 'O' Won! |O|X| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | |X|O| |X| | | |O|O| | | |X|O| |X| | | |O|O| | |X|X|O| |X|O| | |O|O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | |X| | |O|X| | |O| | | | |X|O| |O|X| | |O| |X| | |X|O| 'O' Won! |O|X| | |O| |X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| |O| |O| |X| | | | | |X|X|O| |O| |X| | | | | |X|X|O| |O| |X| | |O| | |X|X|O| |O| |X| |X|O| | |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | |O| |O| | | |X| | |X| | |O|X|O| | | |X| |O|X| | |O|X|O| |X| |X| |O|X| | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X|O| |O| | | |X| | | |X|X|O| |O| | | |X| | | |X|X|O| |O|O| | |X|X| | |X|X|O| |O|O| | |X|X|O| |X|X|O| |O|O| | 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| 'O' Won! | |O| | | |O|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O|O|X| 'X' Won! |X| | | | |X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| |O| | | | |X| | | | |O| |O| |X| | |X| | | | |O| |O| |X| | |X| | | |O|O| |O| |X| |X|X| | | |O|O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| | |O|O| | | | | |X|X|O| |X|O|O| | | | | 'O' Won! |X|X|O| |X|O|O| | | |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O|X| |O|X|O| |X| |X| |O|O|X| 'X' Won! |O|X|O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O| |X| |X| | | |O|X| | |O| |X| |X| |O| |O|X| | |O| |X| |X| |O| |O|X|X| |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | | | |O| |X|O|O| |X|X| | | |O|O| |X|O|O| 'X' Won! |X|X| | |X|O|O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X| | | |O| |O| |X|X| | |X| | | |O| |O| 'O' Won! |X|X| | |X| | | |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O| | | |X|X|O| | |X| | |O|O| | |X|X|O| | |X| | |O|O| | |X|X|O| | |X|X| |O|O| | |X|X|O| |O|X|X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | | |X|X| |O|X| | |O| | | | |X|X| |O|X|O| |O| | | | |X|X| |O|X|O| |O| |X| 'O' Won! |O|X|X| |O|X|O| |O| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | |X| | | | |O|O| |X|X|O| |X| | | | |O|O| 'X' Won! |X|X|O| |X| | | |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| |X| | |O| | |O|X|X| |O| |X| | |O|O| |O|X|X| |O|X|X| | |O|O| 'O' Won! |O|X|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| 'X' Won! |O| | | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | | | 'O' Won! |O| |X| |O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | |X| |X| | | 'O' Won! |O|O|O| | | |X| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | |X|X| | | | |O| | | |O| |X|X| | | | |O| | |X|O| |X|X| | | | |O| | |X|O| |X|X| | | |O|O| | |X|O| |X|X| | |X|O|O| |O|X|O| |X|X| | |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| |X| | |O|O| |X|X| | |O| |X| | |O|O| |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | |O| |X| |O|X| | | | |X| |O| |X| |O|X| | | | |X| |O|O|X| |O|X| | 'X' Won! | | |X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| |O| | | | |X|O| | | |X| |O|X| | | |X|O| | | |X| |O|X| | |O|X|O| 'X' Won! | |X|X| |O|X| | |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| | 'X' Won! |O|O| | |X|O| | |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | |O| | | |O| 'X' Won! |X|X|X| | | |O| | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| | |O|X| | |X| | | 'X' Won! |O|O|X| |O|X| | |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O|X| | | | |O| |X| |O| |O|X| | | | |O| |X|X|O| |O|X| | | | |O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X|X| | |O|O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O|X| 'O' Won! |X|O| | |X|O| | | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| | |O|O| |X| | | |X|O|X| | |O|O| |X| | | |X|O|X| | |O|O| |X| |O| |X|O|X| | |O|O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| | | |O| |X| | |O|O| |X| | | |O| |X| |X|O|O| |X|O| | |O| |X| |X|O|O| |X|O| | |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| | | | | |X| |O|X|O| |X| | | |O| |X| |O|X|O| |X|X| | |O| |X| |O|X|O| |X|X| | |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | | | |O|X|O| | | |X| |X| | | |O|X|O| |O| |X| |X| | | |O|X|O| |O| |X| |X| |X| |O|X|O| 'O' Won! |O| |X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | | |O| | |X| | |X|O| | | |X|O| | |X|O| |X|O| | | |X|O| |X|X|O| |X|O| | | |X|O| 'O' Won! |X|X|O| |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | |X| | | | |O| |O|X|X| | |X| | |O| |O| |O|X|X| 'X' Won! | |X| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| 'X' Won! |X| |X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| |X| 'O' Won! | | |O| |X|O| | |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| | |X| | | | |O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X|X| | |O|O| | |X|O|X| 'O' Won! |X|X| | |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | |X|O|X| |O| | | |O| | | |X|O|X| |O| | | |O| | | |X|O|X| |O| |X| |O| | | |X|O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O| |X| | |X| | |X|O|O| |O| |X| |O|X| | |X|O|O| |O|X|X| |O|X| | It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X|O| | | | | |X|X|O| |O|X|O| | | | | 'O' Won! |X|X|O| |O|X|O| | | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | | |O|X| | | | | |X|O|X| 'O' Won! | |O|X| | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | |X|X|O| |O| |O| | |X| | |X|X|O| 'X' Won! |O|X|O| | |X| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| |X|O| | | | |O| | |X|X| |X|O|O| | |X|O| | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O|X|O| | | | | | |X|O| |O|X|O| | | | | |X|X|O| |O|X|O| | | | | 'O' Won! |X|X|O| |O|X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | |X|O|X| | |O| | 'O' Won! |O|X| | |X|O|X| | |O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | | | |X|O|X| | |O|O| | | | | |X|O|X| | |O|O| | |X| | |X|O|X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'O' Won! |O|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| | | |O|X|O| |X|X| | 'X' Won! |O| |X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | |O|X| |X| | | |O| | | |X|O|X| |X| | | |O| | | |X|O|X| |X|O| | |O|X| | |X|O|X| |X|O| | 'O' Won! |O|X| | |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | |X|O|X| | | |O| |O| | | |X|O|X| | | |O| |O|X| | |X|O|X| |O| |O| |O|X| | |X|O|X| |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| 'X' Won! |X|O| | |O|X| | | | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O|O| | |X| | |X| | | |X|O|O| | |X| | |X| | | |X|O|O| |O|X| | |X| | | 'X' Won! |X|O|O| |O|X| | |X| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | | |X| | | |O| |O|X| | |O| |X| | | |O| |O|X|X| |O| |X| | |O|O| |O|X|X| |O| |X| | |O|O| |O|X|X| |O|X|X| 'O' Won! |O|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| |X| |O| |O| | | |X| |X| |X| |O| |O|O| | |X| |X| 'X' Won! |X| |O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | |X|O| |O| | | |O| |X| | |X|O| |O| |X| |O| |X| | |X|O| 'O' Won! |O| |X| |O| |X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | |X| | | | | |X|O|O| | | |X| | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X| |X| | |O|O| |X|O|O| 'X' Won! |X| |X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X|O|O| | | |X| | |X| | |X|O|O| | | |X| |O|X| | |X|O|O| | | |X| |O|X| | |X|O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O|X| |X| | | | |O|X| |O|O|X| 'X' Won! |X| |X| | |O|X| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | | | |X| | | |X| |O|O| | | |O|X| | | |X| |O|O| | |X|O|X| |O| |X| |O|O| | |X|O|X| 'X' Won! |O| |X| |O|O|X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X|O|O| | | |X| |O| |X| |X|O|O| | | |X| |O| |X| |X|O|O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| | |O|O| | 'X' Won! |X| | | |O|X| | |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X| | | | | |O| |O|O|X| |X| | | |X| |O| 'O' Won! |O|O|X| |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | 'O' Won! | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O| |X| |O|X| | |X|O| | |O|O|X| |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | |O|X| | |O|O| | | |X| | |O|X|X| |O|O| | | |X|O| |O|X|X| |O|O| | |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | |X| |O|X| | | |O|O| | | |X| |O|X| | | |O|O| |X| |X| |O|X|O| | |O|O| |X| |X| 'X' Won! |O|X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| |X| | |X| | | |O| | |O|O|X| |X|X| | | |O| | |O|O|X| |X|X| | |O|O| | |O|O|X| 'X' Won! |X|X|X| |O|O| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X| | | | |O|O| |X| |X| |X| | | | |O|O| |X|O|X| |X| |X| | |O|O| |X|O|X| 'O' Won! |X|O|X| | |O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| 'O' Won! |X| | | |X| | | |O|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | |O| | | |O|X| | |X|X| | |O| | | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | |O|O| |X|O|X| 'O' Won! | |X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X|O| | |X|O|O| | |X| | |X|O| | |X|O|O| | |X|O| |X|O| | 'X' Won! |X|O|O| | |X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| |X| | | |O| |O| 'O' Won! | |X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | |O| | |X| | | |X|O|O| | |O|X| |X| | | |X|O|O| | |O|X| |X| |O| |X|O|O| 'X' Won! | |O|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| | | | |O|X| |X|O|X| |O| | | | |O|X| |X|O|X| |O| | | |O|O|X| 'X' Won! |X|O|X| |O|X| | |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| |X| | | |O| |O| | |X|O| |X| | | |O| |O| | |X|O| |X| |X| |O| |O| |O|X|O| |X| |X| 'X' Won! |O| |O| |O|X|O| |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X|X| |O| | | |X| | | |O|X|X| |O| | | |X|O| | 'X' Won! |O|X|X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| |X| |X| | |O| | | |O|O| |X| |X| |X|O| | | |O|O| |X| |X| 'O' Won! |X|O| | | |O|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| |O| | | | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| |X| |X|X| | |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O| | | |X|O| |X|O|O| |X|O| | 'X' Won! |X|X|O| |X|O|O| |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O|X| | | | | | |O| | 'O' Won! |X|O|X| | |O| | | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| 'O' Won! | | |X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| |X| |O| | | | | | |X|O| |X|O|O| 'X' Won! | | |X| | |X|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | |X| |O| 'O' Won! |O|X| | | |O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X|X| | | | |O| |O|O| | |X|X| | | | |O| |O|O| | |X|X| | |X| |O| 'O' Won! |O|O|O| |X|X| | |X| |O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O| | |O| |O| |X| |X| | |O| | |O|X|O| |X| |X| | |O|O| |O|X|O| 'X' Won! |X|X|X| | |O|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| |X|X|O| | |O| | | |O|X| |X|X|O| | |O| | 'X' Won! | |O|X| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | | |O| |O|X|O| | | |X| |X| |O| |O|X|O| |O| |X| |X| |O| |O|X|O| |O|X|X| |X| |O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X|O| |O|X|X| | | | | | |X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | |X| |O|X|O| 'O' Won! |O|X|X| |O| |X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| |O| | | |O|X|X| |X| |O| |O| | | |O|X|X| |X| |O| |O|O| | |O|X|X| |X|X|O| |O|O| | 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X|X| |O| |O| | |O|X| | |X|X| 'X' Won! |O| |O| | |O|X| |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |O| 'X' Won! | |X| | |O|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X| |X| |X| |O|O| | |O| |X| |X| |X| |O|O| | |O|X|X| |X| |X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | |O| |X| |X| | |X|O| | | |O| |X|O|X| | |X|O| | | |O| |X|O|X| | |X|O| | |X|O| |X|O|X| |O|X|O| | |X|O| 'X' Won! |X|O|X| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X|X| | |O|O| |O| |X| | |X|X| | |O|O| |O|X|X| | |X|X| | |O|O| 'O' Won! |O|X|X| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X|O| | | | | |X| |O| |X|X|O| | | | | |X| |O| 'O' Won! |X|X|O| | | |O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | |X| | | |X| |O| |O|X| | |X| | | |X|O|O| |O|X| | |X|X| | |X|O|O| |O|X| | |X|X|O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| | | | |X|O| |O|O| | |X| |X| | |X|O| |O|O| | |X| |X| |O|X|O| |O|O| | 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| |X| |O|O| | 'O' Won! | | | | |X| |X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| | |X| | | |X| | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | |X|X| | |X|O|O| |O|X| | |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | | |X| |O| | | |X|O| | | |O|X| |O|X| | |X|O| | | |O|X| |O|X|O| |X|O| | | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | |O| |O| 'X' Won! | |X| | | |X| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| |O| | | | |X| | | |O|X| |O| | | | |X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O| | | |O|X|X| |X|O|X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | | |X| | | |O| |O|X|X| |O| |X| | | |O| |O|X|X| |O|X|X| | | |O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | |O| |O| | |X|X| |O| | | |O| |O| | |X|X| |O|X| | 'O' Won! |O| |O| |O|X|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| |O| |O| | | | | |X| 'X' Won! |X| |O| |O|X| | | | |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X| | | |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| | | |O| | | |O| |X|O|X| |X| |O| | | |O| |X|O|X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'O' Won! |O|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| |X| |O| | | |X| |O| |O| |X| |O| |X| |X| |O| |O| |X| |O|O|X| |X| |O| |O| |X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |O| | |O|X| | |O| | |X| |O| | |O|X| |X|O| | |X| |O| |O|O|X| |X|O| | |X|X|O| |O|O|X| |X|O| | It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O|X| | |O| | | |X|X| | |O|X| | |O|O| | |X|X| | |O|X| |X|O|O| | |X|X| |O|O|X| |X|O|O| | |X|X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| |O| | | | | |X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X| | |X|O|X| |O|X|O| |O|X| | It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | |X|X| | |O|O| | | |O| | |X|X| | |O|O| | | |O|X| |X|X| | 'O' Won! |O|O| | | |O|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X| | | | | | |O|X|X| 'O' Won! |O|X| | |O| | | |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | | | | | |X|O|O| |O|X| | | | | | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | |O|X| | 'X' Won! |X|O|O| |O|X| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| |O|X| | | | | | 'X' Won! | |O|X| |O|X| | |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| | |X| | |O| |X| |O|O|X| | |X| | |O| |X| |O|O|X| |X|X| | |O| |X| |O|O|X| |X|X| | |O|O|X| 'X' Won! |O|O|X| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X|X| | |O| | | |X| |O| |X|X|O| |O| | | |X| |O| |X|X|O| |O| |X| |X| |O| |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | |X|O| |O| |X| | | |O| | |X|O| |O| |X| 'X' Won! |X| |O| | |X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | |X| | | |X|X|O| | |O| | |X| | | |X|X|O| | |O| | |X|O| | |X|X|O| | |O|X| |X|O| | |X|X|O| |O|O|X| |X|O| | It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | | |O| | | |X| | |O|X|X| | |O| | | |X| | 'O' Won! |O|X|X| | |O| | | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| | | |X|O| | 'X' Won! |X| |O| |X| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | 'O' Won! | |X| | |O|O|O| | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| |X| | | |O| | |X| | |O|O|X| 'X' Won! |X| |O| | |X| | |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | |O|O| | 'X' Won! | | | | |X|X|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | | | | |O| | | |O|X|X| | | | | |O|X| | |O|X|X| | | | | |O|X|O| 'X' Won! |O|X|X| | |X| | |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X|O| | 'X' Won! |X|O|O| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | | |O| | | | |X| |X|O| | | |O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O| | 'O' Won! |X|O| | |O|O|X| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| | |O|O| | | | | |X| |X| 'X' Won! | |O|O| | | | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| |X| | |O| | |X| |O| |O| |X| | |O| | |X|X|O| |O| |X| | |O| | |X|X|O| |O| |X| |O|O| | |X|X|O| |O| |X| |O|O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | | | |O|X|O| |X| | | | |X| | |O|X|O| |X| | | | |X| | |O|X|O| |X| |O| |X|X| | |O|X|O| |X| |O| |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | |O| | |X| |O| | |O|X| | |O|X| |X| |O| |O|O|X| | |O|X| |X|X|O| |O|O|X| | |O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X|X| | | | |O| |O| | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O|O| | |X|X| | |O|X|O| |O|O|X| |X|X| | |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O|X|O| 'O' Won! |O|X| | |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | 'X' Won! | | |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| | | | | |X|O| | | |O|X| |O| | | |X|O| | |X|O|X| |O| | | 'O' Won! |X|O| | |X|O|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X|X| | | |O|O| 'O' Won! | | |X| |X|X| | |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | | |X| |O| | | |X|O|O| | | |X| |O|X| | |X|O|O| | | |X| |O|X| | |X|O|O| | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |O| |X|X| | | | |O| | | |O| |X|X| | | | |O| | |O|O| |X|X| | | |X|O| | |O|O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X|O|X| | | |O| |X|O|O| 'X' Won! |X|O|X| | |X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| | |O|X| |O| |O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| | |X| | |O| | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X|X| |O| | | |X|O|X| |O|X|X| |O| |O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | | |O|X| | |X|X| | |O| | | |O|X| | |X|X| |O|O| | |X|O|X| | |X|X| |O|O| | 'O' Won! |X|O|X| | |X|X| |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | | | | |X|X| |O|O| | | | | | |O|X|X| |O|O| | | | |X| |O|X|X| 'O' Won! |O|O| | |O| |X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| | | | |O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X|X| | |O|O| | |X|X|O| |X|X|O| 'X' Won! |O|O|X| |X|X|O| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | | | 'O' Won! |O|X|X| | |O| | | | |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| |X|X| | | |O|O| |X| |O| |X|X| | | |O|O| 'O' Won! |X| |O| |X|X|O| | |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | |O| | |X|X|O| | |X| | | |O| | |X|X|O| | |X|O| | |O| | 'X' Won! |X|X|O| | |X|O| | |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| |X|O| | |X|X| | | |O|O| |X|O|O| |X|X| | 'X' Won! | |O|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| |X|X| | |O| |O| 'X' Won! | | |O| |X|X|X| |O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| |X|X| | 'O' Won! | | |O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X|X| |X| | | |O| |O| |O|X|X| 'X' Won! |X| | | |O|X|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | | |O|O| | |X|X| | | | | | |O|O|X| |X|X| | |O| | | |O|O|X| 'X' Won! |X|X|X| |O| | | |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | |O| |X| |O| |X|O| | 'X' Won! |X| |O| |X| |O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | |O| | |O| | | |X| |X| |X|O| | |O| | | |X| |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| |X| |X| |X| |X|O|O| |O|O|X| 'X' Won! |X|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| | | |X|O| |X| | | |O|O|X| | |X|O| |X| |O| |O|O|X| | |X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X| |X| | |O|O| | | |X| |X| |X| | |O|O| | |O|X| 'X' Won! |X|X|X| | |O|O| | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X|O| |X|O| | |O|X| | |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X| | |O|X| |O| | | |O| |X| |X|O|X| |O| | | |O|O|X| |X|O|X| |O| | | |O|O|X| |X|O|X| |O|X| | 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | |X| | | | |X|O| |O| | | |X| | | |O|X|O| |O| | | 'X' Won! |X| | | |O|X|O| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | | |X|O| |X| | | |O| | | |O|X|O| |X| | | |O| | | |O|X|O| |X|X| | |O|O| | |O|X|O| |X|X| | 'X' Won! |O|O| | |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | |X| |O|X| | |O|O| | | | |X| |O|X|X| |O|O| | | | |X| 'O' Won! |O|X|X| |O|O| | |O| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | | | | | |O|O|X| |O|X| | | | |X| |O|O|X| |O|X| | | |O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | |X| | | | |X| | 'O' Won! |O|O|O| |X| | | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X|O| |O|X| | | | | | |X|X|O| |O|X| | | | | | |X|X|O| |O|X| | | | |O| |X|X|O| |O|X|X| | | |O| |X|X|O| |O|X|X| |O| |O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| |O| | | | | |X| |X| |O|X|O| | | | | |X| |X| |O|X|O| | | |O| 'X' Won! |X| |X| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O|X|O| |O| | | |X| |X| |O|X|O| |O|O| | 'X' Won! |X|X|X| |O|X|O| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| | |O| | 'X' Won! |X|O|O| | |X|X| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X|O| | |X|O| | | | | |O|X|O| | |X|O| | | |X| |O|X|O| | |X|O| |O| |X| 'X' Won! |O|X|O| | |X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| |X| |O|X|O| | |X|O| |X|O|X| It's a stalemate! |O|X|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | | |O| | | |X| | |X|O| | | |O| | |O|X| | |X|O| | | |O| | |O|X| | |X|O|X| | |O| | |O|X|O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|O| | |O| | |O|X|X| | |X|O| | |O|O| |O|X|X| | |X|O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X| | |X|X| | | | |O| |O|X| | |X|X| | | |O|O| |O|X| | |X|X| | |X|O|O| |O|X| | |X|X|O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X| |O| |O|O| | |X|O|X| |X| |O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X|O|X| | |O| | | | | | |X|O|X| |X|O| | | | | | 'O' Won! |X|O|X| |X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| | | |X| |O| | |O|O| 'X' Won! |X| | | |X| |O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| |O| |O| | | | | |X| |X|X|O| |O| | | | | |X| |X|X|O| |O| |O| |X| |X| |X|X|O| |O| |O| 'O' Won! |X| |X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X| | |X| |X| |O| | | |O|X|O| |X| |X| |O|X| | |O|X|O| |X| |X| |O|X| | |O|X|O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |X| |X| | | | |O| | |O| |X| |X| | | | |O|X| |O| |X| |X| | | | |O|X| |O| |X| |X| |O| |X|O|X| |O| |X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O| | | |X| | |O|X|X| |O|O| | | |X| | |O|X|X| |O|O| | | |X|X| 'O' Won! |O|X|X| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | |X|O|O| |X| |O| | | |X| |X|O|O| |X| |O| | | |X| |X|O|O| |X|O|O| | |X|X| |X|O|O| |X|O|O| 'O' Won! |O|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | | | |X|O|O| |X|X| | |O| | | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| |X|O|O| 'X' Won! |X|X|X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| |X| | |O| | | |O|X| |O| |X| | |O| | |X|O|X| |O| |X| |O|O| | |X|O|X| |O| |X| 'X' Won! |O|O|X| |X|O|X| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | |X| | |O|O| | | | |X| |O|X| | |O|O| | | | |X| |O|X| | |O|O|X| | | |X| |O|X| | |O|O|X| | |O|X| |O|X| | |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | | |O|X|O| |O|X| | | | |X| |O|X|O| |O|X| | 'O' Won! |O| |X| |O|X|O| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | |O|O| | |X|O| |X| |X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| | |O|O| 'O' Won! |X|X|O| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | |X|X| | | |O|X| | |O| | |X|X|O| | |O|X| | |O|X| |X|X|O| | |O|X| | |O|X| |X|X|O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | | | |O|X|X| | | |O| | | | | |O|X|X| | |O|O| | | | | |O|X|X| | |O|O| |X| | | |O|X|X| | |O|O| |X|O| | |O|X|X| | |O|O| |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | |X| | | |X| | |O| |O| | |X|O| | |X| | |O| |O| |X|X|O| 'O' Won! | |X| | |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X|X| | 'O' Won! |X| | | |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | | | |O| |X|O|O| |X|X| | |O| |O| |X|O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X|O| |O| | | | | |X| |X|X|O| 'O' Won! |O| | | | |O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X|X| | | | | |X|O|O| |O|X|X| | | |X| |X|O|O| |O|X|X| |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X|X| | |O|O| | | |X| | |X|X| | |O|O| | |O|X| | |X|X| |X|O|O| | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | |X| | |O|X| | | |O| 'O' Won! |O| |X| | |O|X| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |O|X| | | |X| | |O|O| | |O|X|X| 'O' Won! | |X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | |O| | |X| |O| | | |X| |X|O| | |X| |O| | | |X| |X|O| | |X|O|O| | | |X| |X|O|X| |X|O|O| | | |X| |X|O|X| |X|O|O| |O| |X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | | | |O| | |X|X| |O|O| | | |X|O| | |X|X| |O|O| | |O|X|O| | |X|X| |O|O| | |O|X|O| | |X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O| | | |O|X|X| |X|O|X| |O| | | |O|X|X| |X|O|X| |O|O| | 'X' Won! |O|X|X| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | | |O|X| |X| | | |O| | | | |O|X| |X| | | |O|O| | | |O|X| |X| |X| |O|O| | 'O' Won! | |O|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O| |X| | |X|X| |O| | | |O|O|X| | |X|X| 'X' Won! |O| |X| |O|O|X| | |X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X| | |X| | 'O' Won! |O| | | |O| |X| |O|X| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| |O|O| | |O|X|X| | |X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| |O| | | |X| | |O| | |X| |O| | |X|X| | |O| | |X|O|O| | |X|X| | |O| | 'X' Won! |X|O|O| | |X|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| | | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| |X|O| | |X| |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'X' Won! |X|O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O| |X| | |X| | |O| |X| 'O' Won! |O| |X| |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | | |O| |O|X|X| | |O| | | |X|O| |O|X|X| | |O| | |O|X|O| |O|X|X| 'X' Won! |X|O| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| | | |X|X|O| |O| |X| |O|O| | |X|X|O| |O| |X| |O|O| | |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O| |X| |O|X| | |X|O|O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| | | |X| | |O|X| | | |O|O| | |X| | |O|X| | |X|O|O| | |X| | |O|X|O| |X|O|O| | |X| | |O|X|O| |X|O|O| | |X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| |O| | | |X| |O| |X| |X| |O| | |O|X| |O| |X| |X| |O| | |O|X| |O| |X| |X|X|O| |O|O|X| |O| |X| |X|X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| 'O' Won! |X|X|O| | | |O| | | |O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | |O|X| | |X|O| |X|O| | | |O|X| | |X|O| |X|O|O| | |O|X| |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| |O| | | |X|X| | | |O|X| |O| | | |X|X| | | |O|X| |O| |O| |X|X| | |X|O|X| |O| |O| 'O' Won! |X|X| | |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | |X|O| | |X| | |X|O| | |O|X|O| | |X| | |X|O| | |O|X|O| |X|X| | |X|O| | |O|X|O| |X|X| | |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| |X|O| | |X| | | |O|O|X| |X|O| | |X| | | |O|O|X| |X|O|X| |X| | | 'O' Won! |O|O|X| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O|X| | |X| | 'O' Won! | | |O| | |O|X| |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O|X| 'O' Won! | |O| | | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | |X| | | |X| |O|O| | 'O' Won! | | |X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O|X| | |O| | | |X|X| |O|O|X| | |O| | | |X|X| 'X' Won! |O|O|X| | |O|X| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| |O|X|O| | | | | |X| |X| |O|X|O| | | | | |X|O|X| |O|X|O| | | |X| |X|O|X| |O|X|O| 'O' Won! |O| |X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X| |X| |O|X|X| 'O' Won! |O| |O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | |X| | | | |X| |O| |O| | |X|O| | | |X| |O| |O| |X|X|O| | | |X| |O| |O| |X|X|O| | |O|X| |O|X|O| |X|X|O| | |O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | |O| |X| |O| | |X|X| | | |O| |X|O|O| | |X|X| | | |O| 'X' Won! |X|O|O| |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | |X| | | |X|O|O| | |X| | |X| | | |X|O|O| |O|X| | |X| | | |X|O|O| |O|X|X| |X|O| | |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| | | |X|X| |O|O| | |X|O| | | |X|X| |O|O|X| |X|O| | | |X|X| |O|O|X| |X|O|O| | |X|X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | |X| |X| | |O|O| | |O| | |X| |X| |X|O|O| | |O| | |X| |X| 'O' Won! |X|O|O| | |O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | |X| | |X| | | |O| |O| | |X| | |X| |O| |O| |O| | |X| | |X| |O| |O|X|O| 'O' Won! | |X|O| |X| |O| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| | | |X| |X| | |X|O| 'O' Won! |O| | | |X|O|X| | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| |X| | |O| | |O|O|X| |X| |X| 'X' Won! | |O| | |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| |O| | | | | |X| |X|X|O| |O| | | |O| |X| |X|X|O| |O| | | |O|X|X| |X|X|O| |O| | | |O|X|X| |X|X|O| |O| |O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| | |O| | |X|O|X| | | |O| |X|O| | |X|O|X| | | |O| |X|O|O| |X|O|X| 'X' Won! |X| |O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X|O| | |O| | | |X| |X| |X|O| | |O| | | |X|O|X| |X|O| | |O| | | |X|O|X| |X|O|X| |O| | | 'O' Won! |X|O|X| |X|O|X| |O|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| | |X| | |X|O|O| | | |O| |X|X| | |X|O|O| 'O' Won! | | |O| |X|X|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| | | |O| | |X| | |O|O|X| | | |O| |X|X| | |O|O|X| |O| |O| |X|X| | 'X' Won! |O|O|X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| |X| | | | | |X| |O| |O| 'O' Won! |X| | | | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | | | |X| |O|O| | |O|X| | | |X|X| |O|O| | |O|X| | | |X|X| |O|O| | |O|X|O| 'X' Won! |X|X|X| |O|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | |O| | |X|O| | |O|X| |X| |O| | |X|O| | |O|X| |X| |O| |O|X|O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X|O| | | | | |O|O| | |X|X|O| | |X| | |O|O| | |X|X|O| |O|X| | |O|O| | 'X' Won! |X|X|O| |O|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O| |O| |X|O| | | |X| | |O|X|O| |X|O|O| | |X| | |O|X|O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| 'X' Won! | | |X| |O|X| | |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| |X| | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O|X|X| | | |O| |X|X|O| |O|X|X| |O| |O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | 'X' Won! |X| |O| |O|X| | | | |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | |O|X| | |X| |O| |O|X| | |O|X|X| |X| |O| |O|X| | |O|X|X| |X|O|O| |O|X| | It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| |O| |O| |X| |X| |X| |O| |O| 'O' Won! |O| |X| |X|O|X| |O| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | |X| | | | | |O| 'X' Won! |X|O| | |X| | | |X| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| |X| | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X|X| | |O|X| | | | |O| |X|X| | |O|X| | |O| |O| |X|X| | |O|X|X| |O| |O| 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| |X| | |O| | | |X| | |O| |X| |O|O| | | |X| | |O| |X| |O|O|X| | |X| | |O|O|X| |O|O|X| |X|X| | |O|O|X| |O|O|X| 'O' Won! |X|X|O| |O|O|X| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | | | | |X| | | |O|X|O| | |O| | |X| | | |O|X|O| | |O| | |X|X| | |O|X|O| |O|O| | |X|X| | 'X' Won! |O|X|O| |O|O| | |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |O| | |X| | |O|X|O| | | |O| |X|X| | 'O' Won! |O|X|O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| |O| | |O|X| |X|X| | |O| |O| | |O|X| |X|X|O| |O| |O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X|O| |X| | | |O| | | | |X|O| |X|X| | |O| | | | |X|O| |X|X| | |O| |O| 'X' Won! | |X|O| |X|X| | |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| | |X|X| |O| |O| | | |O| | |X|X| |O|X|O| | | |O| |O|X|X| |O|X|O| 'X' Won! | |X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | |O|O|X| | |X| | | | | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O| |X| |O|O|X| |X|X| | |O|O|X| |O|O|X| |X|X| | 'X' Won! |O|O|X| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | |X| |X| | |X|O| |O|O| | |X| |X| | |X|O| |O|O| | |X| |X| |X|X|O| 'O' Won! |O|O| | |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | | |O| | |X|O|X| |O|X| | | |O|O| |X|O|X| |O|X|X| | |O|O| |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| |X| | |X| | | | | | |O| |X| | |X|O| | | |X| |O| |X| | |X|O| | | |X| |O| |X| |O|X|O| | | |X| |O|X|X| |O|X|O| 'O' Won! |O| |X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O| |O|X|O| | | | | | |X|O| |O|X|O| |X| | | 'O' Won! | |X|O| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | 'O' Won! |O| |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X| | | |O|X| |X| |O| | |X|O| | |O|X| |X| |O| | |X|O| | |O|X| |X|X|O| 'O' Won! |O|X|O| | |O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O| |X|X| | | |O|O| | | |O| |X|X| | | |O|O| |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| |O| | | |X|X|O| | | |X| |O| | | 'O' Won! |X|X|O| | |O|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O| | | |X| | |X|O|O| | |O| | | |X|X| |X|O|O| | |O| | |O|X|X| |X|O|O| | |O| | |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | |O|X| |O|X| | | |X| | | |O|X| |O|X| | | |X| | |O|O|X| |O|X|X| | |X| | |O|O|X| |O|X|X| | |X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | | | |O| |X|O|O| |X| |X| | |O|O| |X|O|O| |X| |X| 'X' Won! |X|O|O| |X|O|O| |X| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O|O| | |X| |X| |O| | | |O|O|X| |X| |X| |O|O| | |O|O|X| |X| |X| 'X' Won! |O|O|X| |O|O|X| |X| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X|O|O| | |X|X| | | | | |X|O|O| |O|X|X| | | | | |X|O|O| 'X' Won! |O|X|X| | |X| | |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | |O| | | | | |X|X|O| | | |O| |X| | | |X|X|O| |O| |O| |X| | | |X|X|O| |O|X|O| |X| | | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | |X|O| |O|O| | |X| | | | |X|O| |O|O|X| |X| | | | |X|O| 'O' Won! |O|O|X| |X|O| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | |X|O| | | |X|O| | | | | |X|O|O| | |X|O| | | | | |X|O|O| | |X|O| |X| | | |X|O|O| | |X|O| |X|O| | 'X' Won! |X|O|O| | |X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | | |X| | |X| | |O| |O| | | |X| | |X|O| |O| |O| | |X|X| | |X|O| 'O' Won! |O|O|O| | |X|X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X|O|O| 'X' Won! | |O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | |O| | |X|O| | |X|X| | | |O| | |X|O| | |X|X| | | |O| | |X|O|O| |X|X| | | |O|X| |X|O|O| |X|X| | |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | |O| | |X|X|O| | | |X| | |O| | |X|X|O| | | |X| |O|O| | |X|X|O| |X| |X| |O|O| | 'O' Won! |X|X|O| |X| |X| |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| | | |X|O| |X| |O| | |X| | 'O' Won! | |X|O| |X| |O| | |X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| |X| |O| | | | | |X| |O| |X| |O|O| | | | |X| |O|X|X| |O|O| | 'O' Won! |O| |X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | |X|X| |O| | | |O| | | 'O' Won! |O|X|X| |O| | | |O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | | |X|X| | |O| | |O| | | |O|X|X| | |O|X| |O| | | |O|X|X| | |O|X| |O| |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| | |X|X| | |O| | | |X|O| | |X|X|O| |O| | | |X|O| | |X|X|O| |O|X| | |X|O| | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | |O| | | |O|X|X| | |O| | |O|X| | |O|X|X| | |O| | |O|X|O| |O|X|X| 'X' Won! |X|O| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| |X|O| | | | |O| |X|X|O| |X|O| | | | |O| |X|X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'X' Won! |X|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| |O| |O|X| | |X|X| | |O| |O| |O|X|X| |X|X| | |O| |O| |O|X|X| |X|X|O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X| |X| | |O| | |X| |O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'X' Won! |X|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O|O| | | |X| | |X|O| | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | |X| | |O| | | |X|O| | |O|X| | |O| | | |X|O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| | |O|O| | | | | |X|X|O| |X|O|O| | | | | |X|X|O| |X|O|O| | |O| | |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | |X| | | |O|O| | | |X| | |X| |O| |O|O| | | |X| | |X|X|O| |O|O| | | |X| | |X|X|O| |O|O| | | |X|O| |X|X|O| |O|O|X| | |X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | | |O| | | |X| |X| |O|X| | |O|O| | |X| |X| |O|X| | |O|O| | |X| |X| |O|X|X| 'O' Won! |O|O|O| |X| |X| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | |O|X| |X| |O| | |X| | | |O|X| |X| |O| | |X| | | |O|X| |X|O|O| | |X| | |X|O|X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| | |O|O| | | |X| |X|O|X| | |O|O| | |X|X| |X|O|X| | |O|O| |O|X|X| |X|O|X| 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | 'O' Won! |O|X| | |O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O|X| | | |O| | |X| |X| |O|X| | | |O|O| |X| |X| 'X' Won! |O|X| | | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| 'O' Won! |X| | | | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| 'O' Won! |O| |X| |O| | | |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | | |X| |X|O| | | | |O| | | |X| |X|O| | |X| |O| | | |X| |X|O| | |X| |O| | |O|X| |X|O|X| |X| |O| | |O|X| |X|O|X| |X| |O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | |O| | |X| |X| | |O| | | |O|X| |X| |X| | |O| | | |O|X| |X| |X| |O|O| | 'X' Won! | |O|X| |X| |X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X|X| | |O| |O| | |X| | |X|X|O| |O| |O| 'X' Won! | |X| | |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X|X| | |O|O| | | |O| | |X|X| |X|O|O| | | |O| | |X|X| |X|O|O| | | |O| |O|X|X| |X|O|O| |X| |O| |O|X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| | |O|O| |X|X|O| 'X' Won! |X|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O|X| | | |O| |X|X| | | |O|X| | |O|O| |X|X| | |X|O|X| | |O|O| |X|X| | 'O' Won! |X|O|X| |O|O|O| |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X|X| | | | | | |X|O|O| |X|X| | | | |O| 'X' Won! |X|O|O| |X|X| | |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |X|X| | |O| |O| | |X|O| |X|X| | |O| |O| |X|X|O| |X|X| | 'O' Won! |O| |O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | |X| | | |X| |O| |O|O| | |X| | | |X|X|O| |O|O| | |X| | | 'O' Won! |X|X|O| |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O| | | 'X' Won! |O|O|X| |X|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| |O| | | | | |X| |O|X|O| |O|X| | | | |X| |O|X|O| 'O' Won! |O|X| | |O| |X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |O| | | | | |X|O| | |X| |O| | | | | |X|O| | |X| |O| | | |O| 'X' Won! |X|O| | |X| |O| |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| | | | |O|X| | |X|O| |X| |O| | |O|X| | |X|O| |X| |O| | |O|X| |X|X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | | |X| | | | | |O|X|O| | |O|X| | | | | |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | |O| |X|X|O| | | | | | | |O| |X|X|O| | | |X| | | |O| |X|X|O| | |O|X| 'X' Won! |X| |O| |X|X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | |O|X| |X| | | | |O|O| | |O|X| |X| |X| | |O|O| 'O' Won! | |O|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| |O| | | | |X| | |X| |O| |O| |X| |O|X| | |X| |O| |O| |X| |O|X|X| |X| |O| |O| |X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | |O| | | |O| |X| |X|X| | |O| | | |O| |X| |X|X|O| |O| | | |O| |X| |X|X|O| |O| | | |O|X|X| 'O' Won! |X|X|O| |O|O| | |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X|X|O| |O| |O| | | |X| |X|X|O| |O| |O| | |O|X| 'X' Won! |X|X|O| |O|X|O| | |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | |O| |X| |O| | |O|X| | |X|O| 'O' Won! |X| |O| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | |X|X| | | | |O| | | | | |X|X| | | |O|O| | | |X| |X|X| | | |O|O| |O| |X| |X|X| | | |O|O| |O|X|X| |X|X| | | |O|O| 'O' Won! |O|X|X| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | |X|O|O| | | | | | |X| | |X|O|O| | |O| | | |X| | |X|O|O| | |O|X| | |X|O| |X|O|O| | |O|X| | |X|O| |X|O|O| |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X|X|O| |O| | | |X|O| | |X|X|O| |O| | | 'X' Won! |X|O| | |X|X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X|O| |X| | | | |O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X| | | |O|O|X| |X|X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | | | | | |O|O|X| | |X|O| | | | | |O|O|X| |X|X|O| 'O' Won! |O| | | |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | | | |O|X|X| | |O|O| | | | | |O|X|X| |X|O|O| | | | | |O|X|X| 'O' Won! |X|O|O| | |O| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| 'X' Won! | |O|X| | |O|X| | | |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | |O| |O| |X| |O| |X| | | |O| |O| |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O| |O| |X| |X| | |O| | |O|X|O| |X| |X| | |O| | |O|X|O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| |X| | |O| | |O|X|X| |O| |X| | |O|O| |O|X|X| |O| |X| |X|O|O| 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X|X|O| | | | | |O|O| | |X|X|O| |X| | | |O|O| | |X|X|O| |X|O| | |O|O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| |O|X|X| | |X| | | | |O| |O|X|X| | |X|O| | | |O| |O|X|X| | |X|O| |X| |O| |O|X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | |X|O| | | |X| | | |O| |O|X|O| | | |X| | | |O| |O|X|O| | | |X| |X| |O| 'O' Won! |O|X|O| | |O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | | | | | |X|O| |X|O| | | | | | |O|X|O| |X|O| | | | |X| |O|X|O| |X|O| | |O| |X| |O|X|O| |X|O|X| |O| |X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X| |X| |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X| |X|O|X| | | |O| |O| |X| |X|O|X| | | |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| |O| | | |X| | | | |X|O| |O| | | |X| |X| | |X|O| |O|O| | |X| |X| | |X|O| 'X' Won! |O|O| | |X|X|X| | |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| | |O| |O| | |X|X| | |O|X| |O| |O| | |X|X| |O|O|X| |O| |O| | |X|X| |O|O|X| |O|X|O| 'O' Won! |O|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X|X| | |O| | | |O|O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O| | | |O|O| |O|X|X| |X|O|X| | |O|O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| |X| |O| | | | |O|X| |O| |X| 'X' Won! |O| |X| | |O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X|O| | |O| | 'X' Won! |X|O|X| |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| 'O' Won! | | |X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| 'O' Won! |O| | | |X|O|X| | | |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | |X|O|X| |O|X| | |O|X|O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | | |O| | |X| |O| | |X| | | |O|X| |X| |O| | |X| | | |O|X| |X|O|O| | |X| | |X|O|X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | |X| |O| |X| |O| | |O| | 'X' Won! |X| |O| |X| |O| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| | |O| | |X|X| | 'O' Won! |O| |X| | |O| | |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X|X| | |O|O| | |O| | | |X|X| | 'X' Won! |O|O| | |O| | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | |O| | | |X|X| |X| |O| | |O| | | |X|X| |X| |O| | |O|O| | |X|X| |X|X|O| | |O|O| | |X|X| 'O' Won! |X|X|O| | |O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |X|X| | |O| | | | |X|O| |X|X|O| |O| | | | |X|O| |X|X|O| |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| | |O| | |O|X| | | | |X| | |O| | |O|X| | |X| |X| |O|O| | |O|X| | |X| |X| 'X' Won! |O|O|X| |O|X| | |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X| | | |O| |X| | |O| | |X| | | |O|X|X| | |O| | |X| | | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | | | |X|O| | |X| |O| | | | | |X|O| | |X| |O| |O| | | |X|O| | |X| |O| |O| |X| |X|O|O| |X| |O| |O| |X| |X|O|O| |X| |O| |O|X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | |X| | |O| | |X| | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |X| | |O|X| |O|O| | |X| |X| 'X' Won! | |O|X| |O|O|X| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | |X| | | | |X|X| |O|O| | |X|O| | | |X|X| |O|O| | 'X' Won! |X|O| | | |X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |X| | |O|X| | |O|O| |X| |X| | |O|X| |X|O|O| |X| |X| | |O|X| 'O' Won! |X|O|O| |X|O|X| | |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O|X| | |O|O|X| |X|X|O| |O|X| | It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | |X|O| | | | |X| | |O| | |X|O| | | | |X| | |O|O| |X|O| | | | |X| |X|O|O| |X|O| | |O| |X| |X|O|O| |X|O|X| |O| |X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| |O| |O| | |X| | | |X|X| |O| |O| | |X| | | |X|X| |O| |O| | |X|O| 'X' Won! |X|X|X| |O| |O| | |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| |O| | | | |O| | | | |X| |O| | | |X|O| | | | |X| |O|O| | |X|O| | | | |X| |O|O| | |X|O| | |X| |X| 'O' Won! |O|O|O| |X|O| | |X| |X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | |X|O| |X| |O| | |O| | | |X|O| |X| |O| | |O|X| | |X|O| |X| |O| |O|O|X| | |X|O| |X|X|O| |O|O|X| | |X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| | |O|O| |X| | | |X|O|X| | |O|O| |X| | | 'O' Won! |X|O|X| | |O|O| |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | 'O' Won! |X| |O| |X| |O| | | |O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| |X| |X|O| | |O| | | |X|O|X| |X|O|X| |O| | | |X|O|X| |X|O|X| |O| |O| |X|O|X| 'X' Won! |X|O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X|O| | |X|O|O| | |X| | |X|O| | |X|O|O| |O|X| | |X|O| | |X|O|O| |O|X|X| |X|O| | It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X|O|X| |O| | | | | |X| |X|O|X| |O| | | |O| |X| |X|O|X| |O|X| | |O| |X| |X|O|X| |O|X|O| |O| |X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | |X| | | | |O|O| | |X| | 'O' Won! |X| | | |O|O|O| | |X| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | |X| |X|O|O| | | |O| | | |X| |X|O|O| | | |O| | |X|X| |X|O|O| | |O|O| | |X|X| |X|O|O| |X|O|O| | |X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | |X| | |O|X| | | |O| |O| |X| | |O|X| |X| |O| |O| |X| | |O|X| |X| |O| |O|O|X| |X|O|X| |X| |O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| |O| |X| |X| | |O|O| |X| |O| |X|O|X| | |O|O| |X| |O| 'X' Won! |X|O|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | | |X| |O| |O| | |O|X| | | |X| |O| |O| |X|O|X| | |O|X| |O| |O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X| |O| |X| |X|O|O| | | |X| |O| |X| |X|O|O| |O| |X| |O| |X| 'X' Won! |X|O|O| |O|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X| | | |O|O| | |O|X| | |X| | | |O|O| |X|O|X| | |X|O| | |O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| | |O| | | |X| | |O| |X| | |O| | | |X|O| |O| |X| | |O| | | |X|O| |O|X|X| | |O| | |O|X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X|X| 'O' Won! |O|O|O| | |X| | | |X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| |X| |O| |O| | | |X|O|X| |X| |O| |O| | | |X|O|X| |X|X|O| |O| | | |X|O|X| |X|X|O| |O|O| | 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| | | |O| | | |X| |O| |X| |X| |O| |O| |X| |O| |X| |X| |O| 'X' Won! |O| |X| |O|X|X| |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X| | | |X|X| | |O|O| |O|X| | | |X|X| 'X' Won! |X|O|O| |O|X| | | |X|X| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | |O| | | |O|X|X| |X| | | |O| | | |O|X|X| |X| |O| |O| | | |O|X|X| |X| |O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | 'X' Won! |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | |O| | | |X| |O| |X| |X| |O| | | |X| |O| |X| |X| |O| | |O|X| |O|X|X| |X| |O| | |O|X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| |O|X| | | |O|X| | |X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | | |O| | | |X|X| | |O| |X| |O|O| | |X|X| | |O| |X| |O|O|X| |X|X| | |O| |X| |O|O|X| |X|X| | |O|O|X| 'X' Won! |O|O|X| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| | | | | |X| | |O|O| |X| | | | |X|X| | |O|O| |X|O| | | |X|X| | |O|O| |X|O| | | |X|X| |X|O|O| |X|O|O| | |X|X| |X|O|O| 'X' Won! |X|O|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X|O| |X| | | |X|O| | | |X|O| |X| | | |X|O| | | |X|O| |X|O| | 'X' Won! |X|O| | | |X|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | |O|X| |O|X|O| | | |X| | |O|X| |O|X|O| | |O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X| |O| | | |O| | |O|X| |X| |O| | | |O| |X|O|X| |X|O|O| | | |O| |X|O|X| 'X' Won! |X|O|O| |X| |O| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| | |X|O| | |O|X|X| |X|O| | |X|O| | 'O' Won! |O|X|X| |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| |X| | | | |O| | |O| |X| |X| | | | |O| | |O|O|X| |X| | | |X|O| | |O|O|X| 'O' Won! |X| |O| |X|O| | |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | |O|X|O| | | | | 'X' Won! | |X| | |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X|O|O| | | |X| | | | | |X|O|O| | |X|X| | | |O| |X|O|O| | |X|X| |X| |O| |X|O|O| | |X|X| 'O' Won! |X| |O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | | | |X| | | | |X|O| | |O| | 'X' Won! |X| | | | |X|O| | |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| |O| |X| | | | | | |X|O| |O| |X| | | | | |X|X|O| 'O' Won! |O| |X| | |O| | |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | |O| | | |X|X| |X| |O| | |O| | |O|X|X| |X| |O| | |O| | |O|X|X| |X|X|O| | |O| | |O|X|X| |X|X|O| | |O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| |X| |X| | |O| | | |O|O| |X| |X| | |O| | 'X' Won! | |O|O| |X|X|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| | | |O| |O| |X|X| | |O| |X| |O| |O| |X|X| | |O|O|X| |O| |O| |X|X| | |O|O|X| |O|X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| |O| | | | | |X| |X| |O|X|O| | | | | |X| |X| |O|X|O| |O| | | 'X' Won! |X|X|X| |O|X|O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X| | | |X|O| |O| | | |O|X|X| | |X|O| |O|O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| |X| | | |X| | | |O| |O|O|X| |X| |X| | | |O| |O|O|X| |X| |X| |O| |O| |O|O|X| 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| |O| | | 'X' Won! | |O| | |X|X|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| |X| | | | | | |O|O| |X|O|X| | | |X| | |O|O| |X|O|X| 'O' Won! | |O|X| | |O|O| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| | | |X| |X| |O| |O| |X| | | |X|O|X| |O| |O| |X|X| | |X|O|X| |O| |O| 'O' Won! |X|X| | |X|O|X| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O| | | |X| |O|X|O| | | |O| | | |X| |O|X|O| | | |O| | |X|X| |O|X|O| | | |O| |O|X|X| |O|X|O| |X| |O| |O|X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O| |X| |X| |O| | | |X| |O|O|X| |X| |O| | | |X| |O|O|X| |X|X|O| | | |X| |O|O|X| |X|X|O| | |O|X| 'X' Won! |O|O|X| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X|O|O| | |X| | | | |X| |X|O|O| | |X|O| | | |X| |X|O|O| |X|X|O| | | |X| |X|O|O| |X|X|O| | |O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | | |X| | |X| |O| |O|X|O| | |X|X| |X| |O| |O|X|O| |O|X|X| |X| |O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | |X|X| | |O| |O| | | | | |X|X| | |O| |O| | | |O| |X|X| | |O| |O| |X| |O| |X|X| | |O| |O| |X|O|O| 'X' Won! |X|X|X| |O| |O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| | | |X|O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | |O|O| | | |O| |X| |X| |X|O|O| | |O|O| |X| |X| |X|O|O| 'X' Won! | |O|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | |X|O|O| | |X|O| | | | | |X|O|O| |X|X|O| | | | | |X|O|O| |X|X|O| | |O| | |X|O|O| |X|X|O| | |O|X| |X|O|O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| | | |O|O|X| |O| | | |X| |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| |O| 'X' Won! |X|X|X| |O|O|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X|X| 'O' Won! |O| |X| |O| | | |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|O| | | | | |O|X|X| |X|O|O| | | | | 'O' Won! |O|X|X| |X|O|O| | | |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X|O| |X| | | |O|X| | 'X' Won! |O|X|O| |X|X| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | |O|X| |O|X| | | | | | |X|O|X| |O|X| | | | | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X| | |O|X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | |O| |X|X| | | | |O| |X| |O| |X|X| | | | |O| 'O' Won! |X| |O| |X|X|O| | | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | | |X| |X|O| | |O| | | |X| |X| |X|O| | |O| | | |X| |X| |X|O| | |O| |O| |X| |X| |X|O|X| |O| |O| |X|O|X| |X|O|X| |O| |O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | | |O| | |X| | |O|X| | |O| |O| | |X| | |O|X| | |O| |O| |X|X| | |O|X| | |O| |O| |X|X|O| |O|X| | |O| |O| |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |O| |O|X|X| | | | | | | |O| 'O' Won! |O|X|X| | |O| | | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | | | | |X|X| 'O' Won! |O|O|O| | | | | | |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| | |O| | |X| | | |O| |X| | |O| | |X|O| | |O| |X| | |O|X| |X|O| | |O| |X| 'O' Won! | |O|X| |X|O| | |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | |X|O| |O| |X| |X| | | |O|X|O| |O| |X| |X|X| | |O|X|O| |O|O|X| |X|X| | |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | |O| |X| | | |X| |O|O| | |O| |X| |X| |X| 'O' Won! |O|O|O| |O| |X| |X| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| |O| |X| 'X' Won! | |O|X| |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O|O| |X| | | | |X| | | |O|O| |X|X| | | |X| | 'O' Won! |O|O|O| |X|X| | | |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | |O|X| | | |O|X| | | | | |O|X|X| | |O|X| | | | | |O|X|X| |O|O|X| 'X' Won! | | |X| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | |X| |X| | |O|O| | | | | |X| |X| | |O|O| | |O| | |X| |X| |X|O|O| | |O| | |X| |X| |X|O|O| | |O|O| 'X' Won! |X| |X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O| |O| |X| |O| | |X| | |O| |O| |X|X|O| |O|X| | |O| |O| |X|X|O| |O|X|X| |O| |O| |X|X|O| 'O' Won! |O|X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | | | |O| | |X|O| |X| | | | | |O| |X|X|O| |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X|X| | 'O' Won! |O|O|O| |X|X|O| |X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | | |O| |X| |X| |O|O|X| | | |O| |X|O|X| |O|O|X| | | |O| |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X|O| | |X|O| |X|O| | | |X|O| 'O' Won! | |X|O| |X|O|O| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X| |O| | | | | |X|O|O| |X|X|O| | | | | |X|O|O| |X|X|O| | |O| | 'X' Won! |X|O|O| |X|X|O| |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| | |O| | 'O' Won! | |O|X| |X|O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| |O| | |O| | | | |X| |X|X|O| | |O| | | | |X| |X|X|O| |O|O| | | | |X| |X|X|O| |O|O| | |X| |X| 'O' Won! |X|X|O| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X|X| |X| | | | | |O| |O|X|X| |X| | | | |O|O| |O|X|X| |X| |X| | |O|O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | |O| |X| | | 'X' Won! |X| |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |O|X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | | |X|X| |O|X| | |O| |O| | |X|X| |O|X| | 'X' Won! |O|X|O| | |X|X| |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | | | |O| |O| | |X|X| | |X| | |O| |O| 'O' Won! | |X|X| | |X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| |X|O|O| |O| |X| |X| |X| |X|O|O| |O|O|X| |X| |X| 'X' Won! |X|O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | |O| |O| |X| |X|X| | | | |O| |O| |X| |X|X|O| | | |O| 'X' Won! |O| |X| |X|X|O| |X| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | | |O|X| | | |O| |X| | | | |O|X| | |X|O| |X|O| | | |O|X| | |X|O| |X|O| | | |O|X| |X|X|O| |X|O|O| | |O|X| |X|X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O|X| | |X|O| |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | |O| | | |X|X| |X| |O| | |O| | | |X|X| |X|O|O| | |O| | | |X|X| |X|O|O| | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X|X| | | |O| |X| |O| |O|X|X| | | |O| |X|X|O| |O|X|X| | |O|O| |X|X|O| |O|X|X| 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X| |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |X|X| |O|X|O| | |O|X| |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | | |O| | |O|X| | |X| | | |X|O| | |O|X| | |X|O| | |X|O| |X|O|X| | |X|O| | |X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X|O|X| |X| | | |O|X|O| |X|O|X| |X| | | 'O' Won! |O|X|O| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| |O| | |X|X| |X|O| | |O| |O| 'O' Won! |O|X|X| |X|O| | |O| |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | |O| | |X|O| | |X|X| | | |O| | |X|O| | |X|X| | | |O| |O|X|O| 'X' Won! | |X|X| | |X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | |X| |O| |X| | | | |O|O| |X| |O| |X| |X| | |O|O| |X| |O| 'O' Won! |X| |X| |O|O|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | |O| | |O| | |X| |X| | |X|O| | |O| | |X| |X| | |X|O| | |O|O| |X| |X| | |X|O| |X|O|O| |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| |O|X| | | | |O| |X|O|X| |O|X| | | | |O| |X|O|X| |O|X| | |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | |X| |X| | | |O| | |O| | |X| |X| | | |O| | |O|O| 'X' Won! |X|X|X| | | |O| | |O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X|X| | | | |O| |O| | | |X|X| | | | |O| |O| |X| |X|X| | | |O|O| |O| |X| |X|X| | 'X' Won! | |O|O| |O| |X| |X|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O|O| | | | | |X| |X| |X|O|O| | | | | |X|O|X| |X|O|O| | | | | |X|O|X| |X|O|O| | | |X| |X|O|X| |X|O|O| |O| |X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| |X|O| | | | | | |O| |X| |X|O| | |O| | | |O| |X| |X|O| | |O| | | |O|X|X| |X|O|O| |O| | | |O|X|X| 'X' Won! |X|O|O| |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X|O| | | | |O| | | | | |X|O|X| | | |O| | | |O| |X|O|X| | | |O| | | |O| |X|O|X| |X| |O| | |O|O| |X|O|X| |X| |O| 'X' Won! |X|O|O| |X|O|X| |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| | |X| | | |O| | |O|X|X| | |X| | |O|O| | |O|X|X| | |X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | | |O|O| |X| |X| | | | | |X|O|O| |X| |X| |O| | | |X|O|O| |X| |X| |O|X| | |X|O|O| |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | |X| | |X| | |O|O| | | | |X| | |X|O| |O|O|X| | | |X| | |X|O| 'O' Won! |O|O|X| | |O|X| | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| |O| | | | | |X| |X|O|X| |O| | | | | |X| |X|O|X| |O| |O| | | |X| |X|O|X| |O|X|O| | |O|X| |X|O|X| |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | |O| | 'X' Won! |X| | | |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| | |O|O|X| | | | | |X|X| | |O|O|X| | | | | |X|X| | |O|O|X| | |O| | 'X' Won! |X|X|X| |O|O|X| | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | |X|X| |O| | | | |O|O| | |X|X| |O| | | | |O|O| | |X|X| |O|X| | | |O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| |O|X| | It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X| | | |O|O| 'X' Won! | |O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O|X| | |X| |X| |O| |O| |O|X| | |X|O|X| |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O|X| | |O|O| |X| |X| | |O|X| 'X' Won! | |O|O| |X|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O| | |O| |X| |X| | | |X|O| | |O| |X| |X| | | |X|O| | |O|O|X| |X| |X| |X|O| | |O|O|X| |X| |X| |X|O|O| |O|O|X| 'X' Won! |X|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | | | 'O' Won! |O| |X| |O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X| | |O| |O| | | |X| |O|X|X| |O| |O| | |O|X| |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X|O| | |X| | | | |O| |O|X|O| |X|X| | |O| |O| |O|X|O| |X|X| | 'X' Won! |O|X|O| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| |O|X| | | |O| | | |X|X| |O|X| | | |O|O| | |X|X| |O|X| | | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O|O|X| | | |X| | | |O| |O|O|X| |X| |X| | | |O| |O|O|X| |X| |X| | |O|O| 'X' Won! |O|O|X| |X|X|X| | |O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O| | |X|X| |O| | | |X| |O| | |X|X| |O|O| | |X|X|O| | |X|X| |O|O| | 'O' Won! |X|X|O| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|O| |X| | | | |X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| 'X' Won! |X| |X| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X|X| |O| | | |O|O| | | |X|X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O| |O| |O|O|X| | |X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | |X|O|X| | | | | | |O| | 'O' Won! |X|O|X| | |O| | | |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | |X| | | | |O| |X|O| | | |X| | |O| |O| |X|O| | | |X| | |O|X|O| |X|O| | | |X|O| |O|X|O| |X|O| | |X|X|O| |O|X|O| |X|O| | 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| | | |X| | 'O' Won! |O| | | |X|O| | | |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | |O|O| | | |X|O| | |X| | |O|O|X| | |X|O| |O|X| | |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X| | | |O|X| |O|O|X| | |X| | | |O|X| 'X' Won! |O|O|X| | |X|X| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| |X| | |O| | |X|O| | |O| |X| | |O| | |X|O| | |O|X|X| | |O| | |X|O|O| |O|X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O|O|X| |X| |X| |O| | | |O|O|X| |X| |X| |O| |O| |O|O|X| 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| |X| |O| | | |X| |O| |O| |X| |O| |X| |X| |O| |O| |X| |O|O|X| |X| |O| |O| |X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | |O| | |X|X| | | |O|X| | |O| | |X|X| | |O|O|X| | |O| | |X|X| | 'X' Won! |O|O|X| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |O| |O| | |X|X| |X|O| | |O| |O| 'O' Won! |O|X|X| |X|O| | |O| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X| | |X|X| | | | |O| |O|X| | |X|X|O| | | |O| |O|X|X| |X|X|O| |O| |O| |O|X|X| |X|X|O| 'X' Won! |O|X|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | | | | | |O|X|O| |O|X|X| | | | | |O|X|O| 'O' Won! |O|X|X| | |O| | |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | |X|X| | | |O| |O| | | | |X|X| | | |O| |O| |O| | |X|X| | |X|O| |O| |O| 'O' Won! | |X|X| | |X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| | | |X|X| |X| | | |O|O| | | |X|X| |X|O| | 'X' Won! |O|O| | |X|X|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O|X| | | |O| | |X| | |X|O|X| |O| |O| | |X| | |X|O|X| |O| |O| | |X|X| 'O' Won! |X|O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O|X|O| | |X| | | | |O| |O|X|O| | |X| | | | |O| |O|X|O| | |X|X| |O| |O| |O|X|O| | |X|X| 'X' Won! |O| |O| |O|X|O| |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| | | | | |O| |O|X|O| |X|X| | | | |O| 'O' Won! |O|X|O| |X|X|O| | | |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O|X| | | |O| | |X|X| | |O|X|O| | |O| | |X|X| | 'X' Won! |O|X|O| | |O| | |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | |X| | | | |X| | |O|O| |O|X| | | | |X| | |O|O| |O|X|X| | | |X| | |O|O| |O|X|X| | |O|X| 'X' Won! |X|O|O| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O|O| | | |X| | | | | |X|O|O| | |X|X| | | | | |X|O|O| |O|X|X| | |X| | |X|O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| | |X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O|O| |O| | | |X|X| | |X|O|O| |O| | | |X|X| | |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | |O|X| | |X| | |O|O|X| |X|O|X| | |X| | 'O' Won! |O|O|X| |X|O|X| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | |X|X| | | | | 'O' Won! |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | | |X| |X|O| | | |O|O| | | |X| |X|O| | |X|O|O| | | |X| |X|O| | |X|O|O| |O| |X| |X|O| | |X|O|O| |O|X|X| |X|O| | It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| |X| |X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | | |X| | | |X| |O|X|O| |O| |X| | | |X| |O|X|O| |O|X|X| | | |X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | |O|X| | |O|O| |X| |X| | |O|X| 'O' Won! | |O|O| |X|O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| | | |O| |O| | |X|O| |X| | | |O|X|O| | |X|O| 'O' Won! |X| |O| |O|X|O| | |X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | | |O|X|O| |O|X| | 'X' Won! | |X| | |O|X|O| |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| |O|X| | | |O|X| | |O|X| |O|X| | | |O|X| | |O|X| |O|X|O| | |O|X| |X|O|X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | |X|O|X| | |O| | 'O' Won! | |O| | |X|O|X| | |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X|O| | | | | |X|O| | | |X|O| |X| | | |X|O| | | |X|O| |X|O| | |X|O| | 'X' Won! |X|X|O| |X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | |O| |O| | | |X|X| | 'X' Won! | | |O| |O| | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X| |O| |O|O| | |X| | | |X|X|O| |O|O| | |X| |O| |X|X|O| |O|O| | 'X' Won! |X| |O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | | |O| | |X| |X| |O|X| | | |O| | |X| |X| |O|X| | |O|O| | 'X' Won! |X|X|X| |O|X| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O| | |O|X|O| | |X|X| | |O| | |O|X|O| | |X|X| | |O|O| |O|X|O| | |X|X| |X|O|O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | | | |X| | 'O' Won! |O|X| | |O| | | |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | |X|O|X| |O| |O| | |X| | |X|O|X| |O| |O| | |X|X| |X|O|X| |O| |O| |O|X|X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | | | | | |X|X|O| 'O' Won! |O|X| | | |O| | |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | |X|O| |X| | | | | |O| |O|X|O| |X| | | | |X|O| |O|X|O| |X| | | |O|X|O| |O|X|O| |X| | | |O|X|O| |O|X|O| |X| |X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | |X| | | | | | |X|O|O| | |X| | | | |O| |X|O|O| | |X| | | |X|O| |X|O|O| |O|X| | | |X|O| |X|O|O| 'X' Won! |O|X|X| | |X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| |O|X| | |X| | | | |O|O| |O|X|X| 'O' Won! |X| |O| | |O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| | |O|X| |X|O| | | | |X| | |O|X| |X|O| | |O| |X| | |O|X| |X|O| | |O|X|X| |O|O|X| |X|O| | |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| | | | | |X| |X|O|X| |O|O| | | | |X| 'X' Won! |X|O|X| |O|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | |X| |O|O| | | |O|X| | | |X| |O|O| | |X|O|X| | | |X| |O|O| | 'O' Won! |X|O|X| | |O|X| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O|O| 'X' Won! | | |X| | |X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| |O| |X|X| | | |O| | |O|X|O| |X|X| | | |O| | |O|X|O| |X|X|O| | |O| | |O|X|O| |X|X|O| |X|O| | 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O|X| |O| | | | | |X| |O|O|X| |O| | | | | |X| |O|O|X| |O|X| | | | |X| 'O' Won! |O|O|X| |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | |X|X| | | |O| |O| |O| | |X|X| | | |O| |O|X|O| | |X|X| | |O|O| |O|X|O| | |X|X| 'X' Won! |X|O|O| |O|X|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X|O| | |X| | | |O| |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O|X| 'O' Won! |X|O| | |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X|O| | |X| |O| | |O| | |X|O| | |X|X|O| | |O| | |X|O|O| |X|X|O| | |O| | 'X' Won! |X|O|O| |X|X|O| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| |O| | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O|O| | | | |X| | |X|O| |O|O|X| |O| |X| | |X|O| |O|O|X| |O|X|X| | |X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X| | | |X|X| | |O| |O| |X|O| | |X|X| | 'X' Won! |O| |O| |X|O| | |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| | | | | |X|O| | |O| |X| | | | | |X|O| | |O|O|X| | | | | |X|O| | |O|O|X| | |X| | |X|O| | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | | |O| | |X|X|O| |O|X| | | |O| | |X|X|O| |O|X|O| | |O| | 'X' Won! |X|X|O| |O|X|O| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| | | | | |O|X| | |X|X|O| | | | | |O|X| | |X|X|O| | |O| | |O|X|X| |X|X|O| | |O| | |O|X|X| |X|X|O| |O|O| | It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | | |X|O| | | |O|X| | | |O| |X|O| | | |O|X| |X| |O| |X|O| | |O|O|X| |X| |O| |X|O| | |O|O|X| |X| |O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X| |O| | |X| | |O|O| | |X| |O| |X|X| | |O|O| | 'O' Won! |X| |O| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | |O| | | |X|X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | |X| |X| |O| | | |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| |X| 'O' Won! |X| |X| |O|O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O| |X| | | |O| | |X|O| |O| |X| | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O|O|X| | |X|O| |X|X|O| |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | |O| | |O|X|X| |X| | | | |O| | |O|X|X| |X| | | | |O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X|O| | |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |X| |O| | |X| | |X|O|O| |X| |O| | |X|X| |X|O|O| |X| |O| 'O' Won! |O|X|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | | | |X| |O|O|X| |X| | | |O| |X| 'X' Won! |O|O|X| |X| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | |O|X| |X| | | |O| |O| |X|O|X| |X| | | |O| |O| |X|O|X| |X| |O| |O| |O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | |O| | |X| |X| |O| |O| 'X' Won! | |O| | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | | | |X| |O| | |X|O| | |X| | |X| |O| | |X|O| |O|X| | |X| |O| | |X|O| |O|X|X| |X| |O| | |X|O| 'O' Won! |O|X|X| |X|O|O| | |X|O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| |O| | | |O| |O| |X| |X| |O| |X| |O| |O| |X| 'O' Won! |X| |O| |X|O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | |O| | | | |X| |X| |O| | |O|X| | | |X| |X| |O| | |O|X| | |O|X| |X| |O| |X|O|X| | |O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| |O| |X| |O|X| | | |X|O| |O| |X| |O|X| | 'O' Won! | |X|O| |O|O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O|O| | |X| |X| | |O| | |O|O| | |X| |X| |X|O| | |O|O| | |X| |X| |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | | |X| |O|O| | 'X' Won! |O| |X| | | |X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | |O| | | |X|X| | |O|O| | |O| |X| |X|X| | |O|O| | 'O' Won! |O| |X| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | |O|X| |X| | | |O|X| | | |O|X| |X| | | |O|X| | | |O|X| |X|O| | |O|X| | |X|O|X| |X|O| | 'O' Won! |O|X| | |X|O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | | | | |X| | | |O|X|O| | | | | |X|O| | |O|X|O| | | | | |X|O|X| |O|X|O| | |O| | |X|O|X| |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| |X| | | | | |O|O| | |X| |X| | | | | |O|O|X| |X| |X| |O| | | |O|O|X| |X| |X| 'X' Won! |O| | | |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| |X| | | | |O| | |O| |X| |X| | | |O|O| | |O| |X| |X| |X| |O|O| | |O| |X| |X|O|X| |O|O| | |O| |X| 'X' Won! |X|O|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X| |X| | |O| | |O|X|O| 'X' Won! |X|X|X| | |O| | |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O|X| |X| |O| |O| | | | |O|X| 'X' Won! |X| |O| |O|X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| | | |X|O| |O|X| | | |O| | | |X|O| |O|X| | | |O|X| | |X|O| |O|X| | |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | |X| |X|O| | | |O| | 'O' Won! | |O|X| |X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X|O| | | | |O| | | |X| |X|O| | | | |O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O|X| | |O|O| | |X|X| |X|O|X| | |O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | |X| |O| | | |O| |X| | | |X| |O| | |O|O| 'X' Won! |X| | | |X| |O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| | |X|O| |X| |X| | | |O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X|O|X| 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | |X|O| |O| | | | | |X| |O|X|O| |O| | | | | |X| |O|X|O| |O|X| | | | |X| |O|X|O| |O|X|O| | | |X| |O|X|O| |O|X|O| |X| |X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| | |X| |O| | |X|X| |O|O| | |X| |O| | |X|X| 'X' Won! |O|O| | |X| |O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| |O| |O| | | | | |X|X|O| |O| |O| | |X| | |X|X|O| |O| |O| |O|X| | 'X' Won! |X|X|O| |O|X|O| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | |O| | |X|O| | | | |X| | |O|X| |X|O| | | | |X| |O|O|X| |X|O| | | | |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O|O| |X| |X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | | | |O|X| | | |O|X| | |X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X|X| |O|X| | 'O' Won! |O|O|X| |O|X|X| |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| | | |O| |X| |X| | | |O| | |X|O| |X| |X| | |O|O| | |X|O| 'X' Won! |X|X|X| | |O|O| | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X|O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | |X|O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| | |X|O| 'X' Won! |X|O|X| |O|X|O| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X| |O| |X| | | |O|O| | |X| |O| |X| |X| |O|O| | |X| |O| |X|O|X| |O|O|X| |X| |O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| |O| |X| |X| | | |O| |O| |O| |X| |X| |X| |O| |O| |O|O|X| |X| |X| |O|X|O| |O|O|X| |X| |X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O|X| | | |X|O| |O| |X| |O|X| | | |X|O| |O| |X| |O|X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X| | |X|X| |O| |O| | |O|X| | |X|X| 'X' Won! |O| |O| | |O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |O| | |O| | |X|X|O| | |X|O| | |O| | 'O' Won! |X|X|O| | |X|O| | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | | |O|X| | | |O| |X| |X| | |O|X| | | |O| |X|O|X| | |O|X| | |X|O| |X|O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X|X| | |O| | | |O|O| | |X|X| | |O| | | 'X' Won! |O|O| | |X|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O| |X|X| | | |X| | | |O|O| |X|X| | | |X|O| |X|O|O| |X|X| | | |X|O| 'O' Won! |X|O|O| |X|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O|O| | |X| | | |X|O| | |O|O| | |X|X| | |X|O| | 'O' Won! |O|O|O| |X|X| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | | | |X| |O| |O| |X| |X| | | |X| |O| |O| 'O' Won! |X| |X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | 'X' Won! |O|X|O| | |X|O| |X|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X|O| |X|O| | | | |O| |X|X|O| |X|O| | |O| |O| |X|X|O| |X|O| | |O|X|O| |X|X|O| 'O' Won! |X|O|O| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| |X|X|O| | |X|X| 'O' Won! |O|O|O| |X|X|O| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | 'X' Won! | |X| | |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X|X| | |O|O| | | | |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X|O|O| |X|X| | |O|O| | 'X' Won! |X|O|O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| |X| | |O| | |X|X|O| |O| |X| | |O|X| |X|X|O| |O| |X| | |O|X| |X|X|O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O|X| |X| | | |O| | | | |O|X| |X| | | |O|O| | |X|O|X| |X| | | |O|O| | 'O' Won! |X|O|X| |X|O| | |O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O|O| | | | | |O|X|X| | |O|O| | | |X| |O|X|X| 'O' Won! | |O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |X| |O|O|X| | | | | | | |X| 'X' Won! |O|O|X| | | |X| | | |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| |X| | | |O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| |X| |X| |O| |X| | |O|O| |X|O|X| |O|X|X| | |O|O| |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X|O| | |X|X| |O| | | | |X|O| | |X|X| |O| | | |O|X|O| | |X|X| |O| |X| |O|X|O| | |X|X| |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | |O|O| |X| | | |X| | | 'X' Won! |X|O|O| |X| | | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| | | | |O|X| |O|O| | |X|X| | | |O|X| |O|O| | |X|X|O| | |O|X| |O|O| | |X|X|O| |X|O|X| 'O' Won! |O|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| |X| | | | |X| | |O|X|O| |X| | | |O|X| | |O|X|O| |X| |X| |O|X| | 'O' Won! |O|X|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| | |O| | |X| |O| | |X|X| | |O| | |X| |O| |O|X|X| | |O| | |X| |O| |O|X|X| |X|O| | |X|O|O| |O|X|X| |X|O| | 'X' Won! |X|O|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | | | | | 'X' Won! |O|X|O| | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| |X|X| | | |O|O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| | |O|O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |X| |O|O| | |X| |X| |O| |X| 'X' Won! |O|O| | |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | |O| | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| |X|O| | |X|O|X| |O| |O| |X|O| | |X|O|X| |O|X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| | | | | |O| |X|O|X| |O| | | | |X|O| |X|O|X| |O| | | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O|O|X| | | |O| | |X| | |O|O|X| | | |O| |X|X| | |O|O|X| | | |O| |X|X|O| |O|O|X| |X| |O| |X|X|O| 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| |X| |O| | | |X|X| | |O| |X| |O| |O| |X|X| | |O|X|X| |O| |O| 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | | |O|O| |X| | | |O|X| | |X|O|O| |X| | | 'O' Won! |O|X| | |X|O|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | |X|O| |X| | | | |O|O| | |X|O| |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X|O| | 'X' Won! |X|O|O| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | 'O' Won! |X|O| | |X|O| | | |O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| |X| | | |X|O|X| | | |O| |X| | | |X|O|X| | |O|O| 'X' Won! |X| | | |X|O|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | | |X|O| | |O| | |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X|O| | |O|X|O| | |O|X| |X|O|X| |O|X|O| 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|X| | |O| | |X| | | |O|X|X| | |O|O| |X| | | |O|X|X| |X|O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| |X|O| | | | |O| | |X|X| |X|O| | | | |O| |O|X|X| 'X' Won! |X|O| | | |X|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | |O|X|X| | |X| | | |O| | |O|X|X| 'O' Won! | |X|O| | |O| | |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| |O| | | |X|O| | 'X' Won! | | |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | |O| |X| | |O|X| | | | | |O|X|X| | |O|X| | | | | |O|X|X| |O|O|X| 'X' Won! |X| | | |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X| | |O|X|O| | | |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O| | | |X| |X| | |X|O| |O| |O| |X| |X| 'X' Won! | |X|O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | |O|X| | | | | |O|X| | | |O|X| | | | | |O|X|X| | |O|X| | |O| | |O|X|X| | |O|X| |X|O| | |O|X|X| |O|O|X| |X|O| | |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| |O| | | | | | |X|X| |O| |O| | | |X| 'O' Won! | |X|X| |O|O|O| | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| |X| | |O|X| |X|O| | |O| |X| | |O|X| |X|O| | |O|X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O| | |X| |O| |X| |X| | |O| | |X|O|O| 'X' Won! |X|X|X| | |O| | |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| |O| 'X' Won! |O| |X| |O|X| | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | |O| |X| |O|X| | | | | | |O|X|X| |O|X| | | | | | |O|X|X| |O|X| | | |O| | |O|X|X| |O|X|X| | |O| | |O|X|X| |O|X|X| | |O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | | | | |O|X|O| | |X|X| |O| | | |O|X|O| | |X|X| |O| |X| |O|X|O| | |X|X| |O|O|X| |O|X|O| 'X' Won! |X|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | 'O' Won! |O| | | |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| |O| |X| |X| | | | | |O|X|O| |X| |X| |O| | | |O|X|O| |X| |X| 'X' Won! |O| |X| |O|X|O| |X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | |O| |O| |O|O|X| 'X' Won! |X|X|X| |O| |O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O| |O| |X| |X| |O| |X| |O| |O| |X| |X| |O|O|X| |O| |O| |X| |X| 'X' Won! |O|O|X| |O|X|O| |X| |X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| 'O' Won! |O|X| | | |O|X| | | |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O|O|X| | | | | |X| |X| |O|O|X| | | | | |X| |X| |O|O|X| | | |O| |X| |X| |O|O|X| |X| |O| |X| |X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| | | |X| |O| | |O|O| 'X' Won! |X| | | |X| |O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X|X| |O| |O| | |X| | | |X|X| |O| |O| |O|X| | 'X' Won! |X|X|X| |O| |O| |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| |X| 'O' Won! | | | | |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| |O| |X| | | | | | |X|O| |O| |X| |X| | | | |X|O| |O| |X| |X|O| | |X|X|O| |O| |X| |X|O| | |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | |O|X| |X| | | | | |O| | |O|X| |X|X| | | | |O| | |O|X| |X|X| | | |O|O| 'X' Won! | |O|X| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| |O| |X| |O|X| | | | |O| |O|X|X| |O|X| | | | |O| |O|X|X| |O|X|O| 'X' Won! | |X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | 'X' Won! | |X| | |O|X| | |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| |X| | | | |O|O| | |X|O| |X| | | | |O|O| | |X|O| |X| |X| | |O|O| | |X|O| |X|O|X| | |O|O| |X|X|O| |X|O|X| 'O' Won! |O|O|O| |X|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | |O| |X| | | |O|X| | |X| |O| |X| | | |O|X| | |X| |O| |X| |O| |O|X| | |X|X|O| |X| |O| |O|X| | 'O' Won! |X|X|O| |X| |O| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | |X|X| | | | |O| |O|X| | |X|X|O| | | |O| |O|X| | |X|X|O| | | |O| |O|X|X| |X|X|O| |O| |O| |O|X|X| 'X' Won! |X|X|O| |O|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | |X| | | | | | |X|O|O| | |X| | | |O| | |X|O|O| | |X| | |X|O| | |X|O|O| |O|X| | |X|O| | 'X' Won! |X|O|O| |O|X| | |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | |O| |X| | |O|X| | |O| | |O| |X| |X|O|X| |O|O| | |O| |X| |X|O|X| 'X' Won! |O|O|X| |O| |X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | | | | |O|X|O| | |O|X| | | | | |O|X|O| | |O|X| | | |X| |O|X|O| | |O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | |X| | | | |X| | |O| | | |X| | | | |X| | |O|O| | |X| |X| | |X| | |O|O| | |X|O|X| | |X| | |O|O| | |X|O|X| | |X|X| |O|O| | 'O' Won! |X|O|X| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | | |O|X| | | |X|O| |X| | | |O|X| | | |X|O| |X| | | |O|X|O| 'X' Won! | |X|O| |X|X| | |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | | |X| | |O|X|X| |O| | | | |X| | |O|X|X| |O| |O| | |X|X| |O|X|X| |O| |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| |O|X| | | | |O| 'X' Won! | | |X| |O|X| | |X| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | |X| | |O| |X| |O|O| | | |X| | |O| |X| |O|O| | |X|X| | |O|O|X| |O|O| | |X|X| | 'X' Won! |O|O|X| |O|O| | |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X| | | |O| | |X| |X| |O|X| | | |O|O| |X| |X| |O|X| | |X|O|O| |X| |X| |O|X| | |X|O|O| |X|O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X|X| | |O| | | | |O|O| |X|X| | |O| | | |X|O|O| |X|X| | 'O' Won! |O| | | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| | | |O|O| | | |O|X| |X| | | |O|O|X| 'O' Won! | |O|X| |X|O| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | |X|X| | |O| |O| | | | | |X|X| | 'X' Won! |O| |O| | | | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| | | |X| |O| |O|X| | |O| | | |X| |O| |O|X| | |O|X| | |X| |O| |O|X|O| |O|X| | |X| |O| 'X' Won! |O|X|O| |O|X| | |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| |O|O| | | |X| | | |X|X| |O|O| | | |X| | | |X|X| |O|O| | | |X|O| | |X|X| |O|O| | |X|X|O| 'O' Won! |O|X|X| |O|O| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O| | |O| |O| |X|X| | | |O| | |O|X|O| |X|X| | |O|O| | |O|X|O| 'X' Won! |X|X|X| |O|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| |O| |X| |O|X| | | | |O| |O|X|X| |O|X| | | |O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| |O|X| | It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| | | | |O|X| |X| |X| |O|O| | | |O|X| |X| |X| |O|O| | |X|O|X| |X| |X| 'O' Won! |O|O| | |X|O|X| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O| | | |X|O| | |O|X| | |O| |X| |X|O| | |O|X|O| |O| |X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X|O| | |X| | |O| | | |O|X|O| |X|X| | |O| |O| |O|X|O| |X|X| | 'X' Won! |O| |O| |O|X|O| |X|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | |X| | |O|O| |X|X| | |O| |X| | |O|O| |X|X| | |O| |X| |X|O|O| |X|X|O| |O| |X| |X|O|O| It's a stalemate! |X|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | |X| | |O| |O| |X| |X| | |X|O| |O| |O| |X| |X| |X|X|O| |O| |O| |X| |X| 'O' Won! |X|X|O| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | |O|X| | | | | |X| |O| |X|O|X| | | | | |X| |O| |X|O|X| | |O| | |X| |O| 'X' Won! |X|O|X| |X|O| | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O| |X| | | |X| |X|O| | |O|O|X| | |X|X| |X|O| | |O|O|X| | |X|X| |X|O|O| |O|O|X| 'X' Won! |X|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| |O| | | |O|O|X| 'X' Won! | | |X| |O| |X| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| | | | | | 'O' Won! |O|X| | |X|O| | | | |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | | | | | | |X| |O|X| | |O| | | | | |X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | |O| | |O| | | 'X' Won! |X|X|X| | |O| | |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | | | |O| |X|O|O| |X| | | | |X|O| |X|O|O| |X|O| | | |X|O| |X|O|O| |X|O|X| | |X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | 'O' Won! | |X| | |O|O|O| | |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| |X|O| | 'O' Won! |O|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| | | | | | |X|O| |O| |X| | | | | | |X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | | | 'X' Won! |O|X|O| |O|X|X| | |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | |O|X| | |O| |X| | | | | |O|X| | |O| |X| |X| | | |O|X| | |O| |X| |X| |O| |O|X| | |O|X|X| |X| |O| |O|X| | |O|X|X| |X|O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | | | |O|O| | 'X' Won! |X|X|X| | | | | |O|O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | |O| |X| |X| | | |O| | |X|O| |X| |X| | |O|O| | |X|O| |X| |X| | |O|O| |X|X|O| |X| |X| 'O' Won! |O|O|O| |X|X|O| |X| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | |X| |O|O| | |X|X| | | |O|X| |O|O| | |X|X| | |X|O|X| |O|O| | 'O' Won! |X|X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | |O| | |X| |X| | | |O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O| | |X|O|X| |X|X|O| 'O' Won! |O|O| | |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| |O| | | | | | |O| | |X| |O| | |X| | | |O| | |X| |O| |O|X| | | |O| | 'X' Won! |X| |O| |O|X| | | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |O| | |O| | | |X|X| | | |O| |O|O| | | |X|X| | | |O| |O|O| | | |X|X| | |X|O| |O|O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | | |O|O| |O|X|X| |X| | | | |O|O| 'O' Won! |O|X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | | | |O| |X|O|O| |X|X| | |O| |O| |X|O|O| |X|X| | |O|X|O| |X|O|O| |X|X| | 'O' Won! |O|X|O| |X|O|O| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X|O| |O| |X| | | |X| | |X|O| |O| |X| |O| |X| | |X|O| |O| |X| |O| |X| |X|X|O| 'O' Won! |O| |X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| | | | | |X|X|O| | |X|O| |O| | | |X|X|O| |X|X|O| |O| | | |X|X|O| |X|X|O| |O|O| | |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| | | | |X|O| | |X| | |O| | | | |X|O| |O|X| | |O| | | | |X|O| |O|X| | |O| |X| 'O' Won! |O|X|O| |O|X| | |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | | | | |O| |X| 'X' Won! | |O|X| | | |X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X|O| | | |O| | |X|X| | |X|O| | | |O| | |X|X| | |X|O|O| 'X' Won! | |O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | |X| | |O| |O| |X|O|X| | |X| | |O|X|O| |X|O|X| | |X| | 'O' Won! |O|X|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | | |X|X| 'O' Won! |O| | | |O| | | |O|X|X|","title":"1.0.2 Load Dataset"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#11-clean-data","text":"We will first need to organize the data into a parsable format.","title":"1.1 Clean Data"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q1","text":"What is the object data and what does it contain? what are the keys of data? what are the keys of each game? # inspect data below by grabbing the first key in data # what are the three different keys within each game? data [ 'game 0' ] {'board': {1: 'X', 2: ' ', 3: 'O', 4: 'X', 5: 'X', 6: 'O', 7: 'O', 8: 'X', 9: 'O'}, 'starting player': 'X', 'winner': 'O'}","title":"Q1"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q2","text":"Using those keys, iterate through every game in data and append the board, the winner, and the starting player to separate lists. Call these lists: boards, winners, and starters boards = [] winners = [] starters = [] for game in data : # YOUR CODE HERE","title":"Q2"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q3","text":"Make a dataframe out of the list boards and call it df . Make a series out of the list winners . Make a series out of the list starters . Make a new column of df called \"Winner\" and set it equal to the pandas Series of the winners. Make a new column of df called \"Starter\" and set it equal to the pandas Series of the starters. # YOUR CODE HERE .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 Winner Starter 0 X O O O X O O 1 O O X O X X O X O O O 2 O X X X X O O X X 3 X X O O O X X O O Stalemate O 4 O O X O O X X X X O","title":"Q3"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#12-inferential-analysis","text":"We're going to use Bayes Rule or Bayesian Inference to make a probability of winning based on positions of the board. The formula is: P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)} Where \\(\\cap\\) is the intersection of \\(A\\) and \\(B\\). The example we will use is the following: what is the probability of 'O' being the winner, given that they've played the center piece. \\(B\\) = 'O' played the center piece \\(A\\) = 'O' won the game So what is probability? We will define it in terms of frequencies. So if we are for instance asking what is the probability of player 'O' being in the center piece, it would be defined as: P(B) = \\frac{|O_c|} {|O_c| + |X_c| + |empty|} Where the pipes, | | , or cardinality represent the count of the indicated observation or set. In this case \\(O_c\\) (O being in the center) and \\(X_c\\) (X being in the center). Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc_Xc_empty 1000 # example of assessing the probability of B, O playing the center piece player = 'O' Oc = ( df [ 5 ] == player ) . value_counts () Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc / Oc_Xc_empty False 0.577 True 0.423 Name: 5, dtype: float64 # we can also clean this up and replace the denominator with the whole # observation space (which is just the total number of games, df.shape[0]). # example of assesing probabiliy of A ( df [ 'Winner' ] == 'O' ) . value_counts () / df . shape [ 0 ] False 0.571 True 0.429 Name: Winner, dtype: float64 The \\(P(B|A) * P(A)\\) is the intersection of \\(B\\) and \\(A\\). The intersection is defined as the two events occuring together. Continuing with the example, the probablity of 'O' playing the center piece AND 'O' being the winner is the number of times these observations occured together divided by the whole observation space : # in this view, the total times A and B occured together is 247 player = 'O' df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] 247 # the total observation space is 1000 (1000 games) df . shape [ 0 ] 1000 And so we get: P(B|A) * P(A) = \\frac{247} {1000} = 0.247 In code: df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] / df . shape [ 0 ] 0.247","title":"1.2 Inferential Analysis"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#121-behavioral-analysis-of-the-winner","text":"","title":"1.2.1 Behavioral Analysis of the Winner"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q4","text":"define the 3 different board piece types and label them middle , side , and corner . Middle should be an int and the other two should be lists. # define the 3 different board piece types # middle = # side = # corner =","title":"Q4"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#1211-what-is-the-probability-of-winning-after-playing-the-middle-piece","text":"","title":"1.2.1.1 What is the probability of winning after playing the middle piece?"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q5-for-player-x","text":"# A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'X' # define the intersection of A AND B, A_B # A_B = # define prob B # B = # return A_B over B (The prob B given A) A_B / B 0.5732758620689655","title":"Q5: For player X"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q6-for-player-o","text":"# A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'O' # define the intersection of A AND B, A_B # A_B = # define prob B # B = # return A_B over B (The prob B given A) A_B / B 0.5839243498817968","title":"Q6 For player O"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#1212-what-is-the-probability-of-winning-after-playing-a-side-piece","text":"","title":"1.2.1.2 What is the probability of winning after playing a side piece?"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q7-for-player-o","text":"# A intersect B: O played side and O won / tot games # B: O played side / tot games player = 'O' A_B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.4158609451385117","title":"Q7 For player O"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q8-for-player-x","text":"# A intersect B: X played side and X won / tot games # B: X played side / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.38845460012026456","title":"Q8 For player X"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#1213-what-is-the-probability-of-winning-after-playing-a-corner-piece","text":"","title":"1.2.1.3 What is the probability of winning after playing a corner piece?"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q9-for-player-o","text":"# A intersect B: O played corner and O won / tot games # B: O played corner / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.4779116465863454","title":"Q9 For player O"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#q10-for-player-x","text":"# A intersect B: X played corner and X won / tot games # B: X played corner / tot games # player = # SET PLAYER # A_B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values)) & # (df['Winner'] == player)].shape[0] / df.shape[0] # B = df.loc[(df[<SET PIECE>].T.apply(lambda x: player in x.values))].shape[0] /\\ # df.shape[0] A_B / B 0.47386964180857316 Are these results surprising to you? Why? This resource may be illustrative.","title":"Q10 For player X"},{"location":"project/P1_Statistical_Analysis_of_TicTacToe/#13-improving-the-analysis","text":"In this analysis, we only tracked what moves were made, not the order they were made in. It really limited our assessment! How might we change our recording of the games to track order of moves as well? Do we need to track all the moves or just the first and the winner?","title":"1.3 Improving the Analysis"},{"location":"project/P2_Heuristical_TicTacToe_Agents/","text":"Data Science Foundations Project Part 2: Heuristical Agents \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com We makin' some wack AI today 2.0 Preparing Environment and Importing Data \u00b6 back to top 2.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 2.0.2 Load Dataset \u00b6 back to top 2.1 AI Heuristics \u00b6 Develop a better AI based on your analyses of game play so far. Q1 \u00b6 In our groups, let's discuss what rules we would like to hard code in. Harsha, Varsha and I will help you with the flow control to program these rules # we will define some variables to help us define the types of positions middle = 5 side = [ 2 , 4 , 6 , 8 ] corner = [ 1 , 3 , 7 , 9 ] # recall that our board is a dictionary tictactoe = TicTacToe () tictactoe . board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} # and we have a win_patterns object to help us with the algorithm tictactoe . win_patterns [[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 4, 7], [2, 5, 8], [3, 6, 9], [1, 5, 9], [7, 5, 3]] for example, if we want to check if the middle piece is available, and play it if it is. How do we do that? # set some key variables player = 'X' opponent = 'O' avail_moves = [ i for i in tictactoe . board . keys () if tictactoe . board [ i ] == ' ' ] # a variable that will keep track if we've found a move we like or not move_found = False # <- some other moves we might want to make would go here -> # # and now for our middle piece play if move_found == False : # if no other move has been found yet if middle in avail_moves : # if middle is available move_found = True # then change our move_found status move = middle # update our move Note: in the following when I say return a move I mean when we wrap this up in a function we will want the return to be for a move. For now I just mean that the result of your code in Q3 is to take the variable name move and set it equal to the tic-tac-toe board piece the AI will play Our standard approach will be to always return a move by the agent . Whether the agent is heruistical or from some other ML framework we always want to return a move Q2 \u00b6 Write down your algorithm steps in markdown. i.e. play a corner piece play to opposite corner from the opponent, etc. ....etc. Q3 \u00b6 Begin to codify your algorithm from Q3. Make sure that no matter what, you return a move # some starting variables for you self = TicTacToe () # this is useful cheat for when we actually put this in as a method player_label = 'X' opponent = 'O' avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] # temp board will allow us to play hypothetical moves and see where they get us # in case you need it temp_board = self . board . copy () 2.2 Wrapping our Agent \u00b6 Now that we've created a conditional tree for our AI to make a decision, we need to integrate this within the gaming framework we've made so far. How should we do this? Let's define this thought pattern or tree as an agent. Recall our play_game function within GameEngine def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break ######################################################################## ##################### WE WANT TO CHANGE THESE LINES #################### ######################################################################## else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 2.2.1 Redefining the Random Agent \u00b6 In particular, we want to change lines 30-37 to take our gaming agent in as a parameter to make decisions. Let's try this. In setup_game we want to have the option to set the AI type/level. In play_game we want to make a call to that AI to make the move. For instance, our random AI will go from: while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break to: def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ########## our fresh off the assembly line tictactoe playing robot ########### ############################################################################## def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : pass self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Let's test that our random ai works now in this format random . seed ( 12 ) game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| <__main__.GameEngine at 0x7fadbea428d0> Let's try it with a user player: random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2)2 | | | | | | | | | | | | X, what's your move?q quiting the game <__main__.GameEngine at 0x7fadbea25e90> Q4 \u00b6 Now let's fold in our specialized AI agent. Add your code under the heurstic_ai function. Note that the player_label is passed as an input parameter now class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ################### YOUR BADASS HEURISTIC AGENT GOES HERE #################### ############################################################################## def heuristic_ai ( self , player_label ): # SOME HELPER VARIABLES IF YOU NEED THEM opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () ################## YOUR CODE GOES HERE, RETURN THAT MOVE! ################## while True : # DELETE LINES 20 - 25, USED FOR TESTING PURPOSES ONLY move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break ############################################################################ return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Q5 \u00b6 And we'll test that it works! random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2)1 select AI level (1, 2)2 who will go first? (X, (AI), or O (Player))O | | | | | | | | | | | | O, what's your move?5 | | | | | |O| | | | | | | | | | | |O| | | |X| | O, what's your move?9 | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| O, what's your move?1 'O' Won! |O| | | | |O|X| | |X|O| <__main__.GameEngine at 0x7fadbe93f610> Q6 \u00b6 Test the autorun feature! game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| 'O' Won! |O|X| | |O| | | |O| |X| <__main__.GameEngine at 0x7fadbe8cc050>","title":"Heuristical TicTacToe Agents"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#data-science-foundations-project-part-2-heuristical-agents","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com We makin' some wack AI today","title":"Data Science Foundations  Project Part 2: Heuristical Agents"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#20-preparing-environment-and-importing-data","text":"back to top","title":"2.0 Preparing Environment and Importing Data"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#201-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"2.0.1 Import Packages"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#202-load-dataset","text":"back to top","title":"2.0.2 Load Dataset"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#21-ai-heuristics","text":"Develop a better AI based on your analyses of game play so far.","title":"2.1 AI Heuristics"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q1","text":"In our groups, let's discuss what rules we would like to hard code in. Harsha, Varsha and I will help you with the flow control to program these rules # we will define some variables to help us define the types of positions middle = 5 side = [ 2 , 4 , 6 , 8 ] corner = [ 1 , 3 , 7 , 9 ] # recall that our board is a dictionary tictactoe = TicTacToe () tictactoe . board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} # and we have a win_patterns object to help us with the algorithm tictactoe . win_patterns [[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 4, 7], [2, 5, 8], [3, 6, 9], [1, 5, 9], [7, 5, 3]] for example, if we want to check if the middle piece is available, and play it if it is. How do we do that? # set some key variables player = 'X' opponent = 'O' avail_moves = [ i for i in tictactoe . board . keys () if tictactoe . board [ i ] == ' ' ] # a variable that will keep track if we've found a move we like or not move_found = False # <- some other moves we might want to make would go here -> # # and now for our middle piece play if move_found == False : # if no other move has been found yet if middle in avail_moves : # if middle is available move_found = True # then change our move_found status move = middle # update our move Note: in the following when I say return a move I mean when we wrap this up in a function we will want the return to be for a move. For now I just mean that the result of your code in Q3 is to take the variable name move and set it equal to the tic-tac-toe board piece the AI will play Our standard approach will be to always return a move by the agent . Whether the agent is heruistical or from some other ML framework we always want to return a move","title":"Q1"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q2","text":"Write down your algorithm steps in markdown. i.e. play a corner piece play to opposite corner from the opponent, etc. ....etc.","title":"Q2"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q3","text":"Begin to codify your algorithm from Q3. Make sure that no matter what, you return a move # some starting variables for you self = TicTacToe () # this is useful cheat for when we actually put this in as a method player_label = 'X' opponent = 'O' avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] # temp board will allow us to play hypothetical moves and see where they get us # in case you need it temp_board = self . board . copy ()","title":"Q3"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#22-wrapping-our-agent","text":"Now that we've created a conditional tree for our AI to make a decision, we need to integrate this within the gaming framework we've made so far. How should we do this? Let's define this thought pattern or tree as an agent. Recall our play_game function within GameEngine def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break ######################################################################## ##################### WE WANT TO CHANGE THESE LINES #################### ######################################################################## else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"2.2 Wrapping our Agent"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#221-redefining-the-random-agent","text":"In particular, we want to change lines 30-37 to take our gaming agent in as a parameter to make decisions. Let's try this. In setup_game we want to have the option to set the AI type/level. In play_game we want to make a call to that AI to make the move. For instance, our random AI will go from: while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break to: def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ########## our fresh off the assembly line tictactoe playing robot ########### ############################################################################## def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : pass self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Let's test that our random ai works now in this format random . seed ( 12 ) game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| <__main__.GameEngine at 0x7fadbea428d0> Let's try it with a user player: random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2)2 | | | | | | | | | | | | X, what's your move?q quiting the game <__main__.GameEngine at 0x7fadbea25e90>","title":"2.2.1 Redefining the Random Agent"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q4","text":"Now let's fold in our specialized AI agent. Add your code under the heurstic_ai function. Note that the player_label is passed as an input parameter now class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ################### YOUR BADASS HEURISTIC AGENT GOES HERE #################### ############################################################################## def heuristic_ai ( self , player_label ): # SOME HELPER VARIABLES IF YOU NEED THEM opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () ################## YOUR CODE GOES HERE, RETURN THAT MOVE! ################## while True : # DELETE LINES 20 - 25, USED FOR TESTING PURPOSES ONLY move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break ############################################################################ return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"Q4"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q5","text":"And we'll test that it works! random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2)1 select AI level (1, 2)2 who will go first? (X, (AI), or O (Player))O | | | | | | | | | | | | O, what's your move?5 | | | | | |O| | | | | | | | | | | |O| | | |X| | O, what's your move?9 | | | | | |O| | | |X|O| | | | | | |O|X| | |X|O| O, what's your move?1 'O' Won! |O| | | | |O|X| | |X|O| <__main__.GameEngine at 0x7fadbe93f610>","title":"Q5"},{"location":"project/P2_Heuristical_TicTacToe_Agents/#q6","text":"Test the autorun feature! game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| 'O' Won! |O|X| | |O| | | |O| |X| <__main__.GameEngine at 0x7fadbe8cc050>","title":"Q6"},{"location":"project/P3_1_Step_Look_Ahead_Agents/","text":"Data Science Foundations Project Part 3: 1-Step Look Ahead \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we're working on a more advanced AI structure: 1-step lookahead. 3.0 Preparing Environment and Importing Data \u00b6 back to top 3.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## self . ai_level = 2 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 3.0.2 Load Dataset \u00b6 back to top 3.1 Rethinking gameplay \u00b6 To implement the broader strategies used in game theory and machine learning, we need to rebroadcast our approach to creating our AI agent. In the heurstical agent model, we thought in terms of checking for specific move types, defined by what kind of advantage they give us during game play, i.e. see if a winning move is available, a blocking move, if a corner place is free, etc. Rather than thinking with this look and check mindset that is centered around specific strategies and our own prior knowledge about the game (we know that a center piece is statistically likely to give us a higher chance of winning) we will evaluate every available move to the AI, and rate them quantitatively. switching from ordinal to interval Notice the datatype change when we move from giving simple preferences of moves to actual scores of moves. Catalog this in your mind for future reference when considering datatypes! 3.1.1 One-Step Look Ahead \u00b6 For now, when we rate our boards, we will only look 1-step ahead in gameplay. Hence the name we give this AI strategy, 1-step lookahead The beginning portion of our code will look about the same as the heuristic AI model. Recall: def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() but now, instead of searching progressively through our preferred move-types (winning, middle, etc.) . We are going to give every available move (1, 3, 7, etc.) a score. Our score regimen will look like the following: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move Q1 Rewrite avail_moves \u00b6 define avail_moves as a dictionary of available moves with scores for each move as empty strings. We will update this dictionary with numerical scores in the next step # we're going to steal the parameter names to # prototype our new function self = TicTacToe () player_label = 'X' opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] # instead of a list, we want avail_moves to now be a dictionary that will # contain the move and its score # avail_moves = # YOUR CODE HERE temp_board = self . board . copy () Q2 Score each move in avail_moves \u00b6 Now let's fold this into our new one_step_ai function. Remember: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## # avail_moves = # temp_board = self . board . copy () # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score temp_board [ move ] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## ############################################################################## ################### All remaining moves receive a score of 1 ############################################################################## return avail_moves one_step_ai ( self , 'X' ) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} Q3 Test one_step_ai \u00b6 That's great, but how do we check that our code will work when a winning move is available, or a losing move is just around the corner? let's create a unit test for these! # just defining a new game self = TicTacToe () player_label = 'X' # seeding the board with some X's self . board [ 1 ] = 'X' self . board [ 2 ] = 'X' self . board {1: 'X', 2: 'X', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} Now test the winning move. Your code should return 100 at move 3 and 1 everywhere else one_step_ai ( self , player_label ) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} We can test the losing move by reversing the players player_label = 'O' one_step_ai ( self , player_label ) {3: 10, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} great! Let's keep these shorthand codes in mind when we go to write actual unit tests with the one_step_ai function embedded in the GameEngine module. We're not done yet, recall that our other ai agents returned the actual selected move, not a dictionary of the moves with scores. We need to create a move from this dictionary and return it. Here's what the general procedure will look like: Grab the maximum score (after assigning scores to all of avail_moves) Select all moves that have this maximum score Return a random selection of the moves with the max score and then in code format: avail_moves = one_step_ai ( self , player_label ) # 1. grab the maximum score max_score = max ( avail_moves . values ()) # 2. select all moves that have this maximum score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # 3. return a random selection of the moves with the max score move = random . choice ( valid ) move 3 3.2 Putting it all together \u00b6 Q4 Finish one_step_ai to return a move \u00b6 Let's see if we can rewrite our game engine to take new AI models in as a passable parameter. This way our base module will be much cleaner, and allow us to continue to write new functions for the base engine as long as they pass along the same variables. # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( board , win_patterns , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] temp_board = board . copy () # define avail_moves # avail_moves = # your code for giving scores for avail_moves # first grab max score # then select all moves that have this max score # return a random selection of the moves with the max score return move 3.2.1 Allow GameEngine to take an ai agent as a passable parameter \u00b6 Let's rewrite our GameEngine to take an ai agent as a passable parameter under user_ai . The default value will be None Additional user_ai criteria will be that user_ai receives board , win_patterns and player_label and returns move . class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' , user_ai = None ): super () . __init__ () self . setup = setup self . user_ai = user_ai def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self . user_ai == None : level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) else : self . ai_level = 3 if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## if self . user_ai == None : self . ai_level = 2 else : self . ai_level = 3 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) ########## # Our user-defined AI agent ########## elif self . ai_level == 3 : move = self . user_ai ( self . board , self . win_patterns , player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Test the auto and user functions game = GameEngine ( setup = 'user' , user_ai = one_step_ai ) game . setup_game () How many Players? (type 0, 1, or 2)1 who will go first? (X, (AI), or O (Player))X game . play_game () | | | | | | | | | | | | | | | | | | | | | |X| | O, what's your move?5 | | | | | |O| | | |X| | | | |X| | |O| | | |X| | O, what's your move?4 | | |X| |O|O| | | |X| | | | |X| |O|O|X| | |X| | O, what's your move?1 |O| |X| |O|O|X| | |X| | 'X' Won! |O| |X| |O|O|X| | |X|X| <__main__.GameEngine at 0x7f1f812e7d50> 3.3 Write Unit Tests for the New Code \u00b6 def test_user_ai (): random . seed ( 42 ) game = GameEngine ( setup = 'auto' , user_ai = one_step_ai ) game . setup_game () outcome = game . play_game () assert outcome . winner == 'X' , 'X should have won!' test_user_ai () | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| | |X|O| |O| | | 'X' Won! |X|O|X| | |X|O| |O| |X|","title":"1-Step Look Ahead Agents"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#data-science-foundations-project-part-3-1-step-look-ahead","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we're working on a more advanced AI structure: 1-step lookahead.","title":"Data Science Foundations  Project Part 3: 1-Step Look Ahead"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#30-preparing-environment-and-importing-data","text":"back to top","title":"3.0 Preparing Environment and Importing Data"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#301-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## self . ai_level = 2 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"3.0.1 Import Packages"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#302-load-dataset","text":"back to top","title":"3.0.2 Load Dataset"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#31-rethinking-gameplay","text":"To implement the broader strategies used in game theory and machine learning, we need to rebroadcast our approach to creating our AI agent. In the heurstical agent model, we thought in terms of checking for specific move types, defined by what kind of advantage they give us during game play, i.e. see if a winning move is available, a blocking move, if a corner place is free, etc. Rather than thinking with this look and check mindset that is centered around specific strategies and our own prior knowledge about the game (we know that a center piece is statistically likely to give us a higher chance of winning) we will evaluate every available move to the AI, and rate them quantitatively. switching from ordinal to interval Notice the datatype change when we move from giving simple preferences of moves to actual scores of moves. Catalog this in your mind for future reference when considering datatypes!","title":"3.1 Rethinking gameplay"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#311-one-step-look-ahead","text":"For now, when we rate our boards, we will only look 1-step ahead in gameplay. Hence the name we give this AI strategy, 1-step lookahead The beginning portion of our code will look about the same as the heuristic AI model. Recall: def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() but now, instead of searching progressively through our preferred move-types (winning, middle, etc.) . We are going to give every available move (1, 3, 7, etc.) a score. Our score regimen will look like the following: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move","title":"3.1.1 One-Step Look Ahead"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#q1-rewrite-avail_moves","text":"define avail_moves as a dictionary of available moves with scores for each move as empty strings. We will update this dictionary with numerical scores in the next step # we're going to steal the parameter names to # prototype our new function self = TicTacToe () player_label = 'X' opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] # instead of a list, we want avail_moves to now be a dictionary that will # contain the move and its score # avail_moves = # YOUR CODE HERE temp_board = self . board . copy ()","title":"Q1 Rewrite avail_moves"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#q2-score-each-move-in-avail_moves","text":"Now let's fold this into our new one_step_ai function. Remember: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## # avail_moves = # temp_board = self . board . copy () # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score temp_board [ move ] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## ############################################################################## ################### All remaining moves receive a score of 1 ############################################################################## return avail_moves one_step_ai ( self , 'X' ) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}","title":"Q2 Score each move in avail_moves"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#q3-test-one_step_ai","text":"That's great, but how do we check that our code will work when a winning move is available, or a losing move is just around the corner? let's create a unit test for these! # just defining a new game self = TicTacToe () player_label = 'X' # seeding the board with some X's self . board [ 1 ] = 'X' self . board [ 2 ] = 'X' self . board {1: 'X', 2: 'X', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} Now test the winning move. Your code should return 100 at move 3 and 1 everywhere else one_step_ai ( self , player_label ) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} We can test the losing move by reversing the players player_label = 'O' one_step_ai ( self , player_label ) {3: 10, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} great! Let's keep these shorthand codes in mind when we go to write actual unit tests with the one_step_ai function embedded in the GameEngine module. We're not done yet, recall that our other ai agents returned the actual selected move, not a dictionary of the moves with scores. We need to create a move from this dictionary and return it. Here's what the general procedure will look like: Grab the maximum score (after assigning scores to all of avail_moves) Select all moves that have this maximum score Return a random selection of the moves with the max score and then in code format: avail_moves = one_step_ai ( self , player_label ) # 1. grab the maximum score max_score = max ( avail_moves . values ()) # 2. select all moves that have this maximum score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # 3. return a random selection of the moves with the max score move = random . choice ( valid ) move 3","title":"Q3 Test one_step_ai"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#32-putting-it-all-together","text":"","title":"3.2 Putting it all together"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#q4-finish-one_step_ai-to-return-a-move","text":"Let's see if we can rewrite our game engine to take new AI models in as a passable parameter. This way our base module will be much cleaner, and allow us to continue to write new functions for the base engine as long as they pass along the same variables. # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( board , win_patterns , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] temp_board = board . copy () # define avail_moves # avail_moves = # your code for giving scores for avail_moves # first grab max score # then select all moves that have this max score # return a random selection of the moves with the max score return move","title":"Q4 Finish one_step_ai to return a move"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#321-allow-gameengine-to-take-an-ai-agent-as-a-passable-parameter","text":"Let's rewrite our GameEngine to take an ai agent as a passable parameter under user_ai . The default value will be None Additional user_ai criteria will be that user_ai receives board , win_patterns and player_label and returns move . class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' , user_ai = None ): super () . __init__ () self . setup = setup self . user_ai = user_ai def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self . user_ai == None : level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) else : self . ai_level = 3 if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## if self . user_ai == None : self . ai_level = 2 else : self . ai_level = 3 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) ########## # Our user-defined AI agent ########## elif self . ai_level == 3 : move = self . user_ai ( self . board , self . win_patterns , player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Test the auto and user functions game = GameEngine ( setup = 'user' , user_ai = one_step_ai ) game . setup_game () How many Players? (type 0, 1, or 2)1 who will go first? (X, (AI), or O (Player))X game . play_game () | | | | | | | | | | | | | | | | | | | | | |X| | O, what's your move?5 | | | | | |O| | | |X| | | | |X| | |O| | | |X| | O, what's your move?4 | | |X| |O|O| | | |X| | | | |X| |O|O|X| | |X| | O, what's your move?1 |O| |X| |O|O|X| | |X| | 'X' Won! |O| |X| |O|O|X| | |X|X| <__main__.GameEngine at 0x7f1f812e7d50>","title":"3.2.1 Allow GameEngine to take an ai agent as a passable parameter"},{"location":"project/P3_1_Step_Look_Ahead_Agents/#33-write-unit-tests-for-the-new-code","text":"def test_user_ai (): random . seed ( 42 ) game = GameEngine ( setup = 'auto' , user_ai = one_step_ai ) game . setup_game () outcome = game . play_game () assert outcome . winner == 'X' , 'X should have won!' test_user_ai () | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| | |X|O| |O| | | 'X' Won! |X|O|X| | |X|O| |O| |X|","title":"3.3 Write Unit Tests for the New Code"},{"location":"project/P4_N_Step_Look_Ahead_Agents/","text":"Data Science Foundations Project Part 4: N-Step Look Ahead \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com For this lesson, we will be adding N-step Look Ahead algorithm to our ensemble of AI agents! 4.0 Preparing Environment and Importing Data \u00b6 back to top 4.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt def one_step_ai ( board , win_patterns , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = { i : 1 for i in board . keys () if board [ i ] == ' ' } temp_board = board . copy () ######################################## # we're going to change the following lines, instead of caring # whether we've found the best move, we want to update the move # with a score ######################################## # check if the opponent has a winning move first, we will overwrite # the score for this move if it is also a winning move for the current # player for move in avail_moves . keys (): temp_board [ move ] = opponent for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: avail_moves [ move ] = 10 temp_board [ move ] = ' ' for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: avail_moves [ move ] = 100 temp_board [ move ] = ' ' # first grab max score max_score = max ( avail_moves . values ()) # then select all moves that have this max score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # return a random selection of the moves with the max score move = random . choice ( valid ) return move class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' , user_ai = None ): super () . __init__ () self . setup = setup self . user_ai = user_ai def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self . user_ai == None : level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) else : self . ai_level = 3 if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## if self . user_ai == None : self . ai_level = 2 else : self . ai_level = 3 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) ########## # Our user-defined AI agent ########## elif self . ai_level == 3 : move = self . user_ai ( self . board , self . win_patterns , player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 4.1 N-Step Look Ahead and Minimax \u00b6 In the previous tic tac toe module, our AI only looked 1 step ahead, and we can probably see how this has disadvantages. When we play strategy games ourselves, we often do better by looking a number of steps into the future. One new idea that this requires, is how we will anticipate our opponents move. This gets us into game theory . We're not going to borrow a whole lot from here, just the following: we will assume our opponent will work to minimize our score This switching from us wanting to maximize our score to the opponent wanting to minimize our score is called the minimax algorithm. As well look ahead into the future possibility of moves, we will use minimax to set our hypothetical behavior as well as our opponents You can investigate the pseudocode for minmax on wiki. # we're going to pull out and reformat some of our helper functions in the # TicTacToe class win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] def check_winning ( board , win_patterns ): for pattern in win_patterns : values = [ board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ] or values == [ 'O' , 'O' , 'O' ]: return True return False def check_stalemate ( board , win_patterns ): if ( ' ' not in board . values ()) and ( check_winning ( board , win_patterns ) == '' ): return True return False def minimax ( depth , board , maximizing_player , player_label , verbiose = False ): # infer the opponent opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] # set the available moves avail_moves = [ i for i in board . keys () if board [ i ] == ' ' ] # check if the depth is 0, or stalemate/winner has been reached # if so this is the basecase and we want to return get_score() terminal_move = is_terminal_node ( board , avail_moves ) if terminal_move or depth == 0 : score = get_score ( board , player_label , win_patterns ) if verbiose : print ( ' {} score: {} . depth: {} ' . format ( board , score , depth )) return score ### in the following we want to search through every possible board at the ### current level (the possible moves for the current player, given that the ### player is either the one whose turn it is or the imagined opponent) # call minimax where it is the current players turn and so we want to # maximize the score if maximizing_player : score = - np . Inf for move in avail_moves : new_board = board . copy () new_board [ move ] = player_label score = max ( score , minimax ( depth - 1 , new_board , False , player_label , verbiose )) if verbiose : print ( ' {} max. score: {} . depth: {} ' . format ( board , score , depth )) return score # call minimax where it is the opponent players turn and so we want to # minimize the score elif not maximizing_player : score = np . Inf for move in avail_moves : new_board = board . copy () new_board [ move ] = opponent score = min ( score , minimax ( depth - 1 , new_board , True , player_label , verbiose )) if verbiose : print ( ' {} min. score: {} . depth: {} ' . format ( board , score , depth )) return score def is_terminal_node ( board , avail_moves ): if check_winning ( board , win_patterns ): return True elif check_stalemate ( board , win_patterns ): return True else : return False def get_score ( board , player_label , win_patterns ): # this will look somewhat similar to our 1-step lookahead algorithm opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] score = 0 for pattern in win_patterns : values = [ board [ i ] for i in pattern ] # if the opponent wins, the score is -100 if values == [ opponent , opponent , opponent ]: score = - 100 elif values == [ player_label , player_label , player_label ]: score = 100 return score board = TicTacToe () . board board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} minimax ( depth = 1 , board = board , maximizing_player = True , player_label = 'O' ) 0 Finally, we need a couple wrapper functions to handle this. The first is a handler for the top level of the game tree (we want to see the minmax result for every possible move at the current place in the game) verbiose = True verbiose True def get_minimax ( depth , board , player_label , verbiose = False ): score = minimax ( depth - 1 , board , False , player_label , verbiose = verbiose ) return score def n_step_ai_temp ( board , win_patterns , player_label , n_steps , verbiose = False ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = { i : 1 for i in board . keys () if board [ i ] == ' ' } for move in avail_moves . keys (): temp_board = board . copy () temp_board [ move ] = player_label score = get_minimax ( n_steps , temp_board , player_label , verbiose = verbiose ) avail_moves [ move ] = score return avail_moves Let's test our n_step_ai while we're still returning the dictionary of available moves. Does this make sense? board = TicTacToe () . board board [ 1 ] = 'X' board [ 5 ] = 'O' board [ 2 ] = 'X' # with this setup we should see that a good move will be to play 3 # be sure to vary the number of lookahead steps n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 2 ) {3: 100, 4: 0, 6: 0, 7: 0, 8: 0, 9: 0} Looks like it's making sense. Let's also try when the opponent looks like they've got a winning move to be made board = TicTacToe () . board board [ 1 ] = 'X' board [ 5 ] = 'O' board [ 2 ] = 'X' board [ 4 ] = 'O' # with this setup we should see that a good move will be to play 6 or 3 # be sure to vary the number of lookahead steps n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 3 ) Let's look at the following as well. This should be an interesting game situation to you. It is \"X's\" move. What do you notice? Does this outcome make sense? X cannot win this game board = TicTacToe () . board board [ 1 ] = 'O' board [ 5 ] = 'O' board [ 2 ] = 'X' board [ 8 ] = 'X' # be sure to vary the number of lookahead steps n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 4 , verbiose = False ) # as a side note, sometimes it helps to interpret the outcome if you discount # the results from looking further down the game tree. (maybe O will make a mistake) # how would you encode this in your n-step look ahead algorithm? {3: -100, 4: -100, 6: -100, 7: -100, 9: -100} 4.2 Packaging for GameEngine \u00b6 Nice. Let's finish packaging our n_steps_ai so we can feed it to our game engine. def n_step_ai ( board , win_patterns , player_label , n_steps = 3 ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = { i : 1 for i in board . keys () if board [ i ] == ' ' } for move in avail_moves . keys (): temp_board = board . copy () temp_board [ move ] = player_label score = get_minimax ( n_steps , temp_board , player_label ) avail_moves [ move ] = score ########################################## ### The rest of our ai agent harness is the same ########################################## # first grab max score max_score = max ( avail_moves . values ()) # then select all moves that have this max score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # return a random selection of the moves with the max score move = random . choice ( valid ) return move game = GameEngine ( setup = 'user' , user_ai = n_step_ai ) game . setup_game () How many Players? (type 0, 1, or 2)1 who will go first? (X, (AI), or O (Player))X game . play_game () | | | | | | | | | | | | | | | | |X| | | | | | | O, what's your move?5 | | | | |X|O| | | | | | | | | | |X|O| | | |X| | O, what's your move?2 | |O| | |X|O| | | |X| | | |O| | |X|O| | |X|X| | O, what's your move?1 |O|O| | |X|O| | |X|X| | 'X' Won! |O|O| | |X|O| | |X|X|X| <__main__.GameEngine at 0x7fe529a0d850> Let's investigate the behavior of our AI and double check that it makes sense board = game . board board [ 9 ] = ' ' board {1: 'O', 2: 'O', 3: ' ', 4: 'X', 5: 'O', 6: ' ', 7: 'X', 8: 'X', 9: ' '} game . visualize_board () |O|O| | |X|O| | |X|X| | n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 3 ) {3: -100, 6: -100, 9: 100} 4.3 Writing Tests \u00b6 def test_n_step_ai (): random . seed ( 42 ) game = GameEngine ( setup = 'auto' , user_ai = n_step_ai ) game . setup_game () game . play_game () # check that the winner is X assert game . winner == 'X' , \"Winner should be X!\" # check that the ai level is set to 3 which means our engine is properly # accessing the user defined ai assert game . ai_level == 3 , \"The engine is not using the user defined AI!\" test_n_step_ai () | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | 'X' Won! |X|O|X| |O|X|O| | | |X|","title":"N-Step Look Ahead Agents"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#data-science-foundations-project-part-4-n-step-look-ahead","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com For this lesson, we will be adding N-step Look Ahead algorithm to our ensemble of AI agents!","title":"Data Science Foundations  Project Part 4: N-Step Look Ahead"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#40-preparing-environment-and-importing-data","text":"back to top","title":"4.0 Preparing Environment and Importing Data"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#401-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt def one_step_ai ( board , win_patterns , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = { i : 1 for i in board . keys () if board [ i ] == ' ' } temp_board = board . copy () ######################################## # we're going to change the following lines, instead of caring # whether we've found the best move, we want to update the move # with a score ######################################## # check if the opponent has a winning move first, we will overwrite # the score for this move if it is also a winning move for the current # player for move in avail_moves . keys (): temp_board [ move ] = opponent for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: avail_moves [ move ] = 10 temp_board [ move ] = ' ' for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: avail_moves [ move ] = 100 temp_board [ move ] = ' ' # first grab max score max_score = max ( avail_moves . values ()) # then select all moves that have this max score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # return a random selection of the moves with the max score move = random . choice ( valid ) return move class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' , user_ai = None ): super () . __init__ () self . setup = setup self . user_ai = user_ai def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self . user_ai == None : level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) else : self . ai_level = 3 if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## if self . user_ai == None : self . ai_level = 2 else : self . ai_level = 3 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) ########## # Our user-defined AI agent ########## elif self . ai_level == 3 : move = self . user_ai ( self . board , self . win_patterns , player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"4.0.1 Import Packages"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#41-n-step-look-ahead-and-minimax","text":"In the previous tic tac toe module, our AI only looked 1 step ahead, and we can probably see how this has disadvantages. When we play strategy games ourselves, we often do better by looking a number of steps into the future. One new idea that this requires, is how we will anticipate our opponents move. This gets us into game theory . We're not going to borrow a whole lot from here, just the following: we will assume our opponent will work to minimize our score This switching from us wanting to maximize our score to the opponent wanting to minimize our score is called the minimax algorithm. As well look ahead into the future possibility of moves, we will use minimax to set our hypothetical behavior as well as our opponents You can investigate the pseudocode for minmax on wiki. # we're going to pull out and reformat some of our helper functions in the # TicTacToe class win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] def check_winning ( board , win_patterns ): for pattern in win_patterns : values = [ board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ] or values == [ 'O' , 'O' , 'O' ]: return True return False def check_stalemate ( board , win_patterns ): if ( ' ' not in board . values ()) and ( check_winning ( board , win_patterns ) == '' ): return True return False def minimax ( depth , board , maximizing_player , player_label , verbiose = False ): # infer the opponent opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] # set the available moves avail_moves = [ i for i in board . keys () if board [ i ] == ' ' ] # check if the depth is 0, or stalemate/winner has been reached # if so this is the basecase and we want to return get_score() terminal_move = is_terminal_node ( board , avail_moves ) if terminal_move or depth == 0 : score = get_score ( board , player_label , win_patterns ) if verbiose : print ( ' {} score: {} . depth: {} ' . format ( board , score , depth )) return score ### in the following we want to search through every possible board at the ### current level (the possible moves for the current player, given that the ### player is either the one whose turn it is or the imagined opponent) # call minimax where it is the current players turn and so we want to # maximize the score if maximizing_player : score = - np . Inf for move in avail_moves : new_board = board . copy () new_board [ move ] = player_label score = max ( score , minimax ( depth - 1 , new_board , False , player_label , verbiose )) if verbiose : print ( ' {} max. score: {} . depth: {} ' . format ( board , score , depth )) return score # call minimax where it is the opponent players turn and so we want to # minimize the score elif not maximizing_player : score = np . Inf for move in avail_moves : new_board = board . copy () new_board [ move ] = opponent score = min ( score , minimax ( depth - 1 , new_board , True , player_label , verbiose )) if verbiose : print ( ' {} min. score: {} . depth: {} ' . format ( board , score , depth )) return score def is_terminal_node ( board , avail_moves ): if check_winning ( board , win_patterns ): return True elif check_stalemate ( board , win_patterns ): return True else : return False def get_score ( board , player_label , win_patterns ): # this will look somewhat similar to our 1-step lookahead algorithm opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] score = 0 for pattern in win_patterns : values = [ board [ i ] for i in pattern ] # if the opponent wins, the score is -100 if values == [ opponent , opponent , opponent ]: score = - 100 elif values == [ player_label , player_label , player_label ]: score = 100 return score board = TicTacToe () . board board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} minimax ( depth = 1 , board = board , maximizing_player = True , player_label = 'O' ) 0 Finally, we need a couple wrapper functions to handle this. The first is a handler for the top level of the game tree (we want to see the minmax result for every possible move at the current place in the game) verbiose = True verbiose True def get_minimax ( depth , board , player_label , verbiose = False ): score = minimax ( depth - 1 , board , False , player_label , verbiose = verbiose ) return score def n_step_ai_temp ( board , win_patterns , player_label , n_steps , verbiose = False ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = { i : 1 for i in board . keys () if board [ i ] == ' ' } for move in avail_moves . keys (): temp_board = board . copy () temp_board [ move ] = player_label score = get_minimax ( n_steps , temp_board , player_label , verbiose = verbiose ) avail_moves [ move ] = score return avail_moves Let's test our n_step_ai while we're still returning the dictionary of available moves. Does this make sense? board = TicTacToe () . board board [ 1 ] = 'X' board [ 5 ] = 'O' board [ 2 ] = 'X' # with this setup we should see that a good move will be to play 3 # be sure to vary the number of lookahead steps n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 2 ) {3: 100, 4: 0, 6: 0, 7: 0, 8: 0, 9: 0} Looks like it's making sense. Let's also try when the opponent looks like they've got a winning move to be made board = TicTacToe () . board board [ 1 ] = 'X' board [ 5 ] = 'O' board [ 2 ] = 'X' board [ 4 ] = 'O' # with this setup we should see that a good move will be to play 6 or 3 # be sure to vary the number of lookahead steps n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 3 ) Let's look at the following as well. This should be an interesting game situation to you. It is \"X's\" move. What do you notice? Does this outcome make sense? X cannot win this game board = TicTacToe () . board board [ 1 ] = 'O' board [ 5 ] = 'O' board [ 2 ] = 'X' board [ 8 ] = 'X' # be sure to vary the number of lookahead steps n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 4 , verbiose = False ) # as a side note, sometimes it helps to interpret the outcome if you discount # the results from looking further down the game tree. (maybe O will make a mistake) # how would you encode this in your n-step look ahead algorithm? {3: -100, 4: -100, 6: -100, 7: -100, 9: -100}","title":"4.1 N-Step Look Ahead and Minimax"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#42-packaging-for-gameengine","text":"Nice. Let's finish packaging our n_steps_ai so we can feed it to our game engine. def n_step_ai ( board , win_patterns , player_label , n_steps = 3 ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = { i : 1 for i in board . keys () if board [ i ] == ' ' } for move in avail_moves . keys (): temp_board = board . copy () temp_board [ move ] = player_label score = get_minimax ( n_steps , temp_board , player_label ) avail_moves [ move ] = score ########################################## ### The rest of our ai agent harness is the same ########################################## # first grab max score max_score = max ( avail_moves . values ()) # then select all moves that have this max score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # return a random selection of the moves with the max score move = random . choice ( valid ) return move game = GameEngine ( setup = 'user' , user_ai = n_step_ai ) game . setup_game () How many Players? (type 0, 1, or 2)1 who will go first? (X, (AI), or O (Player))X game . play_game () | | | | | | | | | | | | | | | | |X| | | | | | | O, what's your move?5 | | | | |X|O| | | | | | | | | | |X|O| | | |X| | O, what's your move?2 | |O| | |X|O| | | |X| | | |O| | |X|O| | |X|X| | O, what's your move?1 |O|O| | |X|O| | |X|X| | 'X' Won! |O|O| | |X|O| | |X|X|X| <__main__.GameEngine at 0x7fe529a0d850> Let's investigate the behavior of our AI and double check that it makes sense board = game . board board [ 9 ] = ' ' board {1: 'O', 2: 'O', 3: ' ', 4: 'X', 5: 'O', 6: ' ', 7: 'X', 8: 'X', 9: ' '} game . visualize_board () |O|O| | |X|O| | |X|X| | n_step_ai_temp ( board = board , win_patterns = win_patterns , player_label = 'X' , n_steps = 3 ) {3: -100, 6: -100, 9: 100}","title":"4.2 Packaging for GameEngine"},{"location":"project/P4_N_Step_Look_Ahead_Agents/#43-writing-tests","text":"def test_n_step_ai (): random . seed ( 42 ) game = GameEngine ( setup = 'auto' , user_ai = n_step_ai ) game . setup_game () game . play_game () # check that the winner is X assert game . winner == 'X' , \"Winner should be X!\" # check that the ai level is set to 3 which means our engine is properly # accessing the user defined ai assert game . ai_level == 3 , \"The engine is not using the user defined AI!\" test_n_step_ai () | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | 'X' Won! |X|O|X| |O|X|O| | | |X|","title":"4.3 Writing Tests"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/","text":"Data Science Foundations, Lab 1: Data Hunt I \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's a data hunt. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from ipywidgets import interact df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_explorers/main/assets/imdb_movies.csv\" ) # converting years to numbers for easy conditionals df [ 'year' ] = pd . to_numeric ( df [ 'year' ], errors = 'coerce' ) df . shape /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) (85855, 22) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics 0 tt0000009 Miss Jerry Miss Jerry 1894.0 1894-10-09 Romance 45 USA None Alexander Black ... Blanche Bayliss, William Courtenay, Chauncey D... The adventures of a female reporter in the 1890s. 5.9 154 NaN NaN NaN NaN 1.0 2.0 1 tt0000574 The Story of the Kelly Gang The Story of the Kelly Gang 1906.0 1906-12-26 Biography, Crime, Drama 70 Australia None Charles Tait ... Elizabeth Tait, John Tait, Norman Campbell, Be... True story of notorious Australian outlaw Ned ... 6.1 589 $ 2250 NaN NaN NaN 7.0 7.0 2 tt0001892 Den sorte dr\u00f8m Den sorte dr\u00f8m 1911.0 1911-08-19 Drama 53 Germany, Denmark NaN Urban Gad ... Asta Nielsen, Valdemar Psilander, Gunnar Helse... Two men of high rank are both wooing the beaut... 5.8 188 NaN NaN NaN NaN 5.0 2.0 3 rows \u00d7 22 columns Q1 What american director has the highest mean avg_vote? \u00b6 df . groupby ( 'director' )[ 'avg_vote' ] . mean () . sort_values ( ascending = False ) director Msn Surya 9.9 Aalmist Subba 9.8 Sampath Rudra 9.8 Basheed S.K. 9.8 Abner Official 9.8 ... Ramana Reddy B.V. 1.0 Tam\u00e1s Gerencs\u00e9r 1.0 Tommy Yu 1.0 G\u00f6khan G\u00f6k 1.0 Yasutake Torii 1.0 Name: avg_vote, Length: 34733, dtype: float64 Q2 What american director with more than 5 movies, has the highest mean avg_vote? \u00b6 df . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 5 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . mean () . sort_values ( ascending = False ) director Christopher Nolan 8.218182 Satyajit Ray 8.025000 Andrei Tarkovsky 8.014286 Hayao Miyazaki 8.008333 Sergio Leone 7.928571 ... Bill Zebub 2.483333 Mark Polonia 2.433333 Paul T.T. Easter 2.383333 Christopher Forbes 2.000000 Brett Kelly 1.533333 Name: avg_vote, Length: 3047, dtype: float64 Q3 What director has the largest variance in avg_vote? \u00b6 df . groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) director Deniz Denizciler 4.030509 Rudi Lagemann 3.747666 Emilio Ruiz Barrachina 3.676955 Krishna Ghattamaneni 3.676955 Milos Avramovic 3.606245 ... \u00dcmit Degirmenci NaN \u00dcmit El\u00e7i NaN \u00dcmit K\u00f6reken NaN \u00deorsteinn Gunnar Bjarnason NaN \u00de\u00f3rhildur \u00deorleifsd\u00f3ttir NaN Name: avg_vote, Length: 34733, dtype: float64 Q4 What director with more than 10 movies has the largest variance in avg_vote? \u00b6 df . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 10 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) director Harry Baweja 1.869954 Shaji Kailas 1.854502 Zdenek Troska 1.775984 Adam Rifkin 1.711251 Ram Gopal Varma 1.687850 ... Ford Beebe 0.224343 Ray Nazarro 0.210311 Jean Gr\u00e9millon 0.196946 Louis Feuillade 0.156428 Tsutomu Shibayama 0.126121 Name: avg_vote, Length: 1135, dtype: float64 Q5 What american directors with more than 5 movies have the largest variance in avg_vote? \u00b6 df . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 5 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) director Jorge Ameer 2.200606 Tigran Keosayan 2.188150 Nikos Zervos 2.093243 Kundan Shah 2.060502 Feroz Khan 2.036220 ... Sang-il Lee 0.132916 Nate Watt 0.129099 Daisuke Nishio 0.127242 Tsutomu Shibayama 0.126121 Pierre Chenal 0.103280 Name: avg_vote, Length: 3047, dtype: float64 Q6 Where does M. Night Shyamalan fall on this rank scale? \u00b6 (He's number 36/859) var_rank = df . loc [ df [ 'country' ] == 'USA' ] . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 5 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) . reset_index () display ( var_rank . loc [ var_rank [ 'director' ] == 'M. Night Shyamalan' ]) print ( var_rank . shape [ 0 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } director avg_vote 36 M. Night Shyamalan 1.258791 859 what happens when you only include directors who, on average (based on mean), have made most their movies after 1990 and have produced 10 or more movies? (Shyamalan rises to 3/83) var_rank = df . loc [ df [ 'country' ] == 'USA' ] \\ . groupby ( 'director' ) . filter ( lambda x : ( x [ 'year' ] . mean () > 1990 ) & ( x . shape [ 0 ] > 9 )) \\ . groupby ( 'director' )[[ 'avg_vote' , 'votes' ]] . describe () . sort_values ( by = ( 'avg_vote' , 'std' ), ascending = False ) display ( var_rank . iloc [: 10 ]) print ( var_rank . shape [ 0 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } avg_vote votes count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max director Adam Rifkin 15.0 5.053333 1.711251 1.3 4.200 5.80 6.100 6.9 15.0 4417.400000 9414.430237 124.0 525.50 1084.0 1782.50 34958.0 Mark L. Lester 19.0 4.768421 1.262296 2.3 4.200 4.70 5.800 6.7 19.0 11479.052632 32768.240173 298.0 549.00 1219.0 4405.50 143443.0 M. Night Shyamalan 10.0 5.970000 1.258791 4.0 4.975 6.05 6.650 8.1 10.0 239264.500000 262079.154005 731.0 97982.50 169426.0 308493.25 894385.0 Sean McNamara 12.0 5.216667 1.252513 2.9 4.725 5.60 5.950 7.0 12.0 9221.166667 13933.853515 365.0 1085.25 1416.0 12191.50 44808.0 Sam Firstenberg 10.0 4.550000 1.174970 2.8 3.475 4.85 5.325 6.2 10.0 1890.400000 1552.704107 153.0 713.50 1282.0 3317.25 4330.0 Gus Van Sant 12.0 6.583333 1.169952 4.4 6.025 6.95 7.300 8.3 12.0 112868.166667 232569.906962 3674.0 16191.75 38458.5 82270.25 837379.0 John Lyde 16.0 4.937500 1.159813 3.6 3.900 4.80 5.500 7.0 16.0 1021.937500 911.075516 113.0 409.50 802.0 1360.00 3270.0 Michael Polish 12.0 5.458333 1.154011 3.4 4.700 5.45 6.300 7.2 12.0 4396.833333 5662.740034 528.0 1698.75 3395.5 3624.00 21873.0 Randal Kleiser 12.0 5.708333 1.126102 3.5 5.050 5.60 6.750 7.2 12.0 35908.916667 65204.420315 1030.0 2458.75 11245.5 41162.50 232940.0 Brian Brough 12.0 5.575000 1.096378 3.5 5.225 5.80 6.350 7.0 12.0 675.500000 590.071721 104.0 134.00 606.5 1048.75 1842.0 83 var_rank = df . loc [ df [ 'country' ] == 'USA' ] \\ . groupby ( 'director' ) . filter ( lambda x : ( x [ 'avg_vote' ] . max () > 8 ) & ( x [ 'votes' ] . mean () > 1e3 ) & ( x . shape [ 0 ] > 2 )) \\ . groupby ( 'director' )[[ 'avg_vote' , 'votes' ]] . describe () . sort_values ( by = ( 'avg_vote' , 'std' ), ascending = False ) display ( var_rank . iloc [: 10 ]) print ( var_rank . shape [ 0 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } avg_vote votes count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max director Peter Bogdanovich 13.0 6.446154 1.564510 2.9 6.000 7.00 7.600 8.1 13.0 12023.307692 1.510756e+04 870.0 1262.00 2426.0 19253.00 41283.0 Francis Ford Coppola 18.0 6.777778 1.444077 3.1 6.225 6.65 7.550 9.2 18.0 226387.555556 4.392702e+05 199.0 5078.25 23681.5 164311.25 1572674.0 Richard Marquand 5.0 6.320000 1.375500 4.5 5.800 6.50 6.500 8.3 5.0 188499.800000 4.134480e+05 411.0 560.00 598.0 12894.00 928036.0 Curtis Hanson 6.0 6.150000 1.361984 4.3 5.350 6.40 6.550 8.2 6.0 100734.333333 2.069594e+05 209.0 1345.00 20931.0 39724.25 521530.0 Sean Penn 4.0 6.525000 1.322561 4.9 5.950 6.55 7.125 8.1 4.0 157547.000000 2.695851e+05 4409.0 10514.00 32543.5 179576.50 560692.0 Timothy A. Chey 6.0 6.250000 1.291124 4.4 5.775 6.05 6.850 8.2 6.0 1447.500000 5.878295e+02 788.0 923.50 1510.5 1812.50 2235.0 M. Night Shyamalan 10.0 5.970000 1.258791 4.0 4.975 6.05 6.650 8.1 10.0 239264.500000 2.620792e+05 731.0 97982.50 169426.0 308493.25 894385.0 Stanley Kubrick 5.0 7.280000 1.202913 5.5 6.600 7.90 8.000 8.4 5.0 80954.400000 6.855686e+04 9649.0 20806.00 79652.0 121994.00 172671.0 Gus Van Sant 12.0 6.583333 1.169952 4.4 6.025 6.95 7.300 8.3 12.0 112868.166667 2.325699e+05 3674.0 16191.75 38458.5 82270.25 837379.0 Frank Darabont 4.0 7.975000 1.164403 6.9 7.050 7.85 8.775 9.3 4.0 929718.000000 1.008586e+06 51763.0 219886.75 694132.0 1403963.25 2278845.0 66 Q7 How many movies were made each year in US from 2000-2020 \u00b6 df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 )][[ 'year' ]] . value_counts ( sort = False ) year 2000.0 1345 2001.0 1447 2002.0 1405 2003.0 1496 2004.0 1681 2005.0 1827 2006.0 2063 2007.0 2074 2008.0 2175 2009.0 2298 2010.0 2281 2011.0 2429 2012.0 2560 2013.0 2783 2014.0 2942 2015.0 2977 2016.0 3138 2017.0 3329 2018.0 3257 2019.0 2841 2020.0 789 dtype: int64 Q8 Visualize The Results of Q7! \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 5 )) df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 )][[ 'year' ]] . value_counts ( sort = False ) . plot ( kind = 'bar' , ax = ax ) <AxesSubplot:xlabel='year'> Q9 For single country movies, how many movies were made each year in each country from 2000-2020, only include countries that made more than 1000 movies in that timeframe \u00b6 df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 ) & ( ~ df [ 'country' ] . str . contains ( ',' , na = False ))] \\ . groupby ( 'country' ) . filter ( lambda x : x . shape [ 0 ] > 1000 ) \\ . groupby ([ 'country' , 'year' ])[[ 'title' ]] . count () . reset_index () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year title 0 Canada 2000.0 39 1 Canada 2001.0 51 2 Canada 2002.0 49 3 Canada 2003.0 38 4 Canada 2004.0 52 ... ... ... ... 163 USA 2016.0 869 164 USA 2017.0 905 165 USA 2018.0 886 166 USA 2019.0 700 167 USA 2020.0 276 168 rows \u00d7 3 columns Q10 Visualize the results from Q9! \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) countries = df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 ) & ( ~ df [ 'country' ] . str . contains ( ',' , na = False ))] \\ . groupby ( 'country' ) . filter ( lambda x : x . shape [ 0 ] > 1000 ) \\ . groupby ([ 'country' , 'year' ])[[ 'title' ]] . count () . reset_index () for country in countries . groupby ( 'country' ): country [ 1 ] . plot ( x = 'year' , y = 'title' , ax = ax , label = country [ 0 ])","title":"SOLN L1 Descriptive Statistics Data Hunt"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#data-science-foundations-lab-1-data-hunt-i","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's a data hunt. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from ipywidgets import interact df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_explorers/main/assets/imdb_movies.csv\" ) # converting years to numbers for easy conditionals df [ 'year' ] = pd . to_numeric ( df [ 'year' ], errors = 'coerce' ) df . shape /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) (85855, 22) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics 0 tt0000009 Miss Jerry Miss Jerry 1894.0 1894-10-09 Romance 45 USA None Alexander Black ... Blanche Bayliss, William Courtenay, Chauncey D... The adventures of a female reporter in the 1890s. 5.9 154 NaN NaN NaN NaN 1.0 2.0 1 tt0000574 The Story of the Kelly Gang The Story of the Kelly Gang 1906.0 1906-12-26 Biography, Crime, Drama 70 Australia None Charles Tait ... Elizabeth Tait, John Tait, Norman Campbell, Be... True story of notorious Australian outlaw Ned ... 6.1 589 $ 2250 NaN NaN NaN 7.0 7.0 2 tt0001892 Den sorte dr\u00f8m Den sorte dr\u00f8m 1911.0 1911-08-19 Drama 53 Germany, Denmark NaN Urban Gad ... Asta Nielsen, Valdemar Psilander, Gunnar Helse... Two men of high rank are both wooing the beaut... 5.8 188 NaN NaN NaN NaN 5.0 2.0 3 rows \u00d7 22 columns","title":"Data Science Foundations, Lab 1: Data Hunt I"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q1-what-american-director-has-the-highest-mean-avg_vote","text":"df . groupby ( 'director' )[ 'avg_vote' ] . mean () . sort_values ( ascending = False ) director Msn Surya 9.9 Aalmist Subba 9.8 Sampath Rudra 9.8 Basheed S.K. 9.8 Abner Official 9.8 ... Ramana Reddy B.V. 1.0 Tam\u00e1s Gerencs\u00e9r 1.0 Tommy Yu 1.0 G\u00f6khan G\u00f6k 1.0 Yasutake Torii 1.0 Name: avg_vote, Length: 34733, dtype: float64","title":"Q1 What american director has the highest mean  avg_vote?"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q2-what-american-director-with-more-than-5-movies-has-the-highest-mean-avg_vote","text":"df . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 5 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . mean () . sort_values ( ascending = False ) director Christopher Nolan 8.218182 Satyajit Ray 8.025000 Andrei Tarkovsky 8.014286 Hayao Miyazaki 8.008333 Sergio Leone 7.928571 ... Bill Zebub 2.483333 Mark Polonia 2.433333 Paul T.T. Easter 2.383333 Christopher Forbes 2.000000 Brett Kelly 1.533333 Name: avg_vote, Length: 3047, dtype: float64","title":"Q2 What american director with more than 5 movies, has the highest mean avg_vote?"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q3-what-director-has-the-largest-variance-in-avg_vote","text":"df . groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) director Deniz Denizciler 4.030509 Rudi Lagemann 3.747666 Emilio Ruiz Barrachina 3.676955 Krishna Ghattamaneni 3.676955 Milos Avramovic 3.606245 ... \u00dcmit Degirmenci NaN \u00dcmit El\u00e7i NaN \u00dcmit K\u00f6reken NaN \u00deorsteinn Gunnar Bjarnason NaN \u00de\u00f3rhildur \u00deorleifsd\u00f3ttir NaN Name: avg_vote, Length: 34733, dtype: float64","title":"Q3 What director has the largest variance in avg_vote?"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q4-what-director-with-more-than-10-movies-has-the-largest-variance-in-avg_vote","text":"df . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 10 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) director Harry Baweja 1.869954 Shaji Kailas 1.854502 Zdenek Troska 1.775984 Adam Rifkin 1.711251 Ram Gopal Varma 1.687850 ... Ford Beebe 0.224343 Ray Nazarro 0.210311 Jean Gr\u00e9millon 0.196946 Louis Feuillade 0.156428 Tsutomu Shibayama 0.126121 Name: avg_vote, Length: 1135, dtype: float64","title":"Q4 What director with more than 10 movies has the largest variance in avg_vote?"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q5-what-american-directors-with-more-than-5-movies-have-the-largest-variance-in-avg_vote","text":"df . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 5 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) director Jorge Ameer 2.200606 Tigran Keosayan 2.188150 Nikos Zervos 2.093243 Kundan Shah 2.060502 Feroz Khan 2.036220 ... Sang-il Lee 0.132916 Nate Watt 0.129099 Daisuke Nishio 0.127242 Tsutomu Shibayama 0.126121 Pierre Chenal 0.103280 Name: avg_vote, Length: 3047, dtype: float64","title":"Q5 What american directors with more than 5 movies have the largest variance in avg_vote?"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q6-where-does-m-night-shyamalan-fall-on-this-rank-scale","text":"(He's number 36/859) var_rank = df . loc [ df [ 'country' ] == 'USA' ] . groupby ( 'director' ) . filter ( lambda x : x . shape [ 0 ] > 5 ) . \\ groupby ( 'director' )[ 'avg_vote' ] . std () . sort_values ( ascending = False ) . reset_index () display ( var_rank . loc [ var_rank [ 'director' ] == 'M. Night Shyamalan' ]) print ( var_rank . shape [ 0 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } director avg_vote 36 M. Night Shyamalan 1.258791 859 what happens when you only include directors who, on average (based on mean), have made most their movies after 1990 and have produced 10 or more movies? (Shyamalan rises to 3/83) var_rank = df . loc [ df [ 'country' ] == 'USA' ] \\ . groupby ( 'director' ) . filter ( lambda x : ( x [ 'year' ] . mean () > 1990 ) & ( x . shape [ 0 ] > 9 )) \\ . groupby ( 'director' )[[ 'avg_vote' , 'votes' ]] . describe () . sort_values ( by = ( 'avg_vote' , 'std' ), ascending = False ) display ( var_rank . iloc [: 10 ]) print ( var_rank . shape [ 0 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } avg_vote votes count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max director Adam Rifkin 15.0 5.053333 1.711251 1.3 4.200 5.80 6.100 6.9 15.0 4417.400000 9414.430237 124.0 525.50 1084.0 1782.50 34958.0 Mark L. Lester 19.0 4.768421 1.262296 2.3 4.200 4.70 5.800 6.7 19.0 11479.052632 32768.240173 298.0 549.00 1219.0 4405.50 143443.0 M. Night Shyamalan 10.0 5.970000 1.258791 4.0 4.975 6.05 6.650 8.1 10.0 239264.500000 262079.154005 731.0 97982.50 169426.0 308493.25 894385.0 Sean McNamara 12.0 5.216667 1.252513 2.9 4.725 5.60 5.950 7.0 12.0 9221.166667 13933.853515 365.0 1085.25 1416.0 12191.50 44808.0 Sam Firstenberg 10.0 4.550000 1.174970 2.8 3.475 4.85 5.325 6.2 10.0 1890.400000 1552.704107 153.0 713.50 1282.0 3317.25 4330.0 Gus Van Sant 12.0 6.583333 1.169952 4.4 6.025 6.95 7.300 8.3 12.0 112868.166667 232569.906962 3674.0 16191.75 38458.5 82270.25 837379.0 John Lyde 16.0 4.937500 1.159813 3.6 3.900 4.80 5.500 7.0 16.0 1021.937500 911.075516 113.0 409.50 802.0 1360.00 3270.0 Michael Polish 12.0 5.458333 1.154011 3.4 4.700 5.45 6.300 7.2 12.0 4396.833333 5662.740034 528.0 1698.75 3395.5 3624.00 21873.0 Randal Kleiser 12.0 5.708333 1.126102 3.5 5.050 5.60 6.750 7.2 12.0 35908.916667 65204.420315 1030.0 2458.75 11245.5 41162.50 232940.0 Brian Brough 12.0 5.575000 1.096378 3.5 5.225 5.80 6.350 7.0 12.0 675.500000 590.071721 104.0 134.00 606.5 1048.75 1842.0 83 var_rank = df . loc [ df [ 'country' ] == 'USA' ] \\ . groupby ( 'director' ) . filter ( lambda x : ( x [ 'avg_vote' ] . max () > 8 ) & ( x [ 'votes' ] . mean () > 1e3 ) & ( x . shape [ 0 ] > 2 )) \\ . groupby ( 'director' )[[ 'avg_vote' , 'votes' ]] . describe () . sort_values ( by = ( 'avg_vote' , 'std' ), ascending = False ) display ( var_rank . iloc [: 10 ]) print ( var_rank . shape [ 0 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } avg_vote votes count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max director Peter Bogdanovich 13.0 6.446154 1.564510 2.9 6.000 7.00 7.600 8.1 13.0 12023.307692 1.510756e+04 870.0 1262.00 2426.0 19253.00 41283.0 Francis Ford Coppola 18.0 6.777778 1.444077 3.1 6.225 6.65 7.550 9.2 18.0 226387.555556 4.392702e+05 199.0 5078.25 23681.5 164311.25 1572674.0 Richard Marquand 5.0 6.320000 1.375500 4.5 5.800 6.50 6.500 8.3 5.0 188499.800000 4.134480e+05 411.0 560.00 598.0 12894.00 928036.0 Curtis Hanson 6.0 6.150000 1.361984 4.3 5.350 6.40 6.550 8.2 6.0 100734.333333 2.069594e+05 209.0 1345.00 20931.0 39724.25 521530.0 Sean Penn 4.0 6.525000 1.322561 4.9 5.950 6.55 7.125 8.1 4.0 157547.000000 2.695851e+05 4409.0 10514.00 32543.5 179576.50 560692.0 Timothy A. Chey 6.0 6.250000 1.291124 4.4 5.775 6.05 6.850 8.2 6.0 1447.500000 5.878295e+02 788.0 923.50 1510.5 1812.50 2235.0 M. Night Shyamalan 10.0 5.970000 1.258791 4.0 4.975 6.05 6.650 8.1 10.0 239264.500000 2.620792e+05 731.0 97982.50 169426.0 308493.25 894385.0 Stanley Kubrick 5.0 7.280000 1.202913 5.5 6.600 7.90 8.000 8.4 5.0 80954.400000 6.855686e+04 9649.0 20806.00 79652.0 121994.00 172671.0 Gus Van Sant 12.0 6.583333 1.169952 4.4 6.025 6.95 7.300 8.3 12.0 112868.166667 2.325699e+05 3674.0 16191.75 38458.5 82270.25 837379.0 Frank Darabont 4.0 7.975000 1.164403 6.9 7.050 7.85 8.775 9.3 4.0 929718.000000 1.008586e+06 51763.0 219886.75 694132.0 1403963.25 2278845.0 66","title":"Q6 Where does M. Night Shyamalan fall on this rank scale?"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q7-how-many-movies-were-made-each-year-in-us-from-2000-2020","text":"df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 )][[ 'year' ]] . value_counts ( sort = False ) year 2000.0 1345 2001.0 1447 2002.0 1405 2003.0 1496 2004.0 1681 2005.0 1827 2006.0 2063 2007.0 2074 2008.0 2175 2009.0 2298 2010.0 2281 2011.0 2429 2012.0 2560 2013.0 2783 2014.0 2942 2015.0 2977 2016.0 3138 2017.0 3329 2018.0 3257 2019.0 2841 2020.0 789 dtype: int64","title":"Q7 How many movies were made each year in US from 2000-2020"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q8-visualize-the-results-of-q7","text":"fig , ax = plt . subplots ( figsize = ( 10 , 5 )) df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 )][[ 'year' ]] . value_counts ( sort = False ) . plot ( kind = 'bar' , ax = ax ) <AxesSubplot:xlabel='year'>","title":"Q8 Visualize The Results of Q7!"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q9-for-single-country-movies-how-many-movies-were-made-each-year-in-each-country-from-2000-2020-only-include-countries-that-made-more-than-1000-movies-in-that-timeframe","text":"df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 ) & ( ~ df [ 'country' ] . str . contains ( ',' , na = False ))] \\ . groupby ( 'country' ) . filter ( lambda x : x . shape [ 0 ] > 1000 ) \\ . groupby ([ 'country' , 'year' ])[[ 'title' ]] . count () . reset_index () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year title 0 Canada 2000.0 39 1 Canada 2001.0 51 2 Canada 2002.0 49 3 Canada 2003.0 38 4 Canada 2004.0 52 ... ... ... ... 163 USA 2016.0 869 164 USA 2017.0 905 165 USA 2018.0 886 166 USA 2019.0 700 167 USA 2020.0 276 168 rows \u00d7 3 columns","title":"Q9 For single country movies, how many movies were made each year in each country from 2000-2020, only include countries that made more than 1000 movies in that timeframe"},{"location":"solutions/SOLN_L1_Descriptive_Statistics_Data_Hunt/#q10-visualize-the-results-from-q9","text":"fig , ax = plt . subplots ( figsize = ( 10 , 10 )) countries = df . loc [( df [ 'year' ] >= 2000 ) & ( df [ 'year' ] <= 2020 ) & ( ~ df [ 'country' ] . str . contains ( ',' , na = False ))] \\ . groupby ( 'country' ) . filter ( lambda x : x . shape [ 0 ] > 1000 ) \\ . groupby ([ 'country' , 'year' ])[[ 'title' ]] . count () . reset_index () for country in countries . groupby ( 'country' ): country [ 1 ] . plot ( x = 'year' , y = 'title' , ax = ax , label = country [ 0 ])","title":"Q10 Visualize the results from Q9!"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/","text":"Data Science Foundations Lab 2: Data Hunt II \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's the data hunt part TWO. Preparing Environment and Importing Data \u00b6 Import Packages \u00b6 ! pip install - U plotly Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1) Collecting plotly Downloading plotly-5.1.0-py2.py3-none-any.whl (20.6 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.6 MB 1.3 MB/s \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0) Collecting tenacity>=6.2.0 Downloading tenacity-8.0.1-py3-none-any.whl (24 kB) Installing collected packages: tenacity, plotly Attempting uninstall: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-5.1.0 tenacity-8.0.1 # our standard libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns from ipywidgets import interact # our stats libraries import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy # our scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score Import and Clean Data \u00b6 df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_fundamentals/main/assets/truffle_rates.csv\" ) df = df . loc [ df [ 'rate' ] > 0 ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group rate 0 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.167097 1 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.153827 2 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.100299 3 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.333008 4 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.078108 df . shape (9210, 6) Exploratory Data Analysis \u00b6 Q1 Finding Influential Features \u00b6 Which of the five features (base_cake, truffle_type, primary_flavor, secondary_flavor, color_group) of the truffles is most influential on production rate? Back your answer with both a visualization of the distributions (boxplot, kernel denisty estimate, histogram, violin plot) and a statistical test (moods median, ANOVA, t-test) Be sure: everything is labeled (can you improve your labels with additional descriptive statistical information e.g. indicate mean, std, etc.) you meet the assumptions of your statistical test Q1.1 Visualization \u00b6 Use any number of visualizations. Here is an example to get you started: # Example: a KDE of the truffle_type and base_cake columns fig , ax = plt . subplots ( 2 , 1 , figsize = ( 12 , 12 )) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'truffle_type' ], fill = True , ax = ax [ 0 ]) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'base_cake' ], fill = True , ax = ax [ 1 ]) <AxesSubplot:xlabel='rate', ylabel='Density'> fig , ax = plt . subplots ( 5 , 1 , figsize = ( 10 , 40 )) for idx , col in enumerate ( df . columns [: - 1 ]): df . boxplot ( by = col , column = 'rate' , ax = ax [ idx ]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 9 ( ) missing from current font. fig.canvas.print_figure(bytes_io, **kw) Q1.2 Statistical Analysis \u00b6 What statistical tests can you perform to evaluate your hypothesis from the visualizations (maybe you think one particular feature is significant). Here's an ANOVA on the truffle_type column to get you started: model = ols ( 'rate ~ C( {} )' . format ( 'truffle_type' ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Is this P value significant? What is the null hypothesis? How do we check the assumptions of ANOVA? Answer: All groups fail for normal distribution of residuals and homogeneity of variances. So we cannot use ANOVA with this data: confidence_level = 0.05 for idx , col in enumerate ( df . columns [: - 1 ]): model = ols ( 'rate ~ C( {} )' . format ( col ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) print ( \"Shapiro-Wilk\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( * stats . shapiro ( model . resid ))) if stats . shapiro ( model . resid ) . pvalue < confidence_level : shapiro_rej = True print ( f \" \\t reject: { shapiro_rej } \" ) print ( \"Bartlett\" ) gb = df . groupby ( col )[ 'rate' ] w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( w , pvalue )) if pvalue < confidence_level : shapiro_rej = True print ( f \" \\t reject: { shapiro_rej } \" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(base_cake) 331.373550 5.0 2349.684756 0.0 Residual 259.606073 9204.0 NaN NaN Shapiro-Wilk statistic=0.93, pvalue=0.00e+00 reject: True Bartlett statistic=619.37, pvalue=1.32e-131 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Shapiro-Wilk statistic=0.96, pvalue=1.37e-42 reject: True Bartlett statistic=533.02, pvalue=1.80e-116 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(primary_flavor) 159.105452 47.0 71.815842 0.0 Residual 431.874171 9162.0 NaN NaN Shapiro-Wilk statistic=0.97, pvalue=6.49e-38 reject: True Bartlett /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") statistic=1609.00, pvalue=1.85e-306 reject: True .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(secondary_flavor) 115.773877 28.0 79.884192 0.0 Residual 475.205747 9181.0 NaN NaN Shapiro-Wilk statistic=0.97, pvalue=4.34e-39 reject: True Bartlett statistic=1224.49, pvalue=3.55e-240 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(color_group) 33.878491 11.0 50.849974 1.873235e-109 Residual 557.101132 9198.0 NaN NaN Shapiro-Wilk statistic=0.96, pvalue=1.40e-44 reject: True Bartlett statistic=298.64, pvalue=1.69e-57 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") Instead we might use Moods Median Q2 Finding Best and Worst Groups \u00b6 Q2.1 Compare Every Group to the Whole \u00b6 Of the primary flavors (feature), what 5 flavors (groups) would you recommend Truffletopia discontinue? Iterate through every level (i.e. pound, cheese, sponge cakes) of every category (i.e. base cake, primary flavor, secondary flavor) and use moods median testing to compare the group distribution to the grand median rate. moodsdf = pd . DataFrame () target = 'rate' for col in df . columns [: - 1 ]: for truff in df [ col ] . unique (): group = df . loc [ df [ col ] == truff ][ target ] pop = df . loc [ ~ ( df [ col ] == truff )][ target ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , table ]) . T ]) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'table' ] moodsdf [ 'p_value' ] = moodsdf [ 'p_value' ] . astype ( float ) print ( moodsdf . shape ) (98, 9) After you've computed a moods median test on every group, filter any data above a significance level of 0.05 moodsdf = moodsdf . loc [( moodsdf [ 'p_value' ] < confidence_level )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( moodsdf . shape ) (83, 9) Return the groups with the lowest median performance (your table need not look exactly like the one I've created) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 0 secondary_flavor Wild Cherry Cream 56.867453 4.661980e-14 0.310345 0.139998 0.085628 100 [[12, 4593], [88, 4517]] 1 primary_flavor Coconut 56.867453 4.661980e-14 0.310345 0.139998 0.085628 100 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.556345 4.302530e-15 0.310345 0.129178 0.092878 85 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.32026 7.846169e-13 0.310345 0.145727 0.095758 91 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.545249 4.142688e-11 0.310345 0.148964 0.10588 70 [[7, 4598], [63, 4542]] ... ... ... ... ... ... ... ... ... ... 78 secondary_flavor Vanilla 40.484134 1.982191e-10 0.310345 0.559808 0.664252 200 [[145, 4460], [55, 4550]] 79 primary_flavor Orange 60.723646 6.567649e-15 0.310345 0.580579 0.681157 200 [[155, 4450], [45, 4560]] 80 primary_flavor Plum 308.037116 5.845614e-69 0.310345 0.669126 0.681309 300 [[300, 4305], [0, 4605]] 81 primary_flavor Cheesecake 99.085851 2.417896e-23 0.310345 0.745813 0.689802 100 [[100, 4505], [0, 4605]] 82 base_cake Sponge 2107.437614 0.000000e+00 0.310345 0.711797 0.699679 1800 [[1774, 2831], [26, 4579]] 83 rows \u00d7 9 columns We would want to cut the following primary flavors. Check to see that you get a similar answer. rip wild cherry cream. ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] moodsdf . loc [ moodsdf [ 'descriptor' ] == 'primary_flavor' ][: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 1 primary_flavor Coconut 56.867453 4.661980e-14 0.310345 0.139998 0.085628 100 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.556345 4.302530e-15 0.310345 0.129178 0.092878 85 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.32026 7.846169e-13 0.310345 0.145727 0.095758 91 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.545249 4.142688e-11 0.310345 0.148964 0.10588 70 [[7, 4598], [63, 4542]] 10 primary_flavor Gingersnap 131.113519 2.338438e-30 0.310345 0.159268 0.143347 192 [[17, 4588], [175, 4430]] Q2.2 Beyond Statistical Testing: Using Reasoning \u00b6 Let's look at the total profile of the products associated with the five worst primary flavors. Given the number of different products made with any of these flavors, would you alter your answer at all? Answer: We would opt to only discontinue the gingersnap, and wild cherry cream flavors. The other flavors, Pink Lemonade, Chocolate, and Coconut may be subject to Simpson's Paradox since we do not have adequate sampling of them with other categories # 1. filter df for only bottom five flavors # 2. groupby all columns besides rate # 3. describe the rate column. # by doing this we can evaluate just how much sampling variety we have for the # worst performing flavors. bottom_five = [ 'Coconut' , 'Pink Lemonade' , 'Chocolate' , 'Wild Cherry Cream' , 'Gingersnap' ] df . loc [ df [ 'primary_flavor' ] . isin ( bottom_five )] . groupby ( list ( df . columns [: - 1 ]))[ 'rate' ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max base_cake truffle_type primary_flavor secondary_flavor color_group Butter Jelly Filled Pink Lemonade Butter Rum Rose 85.0 0.129178 0.137326 0.000061 0.032887 0.092878 0.171350 0.860045 Chiffon Candy Outer Wild Cherry Cream Rock and Rye Olive 17.0 0.094287 0.059273 0.010464 0.053976 0.077098 0.120494 0.229933 Chocolate Outer Gingersnap Dill Pickle Burgundy 59.0 0.133272 0.080414 0.021099 0.069133 0.137972 0.172066 0.401387 Jelly Filled Chocolate Tutti Frutti Burgundy 91.0 0.145727 0.135230 0.000033 0.044847 0.095758 0.185891 0.586570 Pound Candy Outer Coconut Wild Cherry Cream Taupe 100.0 0.139998 0.147723 0.000705 0.036004 0.085628 0.187318 0.775210 Chocolate Outer Gingersnap Rock and Rye Black 67.0 0.156160 0.110666 0.002846 0.074615 0.139572 0.241114 0.551898 Jelly Filled Gingersnap Kiwi Taupe 66.0 0.185662 0.132272 0.000014 0.086377 0.166340 0.247397 0.593016 Wild Cherry Cream Mango Taupe 53.0 0.166502 0.160090 0.001412 0.056970 0.108918 0.207306 0.787224 Q2.3 The Jelly Filled Conundrum \u00b6 Your boss notices the Jelly filled truffles are being produced much faster than the candy outer truffles and suggests expanding into this product line. What is your response? Use the visualization tool below to help you think about this problem, then create any visualizations or analyses of your own. sunburst charts Answer: Another case of Simpson's Paradox. The real boost is due to the sponge cake base cake type. It is simply that we have been producing more of the sponge cakes that are jelly filled. In fact, jelly filled has a slightly worse performance than chocolate outer when paired with sponge cake. This can be visually verified by switching the menu item in interact to observe the effect of Jelly Filled on all its constituent products def sun ( path = [[ 'base_cake' , 'truffle_type' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ], [ 'truffle_type' , 'base_cake' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ]]): fig = px . sunburst ( df , path = path , color = 'rate' , color_continuous_scale = 'viridis' , ) fig . update_layout ( margin = dict ( l = 20 , r = 20 , t = 20 , b = 20 ), height = 650 ) fig . show () interact ( sun ) interactive(children=(Dropdown(description='path', options=(['base_cake', 'truffle_type', 'primary_flavor', 's\u2026 <function __main__.sun(path=[['base_cake', 'truffle_type', 'primary_flavor', 'secondary_flavor', 'color_group'], ['truffle_type', 'base_cake', 'primary_flavor', 'secondary_flavor', 'color_group']])> This negative impact of Jelly filled is still obscured in the median analysis: moodsdf . loc [ moodsdf [ 'descriptor' ] == 'truffle_type' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 32 truffle_type Candy Outer 369.023049 3.054270e-82 0.310345 0.289822 0.249572 3352 [[1232, 3373], [2120, 2485]] 52 truffle_type Jelly Filled 95.107432 1.803284e-22 0.310345 0.414813 0.365576 3450 [[1952, 2653], [1498, 3107]] 55 truffle_type Chocolate Outer 105.424685 9.857421e-25 0.310345 0.427535 0.380527 2408 [[1421, 3184], [987, 3618]] So we must account for the affect of the sponge cake: fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = 'truffle_type' , y = 'rate' , data = df . loc [ ~ ( df [ 'base_cake' ] == 'Sponge' )]) print ( df . loc [ ~ ( df [ 'base_cake' ] == 'Sponge' )] . groupby ( 'truffle_type' )[[ 'rate' ]] . mean ()) rate truffle_type Candy Outer 0.289822 Chocolate Outer 0.320189 Jelly Filled 0.266877 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = 'truffle_type' , y = 'rate' , data = df . loc [( df [ 'base_cake' ] == 'Sponge' )]) print ( df . loc [( df [ 'base_cake' ] == 'Sponge' )] . groupby ( 'truffle_type' )[[ 'rate' ]] . mean ()) rate truffle_type Chocolate Outer 0.751005 Jelly Filled 0.692193","title":"SOLN L2 Inferential Statistics Data Hunt"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#data-science-foundations-lab-2-data-hunt-ii","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com That's right you heard correctly. It's the data hunt part TWO.","title":"Data Science Foundations  Lab 2: Data Hunt II"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#preparing-environment-and-importing-data","text":"","title":"Preparing Environment and Importing Data"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#import-packages","text":"! pip install - U plotly Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1) Collecting plotly Downloading plotly-5.1.0-py2.py3-none-any.whl (20.6 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.6 MB 1.3 MB/s \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0) Collecting tenacity>=6.2.0 Downloading tenacity-8.0.1-py3-none-any.whl (24 kB) Installing collected packages: tenacity, plotly Attempting uninstall: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-5.1.0 tenacity-8.0.1 # our standard libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns from ipywidgets import interact # our stats libraries import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy # our scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score","title":"Import Packages"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#import-and-clean-data","text":"df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"technology_fundamentals/main/assets/truffle_rates.csv\" ) df = df . loc [ df [ 'rate' ] > 0 ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group rate 0 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.167097 1 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.153827 2 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.100299 3 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.333008 4 Chiffon Candy Outer Cherry Cream Spice Ginger Beer Tiffany 0.078108 df . shape (9210, 6)","title":"Import and Clean Data"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#exploratory-data-analysis","text":"","title":"Exploratory Data Analysis"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q1-finding-influential-features","text":"Which of the five features (base_cake, truffle_type, primary_flavor, secondary_flavor, color_group) of the truffles is most influential on production rate? Back your answer with both a visualization of the distributions (boxplot, kernel denisty estimate, histogram, violin plot) and a statistical test (moods median, ANOVA, t-test) Be sure: everything is labeled (can you improve your labels with additional descriptive statistical information e.g. indicate mean, std, etc.) you meet the assumptions of your statistical test","title":"Q1 Finding Influential Features"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q11-visualization","text":"Use any number of visualizations. Here is an example to get you started: # Example: a KDE of the truffle_type and base_cake columns fig , ax = plt . subplots ( 2 , 1 , figsize = ( 12 , 12 )) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'truffle_type' ], fill = True , ax = ax [ 0 ]) sns . kdeplot ( x = df [ 'rate' ], hue = df [ 'base_cake' ], fill = True , ax = ax [ 1 ]) <AxesSubplot:xlabel='rate', ylabel='Density'> fig , ax = plt . subplots ( 5 , 1 , figsize = ( 10 , 40 )) for idx , col in enumerate ( df . columns [: - 1 ]): df . boxplot ( by = col , column = 'rate' , ax = ax [ idx ]) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 9 ( ) missing from current font. fig.canvas.print_figure(bytes_io, **kw)","title":"Q1.1 Visualization"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q12-statistical-analysis","text":"What statistical tests can you perform to evaluate your hypothesis from the visualizations (maybe you think one particular feature is significant). Here's an ANOVA on the truffle_type column to get you started: model = ols ( 'rate ~ C( {} )' . format ( 'truffle_type' ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Is this P value significant? What is the null hypothesis? How do we check the assumptions of ANOVA? Answer: All groups fail for normal distribution of residuals and homogeneity of variances. So we cannot use ANOVA with this data: confidence_level = 0.05 for idx , col in enumerate ( df . columns [: - 1 ]): model = ols ( 'rate ~ C( {} )' . format ( col ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) print ( \"Shapiro-Wilk\" ) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( * stats . shapiro ( model . resid ))) if stats . shapiro ( model . resid ) . pvalue < confidence_level : shapiro_rej = True print ( f \" \\t reject: { shapiro_rej } \" ) print ( \"Bartlett\" ) gb = df . groupby ( col )[ 'rate' ] w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( \" \\t statistic= {:.2f} , pvalue= {:.2e} \" . format ( w , pvalue )) if pvalue < confidence_level : shapiro_rej = True print ( f \" \\t reject: { shapiro_rej } \" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(base_cake) 331.373550 5.0 2349.684756 0.0 Residual 259.606073 9204.0 NaN NaN Shapiro-Wilk statistic=0.93, pvalue=0.00e+00 reject: True Bartlett statistic=619.37, pvalue=1.32e-131 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(truffle_type) 36.383370 2.0 302.005 9.199611e-128 Residual 554.596254 9207.0 NaN NaN Shapiro-Wilk statistic=0.96, pvalue=1.37e-42 reject: True Bartlett statistic=533.02, pvalue=1.80e-116 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(primary_flavor) 159.105452 47.0 71.815842 0.0 Residual 431.874171 9162.0 NaN NaN Shapiro-Wilk statistic=0.97, pvalue=6.49e-38 reject: True Bartlett /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") statistic=1609.00, pvalue=1.85e-306 reject: True .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(secondary_flavor) 115.773877 28.0 79.884192 0.0 Residual 475.205747 9181.0 NaN NaN Shapiro-Wilk statistic=0.97, pvalue=4.34e-39 reject: True Bartlett statistic=1224.49, pvalue=3.55e-240 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(color_group) 33.878491 11.0 50.849974 1.873235e-109 Residual 557.101132 9198.0 NaN NaN Shapiro-Wilk statistic=0.96, pvalue=1.40e-44 reject: True Bartlett statistic=298.64, pvalue=1.69e-57 reject: True /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000. warnings.warn(\"p-value may not be accurate for N > 5000.\") Instead we might use Moods Median","title":"Q1.2 Statistical Analysis"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q2-finding-best-and-worst-groups","text":"","title":"Q2 Finding Best and Worst Groups"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q21-compare-every-group-to-the-whole","text":"Of the primary flavors (feature), what 5 flavors (groups) would you recommend Truffletopia discontinue? Iterate through every level (i.e. pound, cheese, sponge cakes) of every category (i.e. base cake, primary flavor, secondary flavor) and use moods median testing to compare the group distribution to the grand median rate. moodsdf = pd . DataFrame () target = 'rate' for col in df . columns [: - 1 ]: for truff in df [ col ] . unique (): group = df . loc [ df [ col ] == truff ][ target ] pop = df . loc [ ~ ( df [ col ] == truff )][ target ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , table ]) . T ]) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'table' ] moodsdf [ 'p_value' ] = moodsdf [ 'p_value' ] . astype ( float ) print ( moodsdf . shape ) (98, 9) After you've computed a moods median test on every group, filter any data above a significance level of 0.05 moodsdf = moodsdf . loc [( moodsdf [ 'p_value' ] < confidence_level )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( moodsdf . shape ) (83, 9) Return the groups with the lowest median performance (your table need not look exactly like the one I've created) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 0 secondary_flavor Wild Cherry Cream 56.867453 4.661980e-14 0.310345 0.139998 0.085628 100 [[12, 4593], [88, 4517]] 1 primary_flavor Coconut 56.867453 4.661980e-14 0.310345 0.139998 0.085628 100 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.556345 4.302530e-15 0.310345 0.129178 0.092878 85 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.32026 7.846169e-13 0.310345 0.145727 0.095758 91 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.545249 4.142688e-11 0.310345 0.148964 0.10588 70 [[7, 4598], [63, 4542]] ... ... ... ... ... ... ... ... ... ... 78 secondary_flavor Vanilla 40.484134 1.982191e-10 0.310345 0.559808 0.664252 200 [[145, 4460], [55, 4550]] 79 primary_flavor Orange 60.723646 6.567649e-15 0.310345 0.580579 0.681157 200 [[155, 4450], [45, 4560]] 80 primary_flavor Plum 308.037116 5.845614e-69 0.310345 0.669126 0.681309 300 [[300, 4305], [0, 4605]] 81 primary_flavor Cheesecake 99.085851 2.417896e-23 0.310345 0.745813 0.689802 100 [[100, 4505], [0, 4605]] 82 base_cake Sponge 2107.437614 0.000000e+00 0.310345 0.711797 0.699679 1800 [[1774, 2831], [26, 4579]] 83 rows \u00d7 9 columns We would want to cut the following primary flavors. Check to see that you get a similar answer. rip wild cherry cream. ['Coconut', 'Pink Lemonade', 'Chocolate', 'Wild Cherry Cream', 'Gingersnap'] moodsdf . loc [ moodsdf [ 'descriptor' ] == 'primary_flavor' ][: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 1 primary_flavor Coconut 56.867453 4.661980e-14 0.310345 0.139998 0.085628 100 [[12, 4593], [88, 4517]] 2 primary_flavor Pink Lemonade 61.556345 4.302530e-15 0.310345 0.129178 0.092878 85 [[6, 4599], [79, 4526]] 3 primary_flavor Chocolate 51.32026 7.846169e-13 0.310345 0.145727 0.095758 91 [[11, 4594], [80, 4525]] 4 primary_flavor Wild Cherry Cream 43.545249 4.142688e-11 0.310345 0.148964 0.10588 70 [[7, 4598], [63, 4542]] 10 primary_flavor Gingersnap 131.113519 2.338438e-30 0.310345 0.159268 0.143347 192 [[17, 4588], [175, 4430]]","title":"Q2.1 Compare Every Group to the Whole"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q22-beyond-statistical-testing-using-reasoning","text":"Let's look at the total profile of the products associated with the five worst primary flavors. Given the number of different products made with any of these flavors, would you alter your answer at all? Answer: We would opt to only discontinue the gingersnap, and wild cherry cream flavors. The other flavors, Pink Lemonade, Chocolate, and Coconut may be subject to Simpson's Paradox since we do not have adequate sampling of them with other categories # 1. filter df for only bottom five flavors # 2. groupby all columns besides rate # 3. describe the rate column. # by doing this we can evaluate just how much sampling variety we have for the # worst performing flavors. bottom_five = [ 'Coconut' , 'Pink Lemonade' , 'Chocolate' , 'Wild Cherry Cream' , 'Gingersnap' ] df . loc [ df [ 'primary_flavor' ] . isin ( bottom_five )] . groupby ( list ( df . columns [: - 1 ]))[ 'rate' ] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max base_cake truffle_type primary_flavor secondary_flavor color_group Butter Jelly Filled Pink Lemonade Butter Rum Rose 85.0 0.129178 0.137326 0.000061 0.032887 0.092878 0.171350 0.860045 Chiffon Candy Outer Wild Cherry Cream Rock and Rye Olive 17.0 0.094287 0.059273 0.010464 0.053976 0.077098 0.120494 0.229933 Chocolate Outer Gingersnap Dill Pickle Burgundy 59.0 0.133272 0.080414 0.021099 0.069133 0.137972 0.172066 0.401387 Jelly Filled Chocolate Tutti Frutti Burgundy 91.0 0.145727 0.135230 0.000033 0.044847 0.095758 0.185891 0.586570 Pound Candy Outer Coconut Wild Cherry Cream Taupe 100.0 0.139998 0.147723 0.000705 0.036004 0.085628 0.187318 0.775210 Chocolate Outer Gingersnap Rock and Rye Black 67.0 0.156160 0.110666 0.002846 0.074615 0.139572 0.241114 0.551898 Jelly Filled Gingersnap Kiwi Taupe 66.0 0.185662 0.132272 0.000014 0.086377 0.166340 0.247397 0.593016 Wild Cherry Cream Mango Taupe 53.0 0.166502 0.160090 0.001412 0.056970 0.108918 0.207306 0.787224","title":"Q2.2 Beyond Statistical Testing: Using Reasoning"},{"location":"solutions/SOLN_L2_Inferential_Statistics_Data_Hunt/#q23-the-jelly-filled-conundrum","text":"Your boss notices the Jelly filled truffles are being produced much faster than the candy outer truffles and suggests expanding into this product line. What is your response? Use the visualization tool below to help you think about this problem, then create any visualizations or analyses of your own. sunburst charts Answer: Another case of Simpson's Paradox. The real boost is due to the sponge cake base cake type. It is simply that we have been producing more of the sponge cakes that are jelly filled. In fact, jelly filled has a slightly worse performance than chocolate outer when paired with sponge cake. This can be visually verified by switching the menu item in interact to observe the effect of Jelly Filled on all its constituent products def sun ( path = [[ 'base_cake' , 'truffle_type' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ], [ 'truffle_type' , 'base_cake' , 'primary_flavor' , 'secondary_flavor' , 'color_group' ]]): fig = px . sunburst ( df , path = path , color = 'rate' , color_continuous_scale = 'viridis' , ) fig . update_layout ( margin = dict ( l = 20 , r = 20 , t = 20 , b = 20 ), height = 650 ) fig . show () interact ( sun ) interactive(children=(Dropdown(description='path', options=(['base_cake', 'truffle_type', 'primary_flavor', 's\u2026 <function __main__.sun(path=[['base_cake', 'truffle_type', 'primary_flavor', 'secondary_flavor', 'color_group'], ['truffle_type', 'base_cake', 'primary_flavor', 'secondary_flavor', 'color_group']])> This negative impact of Jelly filled is still obscured in the median analysis: moodsdf . loc [ moodsdf [ 'descriptor' ] == 'truffle_type' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 32 truffle_type Candy Outer 369.023049 3.054270e-82 0.310345 0.289822 0.249572 3352 [[1232, 3373], [2120, 2485]] 52 truffle_type Jelly Filled 95.107432 1.803284e-22 0.310345 0.414813 0.365576 3450 [[1952, 2653], [1498, 3107]] 55 truffle_type Chocolate Outer 105.424685 9.857421e-25 0.310345 0.427535 0.380527 2408 [[1421, 3184], [987, 3618]] So we must account for the affect of the sponge cake: fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = 'truffle_type' , y = 'rate' , data = df . loc [ ~ ( df [ 'base_cake' ] == 'Sponge' )]) print ( df . loc [ ~ ( df [ 'base_cake' ] == 'Sponge' )] . groupby ( 'truffle_type' )[[ 'rate' ]] . mean ()) rate truffle_type Candy Outer 0.289822 Chocolate Outer 0.320189 Jelly Filled 0.266877 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = 'truffle_type' , y = 'rate' , data = df . loc [( df [ 'base_cake' ] == 'Sponge' )]) print ( df . loc [( df [ 'base_cake' ] == 'Sponge' )] . groupby ( 'truffle_type' )[[ 'rate' ]] . mean ()) rate truffle_type Chocolate Outer 0.751005 Jelly Filled 0.692193","title":"Q2.3 The Jelly Filled Conundrum"},{"location":"solutions/SOLN_L3_Feature_Engineering/","text":"Data Science Foundations, Lab 3: Practice with Feature Engineering and Pipelines \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines and feature engineering. We will use the wine dataset. import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn.model_selection import train_test_split from statsmodels.stats.outliers_influence import variance_inflation_factor wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) On Wine Density \u00b6 L1 Q1: Feature Derivation \u00b6 Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 One-Hot encode categorical variables in the wine dataset # Code Cell for L1 Q1 display ( wine . head ()) print ( wine . shape ) str_cols = [ 'type' ] enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () X . pop ( 'type' ) y = X . pop ( 'density' ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) print ( y . shape ) print ( X . shape ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 (6497, 13) (6497,) (6497, 13) L1 Q2: Feature Transformation \u00b6 Use StandardScaler on the input data and evaluate how this affects VIF, kurtosis, and skew You should ignore the one-hot encoded column(s) for this section # Non-one hot encoded columns cols = list ( wine . columns ) cols . remove ( 'density' ) cols . remove ( 'type' ) # Code Cell for L1 Q2 kurt1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . kurt () skew1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . skew () vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( X [:, 2 :], i ) for i in range ( X [:, 2 :] . shape [ 1 ])] vif . index = cols vif [ \"kurtosis\" ] = kurt1 vif [ \"skew\" ] = skew1 vif .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 41.790949 5.070143 1.724131 volatile acidity 9.482732 2.834263 1.496433 citric acid 9.344218 2.404077 0.473142 residual sugar 3.336944 4.360399 1.435221 chlorides 5.398369 50.911457 5.400680 free sulfur dioxide 8.529778 7.906238 1.220066 total sulfur dioxide 13.448130 -0.371664 -0.001177 pH 149.003349 0.374743 0.387234 sulphates 18.402953 8.667071 1.799021 alcohol 114.836088 -0.531687 0.565718 quality 63.455488 0.232322 0.189623 scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) kurt1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . kurt () skew1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . skew () vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( X [:, 2 :], i ) for i in range ( X [:, 2 :] . shape [ 1 ])] vif . index = cols vif [ \"kurtosis\" ] = kurt1 vif [ \"skew\" ] = skew1 vif .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 1.781336 5.070143 1.724131 volatile acidity 1.808525 2.834263 1.496433 citric acid 1.606484 2.404077 0.473142 residual sugar 1.533403 4.360399 1.435221 chlorides 1.564413 50.911457 5.400680 free sulfur dioxide 2.156598 7.906238 1.220066 total sulfur dioxide 2.872586 -0.371664 -0.001177 pH 1.413100 0.374743 0.387234 sulphates 1.364157 8.667071 1.799021 alcohol 1.696986 -0.531687 0.565718 quality 1.408210 0.232322 0.189623 L1 Q3: Modeling \u00b6 Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using logistic regression: dependent variable: wine quality If you are using linear regression: dependent variable: wine density # Code Cell for L1 Q3 model = LinearRegression () X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.963') On Wine Quality \u00b6 L1 Q1: Feature Derivation \u00b6 Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 One-Hot encode categorical variables in the wine dataset # Code Cell for L1 Q1 display ( wine . head ()) print ( wine . shape ) str_cols = [ 'type' ] enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () X . pop ( 'type' ) y = X . pop ( 'quality' ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) print ( y . shape ) print ( X . shape ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 (6497, 13) (6497,) (6497, 13) L1 Q2: Feature Transformation \u00b6 Use StandardScaler on the input data and evaluate how this affects VIF, kurtosis, and skew You should ignore the one-hot encoded column(s) for this section scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) L1 Q3: Modeling \u00b6 Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using logistic regression: dependent variable: wine quality If you are using linear regression: dependent variable: wine density # Code Cell for L1 Q3 model = LogisticRegression ( max_iter = 1e4 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.60 0.07 0.12 46 5 0.58 0.61 0.59 420 6 0.52 0.68 0.59 579 7 0.44 0.19 0.26 221 8 0.00 0.00 0.00 32 accuracy 0.54 1300 macro avg 0.36 0.26 0.26 1300 weighted avg 0.51 0.54 0.50 1300 sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True ) <AxesSubplot:>","title":"SOLN L3 Feature Engineering"},{"location":"solutions/SOLN_L3_Feature_Engineering/#data-science-foundations-lab-3-practice-with-feature-engineering-and-pipelines","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines and feature engineering. We will use the wine dataset. import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn.model_selection import train_test_split from statsmodels.stats.outliers_influence import variance_inflation_factor wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" )","title":"Data Science Foundations, Lab 3: Practice with Feature Engineering and Pipelines"},{"location":"solutions/SOLN_L3_Feature_Engineering/#on-wine-density","text":"","title":"On Wine Density"},{"location":"solutions/SOLN_L3_Feature_Engineering/#l1-q1-feature-derivation","text":"Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 One-Hot encode categorical variables in the wine dataset # Code Cell for L1 Q1 display ( wine . head ()) print ( wine . shape ) str_cols = [ 'type' ] enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () X . pop ( 'type' ) y = X . pop ( 'density' ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) print ( y . shape ) print ( X . shape ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 (6497, 13) (6497,) (6497, 13)","title":"L1 Q1: Feature Derivation"},{"location":"solutions/SOLN_L3_Feature_Engineering/#l1-q2-feature-transformation","text":"Use StandardScaler on the input data and evaluate how this affects VIF, kurtosis, and skew You should ignore the one-hot encoded column(s) for this section # Non-one hot encoded columns cols = list ( wine . columns ) cols . remove ( 'density' ) cols . remove ( 'type' ) # Code Cell for L1 Q2 kurt1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . kurt () skew1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . skew () vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( X [:, 2 :], i ) for i in range ( X [:, 2 :] . shape [ 1 ])] vif . index = cols vif [ \"kurtosis\" ] = kurt1 vif [ \"skew\" ] = skew1 vif .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 41.790949 5.070143 1.724131 volatile acidity 9.482732 2.834263 1.496433 citric acid 9.344218 2.404077 0.473142 residual sugar 3.336944 4.360399 1.435221 chlorides 5.398369 50.911457 5.400680 free sulfur dioxide 8.529778 7.906238 1.220066 total sulfur dioxide 13.448130 -0.371664 -0.001177 pH 149.003349 0.374743 0.387234 sulphates 18.402953 8.667071 1.799021 alcohol 114.836088 -0.531687 0.565718 quality 63.455488 0.232322 0.189623 scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) kurt1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . kurt () skew1 = pd . DataFrame ( X [:, 2 :], columns = cols ) . skew () vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( X [:, 2 :], i ) for i in range ( X [:, 2 :] . shape [ 1 ])] vif . index = cols vif [ \"kurtosis\" ] = kurt1 vif [ \"skew\" ] = skew1 vif .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor kurtosis skew fixed acidity 1.781336 5.070143 1.724131 volatile acidity 1.808525 2.834263 1.496433 citric acid 1.606484 2.404077 0.473142 residual sugar 1.533403 4.360399 1.435221 chlorides 1.564413 50.911457 5.400680 free sulfur dioxide 2.156598 7.906238 1.220066 total sulfur dioxide 2.872586 -0.371664 -0.001177 pH 1.413100 0.374743 0.387234 sulphates 1.364157 8.667071 1.799021 alcohol 1.696986 -0.531687 0.565718 quality 1.408210 0.232322 0.189623","title":"L1 Q2: Feature Transformation"},{"location":"solutions/SOLN_L3_Feature_Engineering/#l1-q3-modeling","text":"Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using logistic regression: dependent variable: wine quality If you are using linear regression: dependent variable: wine density # Code Cell for L1 Q3 model = LinearRegression () X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.963')","title":"L1 Q3: Modeling"},{"location":"solutions/SOLN_L3_Feature_Engineering/#on-wine-quality","text":"","title":"On Wine Quality"},{"location":"solutions/SOLN_L3_Feature_Engineering/#l1-q1-feature-derivation_1","text":"Fill in any missing data in your dataset using imputation and use this new data for Q2-Q3 One-Hot encode categorical variables in the wine dataset # Code Cell for L1 Q1 display ( wine . head ()) print ( wine . shape ) str_cols = [ 'type' ] enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () X . pop ( 'type' ) y = X . pop ( 'quality' ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) print ( y . shape ) print ( X . shape ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 (6497, 13) (6497,) (6497, 13)","title":"L1 Q1: Feature Derivation"},{"location":"solutions/SOLN_L3_Feature_Engineering/#l1-q2-feature-transformation_1","text":"Use StandardScaler on the input data and evaluate how this affects VIF, kurtosis, and skew You should ignore the one-hot encoded column(s) for this section scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :])","title":"L1 Q2: Feature Transformation"},{"location":"solutions/SOLN_L3_Feature_Engineering/#l1-q3-modeling_1","text":"Create a Pipeline using one of the scaling methods in sklearn and linear or logistic regression If you are using logistic regression: dependent variable: wine quality If you are using linear regression: dependent variable: wine density # Code Cell for L1 Q3 model = LogisticRegression ( max_iter = 1e4 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.60 0.07 0.12 46 5 0.58 0.61 0.59 420 6 0.52 0.68 0.59 579 7 0.44 0.19 0.26 221 8 0.00 0.00 0.00 32 accuracy 0.54 1300 macro avg 0.36 0.26 0.26 1300 weighted avg 0.51 0.54 0.50 1300 sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True ) <AxesSubplot:>","title":"L1 Q3: Modeling"},{"location":"solutions/SOLN_L4_Supervised_Learners/","text":"Data Science Foundations Lab 4: Practice with Supervised Learners \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines, feature engineering, and applying learning algorithms. Now that we have covered supervised learning methods, and we've covered Grid Search, we will use these tools to do a sophisticated, search of hyperparameter optimization. import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression , LinearRegression import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.model_selection import train_test_split from sklearn import metrics wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'quality' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ target ] = y to compare, here is our results performing classification on this set of data with just logistic regression: model = LogisticRegression ( max_iter = 1e4 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.60 0.07 0.12 46 5 0.58 0.61 0.59 420 6 0.52 0.68 0.59 579 7 0.44 0.19 0.26 221 8 0.00 0.00 0.00 32 accuracy 0.54 1300 macro avg 0.36 0.26 0.26 1300 weighted avg 0.51 0.54 0.50 1300 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) <AxesSubplot:> \ud83c\udfce\ufe0f Q1: \u00b6 Evaluate the performance of a Random Forest on classifying wine quality # Code Cell for L1 Q2 model = RandomForestClassifier () X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 1.00 0.15 0.26 46 5 0.72 0.77 0.75 420 6 0.67 0.78 0.72 579 7 0.71 0.51 0.59 221 8 1.00 0.22 0.36 32 accuracy 0.70 1300 macro avg 0.68 0.41 0.45 1300 weighted avg 0.71 0.70 0.68 1300 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) <AxesSubplot:> \ud83d\udd2c Q2: \u00b6 Do a grid search to optimize your Random Forest model, use whatever hyperparameters you would like RandomForestClassifier () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Code Cell for L1 Q3 from sklearn.model_selection import GridSearchCV param_grid = { 'bootstrap' : [ True , False ], 'criterion' : [ 'gini' , 'entropy' ], 'min_samples_split' : [ 2 , 4 , 6 ], 'min_samples_leaf' : [ 1 , 3 , 5 ], 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'class_weight' : [ 'balanced' , 'balanced_subsample' , None ]} grid = GridSearchCV ( RandomForestClassifier ( n_jobs =- 1 ), param_grid , cv = 7 ) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=7. warnings.warn( {'bootstrap': True, 'class_weight': None, 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2} model = grid . best_estimator_ model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.75 0.13 0.22 46 5 0.70 0.76 0.73 420 6 0.66 0.78 0.72 579 7 0.73 0.48 0.58 221 8 1.00 0.25 0.40 32 accuracy 0.69 1300 macro avg 0.64 0.40 0.44 1300 weighted avg 0.70 0.69 0.67 1300 <AxesSubplot:>","title":"SOLN L4 Supervised Learners"},{"location":"solutions/SOLN_L4_Supervised_Learners/#data-science-foundations-lab-4-practice-with-supervised-learners","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab we will continue to practice creation of pipelines, feature engineering, and applying learning algorithms. Now that we have covered supervised learning methods, and we've covered Grid Search, we will use these tools to do a sophisticated, search of hyperparameter optimization. import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression , LinearRegression import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score , classification_report , confusion_matrix from sklearn.model_selection import train_test_split from sklearn import metrics wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'quality' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ target ] = y to compare, here is our results performing classification on this set of data with just logistic regression: model = LogisticRegression ( max_iter = 1e4 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.60 0.07 0.12 46 5 0.58 0.61 0.59 420 6 0.52 0.68 0.59 579 7 0.44 0.19 0.26 221 8 0.00 0.00 0.00 32 accuracy 0.54 1300 macro avg 0.36 0.26 0.26 1300 weighted avg 0.51 0.54 0.50 1300 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) <AxesSubplot:>","title":"Data Science Foundations  Lab 4: Practice with Supervised Learners"},{"location":"solutions/SOLN_L4_Supervised_Learners/#q1","text":"Evaluate the performance of a Random Forest on classifying wine quality # Code Cell for L1 Q2 model = RandomForestClassifier () X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) precision recall f1-score support 3 0.00 0.00 0.00 2 4 1.00 0.15 0.26 46 5 0.72 0.77 0.75 420 6 0.67 0.78 0.72 579 7 0.71 0.51 0.59 221 8 1.00 0.22 0.36 32 accuracy 0.70 1300 macro avg 0.68 0.41 0.45 1300 weighted avg 0.71 0.70 0.68 1300 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) <AxesSubplot:>","title":"\ud83c\udfce\ufe0f Q1:"},{"location":"solutions/SOLN_L4_Supervised_Learners/#q2","text":"Do a grid search to optimize your Random Forest model, use whatever hyperparameters you would like RandomForestClassifier () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Code Cell for L1 Q3 from sklearn.model_selection import GridSearchCV param_grid = { 'bootstrap' : [ True , False ], 'criterion' : [ 'gini' , 'entropy' ], 'min_samples_split' : [ 2 , 4 , 6 ], 'min_samples_leaf' : [ 1 , 3 , 5 ], 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'class_weight' : [ 'balanced' , 'balanced_subsample' , None ]} grid = GridSearchCV ( RandomForestClassifier ( n_jobs =- 1 ), param_grid , cv = 7 ) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=7. warnings.warn( {'bootstrap': True, 'class_weight': None, 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2} model = grid . best_estimator_ model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) precision recall f1-score support 3 0.00 0.00 0.00 2 4 0.75 0.13 0.22 46 5 0.70 0.76 0.73 420 6 0.66 0.78 0.72 579 7 0.73 0.48 0.58 221 8 1.00 0.25 0.40 32 accuracy 0.69 1300 macro avg 0.64 0.40 0.44 1300 weighted avg 0.70 0.69 0.67 1300 <AxesSubplot:>","title":"\ud83d\udd2c Q2:"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/","text":"Data Science Foundations Lab 5: Writing Unit Tests \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab, we will try our hand at writing unit tests Import Libraries \u00b6 import random import numpy as np from contextlib import contextmanager import sys , os @contextmanager def suppress_stdout (): with open ( os . devnull , \"w\" ) as devnull : old_stdout = sys . stdout sys . stdout = devnull try : yield finally : sys . stdout = old_stdout Types of Tests \u00b6 There are two main types of tests we want to distinguish: * Unit test : an automatic test to test the internal workings of a class or function. It should be a stand-alone test which is not related to other resources. * Integration test : an automatic test that is done on an environment, it tests the coordination of different classes and functions as well as with the running environment. This usually precedes sending code to a QA team. To this I will add: Acid test : extremely rigorous tests that push beyond the intended use cases for your classes/functions. Written when you, like me, cannot afford QA employees to actually test your code. (word origin: gold acid tests in the 1850s , acid tests in the 70's ) EDIT : you could also call this a corner, or an edge case In this lab we will focus on unit tests . Unit Tests \u00b6 Each unit test should test the smallest portion of your code possible, i.e. a single method or function. Any random number generators should be seeded so that they run the exact same way every time. Unit tests should not rely on any local files or the local environment. Why bother with Unit Tests when we have Integration tests? A major challenge with integration testing is when an integration test fails. It\u2019s very hard to diagnose a system issue without being able to isolate which part of the system is failing. Here comes the unit test to the rescue. Let's take a simple example. If I wanted to test that the sume of two numbers is correct assert sum ([ 2 , 5 ]) == 7 , \"should be 7\" Nothing is sent to the print out because the condition is satisfied. If we run, however: assert sum([2, 4]) == 7, \"should be 7\" we get an error message: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-3-d5724b127818> in <module>() ----> 1 assert sum([2, 4]) == 7, \"should be 7\" AssertionError: should be 7 To make this a Unit Test, you will want to wrap it in a function def test_sum (): assert sum ([ 1 , 2 , 3 ]) == 6 , \"Should be 6\" test_sum () print ( \"Everything passed\" ) Everything passed And if we include a test that does not pass: def test_sum(): assert sum([3, 3]) == 6, \"Should be 6\" def test_my_broken_func(): assert sum([1, 2]) == 5, \"Should be 5\" test_sum() test_my_broken_func() print(\"Everything passed\") Here our test fails, because the sum of 1 and 2 is 3 and not 5. We get a traceback that tells us the source of the error: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-13-8a552fbf52bd> in <module>() 6 7 test_sum() ----> 8 test_my_broken_func() 9 print(\"Everything passed\") <ipython-input-13-8a552fbf52bd> in test_my_broken_func() 3 4 def test_my_broken_func(): ----> 5 assert sum([1, 2]) == 5, \"Should be 5\" 6 7 test_sum() AssertionError: Should be 5 Before sending us on our merry way to practice writing unit tests, we will want to ask, what do I want to write a test about? Here, we've been testing sum(). There are many behaviors in sum() we could check, such as: Does it sum a list of whole numbers (integers)? Can it sum a tuple or set? Can it sum a list of floats? What happens if one of the numbers is negative? etc.. In the end, what you test is up to you, and depends on your intended use cases. As a general rule of thumb, your unit test should test what is relevant. The only caveat to that, is that many continuous integration services (like TravisCI ) will benchmark you based on the percentage of lines of code you have that are covered by your unit tests (ex: 85% coverage ). \u270d\ud83c\udffd Q1 Write a Unit Test \u00b6 Remember our Pokeball discussion in Python Foundations ? We'll return to that here. This time writing unit tests for our classes. Sometimes when writing unit tests, it can be more complicated than checking the return value of a function. Think back on our pokemon example: class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None If I wanted to write a unit test for the release method, I couldn't directly check for the output of a function. I'll have to check for a side effect , in this case, the change of an attribute belonging to a pokeball object; that is the change to the attribute contains . class Pokeball : def __init__ ( self , contains = None , type_name = \"poke ball\" ): self . contains = contains self . type_name = type_name self . catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch ( self , pokemon ): if self . contains == None : if random . random () < self . catch_rate : self . contains = pokemon print ( f \" { pokemon } captured!\" ) else : print ( f \" { pokemon } escaped!\" ) pass else : print ( \"pokeball is not empty!\" ) def release ( self ): if self . contains == None : print ( \"Pokeball is already empty\" ) else : print ( self . contains , \"has been released\" ) self . contains = None In the following cell, finish the code to test the functionality of the release method: def test_release (): ball = Pokeball () ball . contains = 'Pikachu' ball . release () # turn the pseudo code below into an assert statement ### YOUR CODE HERE ### assert ball . contains == None , \"ball is not empty!\" test_release () Pikachu has been released \u26f9\ufe0f Q2 Write a Unit Test for the Catch Rate \u00b6 First, we will check that the succcessful catch is operating correctly. Remember that we depend on random.random and condition our success on whether that random value is less than the catch_rate of the pokeball: if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon so to test whether the successful catch is working we will seed our random number generator with a value that returns less than the catch_rate of the pokeball and then write our assert statement: def test_successful_catch (): # choose a random seed such that # we know the catch call should succeed ### YOUR CODE BELOW ### random . seed ( 1 ) ball = Pokeball () ball . catch ( 'Psyduck' ) # Someone's fave pokemon (bless 'em) ### YOUR CODE BELOW ### assert ball . contains == 'Psyduck' , \"ball did not catch as expected\" NICE. Now we will do the same thing again, this time testing for an unsuccessful catch. SO in order to do this, we need to choose a random seed that will cause our catch to fail: def test_unsuccessful_catch (): # choose a random seed such that # we know the catch call should FAIL ### YOUR CODE BELOW ### random . seed ( 0 ) ball = Pokeball () ball . catch ( 'Psyduck' ) ### YOUR CODE BELOW ### assert ball . contains == None , \"ball did not fail as expected\" When you are finished test your functions below test_unsuccessful_catch () Psyduck escaped! test_successful_catch () Psyduck captured! \u2696\ufe0f Q3 Write a Unit Test that Checks Whether the Overall Catch Rate is 50/50 \u00b6 For this one, we're going to take those same ideas around seeding the random number generator. However, here we'd like to run the catch function multiple times to check whether it is truly creating a 50/50 catch rate situation. Here's a pseudo code outline: seed the random number generator for 100 iterations: create a pokeball try to catch something log whether it was successful check that for the 100 attempts the success was approximately 50/50 note: you can use my suppress stdout() function to suppress the print statements from ball.catch ex: with suppress_stdout(): print(\"HELLO OUT THERE!\") quick segway : what is the actual behavior of random.seed() ? Does it produce the same number every time we call random.random() now? Check for yourself: random . seed ( 42 ) [ random . random () for i in range ( 5 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124] We see that it still produces random numbers with each call to random.random . However, those numbers are the same with every execution of the cell. What happens when we do this: [ random . random () for i in range ( 5 )] [0.5449414806032167, 0.2204406220406967, 0.5892656838759087, 0.8094304566778266, 0.006498759678061017] The numbers are different. BUT: random . seed ( 42 ) [ random . random () for i in range ( 10 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124, 0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] We see them here in the bottom half of the list again. So, random.seed() is seeding the random number generator such that it will produce the same sequence of random numbers every time from the given seed. This will reset whenever random.seed() is set again. This behavior is useful because it allows us to continue using random number generation in our code, (for testing, creating examples and demos, etc.) but it will be reproducable each time. End Segway # 1. seed the random number generator # 2. for 100 iterations: # * create a pokeball # * try to catch something # * log whether it was successful # 3. check that for the 100 attempts the success was approximately 50/50 def test_catch_rate (): ### YOUR CODE HERE ### results = 0 random . seed ( 42 ) for i in range ( 100 ): ball = Pokeball () with suppress_stdout (): ball . catch ( \"Charzard\" ) if ball . contains != None : results += 1 results = results / 100 ### END YOUR CODE ### assert np . abs ( np . mean ( results ) - 0.5 ) < 0.1 , \"catch rate not 50/50\" test_catch_rate () Test Runners \u00b6 When we start to create many tests like this, it can be cumbersome to run them all at once and log which ones fail. To handle our unit tests we use what are called test runners . We won't dedicate time to any single one here but the three most common are: unittest nose2 pytest unittest is built into python. I don't like it because you have to follow a strict class/method structure when writing the tests. nose2 is popular with many useful features and is generally good for high volumes of tests. My favorite is pytest, it's flexible and has an ecosystem of plugins for extensibility.","title":"SOLN L5 Writing Unit Tests"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#data-science-foundations-lab-5-writing-unit-tests","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this lab, we will try our hand at writing unit tests","title":"Data Science Foundations  Lab 5: Writing Unit Tests"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#import-libraries","text":"import random import numpy as np from contextlib import contextmanager import sys , os @contextmanager def suppress_stdout (): with open ( os . devnull , \"w\" ) as devnull : old_stdout = sys . stdout sys . stdout = devnull try : yield finally : sys . stdout = old_stdout","title":"Import Libraries"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#types-of-tests","text":"There are two main types of tests we want to distinguish: * Unit test : an automatic test to test the internal workings of a class or function. It should be a stand-alone test which is not related to other resources. * Integration test : an automatic test that is done on an environment, it tests the coordination of different classes and functions as well as with the running environment. This usually precedes sending code to a QA team. To this I will add: Acid test : extremely rigorous tests that push beyond the intended use cases for your classes/functions. Written when you, like me, cannot afford QA employees to actually test your code. (word origin: gold acid tests in the 1850s , acid tests in the 70's ) EDIT : you could also call this a corner, or an edge case In this lab we will focus on unit tests .","title":"Types of Tests"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#unit-tests","text":"Each unit test should test the smallest portion of your code possible, i.e. a single method or function. Any random number generators should be seeded so that they run the exact same way every time. Unit tests should not rely on any local files or the local environment. Why bother with Unit Tests when we have Integration tests? A major challenge with integration testing is when an integration test fails. It\u2019s very hard to diagnose a system issue without being able to isolate which part of the system is failing. Here comes the unit test to the rescue. Let's take a simple example. If I wanted to test that the sume of two numbers is correct assert sum ([ 2 , 5 ]) == 7 , \"should be 7\" Nothing is sent to the print out because the condition is satisfied. If we run, however: assert sum([2, 4]) == 7, \"should be 7\" we get an error message: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-3-d5724b127818> in <module>() ----> 1 assert sum([2, 4]) == 7, \"should be 7\" AssertionError: should be 7 To make this a Unit Test, you will want to wrap it in a function def test_sum (): assert sum ([ 1 , 2 , 3 ]) == 6 , \"Should be 6\" test_sum () print ( \"Everything passed\" ) Everything passed And if we include a test that does not pass: def test_sum(): assert sum([3, 3]) == 6, \"Should be 6\" def test_my_broken_func(): assert sum([1, 2]) == 5, \"Should be 5\" test_sum() test_my_broken_func() print(\"Everything passed\") Here our test fails, because the sum of 1 and 2 is 3 and not 5. We get a traceback that tells us the source of the error: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-13-8a552fbf52bd> in <module>() 6 7 test_sum() ----> 8 test_my_broken_func() 9 print(\"Everything passed\") <ipython-input-13-8a552fbf52bd> in test_my_broken_func() 3 4 def test_my_broken_func(): ----> 5 assert sum([1, 2]) == 5, \"Should be 5\" 6 7 test_sum() AssertionError: Should be 5 Before sending us on our merry way to practice writing unit tests, we will want to ask, what do I want to write a test about? Here, we've been testing sum(). There are many behaviors in sum() we could check, such as: Does it sum a list of whole numbers (integers)? Can it sum a tuple or set? Can it sum a list of floats? What happens if one of the numbers is negative? etc.. In the end, what you test is up to you, and depends on your intended use cases. As a general rule of thumb, your unit test should test what is relevant. The only caveat to that, is that many continuous integration services (like TravisCI ) will benchmark you based on the percentage of lines of code you have that are covered by your unit tests (ex: 85% coverage ).","title":"Unit Tests"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#q1-write-a-unit-test","text":"Remember our Pokeball discussion in Python Foundations ? We'll return to that here. This time writing unit tests for our classes. Sometimes when writing unit tests, it can be more complicated than checking the return value of a function. Think back on our pokemon example: class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None If I wanted to write a unit test for the release method, I couldn't directly check for the output of a function. I'll have to check for a side effect , in this case, the change of an attribute belonging to a pokeball object; that is the change to the attribute contains . class Pokeball : def __init__ ( self , contains = None , type_name = \"poke ball\" ): self . contains = contains self . type_name = type_name self . catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch ( self , pokemon ): if self . contains == None : if random . random () < self . catch_rate : self . contains = pokemon print ( f \" { pokemon } captured!\" ) else : print ( f \" { pokemon } escaped!\" ) pass else : print ( \"pokeball is not empty!\" ) def release ( self ): if self . contains == None : print ( \"Pokeball is already empty\" ) else : print ( self . contains , \"has been released\" ) self . contains = None In the following cell, finish the code to test the functionality of the release method: def test_release (): ball = Pokeball () ball . contains = 'Pikachu' ball . release () # turn the pseudo code below into an assert statement ### YOUR CODE HERE ### assert ball . contains == None , \"ball is not empty!\" test_release () Pikachu has been released","title":"\u270d\ud83c\udffd Q1 Write a Unit Test"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#q2-write-a-unit-test-for-the-catch-rate","text":"First, we will check that the succcessful catch is operating correctly. Remember that we depend on random.random and condition our success on whether that random value is less than the catch_rate of the pokeball: if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon so to test whether the successful catch is working we will seed our random number generator with a value that returns less than the catch_rate of the pokeball and then write our assert statement: def test_successful_catch (): # choose a random seed such that # we know the catch call should succeed ### YOUR CODE BELOW ### random . seed ( 1 ) ball = Pokeball () ball . catch ( 'Psyduck' ) # Someone's fave pokemon (bless 'em) ### YOUR CODE BELOW ### assert ball . contains == 'Psyduck' , \"ball did not catch as expected\" NICE. Now we will do the same thing again, this time testing for an unsuccessful catch. SO in order to do this, we need to choose a random seed that will cause our catch to fail: def test_unsuccessful_catch (): # choose a random seed such that # we know the catch call should FAIL ### YOUR CODE BELOW ### random . seed ( 0 ) ball = Pokeball () ball . catch ( 'Psyduck' ) ### YOUR CODE BELOW ### assert ball . contains == None , \"ball did not fail as expected\" When you are finished test your functions below test_unsuccessful_catch () Psyduck escaped! test_successful_catch () Psyduck captured!","title":"\u26f9\ufe0f Q2 Write a Unit Test for the Catch Rate"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#q3-write-a-unit-test-that-checks-whether-the-overall-catch-rate-is-5050","text":"For this one, we're going to take those same ideas around seeding the random number generator. However, here we'd like to run the catch function multiple times to check whether it is truly creating a 50/50 catch rate situation. Here's a pseudo code outline: seed the random number generator for 100 iterations: create a pokeball try to catch something log whether it was successful check that for the 100 attempts the success was approximately 50/50 note: you can use my suppress stdout() function to suppress the print statements from ball.catch ex: with suppress_stdout(): print(\"HELLO OUT THERE!\") quick segway : what is the actual behavior of random.seed() ? Does it produce the same number every time we call random.random() now? Check for yourself: random . seed ( 42 ) [ random . random () for i in range ( 5 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124] We see that it still produces random numbers with each call to random.random . However, those numbers are the same with every execution of the cell. What happens when we do this: [ random . random () for i in range ( 5 )] [0.5449414806032167, 0.2204406220406967, 0.5892656838759087, 0.8094304566778266, 0.006498759678061017] The numbers are different. BUT: random . seed ( 42 ) [ random . random () for i in range ( 10 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124, 0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] We see them here in the bottom half of the list again. So, random.seed() is seeding the random number generator such that it will produce the same sequence of random numbers every time from the given seed. This will reset whenever random.seed() is set again. This behavior is useful because it allows us to continue using random number generation in our code, (for testing, creating examples and demos, etc.) but it will be reproducable each time. End Segway # 1. seed the random number generator # 2. for 100 iterations: # * create a pokeball # * try to catch something # * log whether it was successful # 3. check that for the 100 attempts the success was approximately 50/50 def test_catch_rate (): ### YOUR CODE HERE ### results = 0 random . seed ( 42 ) for i in range ( 100 ): ball = Pokeball () with suppress_stdout (): ball . catch ( \"Charzard\" ) if ball . contains != None : results += 1 results = results / 100 ### END YOUR CODE ### assert np . abs ( np . mean ( results ) - 0.5 ) < 0.1 , \"catch rate not 50/50\" test_catch_rate ()","title":"\u2696\ufe0f Q3 Write a Unit Test that Checks Whether the Overall Catch Rate is 50/50"},{"location":"solutions/SOLN_L5_Writing_Unit_Tests/#test-runners","text":"When we start to create many tests like this, it can be cumbersome to run them all at once and log which ones fail. To handle our unit tests we use what are called test runners . We won't dedicate time to any single one here but the three most common are: unittest nose2 pytest unittest is built into python. I don't like it because you have to follow a strict class/method structure when writing the tests. nose2 is popular with many useful features and is generally good for high volumes of tests. My favorite is pytest, it's flexible and has an ecosystem of plugins for extensibility.","title":"Test Runners"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/","text":"Data Science Foundations, Project Part 1: Statistical Analysis \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to perform statistical analysis on data generated from our tictactoe program! 1.0 Preparing Environment and Importing Data \u00b6 back to top 1.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 1.0.2 Load Dataset \u00b6 back to top data = {} for i in range ( 1000 ): game = GameEngine () game . setup_game () board = game . play_game () data [ 'game {} ' . format ( i )] = { 'board' : board . board , 'winner' : board . winner , 'starting player' : board . start_player } | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | |X|X| | | | | |O|O| | |O|X|X| | | | | |O|O| | |O|X|X| | |X| | |O|O| | 'O' Won! |O|X|X| | |X| | |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X|X| | |O| | | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |O| | | | |X|X| |O| | | |O| | | 'O' Won! |O|X|X| |O| | | |O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |O| | |O| | |X| |X| | | |O| | |O| | |X| |X| | | |O| |O|O| | |X| |X| | |X|O| |O|O| | |X|O|X| | |X|O| |O|O| | |X|O|X| |X|X|O| |O|O| | 'O' Won! |X|O|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| |O| |O| 'X' Won! |X|X|X| | | |O| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| |X|O| | | | |O| |O|X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X|O| | |X|O|O| | | | | |X|O|X| |X|O|O| | | |O| |X|O|X| 'X' Won! |X|O|O| | |X|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X|X| |O|O| | |O| | | | |X|X| 'X' Won! |O|O| | |O| | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| | | | |O|X| |X| |O| |X| | | | |O|X| |X| |O| |X| | | |O|O|X| 'X' Won! |X| |O| |X|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X|O|X| | |O| | | |X| | |X|O|X| |O|O| | | |X| | |X|O|X| |O|O| | |X|X| | |X|O|X| 'O' Won! |O|O|O| |X|X| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X| |X| | | | | |X|O|O| |X|O|X| | |X| | |X|O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| |X| |X| | |X|O| |O|O|X| |X| |X| |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| | |O| | |X|X| | |X|O|O| |O|O| | |X|X| | |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | |X| | | |O|X|O| | | |O| |X| | | |O|X|O| |X| |O| |X| | | |O|X|O| 'O' Won! |X| |O| |X| |O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | |X|O| | |X| | |X|O| | |O|X|O| | |X| | |X|O|X| |O|X|O| 'O' Won! | |X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| |O| |X| | | | | |X| |O| |O|X|X| | | | | |X| |O| |O|X|X| |O| | | |X| |O| 'X' Won! |O|X|X| |O|X| | |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X|X| | |X| |O| 'O' Won! | | |O| |X|X|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| |X|O| | | | | | | |X|O| |X|O| | | | |O| | |X|O| |X|O|X| | | |O| | |X|O| |X|O|X| | |O|O| | |X|O| |X|O|X| |X|O|O| 'O' Won! |O|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O| |O| |X|X| | |X|O|O| |O| |O| |X|X| | |X|O|O| |O|X|O| |X|X| | 'O' Won! |X|O|O| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | |X| | |O|X|O| | |O| | | |X| | |O|X|O| | |O| | |X|X| | |O|X|O| | |O| | |X|X|O| |O|X|O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | |O| | |X| |X| 'X' Won! |O| |O| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X|X| | |O| |O| | | | | |X|X| | |O| |O| | | |O| |X|X| | |O| |O| |X| |O| 'O' Won! |X|X|O| |O| |O| |X| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X|O| | | | | | |O|O|X| |X|O|X| | | | | |O|O|X| |X|O|X| |O| | | |O|O|X| |X|O|X| |O|X| | 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | |O| | | |X| | |O|X| |X| |O| | | |X| |O|O|X| |X| |O| | | |X| |O|O|X| |X| |O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | |X| | |O| |O| |X|O| | | |X| | |O| |O| |X|O| | |X|X| | |O| |O| |X|O|O| |X|X| | 'X' Won! |O| |O| |X|O|O| |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | | | | | | |X|X| |O|O| | | | |X| | |X|X| |O|O| | | |O|X| 'X' Won! |X|X|X| |O|O| | | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| | | |O| | | |X| |X|O|O| |X| |O| | |O|X| |X|O|O| |X| |O| 'X' Won! |X|O|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O| |O|X|O| | | | | 'X' Won! | |X|O| |O|X|O| | |X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X|O| | | |O|X| | | |O| |X|O| | | |O|X| | |X|O| |X|O| | |O|O|X| | |X|O| |X|O| | |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| |X|O| | |X|X|O| | |O|O| |X|O| | |X|X|O| | |O|O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X|X| | |O|O| | | | |O| |X|X| | |O|O| | |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | |O|O| | | |X| |X| | | |X|O|O| | | |X| |X| | | |X|O|O| | |O|X| 'X' Won! |X| | | |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O| | 'O' Won! |X|X| | |O|O|O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | |O| |X| |O|X|O| | | | | |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |O|X| 'X' Won! |O| |X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X|X| |X| | | | | |O| |O|X|X| |X| | | | |O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X| |O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X| |O| | |O|X| |O| | | |X| |O| |X|O|X| |O| | | |X|O|O| |X|O|X| |O| |X| |X|O|O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | | |X| | |X| |O| | |O| | |O|X| | |X| |O| | |O| | |O|X|X| |X| |O| | |O| | |O|X|X| |X|O|O| 'X' Won! | |O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | 'O' Won! |X| |X| | | | | |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | | | | |X|O|X| |O| |O| | | | | |X|O|X| |O| |O| | | |X| |X|O|X| |O| |O| | |O|X| |X|O|X| |O| |O| |X|O|X| 'O' Won! |X|O|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X|X| | |O| | |O| |O| | |X|X| 'X' Won! | |O| | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | |O|O| | | |X|X| | |O| | |O|O| | | |X|X| |X|O| | |O|O| | | |X|X| |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| |X| | | | |X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| |X| |O|X|O| 'O' Won! | |X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | |X|O| | | |X| |X| |O| |O|X|O| | |X|X| |X| |O| |O|X|O| | |X|X| |X|O|O| |O|X|O| 'X' Won! |X|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | |O|X| | | |O|X| | |X| | |O|X| | | |O|X| | |X| | |O|X|O| | |O|X| | |X|X| |O|X|O| | |O|X| |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| |O| |O| |X|X|O| |X| |X| |O| |O| 'O' Won! |X|X|O| |X| |X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| |O| | | |X| | | |O| |X|X|O| | | |X| | | |O| |X|X|O| | | |X| |O| |O| |X|X|O| | |X|X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X|X| |X|O| | |O|O| | | |X|X| |X|O| | |O|O| | | |X|X| |X|O|X| 'O' Won! |O|O|O| | |X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| | | |X| |O|O| | | |X|O| | |X|X| |O|O| | | |X|O| 'O' Won! |O|X|X| |O|O| | | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O| | 'X' Won! |X|O|X| | |X|O| |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O| |O| | |X| | | | |X| 'O' Won! |O|O|O| | |X| | | | |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |O| | | |O|X|X| | | | | |O| | | |O|X|X| | | | | |O| |O| |O|X|X| | |X| | |O| |O| 'O' Won! |O|X|X| |O|X| | |O| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X|O| | |X| | |O| | | |O|X|O| |X|X| | |O|O| | |O|X|O| |X|X| | 'X' Won! |O|O|X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X|O|X| | | |X| |O| |O| |X|O|X| | |O|X| |O| |O| |X|O|X| |X|O|X| |O| |O| |X|O|X| 'O' Won! |X|O|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| | | |X|X|O| |O|O|X| |X| |O| |X|X|O| 'X' Won! |O|O|X| |X|X|O| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O|X| | | |X| |O|X|O| | |O|X| 'X' Won! | | |X| |O|X|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | |X|O| | | |X| |O|O| | | |X|O| | | |X| |O|O|X| | |X|O| | | |X| |O|O|X| |O|X|O| | | |X| |O|O|X| |O|X|O| | |X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | |X| | | |O|X|O| |X| | | |X| |O| |O|X|O| |X| | | |X|X|O| |O|X|O| |X| | | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | |O|O|X| | | |X| |X| | | |O|O|X| |O| |X| |X| | | |O|O|X| |O|X|X| |X| | | |O|O|X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | | |O| | | |O| |X|O|X| | |X|O| | | |O| |X|O|X| | |X|O| |O| |O| |X|O|X| |X|X|O| |O| |O| 'O' Won! |X|O|X| |X|X|O| |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O| |O| |O|X|X| | |X| | |O| |O| |O|X|X| | |X|O| |O| |O| 'X' Won! |O|X|X| | |X|O| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | |O|X| |X|O| | | | |X| | |O|X| |X|O| | | | |X| | |O|X| |X|O|O| |X| |X| | |O|X| |X|O|O| 'O' Won! |X|O|X| | |O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | |O| |X| |X| | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| | |O|O| 'X' Won! |X| |O| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | |X|O| | | | |O| |X| | | |X|O| | | | |O| |X| |O| |X|O| | | | |O| |X|X|O| |X|O| | | |O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X| | | 'O' Won! | |X| | |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X| |O| |X| | | |O| |O| |X| |O| |X| | | |O|X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | |O| |X| | |O| | |O|X| | |O| |X| | |O|X| |O|X| | 'O' Won! |O| |X| |O|O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| |O| | | |X| | |O|O| |X|X|O| | | |X| | |O|O| 'O' Won! |X|X|O| | | |X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X| |X| |X|O|O| | |O| | |X| |X| 'X' Won! |X|O|O| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | | |O| | | 'O' Won! |X| |O| |X|O| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | |O| | |O|X|X| | |X| | | |O| | |O|X|X| | |X| | | |O|O| |O|X|X| | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| |O| |X| | |O| | | |O|X| 'X' Won! |O| |X| | |O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | |X|X| | |O| |X| |O| | | |X|X| | |O|O|X| 'X' Won! |O| | | |X|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | | | | |X|O| 'X' Won! | |X|O| | |X| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | |O| |X| |O|X|O| | | | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | |O| | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |O|X| | |O| | | | |X|O| |O|X|X| |O| | | 'O' Won! |O|X|O| |O|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| |X|X| | | |O|O| | | |X| 'O' Won! |X|X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O| |O| |X|O| | |X|X| | |O| |O| |X|O|O| |X|X| | |O| |O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | | |X| | |O|X|O| | | | | |X|X| | |O|X|O| | | | | |X|X| | |O|X|O| | |O| | 'X' Won! |X|X|X| |O|X|O| | |O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | | | |X|X| | 'O' Won! |O|O|O| | | | | |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| | | | | |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| | |X| |O| |O|X| | |X|O| | 'X' Won! |X| |O| |O|X| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O| |X| | | |O| |O|X| | |O| |X| | | |O| |O|X|X| |O| |X| | | |O| 'O' Won! |O|X|X| |O| |X| |O| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| |X| |X| | | | | 'O' Won! |O|O|O| |X| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | |X| | | |X|O|O| |O|X| | |X| | | |X|O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X| |O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | | |X| |O|O| | |X| | | | |X|X| |O|O| | |X| | | | |X|X| |O|O| | |X|O| | 'X' Won! |X|X|X| |O|O| | |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | |X| | 'O' Won! |O|O|O| | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O|O| |X| |O| |X| |X| | |O|O| |X| |O| |X|O|X| | |O|O| 'X' Won! |X| |O| |X|O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | |X|X| | |O| | |O| | | | |X|X| |O|O| | |O| | | | |X|X| |O|O|X| |O|O| | | |X|X| |O|O|X| 'X' Won! |O|O| | |X|X|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | |X| | |O| |O| | |O|X| | |X| | |O| |O| | |O|X| | |X|X| 'O' Won! |O|O|O| | |O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| | |X| | |X|O|O| | | |O| | |X|X| |X|O|O| | | |O| |O|X|X| |X|O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X| |O|X|O| | | |X| | | |X| |O|X|O| | | |X| | |O|X| |O|X|O| | | |X| |X|O|X| |O|X|O| |O| |X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | |O|O| | | | | |X|O|X| | |O|O| | |X| | |X|O|X| 'O' Won! |O|O|O| | |X| | |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| | | |O| | | |X|X|O| |O| |X| |O| | | |X|X|O| |O| |X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| |O| |X| |X|X| | 'X' Won! |O| |O| |O| |X| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | | | | |O|X| | | |O|X| | | |O| |O|X| | | |O|X| |X| |O| |O|X| | | |O|X| |X| |O| |O|X|O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | |O|X| | |O| | |X| | | |X|O|X| | |O| | |X| | | |X|O|X| | |O| | |X| |O| 'X' Won! |X|O|X| |X|O| | |X| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X|O| |X|O| | | |X| | | |X|O| |X|O| | |O|X| | | |X|O| |X|O| | |O|X| | | |X|O| |X|O|X| |O|X| | |O|X|O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | |O| | |X| | |O| |X| | | |O| | |X|X| |O| |X| |O| |O| | |X|X| 'X' Won! |O| |X| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | |X| |O|O| | | |X| | | |O|X| |O|O| | |X|X| | | |O|X| |O|O| | 'O' Won! |X|X|O| | |O|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | |O| |O| | | | |X|X| | | |O| |O|X| | | |X|X| | |O|O| |O|X| | | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |O| | |O| | |X|X|O| | |X|O| | |O| | |X|X|O| |O|X|O| | |O| | |X|X|O| |O|X|O| |X|O| | 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | 'O' Won! |X| | | | | |X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| |O|O| | |O| |X| | | |X| 'X' Won! |O|O|X| |O| |X| | | |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | |O| |X|X|O| | | | | | |X|O| |X|X|O| 'O' Won! | | |O| | |X|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |X| | |X| | |O|X|O| |O| |X| 'X' Won! | |X| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | |O| |X| |O| |X|O| | | | |O| |X|X|O| |X|O| | 'O' Won! | | |O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X|X| | |O| |O| | |O| | |X|X| | |O| |O| | |O|X| |X|X| | |O| |O| |O|O|X| |X|X| | |O|X|O| |O|O|X| |X|X| | 'O' Won! |O|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X| | | | |X|O| |O| |O| |X| |X| | |X|O| |O| |O| |X| |X| |O|X|O| |O| |O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | |X| | | |O|O| |X|X| | |O|X| | 'X' Won! | |O|O| |X|X|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| |X|O| | | | | | | |X|O| |X|O| | | | | | |O|X|O| |X|O|X| | | | | |O|X|O| |X|O|X| |O| | | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | | | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| | | |X|X|O| 'X' Won! |O|X|O| |O|X| | |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | |O| | | |O|O|X| | |X| | |O|X| | |O|O|X| | |X| | |O|X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| |X|X| | 'O' Won! |O|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| |X| | | | | |X|O|O| |X| |X| |O| | | |X|O|O| 'X' Won! |X|X|X| |O| | | |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | |X|X| | | |O| | |O|O| | |X|X| | | |O| | |O|O| | |X|X| | |X|O| | |O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| |O|X|O| It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X| |X| 'O' Won! |O|O|O| |X|X|O| |X| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|O| |X|O| | |O| |X| | |X|O| |X|O|O| |O| |X| | |X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | |O|O| |X| | | |X| |O| | |O|O| |X| |X| |X| |O| | |O|O| |X| |X| |X|O|O| 'X' Won! |X|O|O| |X| |X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | 'X' Won! | | |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X|O| | | | | |X|X|O| | |X|O| | | | | |X|X|O| |O|X|O| | | |X| |X|X|O| |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | |X|X|O| | |O|O| | |X| | |X|X|O| | |O|O| |X|X| | |X|X|O| 'O' Won! |O|O|O| |X|X| | |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X|X| | | | | | |O| |O| |X|X| | | | |O| |O| |O| |X|X| | | |X|O| 'O' Won! |O| |O| |X|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | |O| |O| | | |X| | |X| | |O| |O| | | |X| | |X|O| |O| |O| | |X|X| | |X|O| |O| |O| | |X|X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| | | |O|O|X| |O| | | |X|X| | |O|O|X| |O| | | |X|X|O| |O|O|X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X|X| | |O| | | | |X| |O|X|X| |O|O| | | | |X| |O|X|X| |O|O| | |X| |X| 'O' Won! |O|X|X| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X| | | |X| | | |O|O| |O|X| | |X|X| | | |O|O| |O|X| | 'O' Won! |X|X| | |O|O|O| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | |O| | | |O|X| |X| | | | |O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X| |X| 'O' Won! |O|O| | |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| | | | | |X|O| | |O| |X| |O| | | |X|O| | |O| |X| |O|X| | |X|O| | |O| |X| 'O' Won! |O|X|O| |X|O| | |O| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | |O|X| | |X| |O| | | | | |O|X|X| 'O' Won! |X| |O| | |O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O|X| | |X| | | |O| | |O|O|X| | |X| | |X|O| | |O|O|X| | |X| | |X|O|O| |O|O|X| | |X|X| |X|O|O| |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | |X|X| | | | | |O|O|X| |O|X|X| | | | | |O|O|X| |O|X|X| | |X| | |O|O|X| |O|X|X| | |X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| | | |O| | | |X| |X|O|O| | |X|O| | | |X| |X|O|O| |O|X|O| |X| |X| |X|O|O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| |O| |O| | |X| | |X|O|X| |O| |O| 'O' Won! | |X|O| |X|O|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | |X| | |O| | | |O| |X| | |X|O| |O| | | |O| |X| | |X|O| |O| |X| |O| |X| 'O' Won! | |X|O| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| | | |O| |O| |X| |O| |X| |X| |O| |O| |X| |O| |X|O|X| |O| |O| |X| |O| |X|O|X| |O| |O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | |X| | | | | | |O|X|O| | |X| | | |O| | |O|X|O| | |X| | |X|O| | |O|X|O| 'O' Won! |O|X| | |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| |O|O| | |X| | | |X| |O| |O|O|X| |X|O| | |X| |O| |O|O|X| 'X' Won! |X|O| | |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| |O|O| | | | | | |X| |X| 'X' Won! |O|O| | | | | | |X|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| |X|O| | |O| | | |X|O|X| |X|O| | |O| |X| |X|O|X| |X|O| | 'O' Won! |O| |X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X| | |O| | | |O|O| 'X' Won! |X|X|X| | |O| | | |O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | |X| | | |O|O| |X|X| | | |X|O| | |O|O| |X|X| | |X|X|O| | |O|O| |X|X| | 'O' Won! |X|X|O| |O|O|O| |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | |O| | |X| |X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | |X| | |O|O| | |X|X| | |O|X| | |O|O| | |X|X| | |O|X|X| |O|O| | |X|X|O| |O|X|X| |O|O| | 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | |O| | |X|O| | |X|X| | | |O| | |X|O| | |X|X| | | |O|O| 'X' Won! |X|O| | |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O|X| | | | | | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | 'O' Won! | |X|O| |X|O|O| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | |O|X| | | | |X| | |O|X| |O|X| | | | |X| | |O|X| |O|X| | |O| |X| 'X' Won! | |O|X| |O|X|X| |O| |X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |O| | | | |X|O| | | | | |O|X| | |O|X|O| | | | | |O|X| | |O|X|O| | | |X| |O|X| | 'O' Won! |O|X|O| |O| |X| |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X| | | 'X' Won! |X|O|O| |X| | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | |O| | |O| | | |X|X| | | |O| | |O| |X| |X|X| | |O|O| | |O| |X| |X|X| | |O|O|X| |O| |X| |X|X| | |O|O|X| |O|O|X| 'X' Won! |X|X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | |X| | | | |O| |X|O|O| | |X| | | | |O| |X|O|O| |X|X| | 'O' Won! | | |O| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X| |O| | | |O| | |O|X| |X| |O| | | |O| |X|O|X| |X| |O| | |O|O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | |O|O| | | |X| | |X|X| | |O|O| | | |X| | |X|X|O| |O|O| | | |X| | |X|X|O| |O|O| | |X|X| | 'O' Won! |X|X|O| |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | |O| | |X| | | |O|X|X| | |O| | |X| | | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|X| |X| | | | |X|O| | |O|X| |X|O| | | |X|O| | |O|X| |X|O|X| | |X|O| | |O|X| |X|O|X| |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| 'O' Won! |O| | | |O| |X| |O| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | |X|O|X| | | | | |O|O| | |X|O|X| | | |X| 'O' Won! |O|O| | |X|O|X| | |O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X|O| | | | | | |O|O|X| |X|O| | | |X| | |O|O|X| |X|O| | |O|X| | |O|O|X| |X|O| | |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| | | |X| |X|X|O| |O| |O| | |X|X| |X|X|O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | |X| | | | |X| |O|O| | | |X| | | |O|X| |O|O| | | |X| | |X|O|X| |O|O| | |O|X| | |X|O|X| |O|O| | |O|X|X| |X|O|X| |O|O| | 'O' Won! |O|X|X| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O|X|O| |O| | | |X| |X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | | |O|X| | |X| | |X|O| | | |O|X| |O|X| | |X|O| | | |O|X| |O|X|X| |X|O| | | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | |O| | | |X|X| | |O|O| | |O|X| 'O' Won! | |X|X| |O|O|O| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X|O| |X| |X| |O|X| | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| |X| |O|X| | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X|X| |O|O| | |X|O|X| |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| |X| |X| | | |O| |O| |O| |X| 'X' Won! |X| | | |O|X|O| |O| |X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | |X|X| | | |O| |O| |X| | |X|X| |O| |O| |O| |X| | |X|X| |O|X|O| |O|O|X| | |X|X| |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| |O| |X|O| | |O| | | |X|X|O| |X|O| | |O| | | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| |O| |O| |X| | | |X|O|X| |O| |O| |X| | | |X|O|X| |O| |O| |X|O| | 'X' Won! |X|O|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X|X| | | | | |O|O| | | |X|X| | | |X| |O|O| | | |X|X| | | |X| |O|O| | |O|X|X| | |X|X| |O|O| | |O|X|X| 'O' Won! |O|X|X| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X|O| | |X| | | | |O| |X|X|O| | |X| | | | |O| |X|X|O| |O|X| | |X| |O| |X|X|O| |O|X| | |X|O|O| |X|X|O| |O|X| | 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | |X| |X| | |O| | |O|O| | |X| |X| | |O| | |O|O|X| |X| |X| | |O|O| |O|O|X| |X| |X| 'X' Won! | |O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| | |O| |X| | |O|O| | |X| | |O| |X| |X|O|O| | |X| | |O|O|X| |X|O|O| |X|X| | |O|O|X| |X|O|O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| | |X| | |O| |O| |X|O|X| | |X| | |O| |O| |X|O|X| |X|X| | 'O' Won! |O|O|O| |X|O|X| |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | |O| | | |X|O| | |O|X| | |O|X| | |X|O| | |O|X|O| |O|X| | |X|O| | |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O| | | |X|X| |X| |O| |O|O| | | |X|X| |X|O|O| 'X' Won! |O|O|X| | |X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| | | |X| |X| |O| | |O|X| | |O|X| |X| |O| | |O|X| |X|O|X| |X| |O| 'O' Won! |O|O|X| |X|O|X| |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O|X| |X| | | |O| | | |O|O|X| |X| | | |O| | | |O|O|X| |X| | | |O| |X| |O|O|X| |X|O| | |O| |X| |O|O|X| |X|O| | |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O| | | |X| |O|X|O| | | |O| | | |X| |O|X|O| | | |O| | |X|X| |O|X|O| | |O|O| | |X|X| 'X' Won! |O|X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | |X| | |O| | | |O|X|O| | |X| | 'X' Won! |O|X| | |O|X|O| | |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| | | |X|O|O| 'X' Won! |O|X|X| |O|X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O|O| | |X|X|O| | | |X| |O|O| | 'O' Won! |X|X|O| | | |X| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | 'O' Won! |X| |O| | |O|X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O|X| |X| | | | | |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | |X|X| | |O| |X| |O| |O| |X|X| | |O| |X| 'X' Won! |O| |O| |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O|X| | |X| | |X| | | |O|O|X| | |X|O| |X| | | |O|O|X| |X|X|O| |X|O| | |O|O|X| |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| |X| | | | | |X|O|O| |X| |X| | |O| | |X|O|O| 'X' Won! |X|X|X| | |O| | |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| |X| | |X|O| | | | | |O|O|X| | |X|O| | | | | |O|O|X| |X|X|O| 'O' Won! |O| | | |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X|X| |O|X| | | |O| | |O|X|X| |O|X| | | |O|X| |O|X|X| 'O' Won! |O|X| | |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X| |O| |O| | |X|X| | | |X| |O| |O| | |X|X| | |O|X| |O|X|O| | |X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | |X| | |X|O|X| | | |O| | |X|O| |X|O|X| | | |O| | |X|O| |X|O|X| | |X|O| 'O' Won! |O|X|O| |X|O|X| | |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | |O|X| | |O|X|X| | | | | |O|X| | |O|X|X| | | |O| |O|X| | 'X' Won! |O|X|X| | |X|O| |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X|X| | | |X| | 'O' Won! |O|O|O| |X|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O| | |O| |X| |X| | | | |O|X| |O| |X| |X| | | | |O|X| |O|O|X| |X|X| | | |O|X| |O|O|X| 'O' Won! |X|X|O| | |O|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X|O| | | | | | | |O| |X|X|O| | | | | |X| |O| |X|X|O| |O| | | |X| |O| |X|X|O| |O| | | |X|X|O| 'O' Won! |X|X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | | | | |X|O| | |X|O|O| | | | | |X|O|X| |X|O|O| | | | | |X|O|X| |X|O|O| |O| | | |X|O|X| |X|O|O| |O|X| | It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | | |O| | |O|X| |X|X| | | | |O| | |O|X| |X|X| | | |O|O| 'X' Won! | |O|X| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X| |X| | |O|O| |X|O|O| |X| |X| 'X' Won! | |O|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| |O| | | |X| | | | |X|O| |O| |X| |X| | | | |X|O| |O| |X| |X| | | |O|X|O| |O|X|X| |X| | | |O|X|O| 'O' Won! |O|X|X| |X|O| | |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | |X|O| | | | | |X|O| | | |X|O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | 'X' Won! |X|O|X| |O|X|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X| |X| |O| |O| 'X' Won! |X|O|O| |X|X|X| |O| |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X|O| | | | | |O|X| | | |X|O| | | |X| |O|X| | |O|X|O| | | |X| |O|X| | 'X' Won! |O|X|O| | |X|X| |O|X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O|X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| |O| | | | | | | |X|X|O| |O| | | | |O| | |X|X|O| |O| | | | |O| | |X|X|O| |O|X| | | |O| | |X|X|O| |O|X|O| |X|O| | |X|X|O| |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O|X| | |O| | | | | | |X|O|X| |O|O| | | | | | |X|O|X| |O|O|X| | | | | |X|O|X| |O|O|X| | | |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | |O| | |X|X| |X| |O| | | |O| | |X|X| |X| |O| |O| |O| 'X' Won! |X|X|X| |X| |O| |O| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X|X| | |X|O| | |O|O| | |X|X| | 'X' Won! |X|O| | |O|O| | |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | | | |X| | |O|X| |O| | | | |O|X| 'X' Won! | |O|X| |O| |X| | |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| |X| | | | | |X|O|X| |O| |X| | |O| | 'X' Won! |X|O|X| |O| |X| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | |X| |X| | | |O| |O|O| | |X| |X| |X| |O| 'O' Won! |O|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| | | |X|X| | | |O| | |O|O| | |X|X| | | |O| | |O|O|X| |X|X| | | |O|O| |O|O|X| |X|X| | |X|O|O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | |O| | |X| |O| |X| |X| | |O| | |X| |O| |X| |X| | |O|O| |X| |O| |X| |X| | |O|O| |X|X|O| 'O' Won! |X| |X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X| | | 'O' Won! |X| | | |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| |O| | | |X| | |X| | |O| |O| | | |X| | |X|X| |O| |O| | |O|X| | |X|X| |O| |O| |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | | |X| | |O| |O| |X|X|O| | |X|X| |O| |O| |X|X|O| 'O' Won! | |X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X|X| | 'O' Won! |O|X|O| | |O|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | | |O| | |X|X| |X| |O| | | |O| | |X|X| |X|O|O| | | |O| 'X' Won! |X|X|X| |X|O|O| | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X|O| | |O|X| | | |O| |X|X|O| |O|O|X| | | |O| |X|X|O| |O|O|X| |X| |O| |X|X|O| 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | |O| |X| | |X|O| |X| | | |O| |X| | |X|O| |X| |O| |O| |X| | |X|O| |X| |O| |O|X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | |X|O| |O| | | |X| | | |O|X|O| |O| | | |X| | | |O|X|O| |O|X| | |X| | | |O|X|O| |O|X|O| 'X' Won! |X|X| | |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| | | |O|X| | | | | |X|O| | |O|O|X| | | | | |X|O|X| |O|O|X| | | | | 'O' Won! |X|O|X| |O|O|X| | |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | |X| | | |X|O| |O| |O| | |X| | 'X' Won! | |X|O| |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| | | | |O|O| 'X' Won! |X|X|X| |O| | | | |O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | |O| | | | |X| | |O|X| |X|O| | | | |X| | |O|X| |X|O| | |O| |X| |X|O|X| |X|O| | |O| |X| |X|O|X| |X|O|O| |O| |X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O|X|O| |X| |X| | | |O| |O|X|O| 'X' Won! |X|X|X| | | |O| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | |O|X| | 'X' Won! |X|O|O| |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X|O| | | | |O| |X| | | |X|O| | | | |O| |X|X| | |X|O| | | | |O| |X|X|O| |X|O|X| | | |O| |X|X|O| |X|O|X| |O| |O| |X|X|O| 'X' Won! |X|O|X| |O|X|O| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O|O| | |O|X| | |X| | | |O|O|X| |O|X|O| |X| | | |O|O|X| |O|X|O| |X|X| | |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | |X| |X| | |O| | |O|O|X| |X| |X| | |O| | |O|O|X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| 'O' Won! |O|O|X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X| | |O| |O| | |X| | | |X|X| |O| |O| | |X| | |O|X|X| |O| |O| |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| |O| | | | | |X|O|X| |X| |O| | | | | |X|O|X| |X| |O| | |O| | 'X' Won! |X|O|X| |X| |O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | |X|O|X| | |O| | |O|X|O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X|O| | | |X| | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O|O| | |X| | |X|O|O| |X|O|O| | |X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X|O| | | | | |X|O|X| |O|X|O| | | | | |X|O|X| |O|X|O| | | |O| 'X' Won! |X|O|X| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|O| | |O| | 'X' Won! |X| | | |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | | |X|X|O| | | | | |O| | | |X|X|O| | | |O| |O|X| | |X|X|O| | | |O| |O|X| | |X|X|O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| | | |X|O| | | |X|O| |X| |O| |X|O| | 'X' Won! |X|X|O| |X| |O| |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | |O|O| |O|O|X| 'X' Won! |X|X|X| | |O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | |X|O|O| | |X|O| | | | | |X|O|O| | |X|O| | |X| | |X|O|O| | |X|O| |O|X| | |X|O|O| |X|X|O| |O|X| | |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X|X| | |O|O| | |X| | 'O' Won! | |X|X| |O|O|O| | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | 'O' Won! |O|O|O| | |X| | | |X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X|O| | | |O| 'X' Won! | | |X| |O|X|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | 'X' Won! |X| |O| |X|O|O| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X| |X| |X|O| | |O| |O| |X| |X| |X|O| | |O|X|O| |X| |X| |X|O|O| |O|X|O| 'X' Won! |X|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X|O| | |X| | | |O| |O| |X|O| | |X| |X| |O| |O| |X|O| | |X|O|X| |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | | | |X| | |X|O| |O|X| | |O| |X| | |X|O| |O|X|X| |O| |X| | |X|O| |O|X|X| |O|O|X| 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O| | | | |X| |O|X|X| |O|O| | | | |X| |O|X|X| 'X' Won! |O|O|X| | | |X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| |O| | |X| | | |X| | 'X' Won! |O|X|O| | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | |X| |O|O| | |O|X| | | | |X| |O|O| | |O|X| | |X| |X| |O|O| | |O|X| | |X|O|X| |O|O| | |O|X|X| |X|O|X| |O|O| | 'O' Won! |O|X|X| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| |X| | | |O| |X| | | |O|O|X| | | |O| |X| | | |O|O|X| | | |O| |X|X| | |O|O|X| |O| |O| |X|X| | 'X' Won! |O|O|X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | | |X| |O|O| | | |X| | |X| |X| |O|O| | | |X| | |X|O|X| |O|O| | | |X| | |X|O|X| |O|O| | | |X|X| 'O' Won! |X|O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | |O|O|X| | |X| | | |O| | |O|O|X| | |X|X| | |O|O| |O|O|X| | |X|X| |X|O|O| |O|O|X| | |X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O| | | | |X|X| | |O|X| |O|O| | | |X|X| | |O|X| 'X' Won! |O|O|X| | |X|X| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| | |X|X| | | |O| | | |O|O| |X|X| | | |O| | |X|O|O| |X|X|O| | |O| | |X|O|O| 'X' Won! |X|X|O| |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O| | | |X|O| |X|X| | | |O|O| | |X|O| |X|X| | |X|O|O| | |X|O| |X|X| | 'O' Won! |X|O|O| | |X|O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | |X|O| |O| |O| | | |X| | |X|O| |O|X|O| | | |X| | |X|O| 'O' Won! |O|X|O| | |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| |O| |O| |X| | | |X| |O| |O| |O| |X| | | |X|X|O| 'O' Won! |O| |O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | | | |X| |O|O|X| |X| |O| | | |X| |O|O|X| |X|X|O| | | |X| |O|O|X| |X|X|O| |O| |X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| |O| |X| | |O|O| | | |X| |O|X|X| 'O' Won! | |O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O| |O| |X|X| | 'O' Won! | | | | |O|O|O| |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X| |X| |O|X| | | | |O| |X| |X| |O|X| | | |O|O| 'X' Won! |X| |X| |O|X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | | |O| |X| |X| |X| |O| | |O|O| |X| |X| |X| |O| 'X' Won! |X|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O|O| | | | |X| |X|O| | |O|O|X| |O| |X| |X|O| | |O|O|X| 'X' Won! |O| |X| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |X| | |O| | |X|X|O| | |O|X| 'X' Won! |X|O| | |X|X|O| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| |O| |X| |O| | | | | |X|X|O| |X| |O| | | | | |X|X|O| |X| |O| |O| | | |X|X|O| |X| |O| |O| |X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| | | | |O| | | |X|X| |O| | | |X|O| | | |X|X| |O| | | |X|O|O| | |X|X| |O| |X| |X|O|O| |O|X|X| |O| |X| |X|O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| |X| |O|O| | | | |X| |X| |X| |O|O| | | |O|X| |X| |X| |O|O| | |X|O|X| 'O' Won! |X|O|X| |O|O| | |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X| | | |O|X| |O| | | |O|X|X| | |O|X| 'O' Won! |O| | | |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| |X| | | | | |O|X| | |O|O|X| | | | | |O|X|X| |O|O|X| | | | | |O|X|X| |O|O|X| | |O| | 'X' Won! |O|X|X| |O|O|X| | |O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X|O| | |O| | |O|X|X| | |X|O| | |O| | |O|X|X| |O|X|O| | |O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | | |X| |O| |O| |O| |X| |X| |X| |O| |O| |O| 'O' Won! |X| |X| |X| |O| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | |X| | | | |O|O| |X|O| | |X| | | | |O|O| |X|O|X| |X| | | 'O' Won! | |O|O| |X|O|X| |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| | |O| | |O|O| | 'X' Won! |X|X|X| | |O| | |O|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| 'O' Won! | |X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | |O| |O| |X| |X| | |X| | |O| |O| |X| |X| | |X|O| |O| |O| |X| |X| | |X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| 'X' Won! |X|X|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | |O| | |X| |O| | |O|X| |X|O| | 'O' Won! |X|O|O| | |O|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O| | | | |X| | |O|X|X| 'O' Won! |O| | | |O|X| | |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |O| | | | | |O| | |X|X| |O| | | | | |O| | |X|X| |O| |O| |X| |O| | |X|X| |O| |O| 'O' Won! |X| |O| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | | |X|O| |O|O| | | |X| | | |X|O| |O|O| | | |X|X| |O|X|O| |O|O| | | |X|X| 'X' Won! |O|X|O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | | | | | |X| |O| | |X|O| | | | | |X| |O| | |X|O| | |X| | |X|O|O| | |X|O| | |X| | |X|O|O| | |X|O| |X|X| | |X|O|O| |O|X|O| |X|X| | 'X' Won! |X|O|O| |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | |X| | |X|O| | | |O| |O| |X| | |X|O| | | |O| 'X' Won! |O| |X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|X| |O|O| | |X| | | |O|X|X| |O|O|X| |X| | | |O|X|X| |O|O|X| |X|O| | |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | |X|O| | |O| | | |X|O| | |X|O| | |O| | |X|X|O| | |X|O| 'O' Won! | |O|O| |X|X|O| | |X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | |O| | | | |O| |X| |X| |X|O| | | | |O| |X|O|X| |X|O| | | | |O| |X|O|X| |X|O| | | |X|O| |X|O|X| |X|O| | |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| |O|X| | |X| |O| | | |O| |O|X|X| |X| |O| | |O|O| |O|X|X| |X| |O| |X|O|O| |O|X|X| |X| |O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| |X| |O| | | | |O|O| |X| |X| 'X' Won! |O| | | | |O|O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X|X| | | |O| |X| |O| |O|X|X| | |X|O| |X| |O| |O|X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| |O| | | | | |X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| | | |O|X|X| |X|O|O| |O|X| | |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | | |X| |X|O| | | |O|O| | | |X| |X|O|X| | |O|O| 'O' Won! | |O|X| |X|O|X| | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | | |O|X|O| |X|O| | |X| | | |O|X|O| |X|O| | |X|X| | |O|X|O| |X|O| | |X|X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X|X| | | | | |O|X|O| |O|X|X| 'X' Won! | |X| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X|O| | |O|O| | |X| | |X|X|O| | |O|O| |O|X| | |X|X|O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| | | |X| |X|X|O| |O| |O| | | |X| |X|X|O| |O| |O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |X| | | | | |X|O| | | |O|X| | | | | |X|O|X| | |O|X| | | |O| |X|O|X| | |O|X| | | |O| |X|O|X| |X|O|X| 'O' Won! | |O|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X|O| | | | | |X|O| | | |X|O| |X| | | |X|O| | | |X|O| |X| | | |X|O|O| | |X|O| |X| |X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O|X| | 'O' Won! | | |O| | |O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | 'X' Won! |X| | | |X| |O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| |O| |O|X| | | | |X| |O| |O| |O|X|X| 'O' Won! |O| |X| |O| |O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| |O| |X| |X| |X|O|O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| |O|X| | | |O|X| | |O|X| |O|X| | | |O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| |O| |X| | | |O| | |O|X| |O|X|X| | | |O| | |O|X| |O|X|X| |O| |O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | |O|X| | |X| | | |X|O| | |O|X| | |X| | | |X|O|O| |O|X| | |X|X| | |X|O|O| |O|X| | 'O' Won! |X|X|O| |X|O|O| |O|X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| 'O' Won! |O| |X| | |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X|O| | | | |X| | |O| | |X|O|X| | | |X| |O|O| | |X|O|X| | | |X| |O|O| | |X|O|X| |X| |X| 'O' Won! |O|O|O| |X|O|X| |X| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | | |X| |X|O| | |O| |X| | | |X| |X|O|O| 'X' Won! |O| |X| | |X|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X|X| | | |O|O| |X| | | |X|X|O| | |O|O| 'X' Won! |X| | | |X|X|O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O|X| | | | |X| |O|O|X| |O|X| | |X| |X| |O|O|X| |O|X| | |X|O|X| |O|O|X| |O|X| | 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | |O| | |X|O|X| |X| | | | |O| | |X|O|X| |X| |O| | |O| | |X|O|X| 'X' Won! |X| |O| |X|O| | |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | | | |O|X|X| 'O' Won! |X| |O| | |O| | |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O|O| | | |X|X| |X|O| | |O|O| | | |X|X| |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X|X| | |O| | | |O|O| | |X|X| | |O| | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O|O| | |O|O|X| |X|X| | |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | |X|X| | | | |O| | |O| | |X|X| | | | |O| |O|O| | |X|X| | |X| |O| |O|O| | |X|X|O| |X| |O| |O|O| | |X|X|O| |X| |O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O|X| |O|X|X| | | | | |O|O|X| |O|X|X| | |X| | |O|O|X| |O|X|X| | |X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | |X| | |O| |O| | |X|X| 'O' Won! | |X| | |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| |O|X| | |O|O|X| | | |X| 'X' Won! |O|X|X| |O|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | |X|X| |O| |O| |O| | | | |X|X| |O| |O| |O| | | | |X|X| |O|X|O| |O| |O| | |X|X| |O|X|O| 'X' Won! |O|X|O| | |X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | |O| |X| | |X|O| | |X| | |O| |X| | |X|O| |O|X| | |O| |X| | |X|O| 'X' Won! |O|X| | |O|X|X| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| |O| | | | |X|O| | | |X| |O| | | | |X|O| | |X|X| |O|O| | | |X|O| | |X|X| |O|O|X| | |X|O| | |X|X| |O|O|X| | |X|O| |O|X|X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | | | |O| |X| |X| |O|X|O| | | |O| |X| |X| |O|X|O| |X| |O| |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X| |O| | |O| | |X|X|O| |X| |O| | |O| | |X|X|O| |X| |O| |O|O| | |X|X|O| |X| |O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O| | | | |X| | | |O| |X|O|X| | | |X| | | |O| |X|O|X| | | |X| | |O|O| |X|O|X| | | |X| |X|O|O| 'O' Won! |X|O|X| | |O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | |X| | |O|X| | | | |O| | |X|O| |O|X| | | | |O| | |X|O| |O|X|X| | | |O| |O|X|O| |O|X|X| | | |O| |O|X|O| |O|X|X| |X| |O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| |X| |O| | |X| | | |O|X| |X|O|O| | |X| | | |O|X| |X|O|O| | |X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O| | |X|O|X| | |X| | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X|X| 'O' Won! | |O|O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| | |O|O| |O|X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| |O|O| | |O| | | |X| |X| |O|O| | |O|X| | |X|O|X| |O|O| | |O|X| | |X|O|X| |O|O|X| |O|X| | It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| |O| |X| | | |X|O| | |O| |O| |X|X| | |X|O| | |O| |O| |X|X|O| |X|O|X| |O| |O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | |O| | |O| |X| |X| |X| | |O| | |O| |X| |X| |X| | |O|O| |O|X|X| |X| |X| | |O|O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | |O| | |X| |X| |O|X|O| | |O| | |X|O|X| |O|X|O| | |O| | 'X' Won! |X|O|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | | | | | |O|X| |X| |O| | | |O| | |O|X| |X|X|O| | | |O| | |O|X| |X|X|O| |O| |O| | |O|X| |X|X|O| |O| |O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| |X| | | |O| | |X| | |O| |X| | |O|O| | |X| | |O| |X| | |O|O| | |X|X| 'O' Won! |O| |X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | |X|X| |O| | | | | | | |O|X|X| |O| | | |X| | | |O|X|X| |O| | | |X| | | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | |O|X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|O| |X|O|X| | |X|O| | |O|O| |X|O|X| |X|X|O| 'O' Won! |O|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O|X| |O|X| | 'O' Won! |O|X| | |O|O|X| |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X|X| | |X| | | |O|O| | |X|X| | |X|O| | |O|O| | |X|X| |X|X|O| | |O|O| 'O' Won! | |X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X| | |X| | | |O|O| | |X|X| |O|X| | | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X| | |O| |O| |X| |X| |O|X| | |O| |O| |X| |X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | |X| | |O| | | |O|X| | 'O' Won! |O|X| | |O| | | |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| 'O' Won! |X|X|O| | | |O| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | |X| | | |O| |X| |O| | | |X|O| | 'X' Won! |O| |X| |O|X| | |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |X| |X| |O| |O|O|X| |X| |X| |X|O|O| 'X' Won! |O|O|X| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| |O| |X| | | | |O|O| |X|X|O| 'O' Won! |X| |O| | |O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | |X|X| | | | | |O| |O| | |X|X| 'X' Won! | | | | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | |X|O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| |X| | | |X| | | |O|X|O| |X| | | |X| |O| |O|X|O| |X| | | |X|X|O| 'O' Won! |O|X|O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| | | | | | |X|O| |O| |X| | |X| | | |X|O| |O| |X| | |X| | |O|X|O| |O| |X| | |X|X| |O|X|O| |O| |X| 'O' Won! |O|X|X| |O|X|O| |O| |X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X|O| | | | |X| |O|O| | |X|O|X| 'O' Won! | |O|X| |O|O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| |O| | | |O| |X|X| | |X| |O| | |O|O| |X|X| | |X| |O| 'X' Won! |X|O|O| |X|X| | |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | |O| | | |O|X| | | |X|X| |O| | | |O|X| | | |X|X| |O| |O| |O|X| | | |X|X| |O| |O| |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X|X| | | |O| | | |X| |O|X|X| | |O|O| | | |X| |O|X|X| |X|O|O| | | |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| |X| |X| | | | |O|O| |X|O|X| |X|X| | | |O|O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X|X| |X| | | |O|O| | |O|X|X| |X| | | |O|O| | |O|X|X| |X|X| | |O|O| | |O|X|X| |X|X|O| |O|O| | It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | |X| | | |O| |X| |O| | | |X| | | |O|O|X| |O| | | |X|X| | |O|O|X| |O| | | |X|X| | |O|O|X| |O| |O| 'X' Won! |X|X|X| |O|O|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X|O|X| |O| | | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X| | | |O|O| |X|O|X| |O|X|X| | |O|O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | |X| | | | |O| | |O|X|O| |X| | | | |O| | |O|X|O| |X| | | |X|O| | |O|X|O| |X|O| | |X|O| | |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X|X| | |O| |O| | |X| | |X|X|O| |O| |O| | |X| | |X|X|O| |O| |O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| |X| |O| | |O|X| | | |X| 'O' Won! |X| |O| | |O|X| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O|X|O| | |X| | |X|O| | |O|X|O| | |X|O| |X|O|X| |O|X|O| | |X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | | | | |X|O|X| |O| | | | | |O| |X|O|X| |O| |X| | | |O| |X|O|X| |O|O|X| | | |O| |X|O|X| |O|O|X| |X| |O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| |X| |X| |O| | | | | |O|O|X| |X| |O| | | | | |O|O|X| |X| |O| |X| | | |O|O|X| |X|O|O| |X| | | |O|O|X| |X|O|O| |X| |X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| |O| |O| | | | | |X| 'X' Won! |X| |O| |O|X| | | | |X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| | |X| | | 'X' Won! |X|O| | |X|O| | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | |O| | | | |O|X| |O|X| | |O| |X| | |O|X| |O|X| | |O|O|X| | |O|X| 'X' Won! |O|X|X| |O|O|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| | | |O| | | |X| |X|X|O| | | |O| | |O|X| |X|X|O| | | |O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X| |X| | |O|O| | |X| | |X|O|X| 'X' Won! |X|O|O| | |X| | |X|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O|X| | | | |X| |O|O|X| |O|X| | 'X' Won! | | |X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| | |X| | |O|O| | | |X|X| | |X|O| 'X' Won! |O|O| | |X|X|X| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | |X|O|X| | | | | | |O| | |X|O|X| |O| | | |X|O| | |X|O|X| |O| | | 'O' Won! |X|O|O| |X|O|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| |X|O| | | | | | | |O|X| |X|O|O| | | | | |X|O|X| |X|O|O| 'O' Won! |O| | | |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| |X|X|O| 'O' Won! |O|O|X| | |O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X|O| | | | | | |X|O| |O|X|O| | | |X| | |X|O| |O|X|O| |O| |X| 'X' Won! |X|X|O| |O|X|O| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X|O| | |X|X| |O| | | |O|X|O| | |X|X| |O|O| | |O|X|O| | |X|X| |O|O|X| |O|X|O| 'O' Won! |O|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O|X| | |O| | |X| |O| |X|O|X| | |O| | 'X' Won! |X| |O| |X|O|X| |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| |X| |X| | | |O|O| | |O| |X| |X| |X| |O|O| | |O| |X| 'O' Won! |X| |X| |O|O|O| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| | | | |O| | |X|X|O| |O|X| | | |O| | |X|X|O| |O|X|O| | |O| | |X|X|O| |O|X|O| |X|O| | 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X|X| | |O|O| | | | | | |X|X| |X|O|O| | | | | | |X|X| |X|O|O| | | | | |O|X|X| |X|O|O| |X| | | |O|X|X| |X|O|O| |X| |O| |O|X|X| 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | |X| | | |X|O|O| | |X| | |X| |O| |X|O|O| | |X|X| |X| |O| |X|O|O| | |X|X| |X|O|O| |X|O|O| 'X' Won! |X|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | |X|X| | |O| |O| | | | | |X|X| | |O| |O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X|X| | | |X| | |O| |O| 'O' Won! |X|X| | | |X| | |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | |O| | |O| | | | |X|X| |X|O| | |O| |O| | |X|X| |X|O| | |O|X|O| | |X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O| | 'O' Won! |X|O| | | |O| | |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | 'O' Won! |X| |O| |X| |O| | | |O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | |O| |X| |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| 'X' Won! |X|O| | | |X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X|X| |O|O| | | | |X| |O|X|X| 'X' Won! |O|O|X| | | |X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| 'O' Won! | | |X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | |X|X| | | |O|O| |O|X| | |X|X|O| | |O|O| |O|X|X| |X|X|O| | |O|O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | | | | |O| |X| | |O|X| | | | | 'O' Won! |O| |X| | |O|X| | | |O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | |X| |O| |O| | | | |X| | |X| |O| |O|O| | | |X|X| |X| |O| |O|O| | 'O' Won! | |X|X| |X| |O| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | |X| |O|O|X| 'X' Won! |O| |X| | | |X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | |X| | | | |X| |O| |O| | |X| | | |O|X| |O| |O| |X|X| | | |O|X| 'O' Won! |O|O|O| |X|X| | | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | |X| | | 'O' Won! | |O|X| | |O| | |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | |X| | | | |O|O| |X| | | 'O' Won! |X| | | |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | |O| | | |X| |O|X|X| | | |O| | | |X| |O|X|X| |O| |O| |X| |X| |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|O| | |O| | |O| |X| | |X|O| | |O|X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| | |O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | |O|O| | |X| | |X| |X| | |O|O| 'O' Won! | |X| | |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | |X| | |X| | |O| |O| | | |X| | |X| | |O| |O| | |X|X| | |X| | 'O' Won! |O|O|O| | |X|X| | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | |X| | |X| |X| |O|O| | | |X| | |X|O|X| |O|O| | | |X|X| |X|O|X| |O|O| | 'O' Won! | |X|X| |X|O|X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| |O| | | |X|X| | | |O|X| |O| | | |X|X| | | |O|X| |O|O| | 'X' Won! |X|X|X| | |O|X| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X| | | |X|O| |O| | | | |X| | |X|X|O| |O| | | | |X|O| |X|X|O| |O| | | | |X|O| |X|X|O| |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X|X| | | | | |O|O| | |O|X|X| | | | | |O|O|X| |O|X|X| 'O' Won! |O| | | |O|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | |O|O| | |X|X| | | |X| | |O|O| | |X|X| |O| |X| | |O|O| 'X' Won! |X|X|X| |O| |X| | |O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| | | |X|X|O| | | | | |O| |O| |X|X|O| | | | | |O|X|O| |X|X|O| 'O' Won! | | |O| |O|X|O| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | |X| |X| | | |X|O|O| |O| |X| |X| | | |X|O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X| |O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | |O| | |O| | | | |X|X| | |O|X| |O| | | | |X|X| |O|O|X| |O| | | 'X' Won! | |X|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X|X| | | |X| | 'O' Won! |O|O|O| |X|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| |O|O| | | | |O| |X| |X| |O|O| | |X| |O| |X|O|X| |O|O| | |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| |O| | | |X|X| | |X| |O| |O| | | |X|X|O| |X| |O| |O| | | |X|X|O| |X| |O| |O| |X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | |O|O| | |X| | |X|O| | | |O|O| |X|X| | |X|O| | | |O|O| |X|X|O| |X|O|X| | |O|O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| |O| |O| | | | |X| | |X| |O| |O|X| | |O|X| | |X| |O| |O|X| | |O|X| | |X| |O| |O|X|X| |O|X|O| |X| |O| |O|X|X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O| | |X|O| |O| | | | |X|O| | |X|O| |O| |X| | |X|O| | |X|O| |O|O|X| | |X|O| |X|X|O| |O|O|X| | |X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | | | |O| |O| |X| |X|X| | | | |O| |O| |X| |X|X| | | | |O| |O|O|X| 'X' Won! |X|X| | | |X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| | | |O|O|X| |X| | | |X| | | |O|O|X| |X| |O| |X| | | |O|O|X| |X|X|O| |X| | | |O|O|X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | |X|O| |O| | | | |X| | | |X|O| |O|O| | | |X| | | |X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| 'X' Won! |X|X| | |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | | |O| | | |X| |X|X|O| | | |O| |O| |X| |X|X|O| |X| |O| |O| |X| |X|X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | |O| | | |X|O|X| | | | | |O| | | |X|O|X| |X| | | 'O' Won! |O| | | |X|O|X| |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | | |O| | |X| | |X|O|X| | | |O| 'O' Won! |O|X| | |X|O|X| | | |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | |X| | | | |X| |O| |O| 'O' Won! | |X| | | | |X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | | |O| | |X|O|X| |O|X| | |O|O| | |X|O|X| |O|X| | |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | |O| |O| 'X' Won! |X|X|X| |O| | | |O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| | | |O| | | |O|X|O| |X| | | |O|X| | 'O' Won! |O|X|O| |X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O|X| | | | | |O| |X| | |O|X| | |O| | |O| |X| |X|O|X| | |O| | |O| |X| |X|O|X| |O|O| | |O| |X| 'X' Won! |X|O|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | | | | |O|X| | |O| |X| 'X' Won! |X| | | |O|X| | |O| |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O|O| |X| | | |X| |O| | |O|O| |X| |X| 'O' Won! |X| |O| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X|O| | |O|X| | | | | |X|X|O| | |O|X| | |O| | |X|X|O| | |O|X| | |O|X| |X|X|O| 'O' Won! |O|O|X| | |O|X| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O|O|X| | | | | | |X|X| |O|O|X| | | | | | |X|X| |O|O|X| | |O| | | |X|X| |O|O|X| |X|O| | | |X|X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | |X| |O| | |O|O| 'O' Won! |X|X| | |X| |O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | | | |X| |O|X|O| | |X| | | | |X| |O|X|O| | |X|O| |X| |X| |O|X|O| | |X|O| |X| |X| |O|X|O| |O|X|O| 'X' Won! |X|X|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | | | | | |X|O|O| | |X|X| | | | | |X|O|O| | |X|X| | | |O| |X|O|O| | |X|X| | |X|O| |X|O|O| |O|X|X| | |X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | |X| |X|O| | |O|O| | | | |X| |X|O| | |O|O| | | | |X| |X|O|X| 'O' Won! |O|O| | | |O|X| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| |X|X|O| | |X| | | |O|O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X|X| | | | | | | |O|O| 'O' Won! |X|X| | | | | | |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | | |O| |X| |O| | |X| | |O| |O| |X| |O| | |X| | |O| |O| |X| |O| |X|X| | 'O' Won! |O| |O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X|X| | |O| | |O|O| | | |X|X| | |O| | |O|O|X| | |X|X| | |O|O| |O|O|X| | |X|X| |X|O|O| |O|O|X| | |X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| |X| |X| |O|X| | | |O|O| |X| |X| |O|X|X| | |O|O| |X| |X| |O|X|X| | |O|O| |X|O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O|X| | | | |X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X|X| 'O' Won! |O|X|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| | | |O| |X| | | |X|O|X| | | |O| |X| | | |X|O|X| | |O|O| 'X' Won! |X| | | |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | | |O| |O| | | | |X|X| | |O|O| |O| | | | |X|X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| |O| | |X|X| 'X' Won! |X|O|O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | |X|O| 'O' Won! | | |O| | |X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O| | |X|O| | | | |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O|X| |X|O| | 'O' Won! |O|O|X| |X|O|X| |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | | |X| |O| |O| |O|X|X| | | |X| |O| |O| |O|X|X| | |X|X| 'O' Won! |O| |O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X|X| | |O| | |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| | |O|X| |O|O| | |O|X|X| 'X' Won! | |O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | | |O| |X|O| | |X|X| | | | |O| |X|O| | |X|X|O| | | |O| 'X' Won! |X|O| | |X|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| | | |X|X|O| |O| |X| |O| |O| |X|X|O| |O|X|X| |O| |O| |X|X|O| 'O' Won! |O|X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X|X| |O| | | |X| |O| |O|X|X| |O| | | |X|X|O| |O|X|X| |O| | | |X|X|O| |O|X|X| |O|O| | 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| |O| | | |O| | | |X| |X|X|O| | | |O| | |O|X| |X|X|O| | | |O| |X|O|X| |X|X|O| | |O|O| |X|O|X| |X|X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X|O| |X|O| | | | | | | |X|O| |X|O| | | |X| | | |X|O| |X|O| | | |X|O| | |X|O| |X|O| | |X|X|O| 'O' Won! |O|X|O| |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| |X| | | |O|O| | |O|X|X| |X| | | |O|O| | |O|X|X| |X| |X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|O| |X| | | |O|X|X| | |O|O| |X| |O| |O|X|X| | |O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | |O| | |O|X|O| | |X| | | |O|X| |O|X|O| | |X| | | |O|X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O|X| | |O|X| | | | | | 'O' Won! |O|X| | |O|X| | |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | | | |X|X|O| | | |O| |O| | | |X|X|O| | | |O| |O| | | |X|X|O| |X| |O| |O| | | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| | |O| | | | |X| |X|O|O| | |O| | |X| |X| |X|O|O| |O|O| | |X| |X| |X|O|O| |O|O|X| |X| |X| 'O' Won! |X|O|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| | | |O|O| |X| | | | |X| | | |O|O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| |X|O|O| |X|X| | 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | | | | |X|O| | |O|X| | | |X| | |X|O| | |O|X| | | |X| | |X|O|O| |O|X| | | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X|X| | |O| | | | |O|O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X|O| | |O|O| |X|X| | 'X' Won! |O|X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X|O| |X|O| | | | |X| |O|X|O| |X|O|O| | | |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O|X| | |X|O|O| | |X| | |O|X| | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X|O| |O|X| | 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | |O| |O| | |X|X| | |X| | |O| |O| | |X|X| |O|X| | |O| |O| 'X' Won! | |X|X| |O|X| | |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O|X| |X| | | |X| |O| | |O|X| |X|O| | |X|X|O| | |O|X| |X|O| | |X|X|O| | |O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | |O| |O| |X| | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X|O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |O| 'X' Won! | |O| | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X|X| | |O| | | |O|O| | |X|X| | 'X' Won! |O| | | |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | | | | |O| | | | |O|X| |X| | | |O| | | | |O|X| |X|O| | |O|X| | | |O|X| |X|O| | 'O' Won! |O|X| | | |O|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | |O| | |O|X|X| |X| | | | |O| | |O|X|X| |X| | | |O|O| | |O|X|X| |X|X| | |O|O| | 'O' Won! |O|X|X| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X| | |O| | | | |O|X| |X|X| | |O| | | | |O|X| |X|X| | |O|O| | | |O|X| 'X' Won! |X|X|X| |O|O| | | |O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | |X| | |O|O|X| | | | | | |X|O| |O|O|X| | | | | |X|X|O| |O|O|X| | |O| | |X|X|O| |O|O|X| | |O|X| |X|X|O| |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| |X| |O|O| | | | |X| 'O' Won! |X| |X| |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O|O| 'X' Won! |O|X|X| |O|X| | |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | |O| |X|X| | | |O| | |X| |O| |X|X| | | |O| | |X| |O| |X|X|O| | |O| | 'X' Won! |X| |O| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X|O|X| |O| |X| | | | | |X|O|X| |O| |X| |O| | | |X|O|X| |O|X|X| |O| | | |X|O|X| |O|X|X| |O|O| | |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| |X| |X|O| | | |O| | |X| |X| |X|O| | | |O|O| |X| |X| |X|O|X| | |O|O| 'O' Won! |X| |X| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | | |X|X| | |X|O| | |O|O| | |X|X| |X|X|O| | |O|O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X|O|X| 'X' Won! |X| |O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| 'O' Won! | |O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O|X| | | |O|X| | | |X| |O|X|O| | |O|X| 'X' Won! |X| |X| |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | |O|O| | |X|O| | | |X| | |O|O| | |X|O|X| | |X|O| |O|O| | |X|O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | | |O| |O| | | | |X|X| | | |O| |O|X| | | |X|X| |O| |O| |O|X| | | |X|X| |O| |O| |O|X|X| | |X|X| 'O' Won! |O| |O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| |O|O| | 'X' Won! |O| |X| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | |O| | | |X|O| | |O|X| | |O| | | |X|O| | |O|X|X| 'O' Won! |O| |O| |X|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| |O| | | | | |O| |X|O|X| |O| | | | |X|O| |X|O|X| |O|O| | | |X|O| |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O|X| |O|X| | 'O' Won! |O|X|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | |O| | |X|O| | |X| | | | |O| | |X|O| | |X|X| | | |O|O| |X|O| | |X|X| | | |O|O| |X|O|X| |X|X| | 'O' Won! |O|O|O| |X|O|X| |X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O|X| | | | | | |O| | |X|O|X| |O| | | | |O| | |X|O|X| |O| |X| 'O' Won! | |O| | |X|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X|X|O| 'O' Won! |O|O|O| | | |X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X|O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | |O| | |X|X| | |O|X| | | |O| | |X|X| | |O|X| | |O|O| | 'X' Won! |X|X| | |O|X| | |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X|X| | |O| | |O|O|X| |O|X|X| 'X' Won! | |O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X|X| | | |O|O| | | |X| |X|X| | | |O|O| | |O|X| |X|X| | | |O|O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X|X| | | | | |O|O|X| |O|X|X| | |O| | 'X' Won! |O|O|X| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| |O|X|O| | | | | | |X|O| |O|X|O| |X| | | | |X|O| 'O' Won! |O|X|O| |X| |O| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | |X|X| | |O| |X| |O| | | |X|X|O| |O| |X| |O|X| | |X|X|O| |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | |O| | | |O| |X| |X| |X| |O| | | |O| |X| |X| |X| |O| | | |O| |X|O|X| 'X' Won! |X| |O| |X| |O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O| | | |X| |X| |O| | |O|O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| | |O|O| | |O|X| |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | |X| | |X|O|O| | | |O| | |X| | |X|O|O| | |X|O| | |X| | |X|O|O| | |X|O| |O|X| | |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| |O| |O|X| | | |O| | |X| |O| |O|X|X| |O|O| | |X| |O| |O|X|X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X|X| | | | | 'O' Won! |O|O|X| |O|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| 'X' Won! | |O|O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| 'X' Won! |X|X|X| | |O| | | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | |O| | | |X| |O| |X|X| | |O| | | |X| |O| |X|X|O| |O| | | 'X' Won! |X| |O| |X|X|O| |O| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X|O| |X|O| | 'X' Won! |X|O|X| |O|X|O| |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| |X| |O| | | |X|X| | |O| |X| |O|O| | |X|X| | |O| |X| |O|O|X| |X|X|O| |O| |X| |O|O|X| 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X| | | | |O| | |O| | |X|X| | | |X|O| | |O| | |X|X| | | |X|O| |O|O| | 'X' Won! |X|X|X| | |X|O| |O|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | |O| | |X| |X| |O|X|O| | |O| | |X| |X| |O|X|O| |O|O| | 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| | | |O| |X| |O| |X| |X| | | |O|O|X| |O| |X| |X| | | 'X' Won! |O|O|X| |O|X|X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | |X| | |O| | | |O|X|O| | |X| | |O| | | |O|X|O| | |X|X| |O|O| | |O|X|O| | |X|X| 'X' Won! |O|O| | |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| | | |X| 'X' Won! |X|O|O| |X|X|O| | | |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| |O| |O| 'X' Won! |X| |O| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| 'O' Won! |O|X|X| |O| | | |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O|O| | |X| | | |X|O| | |O|O| | |X|X| | |X|O| | |O|O| | |X|X| | |X|O|O| 'X' Won! |O|O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X|X| | | |O|O| | |X|O| |X|X| | | |O|O| |X|X|O| |X|X| | 'O' Won! | |O|O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X|O| | | | | |X| |O| | |X|O| | | |X| |X| |O| |O|X|O| | | |X| |X| |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X| |X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X| |O| | |O|O| | |X|X| |X| |O| | |O|O| |O|X|X| |X| |O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| |O| |X| |X|X| | |O| |O| 'X' Won! |O| |X| |X|X|X| |O| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X|O| | |O| |X| | | | | |X|O| | |O|X|X| | | | | |X|O|O| |O|X|X| | | | | 'X' Won! |X|O|O| |O|X|X| | | |X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| |X| | | |X|O| | | |O|O| |X| | | |X|O|X| | |O|O| |X| | | 'O' Won! |X|O|X| | |O|O| |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O|O| |X| | | |X|O|X| | |O|O| |X| | | 'O' Won! |X|O|X| | |O|O| |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| | | |X|O|X| | | |X| |O|O| | |X|O|X| | | |X| |O|O| | |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | |X|O| |O| | | | |X| | | |X|O| |O|O| | | |X|X| | |X|O| |O|O| | | |X|X| |O|X|O| |O|O| | | |X|X| |O|X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X|O| | |O|O| | |X|X| | |X|O| | |O|O| | 'O' Won! |X|X|O| |X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| |X| | | |X| | | |O|O|X| |X| | | |X|O| | 'X' Won! |O|O|X| |X|X| | |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | |X| |X| | |O|O| | | | | |X| |X| | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | |O|O| | |X|O| | |X|X| | |O|O| | |X|O| | 'O' Won! |X|X| | |O|O|O| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| |O| | | | | |X|O| | |X|X|O| | | | | |X|O| | |X|X|O| |O| | | |X|O| | |X|X|O| |O| |X| |X|O| | |X|X|O| |O|O|X| |X|O| | It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O|X|X| | | | | |O| | | 'O' Won! |O|X|X| |O| | | |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| |O| | | |X|X| | |X| |O| |O| | | |X|X| | |X| |O| |O|O| | |X|X| | |X|X|O| |O|O| | 'O' Won! |X|X| | |X|X|O| |O|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | | | |O|O|X| | | |X| | | | | 'X' Won! |O|O|X| | | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | | |X| | | |O| |X|O| | | | |X| |O| |O| |X|O|X| | | |X| |O| |O| |X|O|X| |O| |X| |O| |O| |X|O|X| |O| |X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X|O| | |O| | |X| |O| |X|X|O| | |O| | |X|O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | |O| | | | |X| |O| |X| | |O| | | |X|X| |O| |X| | |O| | |O|X|X| 'X' Won! |O| |X| | |O|X| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X|X| | |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X|O| |X| | | 'X' Won! |O| |X| | |X|O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | 'O' Won! |O| |X| |O|X| | |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | | |O| | | |O| |O|X|X| |X| |O| | | |O| |O|X|X| |X| |O| | |O|O| |O|X|X| |X|X|O| | |O|O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| |X| | | | | |O| | |O|X| |X| | | | |O|O| | |O|X| |X| | | |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | | |X|X| | |O|X| | |O|O| | |X|X| | |O|X| | |O|O| | |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| 'X' Won! |X|O| | |O|X| | | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X|O| | |O| | | |X| |O| |X|O| | |O|X| | |X| |O| |X|O| | |O|X|O| |X| |O| |X|O| | |O|X|O| |X|X|O| 'O' Won! |X|O|O| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| |O| | | |X| |O| |X| |X| |O| | | |X| |O|O|X| |X| |O| |X| |X| |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X|X| | |X|O| | |O|O| | |X|X| 'X' Won! | |X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | |X| | |X| | |O|O| | |O| |X| | |X|X| |O|O| | |O| |X| 'O' Won! |O|X|X| |O|O| | |O| |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X|X| | |O| |O| 'O' Won! | |X| | |X|X| | |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X| | |O|X| | | | |O| 'X' Won! |O|X| | |O|X| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O|O| | | |X|X| | |X|O| |O|O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| |O|O| | 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X| | |O| |O| | | | | 'X' Won! |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| |O| |O|O|X| | | |X| |X| |O| |O|O|X| | | |X| |X|O|O| |O|O|X| |X| |X| |X|O|O| 'O' Won! |O|O|X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |X| |O|O|X| | |O| | |X| |X| 'X' Won! |O|O|X| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | | | |O|X| | | |X|O| | | | | |O|X| | |X|X|O| | | |O| |O|X| | |X|X|O| 'X' Won! | |X|O| |O|X| | |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X|X| | |X| |O| |O| |O| |X|X| | 'X' Won! |X| |O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X| | |X| | | | |O|O| | |X| | |X|X| | | |O|O| 'O' Won! | |X| | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | |O| |X| |X| | | |O|O| | |O|X|X| 'O' Won! |X| |O| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | |O|O| | | | |O| 'X' Won! |X|X|X| |O|O| | | | |O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | |O|X| | | |X| 'X' Won! | |O|X| | |O|X| | | |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| | | | |O| | | |X|X| |O|X| | | |O| | | |X|X| |O|X| | |O|O| | | |X|X| |O|X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| | | | | |O|X| | |O| |X| | | | | |O|X| | |O|X|X| | | | | |O|X| | |O|X|X| | | |O| |O|X|X| |O|X|X| | | |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| |O| |O| |O| |X| | | |X| |O| |O| |O| |X| | |X|X| 'O' Won! |O|O|O| |O| |X| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| |O| | | | | |X| |O| |X| 'X' Won! |O| |X| | | |X| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | |X| | |O| |O| |X|O| | | |X| | |O| |O| |X|O| | | |X|X| |O| |O| |X|O|O| | |X|X| |O| |O| 'X' Won! |X|O|O| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | | |X| | | |O| 'O' Won! |O|X| | | |O|X| | | |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| | | |O|X| | |X|X| |O|O| | | |O|X| 'X' Won! |X|X|X| |O|O| | | |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | |O|X| | |X| |O| 'X' Won! |O|X| | |O|X| | |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| | | | | |X|X| | |O|O|X| | |O| | |X|X| | |O|O|X| |X|O| | |X|X| | 'O' Won! |O|O|X| |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X| | | |O|O| | |X| |O| |X| | | |O|O|X| |X| |O| |X|O| | |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X| | | |X|O| |O| | | |O|X| | | |X|O| |O| |X| |O|X| | 'O' Won! | |X|O| |O|O|X| |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O|O| | |O| |X| | | |X| 'X' Won! |O|O|X| |O| |X| | | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | 'X' Won! |O|O| | |O|X| | |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | |X|X| | | |O| |O| |X| | |X|X| | |O|O| |O| |X| | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | |O| | |X| | | |O|X| | |X|O| | |X| | | |O|X|O| |X|O| | |X|X| | |O|X|O| |X|O| | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O|X|X| | | | | | |O|O| |O|X|X| | | | | |X|O|O| |O|X|X| | | | | |X|O|O| |O|X|X| | |O| | 'X' Won! |X|O|O| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| |X| | | |X| 'O' Won! |O|O|O| |X| |X| | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| |X|X|O| | |O| | | | |X| |X|X|O| | |O|O| 'X' Won! | | |X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O|X|X| | | |X| |O| | | |O|X|X| | | |X| |O| |O| |O|X|X| | | |X| |O|X|O| 'O' Won! |O|X|X| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | |O| |X| | | | | |O|X| | |O| |X| | | | | |O|X|X| 'O' Won! |O| |X| |O| | | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | | |X| | |X|X| 'O' Won! |O|O|O| | | |X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| | |X|O| | |X| | |X|O|O| |O|X|O| | |X| | |X|O|O| |O|X|O| | |X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | 'O' Won! |X|O| | | |O| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O|X| |X| | | |O| | | |O|O|X| |X| | | |O|X| | |O|O|X| |X| | | |O|X| | |O|O|X| |X|O| | |O|X|X| |O|O|X| |X|O| | 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| | | |X|O| | | |O|X| |X| |O| |X|O| | 'X' Won! |X|O|X| |X| |O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | |X| | |X| |O| |O| |O| | |X| | |X|X|O| |O| |O| | |X| | |X|X|O| |O| |O| |O|X| | |X|X|O| |O| |O| |O|X|X| 'O' Won! |X|X|O| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| |O| | | | | |X| |X|O|O| |O|X| | |O| |X| |X|O|O| |O|X| | |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| |X| |O| |O| | | |X| |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | | |X| |O|O|X| |O|X|O| |X| |X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |O| | |O| | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O| | | |X|X| |X| |O| |O|O| | | |X|X| |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | | | |O| |O|O|X| | |X| | | |X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| |X|X| | 'O' Won! |O|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | | | |X| |O|O| | |X| | | |O| |X| |O|O| | |X| |X| |O| |X| 'O' Won! |O|O|O| |X| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | |O| | | |O| | |X| |X| | |O| | | |O|X| |X| |X| 'O' Won! | |O| | | |O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X|O| |X| | | | | |O| |O|X|O| |X| | | | |X|O| |O|X|O| |X| | | |O|X|O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | |O|O| | |O|X|X| |X|X| | 'O' Won! |O|O|O| |O|X|X| |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | | | |O|X|O| | | |X| | |X| | |O|X|O| | | |X| | |X|O| |O|X|O| | | |X| |X|X|O| |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X|O|X| | |O| | |O| | | |X|O|X| |X|O| | |O| | | 'O' Won! |X|O|X| |X|O| | |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| |X| | | | |X| | |O|O|X| |X|O| | | |X| | |O|O|X| |X|O|X| |O|X| | |O|O|X| |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| |X| |X| |O| | | | | |O|O|X| 'X' Won! |X| |O| | |X| | |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |O| |X| |O| |O|O|X| |X|X|O| |X| |O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | |O| |O| | | | |X|X| 'X' Won! | | |O| |O| | | |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | |X| | | |X| |O| | |O|O| |X| | | |X| |O| | |O|O| |X|X| | 'O' Won! |X| |O| | |O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X|O| | |X| | |X| |O| |O|X|O| | |X| | |X|O|O| |O|X|O| |X|X| | |X|O|O| |O|X|O| 'O' Won! |X|X|O| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X|X| | | |O|O| 'O' Won! | | | | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X| |O| |X|O| | | |O| | 'X' Won! |X| |O| |X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | | |X| |X|O| | | |O| | |O| |X| |X|O| | | |O| | |O|X|X| |X|O| | | |O| | |O|X|X| |X|O| | |O|O| | |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |X| | |O| | | |O| |X| | |X| | |O| |X| |O| |X| | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O| |X| |X|X| | |O|O|X| |O| |X| |X|X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| |X| | | |O| |X|X|O| |O| |X| 'X' Won! |X| |O| |X|X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| |O|X| | | |O|X| |X|X|O| |O|X| | | |O|X| |X|X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | |O| |O| |O|X|X| | | | | |O|X|O| |O|X|X| | | | | 'O' Won! |O|X|O| |O|X|X| |O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | |O|X| | | | |O| | |X| | |O|X| | | | |O| | |X|X| |O|X| | | | |O| |O|X|X| |O|X|X| | | |O| |O|X|X| 'O' Won! |O|X|X| |O| |O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | |O| | | | |X| |X| |O| |X|O| | | | |X| |X| |O| |X|O| | |O| |X| |X| |O| |X|O|X| |O| |X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | |O|X| | |X| | |O|O|X| | |O|X| |O|X| | |O|O|X| | |O|X| 'X' Won! |O|X|X| |O|O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| |O|X| | | | |O| | |O|X| |O|X| | | |X|O| |O|O|X| |O|X| | | |X|O| |O|O|X| |O|X|X| | |X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| | |X|X| 'O' Won! | |X| | |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| | |O| | |O| |X| |X|O|X| |X|O| | |O| |X| |X|O|X| 'O' Won! |X|O| | |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O| |X| | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O| |X| |X|X| | |O|O|X| |O| |X| |X|X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| |O| |X| | | | |O| | |X|X|O| |X|O| | | |O| | |X|X|O| 'X' Won! |X|O| | |X|O| | |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | |O| |X|O| | |X|X| | | | |O| |X|O| | |X|X|O| | | |O| 'X' Won! |X|O| | |X|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X|X| |O| | | |O|O| | | |X|X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O|O| | 'X' Won! |O|O|X| |X|X|X| |O|O| | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | 'O' Won! |X| |O| | |O|X| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | | |X|O| 'O' Won! |O| |X| |X|O| | | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| |X| |X| |O| |O| | | |O| |X| |X| |O| |O| |X| |O| |X| |X| 'O' Won! |O|O|O| |X| |O| |X| |X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| | | |O|O| 'X' Won! |X|X|X| | |O| | | |O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X|X| | |O| |O| 'O' Won! | | | | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O|X| |O|X| | |O|X|O| | |O|X| |O|X| | |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X|X| | | |O|O| |X| |O| |X|X| | | |O|O| |X|X|O| |X|X| | | |O|O| 'O' Won! |X|X|O| |X|X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | |X|O|X| | | | | | |O| | |X|O|X| | | | | | |O|X| |X|O|X| | | | | |O|O|X| |X|O|X| | |X| | |O|O|X| |X|O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O|X| | | | | | | |X| |O|O|X| |O| | | 'X' Won! | | |X| |O|O|X| |O| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O|O| | | |X| | | | |X| |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| 'X' Won! | |O|X| |O|O| | |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| |X| |X| |O| 'O' Won! |O| |X| |O|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | |O| |X| | | |O|X|O| | | |O| |X| |X| |O|X|O| | |O|O| |X| |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | | |X| | |X| |O| |O|X| | |O|X| | 'X' Won! |X|X|O| |O|X| | |O|X| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | 'X' Won! | |O|X| | |X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X| |O| | |O| | | |X|O| |X|X|O| | |O| | |O|X|O| |X|X|O| | |O| | |O|X|O| |X|X|O| |X|O| | 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O|X| |X| | | |X|O|O| | |O|X| |X| | | 'X' Won! |X|O|O| |X|O|X| |X| | | | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | |O|X| |O|X| | | | |X| | |O|X| |O|X| | |O| |X| | |O|X| |O|X| | |O| |X| |X|O|X| |O|X| | |O|O|X| |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | |X| |O|X| | | |O|O| | | |X| |O|X| | | |O|O| | | |X| |O|X|X| 'O' Won! |O|O|O| | | |X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | | |X| 'O' Won! |X|X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | |O| |O|X| | | | |X| |X| |O| |O|X| | | | |X| |X| |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X|O| | | |O| | |X| |O| |X|O|X| | |O| | |X| |O| |X|O|X| | |O|O| |X| |O| 'X' Won! |X|O|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O| | |X|X| | |X| |O| | |O| | |X|X|O| |X|X|O| | |O| | |X|X|O| 'O' Won! |X|X|O| | |O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X|O| |X|O| | |X|O| | |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | |O| | |O|X|X| |X|O| | | |O| | |O|X|X| |X|O| | | |O|O| |O|X|X| |X|O|X| | |O|O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | | | |O| | | |X|X|O| | | | | |O|X| | |X|X|O| | | | | |O|X|O| 'X' Won! |X|X|O| | |X| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X|X| | |X| | | |O|O| |O|X|X| | |X|X| | |O|O| |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | |X| |O| | |O| | | |X| | |X| |O| | |O| | | |X|O| |X|X|O| | |O| | | |X|O| |X|X|O| |O|O| | | |X|O| |X|X|O| |O|O| | |X|X|O| 'O' Won! |X|X|O| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| |X| | | |X| |O| | | |O| 'O' Won! |X| |O| |X| |O| | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | |X|O| | | | | |X| | | | |X|O| | | |O| |X| |X| | |X|O| | | |O| |X| |X| | |X|O| |O| |O| |X| |X| |X|X|O| |O| |O| |X|O|X| |X|X|O| |O| |O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | |X| | | |O|X|O| |X|O| | |X| | | |O|X|O| |X|O| | |X| |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'X' Won! |X|O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| | | |O|O| | | |O|X| |X| |X| |O|O| | |O|O|X| |X| |X| |O|O| | 'X' Won! |O|O|X| |X| |X| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| |O|O| | | | |X| |X|O|X| |O|O| | | |X|X| |X|O|X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | |O| | |O| | | |X| |X| | |O|X| |O| | | |X| |X| | |O|X| |O|O| | 'X' Won! |X| |X| | |O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | |O| | |X|O| |O|X| | | | |O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | |O| |O| |O|X|X| | | | | |O| |O| |O|X|X| |X| | | |O| |O| |O|X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X|X|O| |O| | | | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O| |X| |O| |O| |X|X|O| |O|X|X| |O| |O| |X|X|O| 'O' Won! |O|X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | |O| | | | |X| |X|O|O| |X|O| | | | |X| |X|O|O| |X|O| | |O| |X| |X|O|O| |X|O|X| |O| |X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | |O|X| | |X| | |O| | | |X|O|X| | |X| | |O| | | |X|O|X| | |X| | |O| |O| |X|O|X| | |X| | |O|X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | | |X| | | |O| |X|O| | | | |X| | |O|O| |X|O| | |X| |X| | |O|O| 'O' Won! |X|O| | |X| |X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O|O| |X| | | |O|X| | |X|O|O| |X| |X| |O|X| | 'O' Won! |X|O|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| | |O|X| |X|X|O| | |O|O| | |O|X| |X|X|O| | |O|O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O|O| | | | |X| |X| |X| |O|O| | | | |X| |X|O|X| |O|O| | | | |X| |X|O|X| |O|O| | |X| |X| 'O' Won! |X|O|X| |O|O| | |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | |X|O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| |X|O| | | |O|X| 'O' Won! | |X|O| |X|O| | |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| |O|O| | |X| | | |O| |X| |O|O| | |X|X| | |O|O|X| |O|O| | |X|X| | |O|O|X| |O|O|X| |X|X| | 'O' Won! |O|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| |O| | |X|X| |O|O| | |X| |O| | |X|X| |O|O| | |X|O|O| | |X|X| |O|O|X| |X|O|O| 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | | |O|O|X| |X| |X| |O| | | |O|O|X| |X| |X| |O|O| | |O|O|X| 'X' Won! |X| |X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | |X| | |O|X| | | |O|O| | |X| | |O|X| | | |O|O| | |X|X| |O|X| | | |O|O| | |X|X| |O|X|O| 'X' Won! | |O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O|O| | | |X| | |X| |X| |O|O| | | |X| | |X|O|X| |O|O| | |X|X| | |X|O|X| |O|O| | 'O' Won! |X|X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | |X| | |X| | | |O| |O| | |X| | |X| |O| |O| |O| | |X| | |X|X|O| 'O' Won! |O|O|O| | |X| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| | | |O| | | |X| |O|X|X| | | |O| | | |X| |O|X|X| | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | | |O| |X| | | |O|X|O| | |X|O| |X| | | |O|X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| |O| |X| | |O|O| | |X|X| |O| |X| | |O|O| | |X|X| |O|O|X| | |O|O| 'X' Won! |X|X|X| |O|O|X| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O|X| | |X| |X| |O| |O| |O|X|O| |X| |X| |O| |O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X|X| |X|O| | |O| | | |O|X|X| |X|O| | |O| |X| |O|X|X| |X|O| | |O|O|X| |O|X|X| 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | 'O' Won! | | |X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O| | | |X|O|X| | |O| | |O| | | |X|O|X| | |O| | |O|X| | |X|O|X| | |O| | |O|X|O| |X|O|X| |X|O| | |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O|X|X| | |X| | 'O' Won! |O| | | |O|X|X| |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| | 'X' Won! |X| | | |X|O| | |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| | |O| | |X| | | |X|O|X| | |O| | 'O' Won! |X|O| | |X|O|X| | |O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | |O| |X| |O| | |O|X| |X| |O| |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | |O| |X| | |O| | |O|X| | |O| |X| |X|O| | |O|X| | |O| |X| |X|O|O| |O|X| | |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O|O| | | |O|X| | |X|X| |O|O| | | |O|X| | |X|X| |O|O| | |O|O|X| 'X' Won! | |X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| |X| |O| |X| | | |O| |O| |X| |O| |X| | | |O| |O|X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | | | |X| |O| | |X|O| |X| | | |X| |O| |O|X|O| |X| | | |X| |O| |O|X|O| |X|X| | |X| |O| 'O' Won! |O|X|O| |X|X|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X| |O| | |O| | |X| |O| |X|X|O| | |O| | |X| |O| |X|X|O| |O|O| | 'X' Won! |X| |O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | |X|O| | |X| | | | |O| |O|X|O| | |X| | | | |O| |O|X|O| |X|X| | | | |O| |O|X|O| |X|X| | |O| |O| 'X' Won! |O|X|O| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O|X| | | | | | |X|O|O| |O|X| | | | | | |X|O|O| |O|X|X| | | | | |X|O|O| |O|X|X| | | |O| |X|O|O| |O|X|X| | |X|O| It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | |O| | |O| | |X|X| | | |X|O| | |O| | |X|X|O| | |X|O| |X|O| | |X|X|O| | |X|O| 'O' Won! |X|O|O| |X|X|O| | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| |X|O|X| |X| |X| |O| |O| |X|O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| | | |X|X| | | |O| |O|O| | 'X' Won! |X|X|X| | | |O| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X|O| | | |X| | | |O| | 'X' Won! |X|O| | | |X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | | | | | |X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | |X| |O|X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| |O| |X| |O| | | | |X|X| |O| |X| |O| | | | |X|X| |O|O|X| |O| | | 'X' Won! | |X|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | |O|X| | | | |X| 'X' Won! |X|O| | |O|X| | | | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| |O| | |X| | |O|O|X| |X| |O| | |X|O| |O|O|X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | |X| |O|O| | |X| | | 'O' Won! | | |X| |O|O|O| |X| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| | | | | |O|X| | |X| |O| |X| | | |O|X| | |X| |O| |X| |O| |O|X| | |X| |O| |X|X|O| 'O' Won! |O|X|O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | |O|O|X| | |X| | | |O|X| |O|O|X| | |X| | | |O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| |O|X| | It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| 'O' Won! |O|O|O| | | | | |X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| |O| | | |X| |X|O|O| |X| |O| 'O' Won! |O| |X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| |X| | | | |O|X| | | |O| |X|O| | | |O|X| | | |O| |X|O| | | |O|X| | |X|O| |X|O| | | |O|X| |O|X|O| |X|O|X| | |O|X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| |O| |O| | | | | | |X|X| |O| |O| | | |X| | |X|X| |O| |O| | | |X| |O|X|X| |O| |O| |X| |X| |O|X|X| 'O' Won! |O|O|O| |X| |X| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | |X|O| | | | |X| | |O| | |X|O|O| | | |X| | |O|X| |X|O|O| | | |X| |O|O|X| |X|O|O| | | |X| |O|O|X| |X|O|O| | |X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | | |X|X| | |O| | |O| |O| | |X|X| | |O| | |O| |O| | |X|X| |X|O| | |O| |O| | |X|X| |X|O|O| 'X' Won! |O| |O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | | |O|X| |X| | | | | |O| | |O|X| |X| |X| | | |O| | |O|X| |X|O|X| | | |O| | |O|X| |X|O|X| | |X|O| 'O' Won! |O|O|X| |X|O|X| | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | | | | |O|O| 'X' Won! |X|X|X| | | | | | |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | | |X| |X| | | |O|O|X| |O| |X| |X| | | 'X' Won! |O|O|X| |O| |X| |X| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| | |O|X| | |X| | | |O|O| |X|O|X| | |X| | | |O|O| |X|O|X| | |X|O| | |O|O| 'X' Won! |X|O|X| | |X|O| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O|O| | | | | |X| |X| |X|O|O| | | | | |X| |X| |X|O|O| | | |O| 'X' Won! |X|X|X| |X|O|O| | | |O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| |X| |X| |X| |O| |O| |O|O|X| 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | | |X| | | |O|X| |O| |O| | |X| | | |O|X| |O|X|O| | |X| | | |O|X| |O|X|O| | |X|O| | |O|X| |O|X|O| |X|X|O| | |O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| |X| |O| | | |X| |O| |X| |X| |O| |O| |X| |O| 'X' Won! |X| |X| |O|X|O| |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | |O|O| | |X| |X| |O| | | |O|O| | |X| |X| |O| |X| |O|O| | |X| |X| |O|O|X| 'X' Won! |O|O| | |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O| | 'O' Won! |X|O|X| |O|O| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| |X|O| | | | |O| |O|X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | |X| |X| |O| |X| 'O' Won! |O|O|O| |X| |X| |O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X|X| 'O' Won! |O| | | |O| |X| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O|X|X| | | |O| | | | | |O|X|X| | | |O| | | |O| |O|X|X| |X| |O| | | |O| |O|X|X| |X| |O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| |O| | | | |X| | | | |O| |O| | | | |X| | |X| |O| |O| | | | |X|O| |X| |O| 'X' Won! |O| |X| | |X|O| |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| |X|O| | | | |O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O| | |X|O|O| 'O' Won! |O|X|X| |X|O| | |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | | | | |O|X|X| |O|O| | | | |X| |O|X|X| |O|O| | | |O|X| |O|X|X| 'X' Won! |O|O|X| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X|O| |O| |X| |X| |O| | |X|O| |O| |X| |X| |O| |O|X|O| |O|X|X| |X| |O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | |X|X| | | | | |O| |O| | |X|X| | | |O| |O| |O| | |X|X| |X| |O| |O| |O| | |X|X| |X|O|O| 'X' Won! |O| |O| |X|X|X| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | |O| | |X| |X| |O|X| | | |O| | |X| |X| |O|X| | | |O|O| |X| |X| |O|X|X| | |O|O| 'O' Won! |X| |X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X|O| | | |X| |O|X| | | |X|O| | | |X| |O|X| | | |X|O| |O| |X| |O|X|X| | |X|O| |O| |X| |O|X|X| | |X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| |O| |X| |X| | | |O| |O| |O|X|X| |X|O| | |O| |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| |O|O| | | |O| | | |X|X| |O|O| | |X|O| | | |X|X| |O|O| | |X|O| | |O|X|X| |O|O| | |X|O|X| |O|X|X| 'O' Won! |O|O|O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | |O| | | |X| | |X| |O| |X|O| | | |X| | |X| |O| |X|O| | | |X| | |X|O|O| 'X' Won! |X|O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X|X|O| 'O' Won! | | |O| | | |O| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|X| | |O| | |O| |X| |O|X|X| | |O| | |O|X|X| |O|X|X| | |O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | |O| | | | |X|X| | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| | |O|O| |O|X|X| | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X|O| | |X|O| 'X' Won! | |X| | |O|X|O| | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X| | | |X| | |O|O|X| |O|X| | | |X|O| 'X' Won! |O|O|X| |O|X| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O|O|X| | |X| | |X|O| | |O|O|X| | |X| | |X|O|O| |O|O|X| |X|X| | |X|O|O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X| |X| | |O| | |O|O| | |X| |X| | |O|X| |O|O| | |X| |X| 'O' Won! | |O|X| |O|O|O| |X| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| |X| |O| | |O|X| |O| |X| |X| |O| | |O|X| |O| |X| |X|X|O| | |O|X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| |O| |O| | |X|O| | | |X| |O|X|O| |O|X|O| | | |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O|X|X| | | | | |O| | | |O|X|X| | | | | |O| |O| |O|X|X| |X| | | |O| |O| 'O' Won! |O|X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X|O| | |O|O| |X| | | |X|X|O| | |O|O| 'O' Won! |X| |O| |X|X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O|O| |X| | | | |X| | |X|O|O| |X| |O| | |X| | |X|O|O| 'X' Won! |X| |O| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| | |O|X| | | | | |X|X|O| |O|O|X| | | |X| |X|X|O| |O|O|X| | |O|X| |X|X|O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O| | |O|X|X| 'O' Won! |O|X|O| | |O| | |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O| |O| |X| |X| 'X' Won! |O| | | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | |X| |X| |O|O| | | | |O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O| | |O|X|O| |X| |X| |O|O| | |O|X|O| |X| |X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | |X| | | | |O| | |X|O| |O|X| | | | |O| 'X' Won! | |X|O| |O|X| | | |X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | |O|X|X| |X|O|X| |O|O| | |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O|O| |X| | | |O| | | |X|O|O| |X|X| | |O| | | |X|O|O| |X|X| | |O|O| | 'X' Won! |X|O|O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | |O|X|O| | | |X| | | | | |O|X|O| | | |X| | | |X| |O|X|O| |O| |X| | | |X| |O|X|O| |O| |X| |X| |X| |O|X|O| |O| |X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O| |X| |O|X| | | |X|O| |O| |X| |O|X| | | |X|O| |O| |X| |O|X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | | |X|O|O| |O|X|X| | | | | |X|O|O| |O|X|X| |X| | | |X|O|O| 'O' Won! |O|X|X| |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | | |O|O| | | | | |X|X| | |X|O|O| | | | | |X|X| | |X|O|O| | | |O| |X|X| | 'X' Won! |X|O|O| |X| |O| |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | |O| | |X| | |X|O| | | | |O| |X|X| | |X|O| | | |O|O| |X|X| | |X|O| | 'X' Won! | |O|O| |X|X|X| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| | | |O|O| |X| |X| |O|X| | | |O|O| |X| |X| |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | |X| | |O|X|O| | | | | |O|X| | |O|X|O| | | | | 'X' Won! |O|X| | |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | |O| | |X|X| |X| |O| | | |O| | |X|X| |X| |O| |O| |O| | |X|X| 'X' Won! |X| |O| |O|X|O| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| |X| | | | |X| | | | |O| |X|O| | | |X| | | | |O| |X|O|X| | |X| | | | |O| |X|O|X| | |X|O| | | |O| 'X' Won! |X|O|X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | |O|X| | |X| |O| | | | | |O|X|X| |X| |O| | | | | |O|X|X| |X| |O| | |O| | |O|X|X| |X| |O| |X|O| | |O|X|X| |X| |O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | |X| | | | |O|O| |X| | | |X| |O| | |O|O| |X|X| | |X| |O| | |O|O| 'O' Won! |X|X| | |X| |O| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | |O|O|X| | |X| | | | |X| |O|O|X| | |X| | | |O|X| |O|O|X| | |X| | | |O|X| |O|O|X| |X|X| | | |O|X| |O|O|X| |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X|O| | |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| 'O' Won! |O| | | |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | |X| | |X| |O| |O| |O| | |X| | |X| |O| |O| |O| | |X|X| |X| |O| |O| |O| |O|X|X| |X| |O| |O|X|O| |O|X|X| |X| |O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | |O| | | | |X|X| |X|O| | |O| | | | |X|X| |X|O| | |O| |O| | |X|X| |X|O|X| |O| |O| | |X|X| |X|O|X| |O| |O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O|X| | |O|O| | | | |X| |O|X| | |O|O|X| | | |X| |O|X| | |O|O|X| | |O|X| |O|X| | 'X' Won! |O|O|X| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X| | |X| | | | |X|O| |O|X| | |X| |O| | |X|O| 'X' Won! |O|X| | |X|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| |O| |X| | | | | |X|X|O| |O| |X| | |O| | |X|X|O| |O| |X| | |O|X| |X|X|O| |O| |X| |O|O|X| |X|X|O| |O| |X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | |O| | |X|X| | | |O| | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O|X| |X|X| | 'O' Won! |O|O| | |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X|O|O| |X| | | | | |X| |X|O|O| |X| | | | |O|X| |X|O|O| |X| | | | |O|X| |X|O|O| |X|X| | | |O|X| |X|O|O| |X|X|O| 'X' Won! |X|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X| | | | |X| |O|X| | |O|X|O| | | |X| 'X' Won! |O|X| | |O|X|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O| | | | |X|X| |X|O|O| |O| | | |O|X|X| |X|O|O| |O|X| | |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | |X|O| |X|O| | | | | | |O|X|O| |X|O| | | | | | |O|X|O| |X|O|X| | | | | |O|X|O| |X|O|X| | |O| | |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| | |X| | |O|X| | | |O|X| | |X| | |O|X| | |O|O|X| 'X' Won! |X|X| | |O|X| | |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | | | | | |X|X| 'O' Won! |O|O|O| | | | | | |X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| |X| | |O| | |O|X| | |O| |X| | |O| | |O|X| | |O|X|X| 'O' Won! |O|O| | |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | |O|X| | |X| | |O| | | |X|O|X| | |X| | |O| | | |X|O|X| | |X|O| |O| | | |X|O|X| | |X|O| |O|X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X| |O| |O| | | | |O|X| |X|X|O| |O| | | | |O|X| |X|X|O| |O| |O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | | | |X| | |O|X|O| 'X' Won! |O|X| | | |X| | |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| |O| |X|O| | | |O|X| |X| |O| |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| 'O' Won! | |O| | | |O|X| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | | | |O| | | |X|X|O| | | |X| |O| | | |X|X|O| |O| |X| |O| | | |X|X|O| |O|X|X| |O| | | |X|X|O| |O|X|X| |O|O| | 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| |X| | |O|X| |O| | | |X| |X| |O|O|X| 'X' Won! |O| | | |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |X| |O| |O| |X| | | | | |X| |O| |O| |X| |X| | | |X| |O| |O| |X| |X| | |O|X| |O|X|O| |X| |X| | |O|X| |O|X|O| |X|O|X| | |O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | | |X|X| |X| |O| |O|O| | | |X|X| 'X' Won! |X| |O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| |O| |O|X| | | | |X| |O|X|O| 'O' Won! |O|X| | | |O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | | | | |X| |X| |O|O| | | |X| | |X| |X| |O|O| | |O|X| | |X| |X| |O|O| | |O|X|X| |X|O|X| |O|O| | |O|X|X| 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| |O|X| | | |O|O| | | |X| |O|X| | | |O|O| | |X|X| 'O' Won! |O|X| | |O|O|O| | |X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| |X| |X| |X| | |O|O| |O| |X| 'X' Won! |X|X|X| | |O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | |O| |X| | | | |O|X| | |X|O| |X| | | | |O|X| |O|X|O| |X| | | | |O|X| |O|X|O| |X|X| | | |O|X| |O|X|O| |X|X| | |O|O|X| 'X' Won! |O|X|O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | | | |X|O|O| |X|O| | 'X' Won! |X| | | |X|O|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | |X|O| | |O| | |O| |X| | |X|O| | |O| | |O| |X| |X|X|O| |O|O| | |O| |X| |X|X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | 'X' Won! | | | | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | | | | | |X|X| |O|O| | |O| | | 'X' Won! |X|X|X| |O|O| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | | | |X| |O| |O| |X|X| | |O| |X| |O| |O| |X|X| | 'X' Won! |O| |X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| | | |X| |X|O|O| |X|X|O| 'O' Won! |O| |X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | |X| | |X| |O| | |O| | | |X| | |X|O|O| | |O| | | |X| | |X|O|O| |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| |X|O| | 'O' Won! |O|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| |O| | |O|X| |X| |X| |O| |O| |O|O|X| |X| |X| |O| |O| 'X' Won! |O|O|X| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| | |X|O|X| |X| | | |O|O| | 'O' Won! |X|O|X| |X| | | |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X|O| | | |X| | |O|O| | |X|O| | |X|X| | 'O' Won! |O|O| | |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O|O| |O| | | |X| |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| |O| |X| |X| |X|O|O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | | | |X| | |O|X|O| |O| | | | |X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O|O| | |X|X| | 'X' Won! |O|X|O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | |X| |O|O| | |X|X| | 'O' Won! | | |X| |O|O|O| |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| |X| | |O| | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | |X| |X| |O|O|X| |O|O| | |X| |X| 'X' Won! |O|O|X| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | | |O| |X| |O|X|X| | | | | |O| |X| |O|X|X| | |O| | |O| |X| 'X' Won! |O|X|X| | |O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X|X| | | | |O| |O| | | |X|X| | |X| |O| |O| | | |X|X| | |X| |O| |O| |O| |X|X| | |X|X|O| |O| |O| |X|X| | 'O' Won! |X|X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O|X| | |X| |O| | |X| | |O|X| | |X|O|O| | |X| | |O|X| | |X|O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | |X| |O| | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| | |X| | | |X|O| |X|O|O| | |X| | | |X|O| |X|O|O| | |X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O| | |X| |O| | |X|O| |X|O| | |X| |O| 'O' Won! |O|X|O| |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X|O| | 'X' Won! |X| |O| |X| |O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X| | | | |X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X| |X| |O|X|X| |O| |O| 'O' Won! |X| |X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| |O| |X|O| | | | | | |X|X|O| |X|O| | | | | | |X|X|O| |X|O|O| 'X' Won! |X| | | |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| |O| | | |O|X|X| | | |X| |O| | | |O|X|X| | | |X| |O|O| | |O|X|X| | |X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | 'X' Won! | |O| | |X|X|X| | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O|X| |O| | | | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O|O|X| | |X| | |O|O|X| 'X' Won! |O|O|X| | |X|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| | |O| | |X|O|X| | | |O| | |O|X| |X|O|X| |O| |O| | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X|O| | | | | |X|O|O| |X|X|O| | |O| | |X|O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| | | |X| 'O' Won! |O|O|O| | | |X| | | |X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X|O|X| | |O| | | |X| | |X|O|X| | |O|O| | |X| | |X|O|X| 'X' Won! |X|O|O| | |X| | |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | |O| |X| |X| |X|O|O| | | |O| |X| |X| |X|O|O| |O| |O| 'X' Won! |X|X|X| |X|O|O| |O| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O| | | | |X| | | |O| |X|O| | | | |X| | |X|O| |X|O| | |O| |X| | |X|O| |X|O|X| |O| |X| | |X|O| |X|O|X| |O| |X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| |X|X| | | |O|X| | |O|O| |X|X| | 'X' Won! | |O|X| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O| |X| | | |O| | |X| | |O|O|X| | | |O| | |X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| |X|X|O| | |X|O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | | | | |O|X| | | |O|X| | | | | |O|X|O| | |O|X| 'X' Won! |X| | | |O|X|O| | |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | |X|O| | |O|X|O| | | | | |X|O| | |O|X|O| | | |X| |X|O| | |O|X|O| | |O|X| |X|O| | |O|X|O| |X|O|X| |X|O| | 'O' Won! |O|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | | |X| |X| | | |O|O| | |O| |X| |X| | | |O|O| | |O| |X| |X|X| | |O|O| | |O| |X| |X|X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | |O|O|X| 'X' Won! |X| | | | |X| | |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | |O| |O| | | |X| | |X| | |O| |O| | |X|X| | |X| | 'O' Won! |O|O|O| | |X|X| | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X|O| | |O| | | |X| | |X|X|O| | |O| | | |X| | 'O' Won! |X|X|O| | |O| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | |O| | | | |X| | |O|X|X| |O| | | | |X| | |O|X|X| |O| |O| | |X|X| |O|X|X| |O| |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| |X|O| | | |O|O| |O|X|X| |X|O| | |X|O|O| |O|X|X| |X|O| | It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O|X| | |O|X| | |X|O| | |O|X| 'X' Won! |X|O|X| | |X|O| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X|X| | |O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | |O|X| |O| |O| |X| |X| | |O|X| |O| |O| |X|O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | |X| |X| | | |O| |X|O| | |X| |X| |O| |O| |X|O| | 'X' Won! |X| |X| |O|X|O| |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | |X| | | |O|X| | |O|O| | |X| | | |O|X| | |O|O| | |X|X| |O|O|X| | |O|O| | |X|X| |O|O|X| |X|O|O| | |X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| | |X| | | | |O| 'X' Won! |O| |X| | |X| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | |X| | | | |O| |O| |X| | |X| | |O| |O| |O|X|X| | |X| | |O| |O| 'O' Won! |O|X|X| |O|X| | |O| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | | | |O|X|O| |O|X| | | | |X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| | | | |O|X| | |X|O| |O| | | | |O|X| | |X|O| |O| |X| | |O|X| | |X|O| |O|O|X| | |O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | |O|X| | |X|X| | |O| | | |O|X| |O|X|X| | |O| | | |O|X| |O|X|X| | |O| | |X|O|X| |O|X|X| |O|O| | |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X|O| | |O|X| | |X| | | |X|O| |O|O|X| | |X| | | |X|O| |O|O|X| |X|X| | | |X|O| |O|O|X| |X|X|O| | |X|O| 'X' Won! |O|O|X| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | |X| |X| | |O| | | |O| | |X| |X| | |O|X| | |O| | |X| |X| | |O|X| |O|O| | |X| |X| |X|O|X| |O|O| | |X| |X| 'O' Won! |X|O|X| |O|O| | |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |O| | | | |X|O| | | | | |O|X| | | |X|O| | | | | |O|X|O| | |X|O| | | |X| |O|X|O| 'O' Won! | |X|O| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| |X| | | |O| |X| |O| |O| |X| | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| |O|X|X| | | |O| | |X|O| |O|X|X| | |O|O| | |X|O| |O|X|X| 'X' Won! |X|O|O| | |X|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X|O| | |X| | |O|O| | |X|X|O| 'O' Won! |O|X| | |O|O| | |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | | |O| | | | |O| |X|X| | | |O|X| | | |O| |X|X|O| | |O|X| | | |O| |X|X|O| | |O|X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | |X|O| |X| | | |O| | | | |X|O| |X| | | |O| |O| | |X|O| |X|X| | |O| |O| 'O' Won! | |X|O| |X|X|O| |O| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X|O|O| | | | | | |O|X| |X|O|O| |X| | | |O|O|X| |X|O|O| |X| | | |O|O|X| |X|O|O| |X|X| | 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| |X|O| | | |X| | |O|O|X| |X|O| | |X|X| | |O|O|X| |X|O| | |X|X| | |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X|O| | | |X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O| | |X|O|O| |O|X| | |X|O|X| |X|O|O| |O|X| | It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| | |O|O| | |X| | | |O|X| | |O|O| |X|X| | 'O' Won! | |O|X| |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | |O|O| |X| |X| | | | | | |O|O| |X|O|X| | | | | | |O|O| |X|O|X| |X| | | | |O|O| 'O' Won! |X|O|X| |X|O| | | |O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | | |O|X| | | |X| |O| | | |O|O|X| | | |X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O|O| | 'X' Won! |O|O|X| | |X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X|O| | | |X| | |X|O| | |X|O| | | |X| | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| | | |O|X|X| | | | | |O| |O| |O|X|X| | | | | |O| |O| |O|X|X| |X| | | 'O' Won! |O|O|O| |O|X|X| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | |X| | | |O|X| | | |O|O| |X| | | |O|X| | | |O|O| |X| | | |O|X|X| | |O|O| |X| |O| |O|X|X| |X|O|O| |X| |O| |O|X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | |O| |X| | |X|O| | |O| | |O|X|X| | |X|O| | |O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| |O|O| | 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | |X|X|O| |O|X| | | | |O| |X|X|O| |O|X| | 'X' Won! | |X|O| |X|X|O| |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| |X| |O| | |X| | | |X|O| 'O' Won! |X| |O| | |X|O| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| |X| | | | | |X|X|O| |O|O|X| | |X| | |X|X|O| |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | |X| | |O|X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | |X|O| 'O' Won! |O|O|X| | |O|X| | |X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X|X| |O| | | |X| |O| | |X|X| |O|O| | 'X' Won! |X| |O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | |O| | |O| | |X| |X| | |X|O| | |O| | |X| |X| |O|X|O| | |O| | |X| |X| 'X' Won! |O|X|O| | |O| | |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |X| |X| |O| 'O' Won! |O|O|X| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X|X| |X|O| | |O| |O| | |X|X| |X|O| | |O|X|O| | |X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | | | | | |O|X|O| | |X| | | | |X| |O|X|O| | |X| | |O| |X| |O|X|O| |X|X| | |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X|X| | |O|O|X| | |O|O| |X|X| | |O|O|X| 'X' Won! |X|O|O| |X|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |O| | |O| | |X| |X| |O|X|O| | |O|O| |X| |X| |O|X|O| 'X' Won! | |O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| |O|X| | | |O|X| | |X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| |O| |X| | | |O| | |X|O| |O| |X| | |X|O| | |X|O| |O| |X| | |X|O| |O|X|O| 'X' Won! |O|X|X| | |X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | |O| | | | |O|X| |O|X| | |O| |X| | |O|X| 'O' Won! |O|X| | |O| |X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X| | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | |O|X| | |O| |O| |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X| | | | |O|X| | |O| | |X| |X| | |O|X| | |O| | |X| |X| | |O|X| | |O|O| |X| |X| | |O|X| |X|O|O| 'O' Won! |X|O|X| | |O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | | | | |O|X| | |O|O|X| | | | | |O|X| | |O|O|X| | |X| | 'O' Won! |O|X| | |O|O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X|O| |O|O|X| | |X|X| | |X|O| |O|O|X| |O|X|X| | |X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| |X| |O| | | |O|X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O|X| | |O|X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X|O| |X| | | 'X' Won! |O|X|X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| |X| | | | | |O|X|O| |X| |X| | | |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X| |X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| |O| | | |O| |X| |X| |X| |O| | |O|O| |X| |X| |X| |O| 'X' Won! |X|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | |O| | | | |O| |X| |X| |X|O| | | | |O| |X| |X| |X|O|O| | | |O| |X| |X| 'X' Won! |X|O|O| | | |O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | |O| 'X' Won! | | |O| |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X| | | |O|O|X| | |X|O| |X| |O| |O|O|X| | |X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| | |X|X| |X| |O| |O| |O| | |X|X| |X|O|O| 'X' Won! |O| |O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| |X|O| | | |O| | |X| |O| |X|O| | 'X' Won! |X|O| | |X| |O| |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X|X| |O|O| | | |O| | | |X|X| |O|O| | |X|O| | | |X|X| 'O' Won! |O|O|O| |X|O| | | |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | |X|O| | |O| |X| | |X| | |X|O|O| |O| |X| | |X|X| |X|O|O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | |X|O| | | | | |X|X|O| 'O' Won! | |X|O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | | | | |X| | |X|O|O| | | | | | |X|O| |X|O|O| | |X| | | |X|O| |X|O|O| | |X| | |O|X|O| |X|O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | |O|X| |X|O| | | | | | | |O|X| |X|O|X| | | | | |O|O|X| |X|O|X| | | | | |O|O|X| |X|O|X| | |X| | |O|O|X| |X|O|X| |O|X| | 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | |X| |O| |X| |O| |O| | | |X| |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| | |O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | 'O' Won! | |O|X| | |O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | | |X|O| | |O|X| | |O| | | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| |O|O|X| | |X|O| | |O|X| 'X' Won! |O|O|X| | |X|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | |O| | |X|X| | |X| |O| | |O| | |X|X| | |X| |O| | |O|O| 'X' Won! |X|X| | |X| |O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | |X|X| | | |O|O| |O|X| | 'O' Won! |X|X|O| | |O|O| |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | | |X| |X|O| | |X|O|O| | | |X| |X|O| | 'O' Won! |X|O|O| | |O|X| |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | |X|X| | |O| | |O| | | |O|X|X| | |O| | |O| | | |O|X|X| | |O|X| |O|O| | |O|X|X| | |O|X| 'X' Won! |O|O|X| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| |O| | | |O| |O| |X| |X| |O|X| | |O| |O| |X| |X| |O|X|O| |O| |O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | | | | | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | |O|X| | 'X' Won! |O|X|O| |X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | | | |O|X|X| |X| |O| | | |O| |O|X|X| |X|X|O| | | |O| |O|X|X| 'O' Won! |X|X|O| | |O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| | |X|O| 'O' Won! |O| | | |X|O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| |O|X| | | | | | | |O|X| |O|X|X| | |O| | | |O|X| |O|X|X| |X|O| | | |O|X| |O|X|X| 'O' Won! |X|O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | | |O| | |O| | 'X' Won! |X|X|X| | | |O| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | |X| |X| | |X|O| |O|O| | |X| |X| | |X|O| |O|O|X| |X| |X| |O|X|O| |O|O|X| |X| |X| 'X' Won! |O|X|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | |X| |X| | | 'O' Won! |O|O|O| | | |X| |X| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O|X| |O| |X| | | | | | |O|X| |O|O|X| | | | | 'X' Won! | |O|X| |O|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O|X| |O| | | | | |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| |O| |X| |X| |O|O|X| 'X' Won! |O| |O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | |X|O| |O|X| | | | | | |O|X|O| |O|X| | | | | | |O|X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | |O| 'X' Won! |O|X|O| |O|X|X| | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | | | | | |O|O|X| |O|X| | |X| | | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | |O|O| |X| |X| |O| |X| | |O|O| 'O' Won! |X| |X| |O| |X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O| |O| | |X|X| 'O' Won! |X|O| | |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X| | | | |X|O| |O|O| | |X| | | | |X|O| |O|O| | |X| | | |X|X|O| |O|O| | |X| |O| |X|X|O| |O|O| | |X|X|O| |X|X|O| 'O' Won! |O|O|O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O| | | | |X|X| | |X|O| |O|O| | | |X|X| | |X|O| |O|O|X| | |X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | | | | 'X' Won! | |O|X| | |X|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | | |O|O| | |X| |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X|O|O| 'X' Won! |X|X|X| |O|O| | |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | | |O| | |X| | |X|X|O| | | |O| | |X| | |X|X|O| | |O|O| | |X| | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | | | | |X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| |O| | |X|X| |X|O|O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X| | |O| |O| |X| |X| |O|X| | |O| |O| |X| |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| | | |X|O|X| |O|O| | |X| | | |X|O|X| |O|O| | |X|X| | |X|O|X| 'O' Won! |O|O|O| |X|X| | |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | |X|O|O| | |O| | |X|X| | |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|O| |X| |X| | |X|O| | |O|O| |X| |X| |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| |X| |O| |X|X| | |O|X|O| |X|O|O| |X|X| | 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| |X|O|X| | | |O| | |O|X| 'O' Won! |X|O|X| | |O|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O| |O| |X| | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O| |X| |O| |O| |X|X|O| |O| |X| |O|X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | |X| |O| |O| |X|O|X| | | |X| |O| |O| |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| | |O|O| | |X| | | | |X|X| |O|O| | |X|O| | | |X|X| |O|O| | 'X' Won! |X|O| | |X|X|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | |O|X| |O| | | |X|X| | | |O|X| |O| | | 'O' Won! |X|X|O| | |O|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| 'O' Won! |O| |X| | |O|X| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | |X|O| | | | | |O|O|X| | |X|O| | | | | 'X' Won! |O|O|X| | |X|O| |X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O| | |X|O|X| | |X|O| 'O' Won! |O|O| | |X|O|X| | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | |O| | |O|X|X| |O|X| | | |O| | |O|X|X| |O|X| | | |O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X|X| |O| | | |X| |O| |O|X|X| |O|X| | |X| |O| |O|X|X| |O|X| | |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| |O|O| | | |O| | | |X|X| 'X' Won! |O|O| | | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | | | |X|O| | 'X' Won! |X| |O| |X| | | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| |O| |O| | |X|X| | | |X| |O| |O| 'O' Won! | |X|X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | | |O|X| |O|X| | | | | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X| | |O| |X| |X|O|X| |O|X| | 'O' Won! |O| |X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | |O|X| |O| | | | | |X| | |O|X| |O| | | | | |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| |O| 'X' Won! |X|X|X| |O|O|X| |O| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| |X| | | | |X| | |O|O|X| |X|O| | | |X| | |O|O|X| |X|O|X| |O|X| | |O|O|X| |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | |X| |O| | | | | | |X|O| |X| |O| | | | | |X|X|O| |X| |O| | | | | |X|X|O| |X|O|O| | | | | |X|X|O| |X|O|O| | |X| | 'O' Won! |X|X|O| |X|O|O| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | | | | 'X' Won! |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| | |O| | |O|O| | |X| |X| |X|O| | |O|O| | |X| |X| |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | |X| | | |O|O|X| |X|O| | |X| | | |O|O|X| |X|O| | |X|X| | 'O' Won! |O|O|X| |X|O| | |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| |O| |X| | |X|O| |O| |X| |O| |X| | |X|O| |O|X|X| 'O' Won! |O| |X| |O|X|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X|X| | | | |O| |X| |O| |X|X| | | |O|O| 'X' Won! |X| |O| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| |O| |X| | | | | |O| 'X' Won! |X| |O| |X| | | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| |X| |O| |X| |O|O| | |X| |X| 'X' Won! |O| |X| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O|X| | |O| | | |X| | | |O|X|O| |O| | | |X| |X| |O|X|O| |O| | | |X| |X| |O|X|O| |O|O| | 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | 'X' Won! | |O|X| | |X| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X|O| | | | |X| |O| | | |X|O| | | |X|X| |O| |O| |X|O| | | |X|X| |O| |O| |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X| | |O|X| | | | |O| 'X' Won! |O|X| | |O|X| | | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | | |O|O| | |X| | |X| | | |X|O|O| | |X| | |X|O| | |X|O|O| 'X' Won! |X|X| | |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| 'O' Won! |O| | | |X|O|X| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | 'O' Won! |O| |X| |O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | |O| | | | |X|O| 'X' Won! |O|X| | |O|X| | | |X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | |X|O|X| | | | | |O|O| | |X|O|X| | |X| | |O|O| | |X|O|X| 'O' Won! | |X| | |O|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | | |O| |X| |X| |X| |O| |O| |O| |X| |X| |X| |O| |O| |O|X|X| 'O' Won! |X| |X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | |O| | | | |X| | |O|X| | |O| | |X| |X| | |O|X| |O|O| | |X| |X| | |O|X| 'X' Won! |O|O|X| |X| |X| | |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| |O| | | | |X| | |O|X|X| |O| | | | |X| | 'O' Won! |O|X|X| |O| | | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | 'X' Won! |O| | | | | |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| | | | | | |X| | |O| |X| | |O| | | |X| | |O| |X| | |O|X| | |X| | |O|O|X| | |O|X| | |X| | |O|O|X| |X|O|X| | |X| | 'O' Won! |O|O|X| |X|O|X| | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| | | |O| | | | |X|X| |O| | | |O|X| | | |X|X| |O| |O| |O|X| | | |X|X| |O| |O| |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | | | |O|O| | 'X' Won! |X|X|X| | | | | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X| |O| | |O| | | |X|O| |X| |O| |X|O| | 'O' Won! | |X|O| |X| |O| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | |O|O| | |X| | | |X|X| | |O|O| | |X| | |O|X|X| | |O|O| | |X|X| |O|X|X| | |O|O| 'O' Won! | |X|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | |X| |O| | | |X|O| | | | |X| |O|O| | |X|O| | | |X|X| |O|O| | |X|O| | |O|X|X| |O|O| | |X|O| | |O|X|X| |O|O| | |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | |X|O| | |X| | | | |O| | |X|O| | |X| | | |O|O| | |X|O| | |X|X| | |O|O| |O|X|O| | |X|X| | |O|O| 'X' Won! |O|X|O| |X|X|X| | |O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | | |X| |O|O| | | |X| | | |O|X| |O|O| | | |X|X| | |O|X| 'O' Won! |O|O|O| | |X|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X| | | |O|X|O| | |X| | |X|O| | |O|X|O| | |X| | |X|O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X| | | |O| | 'X' Won! |O| |X| |O|X| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O| |O| |X| |O|X| | |X| |O| |O| |X| |O|X| | |X| |O| |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |X| | |X|O| |O|X|O| | | |X| | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X|O|X| 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| | | |O| |O| |X|X|O| |X| | | |O| |O| 'O' Won! |X|X|O| |X| | | |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| |X|O| | | | | | |O| |X| |X|O| | |O| | | |O| |X| |X|O| | |O| |X| |O| |X| |X|O|O| |O| |X| |O| |X| 'X' Won! |X|O|O| |O|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | 'X' Won! |X| | | |X| |O| |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | |X|O| | | | |X| |O| | | |X|O| | | |O|X| |O| | | |X|O|X| | |O|X| |O| | | |X|O|X| |O|O|X| |O| | | |X|O|X| |O|O|X| |O|X| | It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| 'O' Won! |X|X|O| |O|X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | | | |X| |O|X| | | |O| | | |O|X| |O|X| | | |O| | 'X' Won! | |O|X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O| |X| | |X|X| 'O' Won! |O| | | |O| |X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | |O|X|X| 'X' Won! | |O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | 'X' Won! |X| |O| |X|O|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| |X|X| | |O| | | | |O|X| |X|X| | |O|O| | | |O|X| |X|X| | |O|O|X| | |O|X| |X|X| | |O|O|X| |O|O|X| 'X' Won! |X|X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | |O| | | | |X|O| |X| | | |O|X| | | |X|O| |X| | | |O|X| | |O|X|O| |X| | | |O|X|X| |O|X|O| |X|O| | |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | |O|X| |O| |X| | | | | 'X' Won! | |O|X| |O| |X| | | |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| |O| |X| | |X| | | |O|O| |O| |X| |X|X| | | |O|O| |O| |X| |X|X|O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | |O|X| |O| |X| | |X| | | |O|X| |O| |X| | |X| | |O|O|X| |O| |X| 'X' Won! | |X|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | | |X| | | |O| |O|X|X| | |O|X| | | |O| |O|X|X| |X|O|X| | | |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | | |O| | |X|O| |O|X| | | | |O| | |X|O| |O|X| | | | |O| |X|X|O| 'O' Won! |O|X|O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | |X|O| | | | |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O| | 'O' Won! |X|O|X| |O|O| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |X|X| | | |X| | |O|O| | |X|X| |O| |X| 'X' Won! | |O|O| |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X|X| | | |X| |O| | | |O|X|X| | |O|X| 'X' Won! |O| |X| |O|X|X| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|X| | | | | 'O' Won! |O|X|X| | |O|X| | | |O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | |X|O| | | |O| |X| | | | |X|O| | |O|O| |X| | | | |X|O| |X|O|O| 'O' Won! |X| |O| | |X|O| |X|O|O| 1.1 Clean Data \u00b6 We will first need to organize the data into a parsable format. Q1 \u00b6 What is the object data and what does it contain? what are the keys of data? what are the keys of each game? # inspect data below by grabbing the first key in data # what are the three different keys within each game? data [ 'game 0' ] {'board': {1: 'O', 2: 'X', 3: 'X', 4: ' ', 5: 'X', 6: ' ', 7: 'O', 8: 'O', 9: 'O'}, 'winner': 'O', 'starting player': 'O'} Q2 \u00b6 Using those keys, iterate through every game in data and append the board, the winner, and the starting player to separate lists. Call these lists: boards, winners, and starters boards = [] winners = [] starters = [] for game in data : boards . append ( data [ game ][ 'board' ]) winners . append ( data [ game ][ 'winner' ]) starters . append ( data [ game ][ 'starting player' ]) Q3 \u00b6 Make a dataframe out of the list boards and call it df . Make a series out of the list winners . Make a series out of the list starters . Make a new column of df called \"Winner\" and set it equal to the pandas Series of the winners. Make a new column of df called \"Starter\" and set it equal to the pandas Series of the starters. # YOUR CODE HERE df = pd . DataFrame ( boards ) df [ \"Winner\" ] = pd . Series ( winners ) df [ \"Starter\" ] = pd . Series ( starters ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 Winner Starter 0 O X X X O O O O O 1 O X O X X X X O O X X 2 O X X O O O O 3 X O X X X O O O O O O 4 X X X O O O X O 1.2 Inferential Analysis \u00b6 We're going to use Bayes Rule or Bayesian Inference to make a probability of winning based on positions of the board. The formula is: $ P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)}$ Where $\\cap$ is the intersection of $A$ and $B$. The example we will use is the following: what is the probability of 'O' being the winner, given that they've played the center piece. $B$ = 'O' played the center piece $A$ = 'O' won the game So what is probability? We will define it in terms of frequencies. So if we are for instance asking what is the probability of player 'O' being in the center piece, it would be defined as: $ P(B) = \\frac{|O_c|} {|O_c| + |X_c| + |empty|}$ Where the pipes, | | , or cardinality represent the count of the indicated observation or set. In this case $O_c$ (O being in the center) and $X_c$ (X being in the center). Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc_Xc_empty 1000 ( df [ 5 ] == 'O' ) . value_counts () False 550 True 450 Name: 5, dtype: int64 # example of assessing the probability of B, O playing the center piece player = 'O' Oc = ( df [ 5 ] == player ) . value_counts () Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc / Oc_Xc_empty False 0.55 True 0.45 Name: 5, dtype: float64 # we can also clean this up and replace the denominator with the whole # observation space (which is just the total number of games, df.shape[0]). # example of assesing probabiliy of A ( df [ 'Winner' ] == 'O' ) . value_counts () / df . shape [ 0 ] False 0.571 True 0.429 Name: Winner, dtype: float64 The $P(B|A) * P(A)$ is the intersection of $B$ and $A$. The intersection is defined as the two events occuring together. Continuing with the example, the probablity of 'O' playing the center piece AND 'O' being the winner is the number of times these observations occured together divided by the whole observation space : # in this view, the total times A and B occured together is 247 player = 'O' df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] 253 # the total observation space is 1000 (1000 games) df . shape [ 0 ] 1000 And so we get: $P(B|A) * P(A) = \\frac{247} {1000} = 0.247 $ In code: df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] / df . shape [ 0 ] 0.253 1.2.1 Behavioral Analysis of the Winner \u00b6 Q4 \u00b6 define the 3 different board piece types and label them middle , side , and corner . Middle should be an int and the other two should be lists. # define the 3 different board piece types middle = 5 side = [ 2 , 4 , 6 , 8 ] corner = [ 1 , 3 , 7 , 9 ] 1.2.1.1 What is the probability of winning after playing the middle piece? \u00b6 Q5 \u00b6 # A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'X' # define the intersection of A AND B, A_B A_B = df . loc [( df [ 'Winner' ] == player ) & ( df [ middle ] == player )] . shape [ 0 ] / \\ df . shape [ 0 ] # define prob B B = ( df [ middle ] == player ) . sum () / \\ df . shape [ 0 ] # return A_B over B (The prob B given A) A_B / B 0.5807962529274004 Q6 \u00b6 # A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'O' # define the intersection of A AND B, A_B A_B = df . loc [( df [ 'Winner' ] == player ) & ( df [ middle ] == player )] . shape [ 0 ] / \\ df . shape [ 0 ] # define prob B B = ( df [ middle ] == player ) . sum () / \\ df . shape [ 0 ] # return A_B over B (The prob B given A) A_B / B 0.5622222222222222 1.2.1.2 What is the probability of winning after playing a side piece? \u00b6 Q7 \u00b6 # A intersect B: O played side and O won / tot games # B: O played side / tot games player = 'O' A_B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.4252136752136752 # A intersect B: X played side and X won / tot games # B: X played side / tot games player = 'X' A_B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.41458106637649617 1.2.1.3 What is the probability of winning after playing a corner piece? \u00b6 Q8 \u00b6 # A intersect B: O played corner and O won / tot games # B: O played corner / tot games player = 'O' A_B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.44235033259423506 Q9 \u00b6 # A intersect B: X played corner and X won / tot games # B: X played corner / tot games player = 'X' A_B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.4375678610206298 Are these results surprising to you? Why? This resource may be illustrative. 1.3 Improving the Analysis \u00b6 In this analysis, we only tracked what moves were made, not the order they were made in. It really limited our assessment! How might we change our recording of the games to track order of moves as well? Do we need to track all the moves or just the first and the winner?","title":"SOLN P1 Statistical Analysis of TicTacToe"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#data-science-foundations-project-part-1-statistical-analysis","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to perform statistical analysis on data generated from our tictactoe program!","title":"Data Science Foundations, Project Part 1: Statistical Analysis"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#10-preparing-environment-and-importing-data","text":"back to top","title":"1.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#101-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"1.0.1 Import Packages"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#102-load-dataset","text":"back to top data = {} for i in range ( 1000 ): game = GameEngine () game . setup_game () board = game . play_game () data [ 'game {} ' . format ( i )] = { 'board' : board . board , 'winner' : board . winner , 'starting player' : board . start_player } | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | | |X|X| | | | | |O|O| | |O|X|X| | | | | |O|O| | |O|X|X| | |X| | |O|O| | 'O' Won! |O|X|X| | |X| | |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X|X| | |O| | | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |O| | | | |X|X| |O| | | |O| | | 'O' Won! |O|X|X| |O| | | |O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |O| | |O| | |X| |X| | | |O| | |O| | |X| |X| | | |O| |O|O| | |X| |X| | |X|O| |O|O| | |X|O|X| | |X|O| |O|O| | |X|O|X| |X|X|O| |O|O| | 'O' Won! |X|O|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | | |O| |X| |X| | | |O| |O| |O| 'X' Won! |X|X|X| | | |O| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| |X|O| | | | |O| |O|X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X|O| | |X|O|O| | | | | |X|O|X| |X|O|O| | | |O| |X|O|X| 'X' Won! |X|O|O| | |X|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X|X| |O|O| | |O| | | | |X|X| 'X' Won! |O|O| | |O| | | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| | | | |O|X| |X| |O| |X| | | | |O|X| |X| |O| |X| | | |O|O|X| 'X' Won! |X| |O| |X|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X|O|X| | |O| | | |X| | |X|O|X| |O|O| | | |X| | |X|O|X| |O|O| | |X|X| | |X|O|X| 'O' Won! |O|O|O| |X|X| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| | | | | | | |X|O|O| |X| |X| | | | | |X|O|O| |X|O|X| | |X| | |X|O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O|O|X| | | |X| | |X|O| |O|O|X| |X| |X| | |X|O| |O|O|X| |X| |X| |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| | |O| | |X|X| | |X|O|O| |O|O| | |X|X| | |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | |X| | | |O|X|O| | | |O| |X| | | |O|X|O| |X| |O| |X| | | |O|X|O| 'O' Won! |X| |O| |X| |O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | |X|O| | |X| | |X|O| | |O|X|O| | |X| | |X|O|X| |O|X|O| 'O' Won! | |X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| |O| |X| | | | | |X| |O| |O|X|X| | | | | |X| |O| |O|X|X| |O| | | |X| |O| 'X' Won! |O|X|X| |O|X| | |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X|X| | |X| |O| 'O' Won! | | |O| |X|X|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| |X|O| | | | | | | |X|O| |X|O| | | | |O| | |X|O| |X|O|X| | | |O| | |X|O| |X|O|X| | |O|O| | |X|O| |X|O|X| |X|O|O| 'O' Won! |O|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O| |O| |X|X| | |X|O|O| |O| |O| |X|X| | |X|O|O| |O|X|O| |X|X| | 'O' Won! |X|O|O| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | |X| | |O|X|O| | |O| | | |X| | |O|X|O| | |O| | |X|X| | |O|X|O| | |O| | |X|X|O| |O|X|O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | |O| | |X| |X| 'X' Won! |O| |O| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X|X| | |O| |O| | | | | |X|X| | |O| |O| | | |O| |X|X| | |O| |O| |X| |O| 'O' Won! |X|X|O| |O| |O| |X| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X|O| | | | | | |O|O|X| |X|O|X| | | | | |O|O|X| |X|O|X| |O| | | |O|O|X| |X|O|X| |O|X| | 'O' Won! |O|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | |O| | | |X| | |O|X| |X| |O| | | |X| |O|O|X| |X| |O| | | |X| |O|O|X| |X| |O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'X' Won! |O|O|X| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| | | |X| | |O| |O| |X|O| | | |X| | |O| |O| |X|O| | |X|X| | |O| |O| |X|O|O| |X|X| | 'X' Won! |O| |O| |X|O|O| |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | | | | | | |X|X| |O|O| | | | |X| | |X|X| |O|O| | | |O|X| 'X' Won! |X|X|X| |O|O| | | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| | | |O| | | |X| |X|O|O| |X| |O| | |O|X| |X|O|O| |X| |O| 'X' Won! |X|O|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | |X|O| | | | | | |X|O| |O|X|O| | | | | 'X' Won! | |X|O| |O|X|O| | |X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X|O| | | |O|X| | | |O| |X|O| | | |O|X| | |X|O| |X|O| | |O|O|X| | |X|O| |X|O| | |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| |X|O| | |X|X|O| | |O|O| |X|O| | |X|X|O| | |O|O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X|X| | |O|O| | | | |O| |X|X| | |O|O| | |X| |O| |X|X| | 'O' Won! |O|O|O| |X| |O| |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | |O|O| | | |X| |X| | | |X|O|O| | | |X| |X| | | |X|O|O| | |O|X| 'X' Won! |X| | | |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O| | 'O' Won! |X|X| | |O|O|O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | |O| |X| |O|X|O| | | | | |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |O|X| 'X' Won! |O| |X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X|X| |X| | | | | |O| |O|X|X| |X| | | | |O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X| |O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X| |O| | |O|X| |O| | | |X| |O| |X|O|X| |O| | | |X|O|O| |X|O|X| |O| |X| |X|O|O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | | |X| | |X| |O| | |O| | |O|X| | |X| |O| | |O| | |O|X|X| |X| |O| | |O| | |O|X|X| |X|O|O| 'X' Won! | |O|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | 'O' Won! |X| |X| | | | | |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| | | | | |X|O|X| |O| |O| | | | | |X|O|X| |O| |O| | | |X| |X|O|X| |O| |O| | |O|X| |X|O|X| |O| |O| |X|O|X| 'O' Won! |X|O|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X|X| | |O| | |O| |O| | |X|X| 'X' Won! | |O| | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | |O|O| | | |X|X| | |O| | |O|O| | | |X|X| |X|O| | |O|O| | | |X|X| |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | | | | |X|O| | |X|O| |X| | | | |X|O| | |X|O| |X| | | |O|X|O| | |X|O| |X| |X| |O|X|O| 'O' Won! | |X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | |X|O| | | |X| |X| |O| |O|X|O| | |X|X| |X| |O| |O|X|O| | |X|X| |X|O|O| |O|X|O| 'X' Won! |X|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | |O|X| | | |O|X| | |X| | |O|X| | | |O|X| | |X| | |O|X|O| | |O|X| | |X|X| |O|X|O| | |O|X| |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| |O| |O| |X|X|O| |X| |X| |O| |O| 'O' Won! |X|X|O| |X| |X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| |O| | | |X| | | |O| |X|X|O| | | |X| | | |O| |X|X|O| | | |X| |O| |O| |X|X|O| | |X|X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'X' Won! |X|X|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X|X| |X|O| | |O|O| | | |X|X| |X|O| | |O|O| | | |X|X| |X|O|X| 'O' Won! |O|O|O| | |X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| | | |X| |O|O| | | |X|O| | |X|X| |O|O| | | |X|O| 'O' Won! |O|X|X| |O|O| | | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O| | 'X' Won! |X|O|X| | |X|O| |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O| |O| | |X| | | | |X| 'O' Won! |O|O|O| | |X| | | | |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |O| | | |O|X|X| | | | | |O| | | |O|X|X| | | | | |O| |O| |O|X|X| | |X| | |O| |O| 'O' Won! |O|X|X| |O|X| | |O| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X|O| | |X| | |O| | | |O|X|O| |X|X| | |O|O| | |O|X|O| |X|X| | 'X' Won! |O|O|X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X|O|X| | | |X| |O| |O| |X|O|X| | |O|X| |O| |O| |X|O|X| |X|O|X| |O| |O| |X|O|X| 'O' Won! |X|O|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| | | |X|X|O| |O|O|X| |X| |O| |X|X|O| 'X' Won! |O|O|X| |X|X|O| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O|X| | | |X| |O|X|O| | |O|X| 'X' Won! | | |X| |O|X|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | |X|O| | | |X| |O|O| | | |X|O| | | |X| |O|O|X| | |X|O| | | |X| |O|O|X| |O|X|O| | | |X| |O|O|X| |O|X|O| | |X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | |X| | | |O|X|O| |X| | | |X| |O| |O|X|O| |X| | | |X|X|O| |O|X|O| |X| | | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | |O|O|X| | | |X| |X| | | |O|O|X| |O| |X| |X| | | |O|O|X| |O|X|X| |X| | | |O|O|X| |O|X|X| |X| |O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | | |O| | | |O| |X|O|X| | |X|O| | | |O| |X|O|X| | |X|O| |O| |O| |X|O|X| |X|X|O| |O| |O| 'O' Won! |X|O|X| |X|X|O| |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O| |O| |O|X|X| | |X| | |O| |O| |O|X|X| | |X|O| |O| |O| 'X' Won! |O|X|X| | |X|O| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | |O|X| |X|O| | | | |X| | |O|X| |X|O| | | | |X| | |O|X| |X|O|O| |X| |X| | |O|X| |X|O|O| 'O' Won! |X|O|X| | |O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | |O| |X| |X| | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| | |O|O| 'X' Won! |X| |O| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | |X|O| | | | |O| |X| | | |X|O| | | | |O| |X| |O| |X|O| | | | |O| |X|X|O| |X|O| | | |O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X| | | 'O' Won! | |X| | |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X| |O| |X| | | |O| |O| |X| |O| |X| | | |O|X|O| |X|O|O| |X| | | |O|X|O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | |O| |X| | |O| | |O|X| | |O| |X| | |O|X| |O|X| | 'O' Won! |O| |X| |O|O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| |O| | | |X| | |O|O| |X|X|O| | | |X| | |O|O| 'O' Won! |X|X|O| | | |X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | | | |X| |X| |X|O|O| | |O| | |X| |X| 'X' Won! |X|O|O| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | | |O| | | 'O' Won! |X| |O| |X|O| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | |O| | |O|X|X| | |X| | | |O| | |O|X|X| | |X| | | |O|O| |O|X|X| | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O|X| |O| |X| | |O| | | |O|X| 'X' Won! |O| |X| | |O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | |X|X| | |O| |X| |O| | | |X|X| | |O|O|X| 'X' Won! |O| | | |X|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | | | | |X|O| 'X' Won! | |X|O| | |X| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | |O| |X| |O|X|O| | | | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | |O| | |X| | |O| | | | |X|O| | |X| | |O| | | | |X|O| |O|X| | |O| | | | |X|O| |O|X|X| |O| | | 'O' Won! |O|X|O| |O|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| |X|X| | | |O|O| | | |X| 'O' Won! |X|X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O| |O| |X|O| | |X|X| | |O| |O| |X|O|O| |X|X| | |O| |O| |X|O|O| |X|X| | |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | | |X| | |O|X|O| | | | | |X|X| | |O|X|O| | | | | |X|X| | |O|X|O| | |O| | 'X' Won! |X|X|X| |O|X|O| | |O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | | | |X|X| | 'O' Won! |O|O|O| | | | | |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| |X| |X| | | | | |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| | |X| |O| |O|X| | |X|O| | 'X' Won! |X| |O| |O|X| | |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O| |X| | | |O| |O|X| | |O| |X| | | |O| |O|X|X| |O| |X| | | |O| 'O' Won! |O|X|X| |O| |X| |O| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| |X| |X| | | | | 'O' Won! |O|O|O| |X| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X| | |X| | | |X|O|O| |O|X| | |X| | | |X|O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X| |O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | | |X| |O|O| | |X| | | | |X|X| |O|O| | |X| | | | |X|X| |O|O| | |X|O| | 'X' Won! |X|X|X| |O|O| | |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | |X| | 'O' Won! |O|O|O| | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| | |O|O| |X| |O| |X| |X| | |O|O| |X| |O| |X|O|X| | |O|O| 'X' Won! |X| |O| |X|O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | |X|X| | |O| | |O| | | | |X|X| |O|O| | |O| | | | |X|X| |O|O|X| |O|O| | | |X|X| |O|O|X| 'X' Won! |O|O| | |X|X|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | |X| | |O| |O| | |O|X| | |X| | |O| |O| | |O|X| | |X|X| 'O' Won! |O|O|O| | |O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| | |X| | |X|O|O| | | |O| | |X|X| |X|O|O| | | |O| |O|X|X| |X|O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X| |O|X|O| | | |X| | | |X| |O|X|O| | | |X| | |O|X| |O|X|O| | | |X| |X|O|X| |O|X|O| |O| |X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | |O|O| | | | | |X|O|X| | |O|O| | |X| | |X|O|X| 'O' Won! |O|O|O| | |X| | |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| | | |O| | | |X|X|O| |O| |X| |O| | | |X|X|O| |O| |X| |O| |O| |X|X|O| |O|X|X| |O| |O| 'O' Won! |X|X|O| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| |O| |X| |X|X| | 'X' Won! |O| |O| |O| |X| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | | | | |O|X| | | |O|X| | | |O| |O|X| | | |O|X| |X| |O| |O|X| | | |O|X| |X| |O| |O|X|O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | |O|X| | |O| | |X| | | |X|O|X| | |O| | |X| | | |X|O|X| | |O| | |X| |O| 'X' Won! |X|O|X| |X|O| | |X| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X|O| |X|O| | | |X| | | |X|O| |X|O| | |O|X| | | |X|O| |X|O| | |O|X| | | |X|O| |X|O|X| |O|X| | |O|X|O| |X|O|X| 'X' Won! |O|X|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | |O| | |X| | |O| |X| | | |O| | |X|X| |O| |X| |O| |O| | |X|X| 'X' Won! |O| |X| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | |X| |O|O| | | |X| | | |O|X| |O|O| | |X|X| | | |O|X| |O|O| | 'O' Won! |X|X|O| | |O|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | |O| |O| | | | |X|X| | | |O| |O|X| | | |X|X| | |O|O| |O|X| | | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |O| | |O| | |X|X|O| | |X|O| | |O| | |X|X|O| |O|X|O| | |O| | |X|X|O| |O|X|O| |X|O| | 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | 'O' Won! |X| | | | | |X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O| |X| | | |X| |O|O| | |O| |X| | | |X| 'X' Won! |O|O|X| |O| |X| | | |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | |O| |X|X|O| | | | | | |X|O| |X|X|O| 'O' Won! | | |O| | |X|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |X| | |X| | |O|X|O| |O| |X| 'X' Won! | |X| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | |O| |X| |O| |X|O| | | | |O| |X|X|O| |X|O| | 'O' Won! | | |O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X|X| | |O| |O| | |O| | |X|X| | |O| |O| | |O|X| |X|X| | |O| |O| |O|O|X| |X|X| | |O|X|O| |O|O|X| |X|X| | 'O' Won! |O|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X| | | | |X|O| |O| |O| |X| |X| | |X|O| |O| |O| |X| |X| |O|X|O| |O| |O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | |X| | | |O|O| |X|X| | |O|X| | 'X' Won! | |O|O| |X|X|X| |O|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| |X|O| | | | | | | |X|O| |X|O| | | | | | |O|X|O| |X|O|X| | | | | |O|X|O| |X|O|X| |O| | | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | | | | |X|X| | |O| |O| |O| | | |X|X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O| | | |X|X|O| 'X' Won! |O|X|O| |O|X| | |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | |O| | | |O|O|X| | |X| | |O|X| | |O|O|X| | |X| | |O|X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| |X|X| | 'O' Won! |O|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| |X| | | | | |X|O|O| |X| |X| |O| | | |X|O|O| 'X' Won! |X|X|X| |O| | | |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | |X|X| | | |O| | |O|O| | |X|X| | | |O| | |O|O| | |X|X| | |X|O| | |O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| |O|X|O| It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | |X| | | | |O|O| |X|X|O| |X| | | | |O|O| |X|X|O| |X| |X| 'O' Won! |O|O|O| |X|X|O| |X| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|O| |X|O| | |O| |X| | |X|O| |X|O|O| |O| |X| | |X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | |O|O| |X| | | |X| |O| | |O|O| |X| |X| |X| |O| | |O|O| |X| |X| |X|O|O| 'X' Won! |X|O|O| |X| |X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | 'X' Won! | | |X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X|O| | | | | |X|X|O| | |X|O| | | | | |X|X|O| |O|X|O| | | |X| |X|X|O| |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | |X|X|O| | |O|O| | |X| | |X|X|O| | |O|O| |X|X| | |X|X|O| 'O' Won! |O|O|O| |X|X| | |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X|X| | | | | | |O| |O| |X|X| | | | |O| |O| |O| |X|X| | | |X|O| 'O' Won! |O| |O| |X|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | |O| |O| | | |X| | |X| | |O| |O| | | |X| | |X|O| |O| |O| | |X|X| | |X|O| |O| |O| | |X|X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| | | |O|O|X| |O| | | |X|X| | |O|O|X| |O| | | |X|X|O| |O|O|X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X|X| | |O| | | | |X| |O|X|X| |O|O| | | | |X| |O|X|X| |O|O| | |X| |X| 'O' Won! |O|X|X| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X| | | |X| | | |O|O| |O|X| | |X|X| | | |O|O| |O|X| | 'O' Won! |X|X| | |O|O|O| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O|X| |X| | | | |O| | | |O|X| |X| | | | |O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X| | | |O|O| | |X|O|X| |X| |X| 'O' Won! |O|O| | |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | |O| | |O| |X| | | | | |X|O| | |O| |X| |O| | | |X|O| | |O| |X| |O|X| | |X|O| | |O| |X| 'O' Won! |O|X|O| |X|O| | |O| |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | |O|X| | |X| |O| | | | | |O|X|X| 'O' Won! |X| |O| | |O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O|X| | |X| | | |O| | |O|O|X| | |X| | |X|O| | |O|O|X| | |X| | |X|O|O| |O|O|X| | |X|X| |X|O|O| |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | |X|X| | | | | |O|O|X| |O|X|X| | | | | |O|O|X| |O|X|X| | |X| | |O|O|X| |O|X|X| | |X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| | | |O| | | |X| |X|O|O| | |X|O| | | |X| |X|O|O| |O|X|O| |X| |X| |X|O|O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| |O| |O| | |X| | |X|O|X| |O| |O| 'O' Won! | |X|O| |X|O|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | |X| | |O| | | |O| |X| | |X|O| |O| | | |O| |X| | |X|O| |O| |X| |O| |X| 'O' Won! | |X|O| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X| | | |O| |O| |X| |O| |X| |X| |O| |O| |X| |O| |X|O|X| |O| |O| |X| |O| |X|O|X| |O| |O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | |X| | | | | | |O|X|O| | |X| | | |O| | |O|X|O| | |X| | |X|O| | |O|X|O| 'O' Won! |O|X| | |X|O| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| |O|O| | |X| | | |X| |O| |O|O|X| |X|O| | |X| |O| |O|O|X| 'X' Won! |X|O| | |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| |O|O| | | | | | |X| |X| 'X' Won! |O|O| | | | | | |X|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| |X|O| | |O| | | |X|O|X| |X|O| | |O| |X| |X|O|X| |X|O| | 'O' Won! |O| |X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X| | |O| | | |O|O| 'X' Won! |X|X|X| | |O| | | |O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | | |O|O| |X|X| | | |X| | | |O|O| |X|X| | | |X|O| | |O|O| |X|X| | |X|X|O| | |O|O| |X|X| | 'O' Won! |X|X|O| |O|O|O| |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | |O| | |X| |X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | |X| | |O|O| | |X|X| | |O|X| | |O|O| | |X|X| | |O|X|X| |O|O| | |X|X|O| |O|X|X| |O|O| | 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | |O| | |X|O| | |X|X| | | |O| | |X|O| | |X|X| | | |O|O| 'X' Won! |X|O| | |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O|X| | | | | | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | 'O' Won! | |X|O| |X|O|O| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | |O|X| | | | |X| | |O|X| |O|X| | | | |X| | |O|X| |O|X| | |O| |X| 'X' Won! | |O|X| |O|X|X| |O| |X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |O| | | | |X|O| | | | | |O|X| | |O|X|O| | | | | |O|X| | |O|X|O| | | |X| |O|X| | 'O' Won! |O|X|O| |O| |X| |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X| | | 'X' Won! |X|O|O| |X| | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | |O| | |O| | | |X|X| | | |O| | |O| |X| |X|X| | |O|O| | |O| |X| |X|X| | |O|O|X| |O| |X| |X|X| | |O|O|X| |O|O|X| 'X' Won! |X|X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | |X| | | | |O| |X|O|O| | |X| | | | |O| |X|O|O| |X|X| | 'O' Won! | | |O| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X| |O| | | |O| | |O|X| |X| |O| | | |O| |X|O|X| |X| |O| | |O|O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | |O|O| | | |X| | |X|X| | |O|O| | | |X| | |X|X|O| |O|O| | | |X| | |X|X|O| |O|O| | |X|X| | 'O' Won! |X|X|O| |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | |O| | |X| | | |O|X|X| | |O| | |X| | | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|X| |X| | | | |X|O| | |O|X| |X|O| | | |X|O| | |O|X| |X|O|X| | |X|O| | |O|X| |X|O|X| |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| 'O' Won! |O| | | |O| |X| |O| |X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | |X|O|X| | | | | |O|O| | |X|O|X| | | |X| 'O' Won! |O|O| | |X|O|X| | |O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X|O| | | | | | |O|O|X| |X|O| | | |X| | |O|O|X| |X|O| | |O|X| | |O|O|X| |X|O| | |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| | | |X| |X|X|O| |O| |O| | |X|X| |X|X|O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | |X| | | | |X| |O|O| | | |X| | | |O|X| |O|O| | | |X| | |X|O|X| |O|O| | |O|X| | |X|O|X| |O|O| | |O|X|X| |X|O|X| |O|O| | 'O' Won! |O|X|X| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O|X|O| |O| | | |X| |X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | | |O|X| | |X| | |X|O| | | |O|X| |O|X| | |X|O| | | |O|X| |O|X|X| |X|O| | | |O|X| |O|X|X| |X|O|O| | |O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | |O| | | |X|X| | |O|O| | |O|X| 'O' Won! | |X|X| |O|O|O| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X|O| |X| |X| |O|X| | |O|X|O| |X|O|X| |O|X| | |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| |X| |O|X| | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X|X| |O|O| | |X|O|X| |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| |X| |X| | | |O| |O| |O| |X| 'X' Won! |X| | | |O|X|O| |O| |X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | |O| |O| |X| | |X|X| | | |O| |O| |X| | |X|X| |O| |O| |O| |X| | |X|X| |O|X|O| |O|O|X| | |X|X| |O|X|O| 'X' Won! |O|O|X| |X|X|X| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| |O| |X|O| | |O| | | |X|X|O| |X|O| | |O| | | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| |O| |O| |X| | | |X|O|X| |O| |O| |X| | | |X|O|X| |O| |O| |X|O| | 'X' Won! |X|O|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X|X| | | | | |O|O| | | |X|X| | | |X| |O|O| | | |X|X| | | |X| |O|O| | |O|X|X| | |X|X| |O|O| | |O|X|X| 'O' Won! |O|X|X| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X|O| | |X| | | | |O| |X|X|O| | |X| | | | |O| |X|X|O| |O|X| | |X| |O| |X|X|O| |O|X| | |X|O|O| |X|X|O| |O|X| | 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | |X| |X| | |O| | |O|O| | |X| |X| | |O| | |O|O|X| |X| |X| | |O|O| |O|O|X| |X| |X| 'X' Won! | |O|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| | |O| |X| | |O|O| | |X| | |O| |X| |X|O|O| | |X| | |O|O|X| |X|O|O| |X|X| | |O|O|X| |X|O|O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| | |X| | |O| |O| |X|O|X| | |X| | |O| |O| |X|O|X| |X|X| | 'O' Won! |O|O|O| |X|O|X| |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | |O| | | |X|O| | |O|X| | |O|X| | |X|O| | |O|X|O| |O|X| | |X|O| | |O|X|O| |O|X| | |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O| | | |X|X| |X| |O| |O|O| | | |X|X| |X|O|O| 'X' Won! |O|O|X| | |X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| | | |X| |X| |O| | |O|X| | |O|X| |X| |O| | |O|X| |X|O|X| |X| |O| 'O' Won! |O|O|X| |X|O|X| |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O|X| |X| | | |O| | | |O|O|X| |X| | | |O| | | |O|O|X| |X| | | |O| |X| |O|O|X| |X|O| | |O| |X| |O|O|X| |X|O| | |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | |O| 'X' Won! | |O| | |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | |X|O| | | |O| | | |X| |O|X|O| | | |O| | | |X| |O|X|O| | | |O| | |X|X| |O|X|O| | |O|O| | |X|X| 'X' Won! |O|X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | | |X| | |O| | | |O|X|O| | |X| | 'X' Won! |O|X| | |O|X|O| | |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X|X| | | | | | |O| | |O|X|X| |O| | | | |O| | |O|X|X| |O| | | |X|O| | |O|X|X| |O| | | |X|O|O| 'X' Won! |O|X|X| |O|X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O|O| | |X|X|O| | | |X| |O|O| | 'O' Won! |X|X|O| | | |X| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | 'O' Won! |X| |O| | |O|X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O|X| |X| | | | | |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O| | |X| |X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | |X|X| | |O| |X| |O| |O| |X|X| | |O| |X| 'X' Won! |O| |O| |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O|X| | |X| | |X| | | |O|O|X| | |X|O| |X| | | |O|O|X| |X|X|O| |X|O| | |O|O|X| |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O|O| |X| |X| | | | | |X|O|O| |X| |X| | |O| | |X|O|O| 'X' Won! |X|X|X| | |O| | |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| |X| | |X|O| | | | | |O|O|X| | |X|O| | | | | |O|O|X| |X|X|O| 'O' Won! |O| | | |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O| | |O|X|X| |O|X| | | |O| | |O|X|X| |O|X| | | |O|X| |O|X|X| 'O' Won! |O|X| | |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |X| |O| |O| | | |X| | | |X| |O| |O| | |X|X| | | |X| |O| |O| | |X|X| | |O|X| |O|X|O| | |X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | |X| | |X|O|X| | | |O| | |X|O| |X|O|X| | | |O| | |X|O| |X|O|X| | |X|O| 'O' Won! |O|X|O| |X|O|X| | |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | |O|X| | |O|X|X| | | | | |O|X| | |O|X|X| | | |O| |O|X| | 'X' Won! |O|X|X| | |X|O| |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |X| | |O| |O| |X| | | | |X| | |O| |O| |X|X| | | |X| | 'O' Won! |O|O|O| |X|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O| | |O| |X| |X| | | | |O|X| |O| |X| |X| | | | |O|X| |O|O|X| |X|X| | | |O|X| |O|O|X| 'O' Won! |X|X|O| | |O|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X|X| | | | | | | | |O| |X|X|O| | | | | | | |O| |X|X|O| | | | | |X| |O| |X|X|O| |O| | | |X| |O| |X|X|O| |O| | | |X|X|O| 'O' Won! |X|X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | | | | |X|O| | |X|O|O| | | | | |X|O|X| |X|O|O| | | | | |X|O|X| |X|O|O| |O| | | |X|O|X| |X|O|O| |O|X| | It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | | |O| | |O|X| |X|X| | | | |O| | |O|X| |X|X| | | |O|O| 'X' Won! | |O|X| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X| |X| | |O|O| |X|O|O| |X| |X| 'X' Won! | |O|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | |X| | | | |X|O| |O| | | |X| | | | |X|O| |O| |X| |X| | | | |X|O| |O| |X| |X| | | |O|X|O| |O|X|X| |X| | | |O|X|O| 'O' Won! |O|X|X| |X|O| | |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | |X|O| | | | | |X|O| | | |X|O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| |O|X|O| | | | | 'X' Won! |X|O|X| |O|X|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |O| | | |O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X| |X| |O| |O| 'X' Won! |X|O|O| |X|X|X| |O| |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X|O| | | | | |O|X| | | |X|O| | | |X| |O|X| | |O|X|O| | | |X| |O|X| | 'X' Won! |O|X|O| | |X|X| |O|X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O|X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| |O| | | | | | | |X|X|O| |O| | | | |O| | |X|X|O| |O| | | | |O| | |X|X|O| |O|X| | | |O| | |X|X|O| |O|X|O| |X|O| | |X|X|O| |O|X|O| 'O' Won! |X|O|O| |X|X|O| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O|X| | |O| | | | | | |X|O|X| |O|O| | | | | | |X|O|X| |O|O|X| | | | | |X|O|X| |O|O|X| | | |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | |O| | |X|X| |X| |O| | | |O| | |X|X| |X| |O| |O| |O| 'X' Won! |X|X|X| |X| |O| |O| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X|X| | |X|O| | |O|O| | |X|X| | 'X' Won! |X|O| | |O|O| | |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | | | |X| | |O|X| |O| | | | |O|X| 'X' Won! | |O|X| |O| |X| | |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| |O| | | | | | | |X|O|X| |O| |X| | | | | |X|O|X| |O| |X| | |O| | 'X' Won! |X|O|X| |O| |X| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | |X| |X| | | |O| |O|O| | |X| |X| |X| |O| 'O' Won! |O|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| | | |X|X| | | |O| | |O|O| | |X|X| | | |O| | |O|O|X| |X|X| | | |O|O| |O|O|X| |X|X| | |X|O|O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | |O| | |X| |O| |X| |X| | |O| | |X| |O| |X| |X| | |O|O| |X| |O| |X| |X| | |O|O| |X|X|O| 'O' Won! |X| |X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X| | | 'O' Won! |X| | | |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| |O| | | |X| | |X| | |O| |O| | | |X| | |X|X| |O| |O| | |O|X| | |X|X| |O| |O| |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | | |X| | |O| |O| |X|X|O| | |X|X| |O| |O| |X|X|O| 'O' Won! | |X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X| | | |O|X|O| | |O|X| |X|X| | 'O' Won! |O|X|O| | |O|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | | |O| | |X|X| |X| |O| | | |O| | |X|X| |X|O|O| | | |O| 'X' Won! |X|X|X| |X|O|O| | | |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X|O| | |O|X| | | |O| |X|X|O| |O|O|X| | | |O| |X|X|O| |O|O|X| |X| |O| |X|X|O| 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | |O| |X| | |X|O| |X| | | |O| |X| | |X|O| |X| |O| |O| |X| | |X|O| |X| |O| |O|X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | |X|O| |O| | | |X| | | |O|X|O| |O| | | |X| | | |O|X|O| |O|X| | |X| | | |O|X|O| |O|X|O| 'X' Won! |X|X| | |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | | | |X|O| | | |O|X| | | | | |X|O| | |O|O|X| | | | | |X|O|X| |O|O|X| | | | | 'O' Won! |X|O|X| |O|O|X| | |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | |X| | | |X|O| |O| |O| | |X| | 'X' Won! | |X|O| |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X|X| | |O| | | | | |O| |X|X| | |O| | | | |O|O| 'X' Won! |X|X|X| |O| | | | |O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | |O| | | | |X| | |O|X| |X|O| | | | |X| | |O|X| |X|O| | |O| |X| |X|O|X| |X|O| | |O| |X| |X|O|X| |X|O|O| |O| |X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O|X|O| |X| |X| | | |O| |O|X|O| 'X' Won! |X|X|X| | | |O| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O|X| | | |X| | |X|O|O| |O|X| | |O|X| | 'X' Won! |X|O|O| |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X|O| | | | |O| |X| | | |X|O| | | | |O| |X|X| | |X|O| | | | |O| |X|X|O| |X|O|X| | | |O| |X|X|O| |X|O|X| |O| |O| |X|X|O| 'X' Won! |X|O|X| |O|X|O| |X|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O|O| | |O|X| | |X| | | |O|O|X| |O|X|O| |X| | | |O|O|X| |O|X|O| |X|X| | |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | |X| |X| | | | | |O|O| | |X| |X| | |O| | |O|O|X| |X| |X| | |O| | |O|O|X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| 'O' Won! |O|O|X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X| | |O| |O| | |X| | | |X|X| |O| |O| | |X| | |O|X|X| |O| |O| |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| |O| | | | | |X|O|X| |X| |O| | | | | |X|O|X| |X| |O| | |O| | 'X' Won! |X|O|X| |X| |O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | |X|O|X| | |O| | |O|X|O| |X|O|X| 'X' Won! | |O|X| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | | |X| | | |O|O| |X|O| | | |X| | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O|O| | |X| | |X|O|O| |X|O|O| | |X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | | | | | |X|O| | |O|X|O| | | | | |X|O|X| |O|X|O| | | | | |X|O|X| |O|X|O| | | |O| 'X' Won! |X|O|X| |O|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|O| | |O| | 'X' Won! |X| | | |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| | | |X|X|O| | | | | |O| | | |X|X|O| | | |O| |O|X| | |X|X|O| | | |O| |O|X| | |X|X|O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| |X| | | | | | | | |X|O| |X| | | | |O| | | |X|O| |X| | | |X|O| | | |X|O| |X| |O| |X|O| | 'X' Won! |X|X|O| |X| |O| |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | |O|O| |O|O|X| 'X' Won! |X|X|X| | |O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | |X|O|O| | |X|O| | | | | |X|O|O| | |X|O| | |X| | |X|O|O| | |X|O| |O|X| | |X|O|O| |X|X|O| |O|X| | |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X|X| | |O|O| | |X| | 'O' Won! | |X|X| |O|O|O| | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | 'O' Won! |O|O|O| | |X| | | |X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X|O| | | |O| 'X' Won! | | |X| |O|X|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | 'X' Won! |X| |O| |X|O|O| |X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| |X| |X| |X| | | |O| |O| |X| |X| |X|O| | |O| |O| |X| |X| |X|O| | |O|X|O| |X| |X| |X|O|O| |O|X|O| 'X' Won! |X|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| | | |O| |O| |X|O| | |X| | | |O| |O| |X|O| | |X| |X| |O| |O| |X|O| | |X|O|X| |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | | | |X| | |X|O| |O|X| | |O| |X| | |X|O| |O|X|X| |O| |X| | |X|O| |O|X|X| |O|O|X| 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O| | | | |X| |O|X|X| |O|O| | | | |X| |O|X|X| 'X' Won! |O|O|X| | | |X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| |O| | |X| | | |X| | 'X' Won! |O|X|O| | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | |X| |O|O| | |O|X| | | | |X| |O|O| | |O|X| | |X| |X| |O|O| | |O|X| | |X|O|X| |O|O| | |O|X|X| |X|O|X| |O|O| | 'O' Won! |O|X|X| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| |X| | | |O| |X| | | |O|O|X| | | |O| |X| | | |O|O|X| | | |O| |X|X| | |O|O|X| |O| |O| |X|X| | 'X' Won! |O|O|X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | | |X| |O|O| | | |X| | |X| |X| |O|O| | | |X| | |X|O|X| |O|O| | | |X| | |X|O|X| |O|O| | | |X|X| 'O' Won! |X|O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | |O|O|X| | |X| | | |O| | |O|O|X| | |X|X| | |O|O| |O|O|X| | |X|X| |X|O|O| |O|O|X| | |X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O| | | | |X|X| | |O|X| |O|O| | | |X|X| | |O|X| 'X' Won! |O|O|X| | |X|X| | |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| | |X|X| | | |O| | | |O|O| |X|X| | | |O| | |X|O|O| |X|X|O| | |O| | |X|O|O| 'X' Won! |X|X|O| |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | |O| | | | |O| |X|X| | | |O| | | |X|O| |X|X| | | |O|O| | |X|O| |X|X| | |X|O|O| | |X|O| |X|X| | 'O' Won! |X|O|O| | |X|O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | |X|O| |O| |O| | | |X| | |X|O| |O|X|O| | | |X| | |X|O| 'O' Won! |O|X|O| | |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| |O| |O| |X| | | |X| |O| |O| |O| |X| | | |X|X|O| 'O' Won! |O| |O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | | | |X| |O|O|X| |X| |O| | | |X| |O|O|X| |X|X|O| | | |X| |O|O|X| |X|X|O| |O| |X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| |O| |X| | |O|O| | | |X| |O|X|X| 'O' Won! | |O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| |O| | |X| | | | | | |O| |O| |X|X| | 'O' Won! | | | | |O|O|O| |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X| |X| |O|X| | | | |O| |X| |X| |O|X| | | |O|O| 'X' Won! |X| |X| |O|X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | | |O| |X| |X| |X| |O| | |O|O| |X| |X| |X| |O| 'X' Won! |X|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O|O| | | | |X| |X|O| | |O|O|X| |O| |X| |X|O| | |O|O|X| 'X' Won! |O| |X| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| | |X|X|O| | | | | | |O| | |X|X|O| | | |X| | |O| | |X|X|O| | |O|X| 'X' Won! |X|O| | |X|X|O| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| |O| |X| |O| | | | | |X|X|O| |X| |O| | | | | |X|X|O| |X| |O| |O| | | |X|X|O| |X| |O| |O| |X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| | | | |O| | | |X|X| |O| | | |X|O| | | |X|X| |O| | | |X|O|O| | |X|X| |O| |X| |X|O|O| |O|X|X| |O| |X| |X|O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| |X| |O|O| | | | |X| |X| |X| |O|O| | | |O|X| |X| |X| |O|O| | |X|O|X| 'O' Won! |X|O|X| |O|O| | |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X| | | |O|X| |O| | | |O|X|X| | |O|X| 'O' Won! |O| | | |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| |X| | | | | |O|X| | |O|O|X| | | | | |O|X|X| |O|O|X| | | | | |O|X|X| |O|O|X| | |O| | 'X' Won! |O|X|X| |O|O|X| | |O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X|O| | |O| | |O|X|X| | |X|O| | |O| | |O|X|X| |O|X|O| | |O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | | |X| |O| |O| |O| |X| |X| |X| |O| |O| |O| 'O' Won! |X| |X| |X| |O| |O|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| | | |X| | | | |O|O| |X|O| | |X| | | | |O|O| |X|O|X| |X| | | 'O' Won! | |O|O| |X|O|X| |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| | |O| | |O|O| | 'X' Won! |X|X|X| | |O| | |O|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| 'O' Won! | |X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | |O| |O| |X| |X| | |X| | |O| |O| |X| |X| | |X|O| |O| |O| |X| |X| | |X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| 'X' Won! |X|X|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | |O| | |X| |O| | |O|X| |X|O| | 'O' Won! |X|O|O| | |O|X| |X|O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O| | | | |X| | |O|X|X| 'O' Won! |O| | | |O|X| | |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |O| | | | | |O| | |X|X| |O| | | | | |O| | |X|X| |O| |O| |X| |O| | |X|X| |O| |O| 'O' Won! |X| |O| | |X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | | |X|O| |O|O| | | |X| | | |X|O| |O|O| | | |X|X| |O|X|O| |O|O| | | |X|X| 'X' Won! |O|X|O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | | | | | |X| |O| | |X|O| | | | | |X| |O| | |X|O| | |X| | |X|O|O| | |X|O| | |X| | |X|O|O| | |X|O| |X|X| | |X|O|O| |O|X|O| |X|X| | 'X' Won! |X|O|O| |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | |X| | |X|O| | | |O| |O| |X| | |X|O| | | |O| 'X' Won! |O| |X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X|X| |O|O| | |X| | | |O|X|X| |O|O|X| |X| | | |O|X|X| |O|O|X| |X|O| | |O|X|X| 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | | |X|O| | |O| | | |X|O| | |X|O| | |O| | |X|X|O| | |X|O| 'O' Won! | |O|O| |X|X|O| | |X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | |O| | | | |O| |X| |X| |X|O| | | | |O| |X|O|X| |X|O| | | | |O| |X|O|X| |X|O| | | |X|O| |X|O|X| |X|O| | |O|X|O| It's a stalemate! |X|O|X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | |X| | |X| |O| | | |O| |O|X| | |X| |O| | | |O| |O|X|X| |X| |O| | |O|O| |O|X|X| |X| |O| |X|O|O| |O|X|X| |X| |O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| |X| |O| | | | |O|O| |X| |X| 'X' Won! |O| | | | |O|O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X|X| | | |O| |X| |O| |O|X|X| | |X|O| |X| |O| |O|X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| |O| | | | | |X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| | | |O|X|X| |X|O|O| |O|X| | |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | | |X| |X|O| | | |O|O| | | |X| |X|O|X| | |O|O| 'O' Won! | |O|X| |X|O|X| | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | | |O|X|O| |X|O| | |X| | | |O|X|O| |X|O| | |X|X| | |O|X|O| |X|O| | |X|X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O|X| | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X|X| | | | | |O|X|O| |O|X|X| 'X' Won! | |X| | |O|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X|O| | |O|O| | |X| | |X|X|O| | |O|O| |O|X| | |X|X|O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| | | |X| |X|X|O| |O| |O| | | |X| |X|X|O| |O| |O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | |X| | | | | |X|O| | | |O|X| | | | | |X|O|X| | |O|X| | | |O| |X|O|X| | |O|X| | | |O| |X|O|X| |X|O|X| 'O' Won! | |O|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X|O| | | | | |X|O| | | |X|O| |X| | | |X|O| | | |X|O| |X| | | |X|O|O| | |X|O| |X| |X| |X|O|O| | |X|O| |X|O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O|X| | 'O' Won! | | |O| | |O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | 'X' Won! |X| | | |X| |O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| |O| |O|X| | | | |X| |O| |O| |O|X|X| 'O' Won! |O| |X| |O| |O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| |O| |X| |X| |X|O|O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| |O|X| | | |O|X| | |O|X| |O|X| | | |O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| |O| |X| | | |O| | |O|X| |O|X|X| | | |O| | |O|X| |O|X|X| |O| |O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | |O|X| | |X| | | |X|O| | |O|X| | |X| | | |X|O|O| |O|X| | |X|X| | |X|O|O| |O|X| | 'O' Won! |X|X|O| |X|O|O| |O|X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| 'O' Won! |O| |X| | |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X|O| | | | |X| | |O| | |X|O|X| | | |X| |O|O| | |X|O|X| | | |X| |O|O| | |X|O|X| |X| |X| 'O' Won! |O|O|O| |X|O|X| |X| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | | |X| |X|O| | |O| |X| | | |X| |X|O|O| 'X' Won! |O| |X| | |X|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X|X| | | |O|O| |X| | | |X|X|O| | |O|O| 'X' Won! |X| | | |X|X|O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O|X| | | | |X| |O|O|X| |O|X| | |X| |X| |O|O|X| |O|X| | |X|O|X| |O|O|X| |O|X| | 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | |O| | |X|O|X| |X| | | | |O| | |X|O|X| |X| |O| | |O| | |X|O|X| 'X' Won! |X| |O| |X|O| | |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | | | |O|X|X| 'O' Won! |X| |O| | |O| | |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | | | |X| |X|O| | |O| | | | | |X| |X|O| | |O|O| | | |X|X| |X|O| | |O|O| | | |X|X| |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X|X| | |O| | | |O|O| | |X|X| | |O| | | |O|O|X| |X|X| | |O| | | |O|O|X| |X|X| | |O|O| | |O|O|X| |X|X| | |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | |X|X| | | | |O| | |O| | |X|X| | | | |O| |O|O| | |X|X| | |X| |O| |O|O| | |X|X|O| |X| |O| |O|O| | |X|X|O| |X| |O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O|X| |O|X|X| | | | | |O|O|X| |O|X|X| | |X| | |O|O|X| |O|X|X| | |X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | |X| | |O| |O| | |X|X| 'O' Won! | |X| | |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| |O|X| | |O|O|X| | | |X| 'X' Won! |O|X|X| |O|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | |X|X| |O| |O| |O| | | | |X|X| |O| |O| |O| | | | |X|X| |O|X|O| |O| |O| | |X|X| |O|X|O| 'X' Won! |O|X|O| | |X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | |O| |X| | |X|O| | |X| | |O| |X| | |X|O| |O|X| | |O| |X| | |X|O| 'X' Won! |O|X| | |O|X|X| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |X| |O| | | | |X|O| | | |X| |O| | | | |X|O| | |X|X| |O|O| | | |X|O| | |X|X| |O|O|X| | |X|O| | |X|X| |O|O|X| | |X|O| |O|X|X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | | | |O| |X| |X| |O|X|O| | | |O| |X| |X| |O|X|O| |X| |O| |X| |X| |O|X|O| |X|O|O| |X| |X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X| |O| | |O| | |X|X|O| |X| |O| | |O| | |X|X|O| |X| |O| |O|O| | |X|X|O| |X| |O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O| | | | |X| | | |O| |X|O|X| | | |X| | | |O| |X|O|X| | | |X| | |O|O| |X|O|X| | | |X| |X|O|O| 'O' Won! |X|O|X| | |O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | |X| | |O|X| | | | |O| | |X|O| |O|X| | | | |O| | |X|O| |O|X|X| | | |O| |O|X|O| |O|X|X| | | |O| |O|X|O| |O|X|X| |X| |O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| |X| |O| | |X| | | |O|X| |X|O|O| | |X| | | |O|X| |X|O|O| | |X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |X|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O| | |X|O|X| | |X| | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X|X| 'O' Won! | |O|O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| | |O|O| |O|X|X| | |O|X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| |O|O| | |O| | | |X| |X| |O|O| | |O|X| | |X|O|X| |O|O| | |O|X| | |X|O|X| |O|O|X| |O|X| | It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| |O| |X| | | |X|O| | |O| |O| |X|X| | |X|O| | |O| |O| |X|X|O| |X|O|X| |O| |O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | |O| | |O| |X| |X| |X| | |O| | |O| |X| |X| |X| | |O|O| |O|X|X| |X| |X| | |O|O| 'O' Won! |O|X|X| |X| |X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | |O| | |X| |X| |O|X|O| | |O| | |X|O|X| |O|X|O| | |O| | 'X' Won! |X|O|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | | | | | |O|X| |X| |O| | | |O| | |O|X| |X|X|O| | | |O| | |O|X| |X|X|O| |O| |O| | |O|X| |X|X|O| |O| |O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| |X| | | |O| | |X| | |O| |X| | |O|O| | |X| | |O| |X| | |O|O| | |X|X| 'O' Won! |O| |X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | |X|X| |O| | | | | | | |O|X|X| |O| | | |X| | | |O|X|X| |O| | | |X| | | |O|X|X| |O|O| | |X|X| | |O|X|X| |O|O| | 'O' Won! |X|X| | |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | | |O|X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|O| |X|O|X| | |X|O| | |O|O| |X|O|X| |X|X|O| 'O' Won! |O|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O|X| |O|X| | 'O' Won! |O|X| | |O|O|X| |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X|X| | |X| | | |O|O| | |X|X| | |X|O| | |O|O| | |X|X| |X|X|O| | |O|O| 'O' Won! | |X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | | | |O|O| | |X|X| | |X| | | |O|O| | |X|X| |O|X| | | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| 'X' Won! |X|O|O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X| | |O| |O| |X| |X| |O|X| | |O| |O| |X| |X| |O|X|O| |O| |O| 'X' Won! |X|X|X| |O|X|O| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | |X| | |O| | | |O|X| | 'O' Won! |O|X| | |O| | | |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |O| |X|X| | | | |O| | | |O| 'O' Won! |X|X|O| | | |O| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | |X| | | |O| |X| |O| | | |X|O| | 'X' Won! |O| |X| |O|X| | |X|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| |O| | |O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |X| |X| |O| |O|O|X| |X| |X| |X|O|O| 'X' Won! |O|O|X| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| |O| |X| | | | |O|O| |X|X|O| 'O' Won! |X| |O| | |O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | |X|X| | | | | |O| |O| | |X|X| 'X' Won! | | | | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | |X|O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| |X| | | |X| | | |O|X|O| |X| | | |X| |O| |O|X|O| |X| | | |X|X|O| 'O' Won! |O|X|O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| | | | | | |X|O| |O| |X| | |X| | | |X|O| |O| |X| | |X| | |O|X|O| |O| |X| | |X|X| |O|X|O| |O| |X| 'O' Won! |O|X|X| |O|X|O| |O| |X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X|O| | | | |X| |O|O| | |X|O|X| 'O' Won! | |O|X| |O|O| | |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| |O| | | |O| |X|X| | |X| |O| | |O|O| |X|X| | |X| |O| 'X' Won! |X|O|O| |X|X| | |X| |O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | |O| | | |O|X| | | |X|X| |O| | | |O|X| | | |X|X| |O| |O| |O|X| | | |X|X| |O| |O| |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| |O|X| | | | |O| | | |X| |O|X|X| | | |O| | | |X| |O|X|X| | |O|O| | | |X| |O|X|X| |X|O|O| | | |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X| | | | |O|O| |X| |X| |X| | | | |O|O| |X|O|X| |X|X| | | |O|O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X|X| |X| | | |O|O| | |O|X|X| |X| | | |O|O| | |O|X|X| |X|X| | |O|O| | |O|X|X| |X|X|O| |O|O| | It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | |X| | | |O| |X| |O| | | |X| | | |O|O|X| |O| | | |X|X| | |O|O|X| |O| | | |X|X| | |O|O|X| |O| |O| 'X' Won! |X|X|X| |O|O|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X|O|X| |O| | | | |O| | |X|O|X| |O|X| | | |O| | |X|O|X| |O|X| | | |O|O| |X|O|X| |O|X|X| | |O|O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | |X| | | | |O| | |O|X|O| |X| | | | |O| | |O|X|O| |X| | | |X|O| | |O|X|O| |X|O| | |X|O| | |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X|X| | |O| |O| | |X| | |X|X|O| |O| |O| | |X| | |X|X|O| |O| |O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| |X| |O| | |O|X| | | |X| 'O' Won! |X| |O| | |O|X| |O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O|X|O| | |X| | |X|O| | |O|X|O| | |X|O| |X|O|X| |O|X|O| | |X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | | | | | |X|O|X| |O| | | | | |O| |X|O|X| |O| |X| | | |O| |X|O|X| |O|O|X| | | |O| |X|O|X| |O|O|X| |X| |O| |X|O|X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| |X| |X| |O| | | | | |O|O|X| |X| |O| | | | | |O|O|X| |X| |O| |X| | | |O|O|X| |X|O|O| |X| | | |O|O|X| |X|O|O| |X| |X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| |O| |O| | | | | |X| 'X' Won! |X| |O| |O|X| | | | |X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| | |X| | | 'X' Won! |X|O| | |X|O| | |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | |O| | | | |O|X| |O|X| | |O| |X| | |O|X| |O|X| | |O|O|X| | |O|X| 'X' Won! |O|X|X| |O|O|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | | |O| | | | | |X|X|O| | | |O| | | |X| |X|X|O| | | |O| | |O|X| |X|X|O| | | |O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X| |X| | |O|O| | |X| | |X|O|X| 'X' Won! |X|O|O| | |X| | |X|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| |O| | | | | |X| | |O|X| |O|X| | | | |X| |O|O|X| |O|X| | 'X' Won! | | |X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| | |X| | |O|O| | | |X|X| | |X|O| 'X' Won! |O|O| | |X|X|X| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| | |X|O|X| | | | | | |O| | |X|O|X| |O| | | |X|O| | |X|O|X| |O| | | 'O' Won! |X|O|O| |X|O|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| |X|O| | | | | | | |O|X| |X|O|O| | | | | |X|O|X| |X|O|O| 'O' Won! |O| | | |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | |O| | |O|X| | | |X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| |X|X|O| 'O' Won! |O|O|X| | |O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X|O| | | | | | |X|O| |O|X|O| | | |X| | |X|O| |O|X|O| |O| |X| 'X' Won! |X|X|O| |O|X|O| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X|O| | |X|X| |O| | | |O|X|O| | |X|X| |O|O| | |O|X|O| | |X|X| |O|O|X| |O|X|O| 'O' Won! |O|X|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O|X| | |O| | |X| |O| |X|O|X| | |O| | 'X' Won! |X| |O| |X|O|X| |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O| |X| |X| | | |O|O| | |O| |X| |X| |X| |O|O| | |O| |X| 'O' Won! |X| |X| |O|O|O| |O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| | | | |O| | |X|X|O| |O|X| | | |O| | |X|X|O| |O|X|O| | |O| | |X|X|O| |O|X|O| |X|O| | 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X|X| | |O|O| | | | | | |X|X| |X|O|O| | | | | | |X|X| |X|O|O| | | | | |O|X|X| |X|O|O| |X| | | |O|X|X| |X|O|O| |X| |O| |O|X|X| 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | |X| | | |X|O|O| | |X| | |X| |O| |X|O|O| | |X|X| |X| |O| |X|O|O| | |X|X| |X|O|O| |X|O|O| 'X' Won! |X|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| |O| | | | | |X|X| | |O| |O| | | | | |X|X| | |O| |O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X|X| | | |X| | |O| |O| 'O' Won! |X|X| | | |X| | |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | |O| | |O| | | | |X|X| |X|O| | |O| |O| | |X|X| |X|O| | |O|X|O| | |X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O| | 'O' Won! |X|O| | | |O| | |X|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | 'O' Won! |X| |O| |X| |O| | | |O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | |O| |X| |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| 'X' Won! |X|O| | | |X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | |X| |O|X| | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X|X| |O|O| | | | |X| |O|X|X| 'X' Won! |O|O|X| | | |X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| 'O' Won! | | |X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | |X|X| | | |O|O| |O|X| | |X|X|O| | |O|O| |O|X|X| |X|X|O| | |O|O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | | | | |O| |X| | |O|X| | | | | 'O' Won! |O| |X| | |O|X| | | |O| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | |X| |O| |O| | | | |X| | |X| |O| |O|O| | | |X|X| |X| |O| |O|O| | 'O' Won! | |X|X| |X| |O| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| |O| | | | | |X| |O|O|X| 'X' Won! |O| |X| | | |X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | |X| | | | |X| |O| |O| | |X| | | |O|X| |O| |O| |X|X| | | |O|X| 'O' Won! |O|O|O| |X|X| | | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O| | |X| | | 'O' Won! | |O|X| | |O| | |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | |X| | | | |O|O| |X| | | 'O' Won! |X| | | |O|O|O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | |O| | | |X| |O|X|X| | | |O| | | |X| |O|X|X| |O| |O| |X| |X| |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|O| | |O| | |O| |X| | |X|O| | |O|X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| | |O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | |O|O| | |X| | |X| |X| | |O|O| 'O' Won! | |X| | |X| |X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | |X| | |X| | |O| |O| | | |X| | |X| | |O| |O| | |X|X| | |X| | 'O' Won! |O|O|O| | |X|X| | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | |X| | |X| |X| |O|O| | | |X| | |X|O|X| |O|O| | | |X|X| |X|O|X| |O|O| | 'O' Won! | |X|X| |X|O|X| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| |O| | | |X|X| | | |O|X| |O| | | |X|X| | | |O|X| |O|O| | 'X' Won! |X|X|X| | |O|X| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X| | | |X|O| |O| | | | |X| | |X|X|O| |O| | | | |X|O| |X|X|O| |O| | | | |X|O| |X|X|O| |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X|X| | | | | |O|O| | |O|X|X| | | | | |O|O|X| |O|X|X| 'O' Won! |O| | | |O|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | |O|O| | |X|X| | | |X| | |O|O| | |X|X| |O| |X| | |O|O| 'X' Won! |X|X|X| |O| |X| | |O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X|O| | | | | |O| | | |X|X|O| | | | | |O| |O| |X|X|O| | | | | |O|X|O| |X|X|O| 'O' Won! | | |O| |O|X|O| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | |X| |X| | | |X|O|O| |O| |X| |X| | | |X|O|O| |O|X|X| |X| | | |X|O|O| |O|X|X| |X| |O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | |O| | |O| | | | |X|X| | |O|X| |O| | | | |X|X| |O|O|X| |O| | | 'X' Won! | |X|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X|X| | | |X| | 'O' Won! |O|O|O| |X|X| | | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| |X| |X| | |O| | | | |O| |X| |X| |O|O| | | | |O| |X| |X| |O|O| | |X| |O| |X|O|X| |O|O| | |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| |O| | | |X|X| | |X| |O| |O| | | |X|X|O| |X| |O| |O| | | |X|X|O| |X| |O| |O| |X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | |X|O| | | | |O| | |X| | |X|O| | | |O|O| | |X| | |X|O| | | |O|O| |X|X| | |X|O| | | |O|O| |X|X|O| |X|O|X| | |O|O| |X|X|O| 'O' Won! |X|O|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| |O| |O| | | | |X| | |X| |O| |O|X| | |O|X| | |X| |O| |O|X| | |O|X| | |X| |O| |O|X|X| |O|X|O| |X| |O| |O|X|X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | |X|O| | | | | | |X|O| | |X|O| |O| | | | |X|O| | |X|O| |O| |X| | |X|O| | |X|O| |O|O|X| | |X|O| |X|X|O| |O|O|X| | |X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | | | |O| |O| |X| |X|X| | | | |O| |O| |X| |X|X| | | | |O| |O|O|X| 'X' Won! |X|X| | | |X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X| | | |O|O|X| |X| | | |X| | | |O|O|X| |X| |O| |X| | | |O|O|X| |X|X|O| |X| | | |O|O|X| |X|X|O| |X| |O| |O|O|X| 'X' Won! |X|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | |X|O| |O| | | | |X| | | |X|O| |O|O| | | |X| | | |X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| 'X' Won! |X|X| | |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | | |O| | | |X| |X|X|O| | | |O| |O| |X| |X|X|O| |X| |O| |O| |X| |X|X|O| |X|O|O| |O| |X| |X|X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | |O| | | |X|O|X| | | | | |O| | | |X|O|X| |X| | | 'O' Won! |O| | | |X|O|X| |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | | | |O| | |X| | |X|O|X| | | |O| 'O' Won! |O|X| | |X|O|X| | | |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | |X| | | | |X| |O| |O| 'O' Won! | |X| | | | |X| |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | | |O| | |X|O|X| |O|X| | |O|O| | |X|O|X| |O|X| | |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | |O| |O| 'X' Won! |X|X|X| |O| | | |O| |O| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| | | |O| | | |O|X|O| |X| | | |O|X| | 'O' Won! |O|X|O| |X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O|X| | | | | |O| |X| | |O|X| | |O| | |O| |X| |X|O|X| | |O| | |O| |X| |X|O|X| |O|O| | |O| |X| 'X' Won! |X|O|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | | | | |O|X| | |O| |X| 'X' Won! |X| | | |O|X| | |O| |X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O|O| |X| | | |X| |O| | |O|O| |X| |X| 'O' Won! |X| |O| |O|O|O| |X| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X|O| | |O|X| | | | | |X|X|O| | |O|X| | |O| | |X|X|O| | |O|X| | |O|X| |X|X|O| 'O' Won! |O|O|X| | |O|X| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X| |O|O|X| | | | | | |X|X| |O|O|X| | | | | | |X|X| |O|O|X| | |O| | | |X|X| |O|O|X| |X|O| | | |X|X| |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X| | | | |O| | |O|O| |X|X| | |X| |O| | |O|O| 'O' Won! |X|X| | |X| |O| |O|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | | | |X| |O|X|O| | |X| | | | |X| |O|X|O| | |X|O| |X| |X| |O|X|O| | |X|O| |X| |X| |O|X|O| |O|X|O| 'X' Won! |X|X|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O|O| | |X| | | | | | |X|O|O| | |X|X| | | | | |X|O|O| | |X|X| | | |O| |X|O|O| | |X|X| | |X|O| |X|O|O| |O|X|X| | |X|O| It's a stalemate! |X|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | | | |X| |X|O| | |O|O| | | | |X| |X|O| | |O|O| | | | |X| |X|O|X| 'O' Won! |O|O| | | |O|X| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| |X|X|O| | |X| | | |O|O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X|X| | | | | | | |O|O| 'O' Won! |X|X| | | | | | |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | | |O| |X| |O| | |X| | |O| |O| |X| |O| | |X| | |O| |O| |X| |O| |X|X| | 'O' Won! |O| |O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X|X| | |O| | |O|O| | | |X|X| | |O| | |O|O|X| | |X|X| | |O|O| |O|O|X| | |X|X| |X|O|O| |O|O|X| | |X|X| 'O' Won! |X|O|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| |X| |X| |O|X| | | |O|O| |X| |X| |O|X|X| | |O|O| |X| |X| |O|X|X| | |O|O| |X|O|X| It's a stalemate! |O|X|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O|X| | | | |X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X|X| 'O' Won! |O|X|O| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O|X| | | | | | | | | |X|O|X| | | |O| |X| | | |X|O|X| | | |O| |X| | | |X|O|X| | |O|O| 'X' Won! |X| | | |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | | |O| |O| | | | |X|X| | |O|O| |O| | | | |X|X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| |O| | |X|X| 'X' Won! |X|O|O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | |O| | | | | | |X|O| | |X|O| 'O' Won! | | |O| | |X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | | | | |X|O| | |X|O| | | | |X| |X|O| | |X|O| | |O| |X| |X|O| | |X|O| | |O| |X| |X|O|X| |X|O| | 'O' Won! |O|O|X| |X|O|X| |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | |X|X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | | |X| |O| |O| |O|X|X| | | |X| |O| |O| |O|X|X| | |X|X| 'O' Won! |O| |O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X|X| | |O| | |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| | |O|X| |O|O| | |O|X|X| 'X' Won! | |O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | |X| |X| | |X|O| | |O| | |X|O|X| | |X|O| | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | | |O| |X|O| | |X|X| | | | |O| |X|O| | |X|X|O| | | |O| 'X' Won! |X|O| | |X|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| | | |X|X|O| |O| |X| |O| |O| |X|X|O| |O|X|X| |O| |O| |X|X|O| 'O' Won! |O|X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X|X| |O| | | |X| |O| |O|X|X| |O| | | |X|X|O| |O|X|X| |O| | | |X|X|O| |O|X|X| |O|O| | 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| |O| | | |O| | | |X| |X|X|O| | | |O| | |O|X| |X|X|O| | | |O| |X|O|X| |X|X|O| | |O|O| |X|O|X| |X|X|O| 'X' Won! |X|O|O| |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X|O| |X|O| | | | | | | |X|O| |X|O| | | |X| | | |X|O| |X|O| | | |X|O| | |X|O| |X|O| | |X|X|O| 'O' Won! |O|X|O| |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | |O|O| | | |X|X| | | | | |O|O| | | |X|X| |X| | | |O|O| | |O|X|X| |X| | | |O|O| | |O|X|X| |X| |X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|O| |X| | | |O|X|X| | |O|O| |X| |O| |O|X|X| | |O|O| |X| |O| |O|X|X| |X|O|O| It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | |O| | |O|X|O| | |X| | | |O|X| |O|X|O| | |X| | | |O|X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O|X| | |O|X| | | | | | 'O' Won! |O|X| | |O|X| | |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | | | |X|X|O| | | |O| |O| | | |X|X|O| | | |O| |O| | | |X|X|O| |X| |O| |O| | | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| | |O| | | | |X| |X|O|O| | |O| | |X| |X| |X|O|O| |O|O| | |X| |X| |X|O|O| |O|O|X| |X| |X| 'O' Won! |X|O|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| | | |O|O| |X| | | | |X| | | |O|O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X| | |X|O|O| |X|X| | |O|X|O| |X|O|O| |X|X| | 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | | | | |X|O| | |O|X| | | |X| | |X|O| | |O|X| | | |X| | |X|O|O| |O|X| | | |X|X| |X|O|O| |O|X|O| | |X|X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X|X| | |O| | | | |O|O| |X|X| | |O|X| | | |O|O| |X|X| | |O|X|O| | |O|O| |X|X| | 'X' Won! |O|X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | | | |X| |O|X| | | |O| | | | |X| |O|X|O| |X|O| | | | |X| |O|X|O| |X|O|O| | | |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| | | |X| | |O|X| | |X|O|O| | |X| | |O|X| | |X|O|O| |X|X| | |O|X| | |X|O|O| |X|X|O| |O|X| | 'X' Won! |X|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | |O| |O| | |X|X| | |X| | |O| |O| | |X|X| |O|X| | |O| |O| 'X' Won! | |X|X| |O|X| | |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O|X| |X| | | |X| |O| | |O|X| |X|O| | |X|X|O| | |O|X| |X|O| | |X|X|O| | |O|X| |X|O|O| 'X' Won! |X|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | |O| |O| |X| | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X|O| | 'X' Won! |O|X|X| |O|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |O| 'X' Won! | |O| | |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X|X| | |O| | | |O|O| | |X|X| | 'X' Won! |O| | | |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | | | | |O| | | | |O|X| |X| | | |O| | | | |O|X| |X|O| | |O|X| | | |O|X| |X|O| | 'O' Won! |O|X| | | |O|X| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | |O| | |O|X|X| |X| | | | |O| | |O|X|X| |X| | | |O|O| | |O|X|X| |X|X| | |O|O| | 'O' Won! |O|X|X| |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X| | |O| | | | |O|X| |X|X| | |O| | | | |O|X| |X|X| | |O|O| | | |O|X| 'X' Won! |X|X|X| |O|O| | | |O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | |X| | |O|O|X| | | | | | |X|O| |O|O|X| | | | | |X|X|O| |O|O|X| | |O| | |X|X|O| |O|O|X| | |O|X| |X|X|O| |O|O|X| It's a stalemate! |O|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| |X| |O|O| | | | |X| 'O' Won! |X| |X| |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O|O| 'X' Won! |O|X|X| |O|X| | |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | |O| |X|X| | | |O| | |X| |O| |X|X| | | |O| | |X| |O| |X|X|O| | |O| | 'X' Won! |X| |O| |X|X|O| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X| |X| |O| | | | | | | |X|O|X| |O| |X| | | | | |X|O|X| |O| |X| |O| | | |X|O|X| |O|X|X| |O| | | |X|O|X| |O|X|X| |O|O| | |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O| | | |O| | |X| |X| |X|O| | | |O| | |X| |X| |X|O| | | |O|O| |X| |X| |X|O|X| | |O|O| 'O' Won! |X| |X| |X|O|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | | |X|X| | |X|O| | |O|O| | |X|X| |X|X|O| | |O|O| | |X|X| 'O' Won! |X|X|O| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O|O| |X| | | | | | | | |O|O| |X| |X| | | |O| | |O|O| |X| |X| | | |O| |X|O|O| |X| |X| | | |O| |X|O|O| |X|O|X| 'X' Won! |X| |O| |X|O|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| | |O| | |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| 'O' Won! | |O|X| |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O|X| | | |O|X| | | |X| |O|X|O| | |O|X| 'X' Won! |X| |X| |O|X|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| | |O|O| | |X|O| | | |X| | |O|O| | |X|O|X| | |X|O| |O|O| | |X|O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | | |O| |O| | | | |X|X| | | |O| |O|X| | | |X|X| |O| |O| |O|X| | | |X|X| |O| |O| |O|X|X| | |X|X| 'O' Won! |O| |O| |O|X|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | |O| |X| | | |X| | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| |O|O| | 'X' Won! |O| |X| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | |X|O| | |O|X| | |O| | | |X|O| | |O|X| | |O| | | |X|O| | |O|X|X| 'O' Won! |O| |O| |X|O| | |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| |X|O|X| | | | | | | |O| |X|O|X| |O| | | | | |O| |X|O|X| |O| | | | |X|O| |X|O|X| |O|O| | | |X|O| |X|O|X| |O|O|X| | |X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O|X| |O|X| | 'O' Won! |O|X|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | | |O| | |X|O| | |X| | | | |O| | |X|O| | |X|X| | | |O|O| |X|O| | |X|X| | | |O|O| |X|O|X| |X|X| | 'O' Won! |O|O|O| |X|O|X| |X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O|X| | | | | | |O| | |X|O|X| |O| | | | |O| | |X|O|X| |O| |X| 'O' Won! | |O| | |X|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X|X|O| 'O' Won! |O|O|O| | | |X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O|X| |O|X| | |O|X| | | |O|X| |O|X| | |O|X|O| | |O|X| |O|X|X| |O|X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | |O| | |X|X| | |O|X| | | |O| | |X|X| | |O|X| | |O|O| | 'X' Won! |X|X| | |O|X| | |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X|X| | |O| | |O|O|X| |O|X|X| 'X' Won! | |O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | | |X| |X| | | | |O|O| | | |X| |X|X| | | |O|O| | | |X| |X|X| | | |O|O| | |O|X| |X|X| | | |O|O| |X|O|X| |X|X|O| | |O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X|X| | | | | |O|O|X| |O|X|X| | |O| | 'X' Won! |O|O|X| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| |O|X|O| | | | | | |X|O| |O|X|O| |X| | | | |X|O| 'O' Won! |O|X|O| |X| |O| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | |X|X| | |O| |X| |O| | | |X|X|O| |O| |X| |O|X| | |X|X|O| |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | |O| | | |O| |X| |X| |X| |O| | | |O| |X| |X| |X| |O| | | |O| |X|O|X| 'X' Won! |X| |O| |X| |O| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O| | | |X| |X| |O| | |O|O| | | |X| |X|X|O| | |O|O| | | |X| |X|X|O| | |O|O| | |O|X| |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | |X| | |X|O|O| | | |O| | |X| | |X|O|O| | |X|O| | |X| | |X|O|O| | |X|O| |O|X| | |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| |O| |O|X| | | |O| | |X| |O| |O|X|X| |O|O| | |X| |O| |O|X|X| |O|O|X| |X| |O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X|X| | | | | 'O' Won! |O|O|X| |O|X|X| |O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | |O|O|X| | | | | | | |X| |O|O|X| | | |O| | | |X| |O|O|X| | | |O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| 'X' Won! | |O|O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| 'X' Won! |X|X|X| | |O| | | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | | | | | |X| |O| | |X| | |O| | | |X| |O| |X|X| | |O| | | |X| |O| |X|X|O| |O| | | 'X' Won! |X| |O| |X|X|O| |O| |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X|O| |X|O| | 'X' Won! |X|O|X| |O|X|O| |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| |X| |O| | | |X|X| | |O| |X| |O|O| | |X|X| | |O| |X| |O|O|X| |X|X|O| |O| |X| |O|O|X| 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X| | | | |O| | |O| | |X|X| | | |X|O| | |O| | |X|X| | | |X|O| |O|O| | 'X' Won! |X|X|X| | |X|O| |O|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | | |X| |O|X|O| | | | | | | |X| |O|X|O| | |O| | |X| |X| |O|X|O| | |O| | |X| |X| |O|X|O| |O|O| | 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| | | |O| |X| |O| |X| |X| | | |O|O|X| |O| |X| |X| | | 'X' Won! |O|O|X| |O|X|X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X|O| | | | | | | | | |O|X|O| | |X| | |O| | | |O|X|O| | |X| | |O| | | |O|X|O| | |X|X| |O|O| | |O|X|O| | |X|X| 'X' Won! |O|O| | |O|X|O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| | | |X| 'X' Won! |X|O|O| |X|X|O| | | |X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | | | |X| |O| | | |X| | | |O| |X| |O| |X| |X| | | |O| |X| |O| |X| |X| |O| |O| 'X' Won! |X| |O| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| 'O' Won! |O|X|X| |O| | | |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | |O| | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O|O| | |X| | | |X|O| | |O|O| | |X|X| | |X|O| | |O|O| | |X|X| | |X|O|O| 'X' Won! |O|O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | |X| | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X|X| | | |O|O| | |X|O| |X|X| | | |O|O| |X|X|O| |X|X| | 'O' Won! | |O|O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | |X| | | | | | |X| |O| | |X|O| | | | | |X| |O| | |X|O| | | |X| |X| |O| |O|X|O| | | |X| |X| |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X| |X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | |X| | |X| | | | |O|O| | |X| | |X| |O| | |O|O| | |X|X| |X| |O| | |O|O| |O|X|X| |X| |O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X|X| | |O| |O| |O| |X| |X|X| | |O| |O| 'X' Won! |O| |X| |X|X|X| |O| |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| |X| | | | | |X|O| | |O| |X| | | | | |X|O| | |O|X|X| | | | | |X|O|O| |O|X|X| | | | | 'X' Won! |X|O|O| |O|X|X| | | |X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| |X| | | |X|O| | | |O|O| |X| | | |X|O|X| | |O|O| |X| | | 'O' Won! |X|O|X| | |O|O| |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O|O| |X| | | |X|O|X| | |O|O| |X| | | 'O' Won! |X|O|X| | |O|O| |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| | | |X|O|X| | | |X| |O|O| | |X|O|X| | | |X| |O|O| | |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |O| |O| | | | |X| | | |X|O| |O| | | | |X| | | |X|O| |O|O| | | |X|X| | |X|O| |O|O| | | |X|X| |O|X|O| |O|O| | | |X|X| |O|X|O| |O|O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X| | | |X|O| | |O|O| | |X|X| | |X|O| | |O|O| | 'O' Won! |X|X|O| |X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| |X| | | |X| | | |O|O|X| |X| | | |X|O| | 'X' Won! |O|O|X| |X|X| | |X|O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | |X| |X| | |O|O| | | | | |X| |X| | |O|O| | | |O| 'X' Won! |X|X|X| | |O|O| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | |O|O| | |X|O| | |X|X| | |O|O| | |X|O| | 'O' Won! |X|X| | |O|O|O| |X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | |X| | | | | | | |X|O| | |X| |O| | | | | |X|O| | |X|X|O| | | | | |X|O| | |X|X|O| |O| | | |X|O| | |X|X|O| |O| |X| |X|O| | |X|X|O| |O|O|X| |X|O| | It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O|X|X| | | | | |O| | | 'O' Won! |O|X|X| |O| | | |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| |O| | | |X|X| | |X| |O| |O| | | |X|X| | |X| |O| |O|O| | |X|X| | |X|X|O| |O|O| | 'O' Won! |X|X| | |X|X|O| |O|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | | | | |O|O|X| | | |X| | | | | 'X' Won! |O|O|X| | | |X| | | |X| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | | |X| | | |O| |X|O| | | | |X| |O| |O| |X|O|X| | | |X| |O| |O| |X|O|X| |O| |X| |O| |O| |X|O|X| |O| |X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X|O| | |O| | |X| |O| |X|X|O| | |O| | |X|O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | |O| | | | |X| |O| |X| | |O| | | |X|X| |O| |X| | |O| | |O|X|X| 'X' Won! |O| |X| | |O|X| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O|O| | |X| | | |X| | | |O|O|X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X|X| | |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X|O| |X| | | 'X' Won! |O| |X| | |X|O| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | 'O' Won! |O| |X| |O|X| | |O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | | |O| | | |O| |O|X|X| |X| |O| | | |O| |O|X|X| |X| |O| | |O|O| |O|X|X| |X|X|O| | |O|O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| |X| | | | | |O| | |O|X| |X| | | | |O|O| | |O|X| |X| | | |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| |X|O|O| | |O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | |X| | | | |X| | |O| | | |X| | | |O|X| | |O| | | |X|X| | |O|X| | |O|O| | |X|X| | |O|X| | |O|O| | |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| 'X' Won! |X|O| | |O|X| | | |O|X| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X|O| | |O| | | |X| |O| |X|O| | |O|X| | |X| |O| |X|O| | |O|X|O| |X| |O| |X|O| | |O|X|O| |X|X|O| 'O' Won! |X|O|O| |O|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| |O| | | |X| |O| |X| |X| |O| | | |X| |O|O|X| |X| |O| |X| |X| |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X|X| | |X|O| | |O|O| | |X|X| 'X' Won! | |X|O| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | | |X| | |X| | |O|O| | |O| |X| | |X|X| |O|O| | |O| |X| 'O' Won! |O|X|X| |O|O| | |O| |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X|X| | |O| |O| 'O' Won! | |X| | |X|X| | |O|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X| | |O|X| | | | |O| 'X' Won! |O|X| | |O|X| | | |X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O|O| | | |X|X| | |X|O| |O|O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| |O|O| | 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X| | |O| |O| | | | | 'X' Won! |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| |O| |O|O|X| | | |X| |X| |O| |O|O|X| | | |X| |X|O|O| |O|O|X| |X| |X| |X|O|O| 'O' Won! |O|O|X| |X|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| | |O| | |X| | | | |O|X| | |O| | |X| |X| |O|O|X| | |O| | |X| |X| 'X' Won! |O|O|X| | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | | | |O|X| | | |X|O| | | | | |O|X| | |X|X|O| | | |O| |O|X| | |X|X|O| 'X' Won! | |X|O| |O|X| | |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O| |X|X| | |X| |O| |O| |O| |X|X| | 'X' Won! |X| |O| |O| |O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| | |X| | |X| | | | |O|O| | |X| | |X|X| | | |O|O| 'O' Won! | |X| | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | |O| |X| |X| | | |O|O| | |O|X|X| 'O' Won! |X| |O| |O|O| | |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | |O| |X|X| | | |O| | | | |O| |X|X| | |O|O| | | | |O| 'X' Won! |X|X|X| |O|O| | | | |O| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | |O|X| | | |X| 'X' Won! | |O|X| | |O|X| | | |X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| | | | |O| | | |X|X| |O|X| | | |O| | | |X|X| |O|X| | |O|O| | | |X|X| |O|X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| | | | | |O|X| | |O| |X| | | | | |O|X| | |O|X|X| | | | | |O|X| | |O|X|X| | | |O| |O|X|X| |O|X|X| | | |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | | |X| |O| |O| |O| |X| | | |X| |O| |O| |O| |X| | |X|X| 'O' Won! |O|O|O| |O| |X| | |X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| |O| | | | | |X| |O| |X| 'X' Won! |O| |X| | | |X| |O| |X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | |X| | |O| |O| |X|O| | | |X| | |O| |O| |X|O| | | |X|X| |O| |O| |X|O|O| | |X|X| |O| |O| 'X' Won! |X|O|O| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | | |X| | | |O| 'O' Won! |O|X| | | |O|X| | | |O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X|X| | |O| | | |O|X| | |X|X| |O|O| | | |O|X| 'X' Won! |X|X|X| |O|O| | | |O|X| | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | |O|X| | |X| |O| 'X' Won! |O|X| | |O|X| | |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| | | | | |X|X| | |O|O|X| | |O| | |X|X| | |O|O|X| |X|O| | |X|X| | 'O' Won! |O|O|X| |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X| | | |O|O| | |X| |O| |X| | | |O|O|X| |X| |O| |X|O| | |O|O|X| |X| |O| |X|O|X| |O|O|X| |X| |O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X| | | |X|O| |O| | | |O|X| | | |X|O| |O| |X| |O|X| | 'O' Won! | |X|O| |O|O|X| |O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O|O| | |O| |X| | | |X| 'X' Won! |O|O|X| |O| |X| | | |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O| | | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | 'X' Won! |O|O| | |O|X| | |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | |X|X| | | |O| |O| |X| | |X|X| | |O|O| |O| |X| | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | |O| | |X| | | |O|X| | |X|O| | |X| | | |O|X|O| |X|O| | |X|X| | |O|X|O| |X|O| | |X|X|O| |O|X|O| |X|O| | 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O| | |O|X|X| | | | | | |O|O| |O|X|X| | | | | |X|O|O| |O|X|X| | | | | |X|O|O| |O|X|X| | |O| | 'X' Won! |X|O|O| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| |X| | | |X| 'O' Won! |O|O|O| |X| |X| | | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| |X|X|O| | |O| | | | |X| |X|X|O| | |O|O| 'X' Won! | | |X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | |O| |X| | | |X| |O| | | |O|X|X| | | |X| |O| | | |O|X|X| | | |X| |O| |O| |O|X|X| | | |X| |O|X|O| 'O' Won! |O|X|X| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | |O| |X| | | | | |O|X| | |O| |X| | | | | |O|X|X| 'O' Won! |O| |X| |O| | | |O|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | |O|O| | | | | | |X|X| | |O|O| | | |X| | |X|X| 'O' Won! |O|O|O| | | |X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| | |X|O| | |X| | |X|O|O| |O|X|O| | |X| | |X|O|O| |O|X|O| | |X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | | | | |X|O| | 'O' Won! |X|O| | | |O| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O|X| |X| | | |O| | | |O|O|X| |X| | | |O|X| | |O|O|X| |X| | | |O|X| | |O|O|X| |X|O| | |O|X|X| |O|O|X| |X|O| | 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| | | |X|O| | | |O|X| |X| |O| |X|O| | 'X' Won! |X|O|X| |X| |O| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | |X| | |X| |O| |O| |O| | |X| | |X|X|O| |O| |O| | |X| | |X|X|O| |O| |O| |O|X| | |X|X|O| |O| |O| |O|X|X| 'O' Won! |X|X|O| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X|O|O| | | | | | | |X| |X|O|O| | | | | | | |X| |X|O|O| |O| | | | | |X| |X|O|O| |O|X| | |O| |X| |X|O|O| |O|X| | |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| |X| |O| |O| | | |X| |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | | |X| |O|O|X| |O|X|O| |X| |X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |O| | |O| | | | |X| |X| |O| |O|O| | | | |X| |X| |O| |O|O| | | |X|X| |X| |O| |O|O| | | |X|X| |X|O|O| 'X' Won! |O|O| | |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | | | |O| |O|O|X| | |X| | | |X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| | |X| | |O|X|O| |O|O|X| |X|X| | 'O' Won! |O|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | | | |X| |O|O| | |X| | | |O| |X| |O|O| | |X| |X| |O| |X| 'O' Won! |O|O|O| |X| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | |O| | | |O| | |X| |X| | |O| | | |O|X| |X| |X| 'O' Won! | |O| | | |O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X|O| |X| | | | | |O| |O|X|O| |X| | | | |X|O| |O|X|O| |X| | | |O|X|O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| | |X|X| | |O|O| | |O|X| | |X|X| | |O|O| | |O|X|X| |X|X| | 'O' Won! |O|O|O| |O|X|X| |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | |X| | | | | | |X|O| | | |X| | | | | |O|X|O| | | |X| | |X| | |O|X|O| | | |X| | |X|O| |O|X|O| | | |X| |X|X|O| |O|X|O| |O| |X| |X|X|O| |O|X|O| 'X' Won! |O|X|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X|O|X| | |O| | |O| | | |X|O|X| |X|O| | |O| | | 'O' Won! |X|O|X| |X|O| | |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| |X| | | | |X| | |O|O|X| |X|O| | | |X| | |O|O|X| |X|O|X| |O|X| | |O|O|X| |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| |X| |X| |O| | | | | |O|O|X| 'X' Won! |X| |O| | |X| | |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |O| |X| |O| |O|O|X| |X|X|O| |X| |O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X|X| | | |O| |O| | | | |X|X| 'X' Won! | | |O| |O| | | |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| | |O| | |X| | | |X| |O| | |O|O| |X| | | |X| |O| | |O|O| |X|X| | 'O' Won! |X| |O| | |O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | |X| |O| |O|X| | | | | | |X| |O| |O|X|O| | |X| | |X| |O| |O|X|O| | |X| | |X|O|O| |O|X|O| |X|X| | |X|O|O| |O|X|O| 'O' Won! |X|X|O| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X|X| | | |O|O| 'O' Won! | | | | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X|O| | | |O| | |X| | | |X|O| | | |O| | |X| |O| |X|O| | | |O| | 'X' Won! |X| |O| |X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| | |O| | | |O| | | | |X| |X|O| | | |O| | |O| |X| |X|O| | | |O| | |O|X|X| |X|O| | | |O| | |O|X|X| |X|O| | |O|O| | |O|X|X| |X|O| | |O|O|X| It's a stalemate! |O|X|X| |X|O|O| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |X| | |O| | | |O| |X| | |X| | |O| |X| |O| |X| | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O| |X| |X|X| | |O|O|X| |O| |X| |X|X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| |X| | | |O| |X|X|O| |O| |X| 'X' Won! |X| |O| |X|X|O| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| |O|X| | | |O|X| |X|X|O| |O|X| | | |O|X| |X|X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | |O| |O| |O|X|X| | | | | |O|X|O| |O|X|X| | | | | 'O' Won! |O|X|O| |O|X|X| |O| | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | |O|X| | | | |O| | |X| | |O|X| | | | |O| | |X|X| |O|X| | | | |O| |O|X|X| |O|X|X| | | |O| |O|X|X| 'O' Won! |O|X|X| |O| |O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | |O| | | | |X| |X| |O| |X|O| | | | |X| |X| |O| |X|O| | |O| |X| |X| |O| |X|O|X| |O| |X| |X| |O| |X|O|X| |O|O|X| |X| |O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O|O| | | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | |O|X| | |X| | |O|O|X| | |O|X| |O|X| | |O|O|X| | |O|X| 'X' Won! |O|X|X| |O|O|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | |O| | | |X| | | | |O| | |O|X| | |X| | | | |O| | |O|X| |O|X| | | | |O| | |O|X| |O|X| | | |X|O| |O|O|X| |O|X| | | |X|O| |O|O|X| |O|X|X| | |X|O| 'O' Won! |O|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | |O|O| | |X|X| 'O' Won! | |X| | |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| | |O| | |O| |X| |X|O|X| |X|O| | |O| |X| |X|O|X| 'O' Won! |X|O| | |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X| | |O| |X| |O| | | | |X| | |O| |X| |O| |X| | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O| |X| |X|X| | |O|O|X| |O| |X| |X|X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| |O| |X| | | | |O| | |X|X|O| |X|O| | | |O| | |X|X|O| 'X' Won! |X|O| | |X|O| | |X|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | |O| |X|O| | |X|X| | | | |O| |X|O| | |X|X|O| | | |O| 'X' Won! |X|O| | |X|X|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X|X| |O| | | |O|O| | | |X|X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O|O| | 'X' Won! |O|O|X| |X|X|X| |O|O| | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | 'O' Won! |X| |O| | |O|X| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | | |X|O| 'O' Won! |O| |X| |X|O| | | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | |O| | | |X|O| |X| |X| | |O| | | |X|O| |X|O|X| | |O| | | |X|O| |X|O|X| | |O| | |X|X|O| |X|O|X| |O|O| | |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| |O| | | | | |O| | | |X| |O| | | | | |O| |X| |X| |O| |O| | | |O| |X| |X| |O| |O| |X| |O| |X| |X| 'O' Won! |O|O|O| |X| |O| |X| |X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |O| | |X|X| | |O| | | | |O| | |X|X| | |O| | | |O|O| 'X' Won! |X|X|X| | |O| | | |O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X|X| | |O| |O| 'O' Won! | | | | |X|X| | |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O|X| |O|X| | |O|X|O| | |O|X| |O|X| | |O|X|O| |X|O|X| It's a stalemate! |O|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| |O|O| | | | | | 'X' Won! |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O|O| |X| | | |X|X| | | |O|O| |X| |O| |X|X| | | |O|O| |X|X|O| |X|X| | | |O|O| 'O' Won! |X|X|O| |X|X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | |X|O|X| | | | | | |O| | |X|O|X| | | | | | |O|X| |X|O|X| | | | | |O|O|X| |X|O|X| | |X| | |O|O|X| |X|O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |X|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O|X| | | | | | | |X| |O|O|X| |O| | | 'X' Won! | | |X| |O|O|X| |O| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |X| | | | |X| |O|O| | | |X| | | | |X| |O|O| | | |X|X| | |O|X| |O|O| | | |X|X| 'X' Won! | |O|X| |O|O| | |X|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | |X| |O| |O| |X| |O| | | |X| |O| |O| |X| |O| |X| |X| |O| 'O' Won! |O| |X| |O|O|X| |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | |O| |X| | | |O|X|O| | | |O| |X| |X| |O|X|O| | |O|O| |X| |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'O' Won! |X|O|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | |X| |O| |O|X| | | |X| | |X| |O| |O|X| | |O|X| | 'X' Won! |X|X|O| |O|X| | |O|X| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | 'X' Won! | |O|X| | |X| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X| |O| | |O| | | |X|O| |X|X|O| | |O| | |O|X|O| |X|X|O| | |O| | |O|X|O| |X|X|O| |X|O| | 'O' Won! |O|X|O| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O|X| |X| | | |X|O|O| | |O|X| |X| | | 'X' Won! |X|O|O| |X|O|X| |X| | | | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | |O|X| |O|X| | | | |X| | |O|X| |O|X| | |O| |X| | |O|X| |O|X| | |O| |X| |X|O|X| |O|X| | |O|O|X| |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | | | |X| |O|X| | | |O|O| | | |X| |O|X| | | |O|O| | | |X| |O|X|X| 'O' Won! |O|O|O| | | |X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | | |X| 'O' Won! |X|X| | |O|O|O| | | |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| | | |O| |O|X| | | | |X| |X| |O| |O|X| | | | |X| |X| |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X|O|X| |X| |O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X|O| | | |O| | |X| |O| |X|O|X| | |O| | |X| |O| |X|O|X| | |O|O| |X| |O| 'X' Won! |X|O|X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| |O| | |O| | |X| | | |X| |O| | |O| | |X|X| | |X| |O| | |O| | |X|X|O| |X|X|O| | |O| | |X|X|O| 'O' Won! |X|X|O| | |O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | |O|X| | | |O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X|O| |X|O| | |X|O| | |O|X|O| |X|O| | |X|O|X| It's a stalemate! |O|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | |O| | | | | | |O| |X| | |O| | | | | | |O|X|X| | |O| | | |O| | |O|X|X| |X|O| | | |O| | |O|X|X| |X|O| | | |O|O| |O|X|X| |X|O|X| | |O|O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | | | |O| | | |X|X|O| | | | | |O|X| | |X|X|O| | | | | |O|X|O| 'X' Won! |X|X|O| | |X| | |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | | |O| |O|X| | | |X| | | | |O| |O|X|X| | |X| | | |O|O| |O|X|X| | |X|X| | |O|O| |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | |O| | |O| | | |X| | |X| |O| | |O| | | |X| | |X| |O| | |O| | | |X|O| |X|X|O| | |O| | | |X|O| |X|X|O| |O|O| | | |X|O| |X|X|O| |O|O| | |X|X|O| 'O' Won! |X|X|O| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| |X| | | |X| |O| | | |O| 'O' Won! |X| |O| |X| |O| | | |O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | |X|O| | | | | |X| | | | |X|O| | | |O| |X| |X| | |X|O| | | |O| |X| |X| | |X|O| |O| |O| |X| |X| |X|X|O| |O| |O| |X|O|X| |X|X|O| |O| |O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | |X| | | |O|X|O| |X|O| | |X| | | |O|X|O| |X|O| | |X| |X| |O|X|O| |X|O|O| |X| |X| |O|X|O| 'X' Won! |X|O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| | | |O|O| | | |O|X| |X| |X| |O|O| | |O|O|X| |X| |X| |O|O| | 'X' Won! |O|O|X| |X| |X| |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O| | | | |X| | |O|X| | |O| | | | |X| |X|O|X| |O|O| | | | |X| |X|O|X| |O|O| | | |X|X| |X|O|X| |O|O| | |O|X|X| |X|O|X| 'X' Won! |O|O|X| |O|X|X| |X|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | |O| | |O| | | |X| |X| | |O|X| |O| | | |X| |X| | |O|X| |O|O| | 'X' Won! |X| |X| | |O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | |O| | |X|O| |O|X| | | | |O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X| | | |O|O| |X|X|O| |O|X|X| 'O' Won! |O|O|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | |O| |O| |O|X|X| | | | | |O| |O| |O|X|X| |X| | | |O| |O| |O|X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O| |X|X|O| |O| | | | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O| |X| |O| |O| |X|X|O| |O|X|X| |O| |O| |X|X|O| 'O' Won! |O|X|X| |O|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| |X|O|O| | |O| | | | |X| |X|O|O| |X|O| | | | |X| |X|O|O| |X|O| | |O| |X| |X|O|O| |X|O|X| |O| |X| |X|O|O| 'O' Won! |X|O|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | |O|X| | |X| | |O| | | |X|O|X| | |X| | |O| | | |X|O|X| | |X| | |O| |O| |X|O|X| | |X| | |O|X|O| |X|O|X| | |X|O| |O|X|O| It's a stalemate! |X|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | | |O| |X|O| | | | |X| | | |O| |X|O| | | | |X| | |O|O| |X|O| | |X| |X| | |O|O| 'O' Won! |X|O| | |X| |X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O|O| |X| | | |O|X| | |X|O|O| |X| |X| |O|X| | 'O' Won! |X|O|O| |X|O|X| |O|X| | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | | |O|O| | |O|X| |X|X|O| | |O|O| | |O|X| |X|X|O| | |O|O| |X|O|X| 'O' Won! |X|X|O| |O|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | |X| | | |X| |O|O| | | | |X| |X| |X| |O|O| | | | |X| |X|O|X| |O|O| | | | |X| |X|O|X| |O|O| | |X| |X| 'O' Won! |X|O|X| |O|O| | |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | |X|O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| |X|O| | | |O|X| 'O' Won! | |X|O| |X|O| | |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | |O| | | | |O| | |X| | | |O| |X| | |O| | |X| | | |O| |X| |O|O| | |X| | | |O| |X| |O|O| | |X|X| | |O|O|X| |O|O| | |X|X| | |O|O|X| |O|O|X| |X|X| | 'O' Won! |O|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| |O| | |X|X| |O|O| | |X| |O| | |X|X| |O|O| | |X|O|O| | |X|X| |O|O|X| |X|O|O| 'O' Won! |O|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| |X| | | |X| |O| | | |O|O|X| |X| |X| |O| | | |O|O|X| |X| |X| |O|O| | |O|O|X| 'X' Won! |X| |X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | |O|X| | | | |O| | |X| | |O|X| | | |O|O| | |X| | |O|X| | | |O|O| | |X|X| |O|X| | | |O|O| | |X|X| |O|X|O| 'X' Won! | |O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O|O| | | |X| | |X| |X| |O|O| | | |X| | |X|O|X| |O|O| | |X|X| | |X|O|X| |O|O| | 'O' Won! |X|X|O| |X|O|X| |O|O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | |X| | |X| | | |O| |O| | |X| | |X| |O| |O| |O| | |X| | |X|X|O| 'O' Won! |O|O|O| | |X| | |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| | | |O| | | |X| |O|X|X| | | |O| | | |X| |O|X|X| | |O|O| | |X|X| |O|X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O|X|O| | | | | |X| | | |O|X|O| | | |O| |X| | | |O|X|O| | |X|O| |X| | | |O|X|O| |O|X|O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| |O| |X| | |O|O| | |X|X| |O| |X| | |O|O| | |X|X| |O|O|X| | |O|O| 'X' Won! |X|X|X| |O|O|X| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O|X| | |X| |X| |O| |O| |O|X|O| |X| |X| |O| |O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X|X| |X|O| | |O| | | |O|X|X| |X|O| | |O| |X| |O|X|X| |X|O| | |O|O|X| |O|X|X| 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O|O| | | | |X| |X| | | |O|O| | 'O' Won! | | |X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X| |X| | |O| | |O| | | |X|O|X| | |O| | |O| | | |X|O|X| | |O| | |O|X| | |X|O|X| | |O| | |O|X|O| |X|O|X| |X|O| | |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O|X|X| | |X| | 'O' Won! |O| | | |O|X|X| |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X|O| | 'X' Won! |X| | | |X|O| | |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| | | | | |X| | | | |O|X| | |O| | |X| | | |X|O|X| | |O| | 'O' Won! |X|O| | |X|O|X| | |O| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | |X| |O| | |O|X| | | | | |X| |O| | |O|X| | | |O| |X| |O| | |O|X| |X| |O| |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | |X| | |O| | |O|X| | |O| |X| | |O| | |O|X| | |O| |X| |X|O| | |O|X| | |O| |X| |X|O|O| |O|X| | |O| |X| |X|O|O| |O|X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O|O| | | |O|X| | |X|X| |O|O| | | |O|X| | |X|X| |O|O| | |O|O|X| 'X' Won! | |X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| |X| |O| |X| | | |O| |O| |X| |O| |X| | | |O| |O|X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |X| | | | |X|O| | | | | |X| |O| | |X|O| |X| | | |X| |O| |O|X|O| |X| | | |X| |O| |O|X|O| |X|X| | |X| |O| 'O' Won! |O|X|O| |X|X|O| |X| |O| | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| |X| | | | |O| | |X| |O| |X| |O| | |O| | |X| |O| |X|X|O| | |O| | |X| |O| |X|X|O| |O|O| | 'X' Won! |X| |O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | |X|O| | |X| | | | |O| |O|X|O| | |X| | | | |O| |O|X|O| |X|X| | | | |O| |O|X|O| |X|X| | |O| |O| 'X' Won! |O|X|O| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O|X| | | | | | |X|O|O| |O|X| | | | | | |X|O|O| |O|X|X| | | | | |X|O|O| |O|X|X| | | |O| |X|O|O| |O|X|X| | |X|O| It's a stalemate! |X|O|O| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | | | |O| | |X|X| | | | |O| | |O| | |X|X| | | |X|O| | |O| | |X|X|O| | |X|O| |X|O| | |X|X|O| | |X|O| 'O' Won! |X|O|O| |X|X|O| | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O| | |X| |X| |O| |O| |X|O| | |X| |X| |O| |O| |X|O|X| |X| |X| |O| |O| |X|O|X| |X|O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |O| | | |X|X| | | |O| | |O| | | |X|X| | | |O| |O|O| | 'X' Won! |X|X|X| | | |O| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X|O| | | |X| | | |O| | 'X' Won! |X|O| | | |X| | | |O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | | | | | |X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | |X| |O|X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| |O| |X| |O| | | | |X|X| |O| |X| |O| | | | |X|X| |O|O|X| |O| | | 'X' Won! | |X|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | |O|X| | | | |X| 'X' Won! |X|O| | |O|X| | | | |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| | |X| |O| | |X| | |O|O|X| |X| |O| | |X|O| |O|O|X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | |X| |O|O| | |X| | | 'O' Won! | | |X| |O|O|O| |X| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| |O| | | | | |O|X| | |X| |O| |X| | | |O|X| | |X| |O| |X| |O| |O|X| | |X| |O| |X|X|O| 'O' Won! |O|X|O| |X| |O| |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| | | |O|X| | |X| | | |O| | |O|O|X| | |X| | | |O|X| |O|O|X| | |X| | | |O|X| |O|O|X| |O|X| | |X|O|X| |O|O|X| |O|X| | It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| 'O' Won! |O|O|O| | | | | |X| |X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| |X| | | | | |X| | |O|O| |X| |O| | | |X| |X|O|O| |X| |O| 'O' Won! |O| |X| |X|O|O| |X| |O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| |X| | | | |O|X| | | |O| |X|O| | | |O|X| | | |O| |X|O| | | |O|X| | |X|O| |X|O| | | |O|X| |O|X|O| |X|O|X| | |O|X| |O|X|O| It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| |O| |O| | | | | | |X|X| |O| |O| | | |X| | |X|X| |O| |O| | | |X| |O|X|X| |O| |O| |X| |X| |O|X|X| 'O' Won! |O|O|O| |X| |X| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | |X|O| | | | |X| | |O| | |X|O|O| | | |X| | |O|X| |X|O|O| | | |X| |O|O|X| |X|O|O| | | |X| |O|O|X| |X|O|O| | |X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O| | | | |X|X| | |O| | |O| |O| | |X|X| | |O| | |O| |O| | |X|X| |X|O| | |O| |O| | |X|X| |X|O|O| 'X' Won! |O| |O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | | |O|X| |X| | | | | |O| | |O|X| |X| |X| | | |O| | |O|X| |X|O|X| | | |O| | |O|X| |X|O|X| | |X|O| 'O' Won! |O|O|X| |X|O|X| | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | |X| |X| | | | | | |O| | |X| |X| | | | | | |O|O| 'X' Won! |X|X|X| | | | | | |O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | |O|O|X| | | | | |X| | | |O|O|X| | | |X| |X| | | |O|O|X| |O| |X| |X| | | 'X' Won! |O|O|X| |O| |X| |X| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| | |O|X| | |X| | | |O|O| |X|O|X| | |X| | | |O|O| |X|O|X| | |X|O| | |O|O| 'X' Won! |X|O|X| | |X|O| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | |X| | | |X|O| | | | | | |X| | | |X|O|O| | | | | |X| |X| |X|O|O| | | | | |X| |X| |X|O|O| | | |O| 'X' Won! |X|X|X| |X|O|O| | | |O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| |X| |X| |X| |O| |O| |O|O|X| 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| |O| | | | | | | | |O|X| |O| | | | |X| | | |O|X| |O| |O| | |X| | | |O|X| |O|X|O| | |X| | | |O|X| |O|X|O| | |X|O| | |O|X| |O|X|O| |X|X|O| | |O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| |O| |X| |X| |O| | | |X| |O| |X| |X| |O| |O| |X| |O| 'X' Won! |X| |X| |O|X|O| |X| |O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | |X| |X| |O| | | |O|O| | |X| |X| |O| | | |O|O| | |X| |X| |O| |X| |O|O| | |X| |X| |O|O|X| 'X' Won! |O|O| | |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O| | 'O' Won! |X|O|X| |O|O| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| |X|O| | | | |O| |O|X|X| |X|O|O| | |X|O| |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O| | | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | |X| |X| |O| |X| 'O' Won! |O|O|O| |X| |X| |O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| |O|X| | |O| | | | | |X| |O|X|X| 'O' Won! |O| | | |O| |X| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O|X|X| | | |O| | | | | |O|X|X| | | |O| | | |O| |O|X|X| |X| |O| | | |O| |O|X|X| |X| |O| |O| |O| |O|X|X| |X|X|O| |O| |O| 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| |O| | | | |X| | | | |O| |O| | | | |X| | |X| |O| |O| | | | |X|O| |X| |O| 'X' Won! |O| |X| | |X|O| |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | | | |X|X| | |O| | | | |O| | |X|X| |X|O| | | | |O| | |X|X| |X|O| | | |O|O| | |X|X| |X|O| | |X|O|O| 'O' Won! |O|X|X| |X|O| | |X|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| |O|O| | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | | | | |O|X|X| |O|O| | | | |X| |O|X|X| |O|O| | | |O|X| |O|X|X| 'X' Won! |O|O|X| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | |O| | | | | |O| | |X| | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X|O| |O| |X| |X| |O| | |X|O| |O| |X| |X| |O| |O|X|O| |O|X|X| |X| |O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | |X|X| | | | | |O| |O| | |X|X| | | |O| |O| |O| | |X|X| |X| |O| |O| |O| | |X|X| |X|O|O| 'X' Won! |O| |O| |X|X|X| |X|O|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | | |O| | |X| |X| |O|X| | | |O| | |X| |X| |O|X| | | |O|O| |X| |X| |O|X|X| | |O|O| 'O' Won! |X| |X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | |X| | | |X| |O| | | | | |X| | | |X|O|O| | | | | 'X' Won! |X| | | |X|O|O| |X| | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | | |X| |O| | | | |X|O| | | |X| |O|X| | | |X|O| | | |X| |O|X| | | |X|O| |O| |X| |O|X|X| | |X|O| |O| |X| |O|X|X| | |X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| |O| |X| |X| | | |O| |O| |O|X|X| |X|O| | |O| |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| |O|O| | | |O| | | |X|X| |O|O| | |X|O| | | |X|X| |O|O| | |X|O| | |O|X|X| |O|O| | |X|O|X| |O|X|X| 'O' Won! |O|O|O| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | |O| | | |X| | |X| |O| |X|O| | | |X| | |X| |O| |X|O| | | |X| | |X|O|O| 'X' Won! |X|O|X| | |X| | |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X|X|O| 'O' Won! | | |O| | | |O| |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X|X| | |O| | |O| |X| |O|X|X| | |O| | |O|X|X| |O|X|X| | |O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | |O| | | | |X|X| | |O| | |O| |X| | |X|X| | |O| | |O| |X| | |X|X| | |O|O| |O|X|X| | |X|X| | |O|O| |O|X|X| |O|X|X| | |O|O| 'X' Won! |O|X|X| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |O| | | | | |O|X| | | |X|O| | | | | |O|X|O| | |X|O| 'X' Won! | |X| | |O|X|O| | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O|O|X| | | | | | | | | |O|O|X| | |X| | | | | | |O|O|X| |O|X| | | | | | |O|O|X| |O|X| | | |X| | |O|O|X| |O|X| | | |X|O| 'X' Won! |O|O|X| |O|X| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | |X| | | |O| | |O| |X| | |X| | | |O| | |O|O|X| | |X| | |X|O| | |O|O|X| | |X| | |X|O|O| |O|O|X| |X|X| | |X|O|O| |O|O|X| 'O' Won! |X|X|O| |X|O|O| |O|O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X| |X| | |O| | |O|O| | |X| |X| | |O|X| |O|O| | |X| |X| 'O' Won! | |O|X| |O|O|O| |X| |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | |O|X| | | |X| |X| |O| | |O|X| |O| |X| |X| |O| | |O|X| |O| |X| |X|X|O| | |O|X| |O| |X| |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O|X| |X| | | | | |O| |O|O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| | | |X| | | |O| | |X|O| | | |X| |O| |O| | |X|O| | | |X| |O|X|O| |O|X|O| | | |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| | | |O|X|X| | | | | |O| | | |O|X|X| | | | | |O| |O| |O|X|X| |X| | | |O| |O| 'O' Won! |O|X|X| |X| | | |O|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X|O| | |O|O| |X| | | |X|X|O| | |O|O| 'O' Won! |X| |O| |X|X|O| | |O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O|O| |X| | | | |X| | |X|O|O| |X| |O| | |X| | |X|O|O| 'X' Won! |X| |O| |X|X| | |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X|O| | |O| | | | | | |X|X|O| | |O|X| | | | | |X|X|O| |O|O|X| | | |X| |X|X|O| |O|O|X| | |O|X| |X|X|O| |O|O|X| 'X' Won! |X|O|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | | |O| | |O|X|X| 'O' Won! |O|X|O| | |O| | |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | | |O| | | |X| |X| |O| | | |O| |O| |X| |X| 'X' Won! |O| | | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | |X| |X| |O|O| | | | |O| |X| |X| |O|O| | | |X|O| |X| |X| |O|O| | |O|X|O| |X| |X| |O|O| | |O|X|O| |X| |X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | | |O| | |X|O| | |X| | | | |O| | |X|O| |O|X| | | | |O| 'X' Won! | |X|O| |O|X| | | |X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | |O|X|X| |X|O|X| |O|O| | |O|X|X| 'O' Won! |X|O|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | |X|O| | |X| | | |O| | | |X|O|O| |X| | | |O| | | |X|O|O| |X|X| | |O| | | |X|O|O| |X|X| | |O|O| | 'X' Won! |X|O|O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | |O|X|O| | | |X| | | | | |O|X|O| | | |X| | | |X| |O|X|O| |O| |X| | | |X| |O|X|O| |O| |X| |X| |X| |O|X|O| |O| |X| |X|O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | |X| | | | | | |O|X| | | |X| | |O| | | |O|X| | | |X| | |O| |X| |O|X| | | |X|O| |O| |X| |O|X| | | |X|O| |O| |X| |O|X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | |X|X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | | |X|O|O| |O|X|X| | | | | |X|O|O| |O|X|X| |X| | | |X|O|O| 'O' Won! |O|X|X| |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | |X|X| | | |O|O| | | | | |X|X| | |X|O|O| | | | | |X|X| | |X|O|O| | | |O| |X|X| | 'X' Won! |X|O|O| |X| |O| |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | |O| | |X| | |X|O| | | | |O| |X|X| | |X|O| | | |O|O| |X|X| | |X|O| | 'X' Won! | |O|O| |X|X|X| |X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| | | |O|O| |X| |X| |O|X| | | |O|O| |X| |X| |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | |O|X|O| | | | | | |X| | |O|X|O| | | | | |O|X| | |O|X|O| | | | | 'X' Won! |O|X| | |O|X|O| | |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | |O| | |X|X| |X| |O| | | |O| | |X|X| |X| |O| |O| |O| | |X|X| 'X' Won! |X| |O| |O|X|O| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| |X| | | | |X| | | | |O| |X|O| | | |X| | | | |O| |X|O|X| | |X| | | | |O| |X|O|X| | |X|O| | | |O| 'X' Won! |X|O|X| | |X|O| |X| |O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | |O|X| | |X| |O| | | | | |O|X|X| |X| |O| | | | | |O|X|X| |X| |O| | |O| | |O|X|X| |X| |O| |X|O| | |O|X|X| |X| |O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | | | |O|O| |X| | | |X| | | | |O|O| |X| | | |X| |O| | |O|O| |X|X| | |X| |O| | |O|O| 'O' Won! |X|X| | |X| |O| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | |O|O|X| | |X| | | | |X| |O|O|X| | |X| | | |O|X| |O|O|X| | |X| | | |O|X| |O|O|X| |X|X| | | |O|X| |O|O|X| |X|X|O| It's a stalemate! |X|O|X| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X|O| | |O|X|X| |O| |O| |X|O|X| |O|X|X| |O| |O| 'O' Won! |X|O|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| |O| | | | | | | |O| |X| |O| | | | | |X| |O| |X| 'O' Won! |O| | | |O| |X| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | | |X| | |X| |O| |O| |O| | |X| | |X| |O| |O| |O| | |X|X| |X| |O| |O| |O| |O|X|X| |X| |O| |O|X|O| |O|X|X| |X| |O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | |O| | | | |X|X| |X|O| | |O| | | | |X|X| |X|O| | |O| |O| | |X|X| |X|O|X| |O| |O| | |X|X| |X|O|X| |O| |O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O|X| | |O|O| | | | |X| |O|X| | |O|O|X| | | |X| |O|X| | |O|O|X| | |O|X| |O|X| | 'X' Won! |O|O|X| | |O|X| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | | | | |O|X|X| |O|O| | | | | | |O|X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | |X| | |X| | | | | |O| |O|X| | |X| | | | | |O| |O|X| | |X| | | | |X|O| |O|X| | |X| |O| | |X|O| 'X' Won! |O|X| | |X|X|O| | |X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | |X|O| | | |X| | | | | | |X|O| |O| |X| | | | | |X|X|O| |O| |X| | |O| | |X|X|O| |O| |X| | |O|X| |X|X|O| |O| |X| |O|O|X| |X|X|O| |O| |X| It's a stalemate! |O|O|X| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | |O| | |X|X| | | |O| | |X|O| | |X|X| | |O|O| | |X|O| | |X|X| | |O|O| | |X|O|X| |X|X| | 'O' Won! |O|O| | |X|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | |X| |O| |X| | | | | | | |X|O|O| |X| | | | | |X| |X|O|O| |X| | | | |O|X| |X|O|O| |X| | | | |O|X| |X|O|O| |X|X| | | |O|X| |X|O|O| |X|X|O| 'X' Won! |X|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | |O|X| | |O|X| | | | | | |O|X| | |O|X| | | | |X| |O|X| | |O|X|O| | | |X| 'X' Won! |O|X| | |O|X|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | |O|O| | | | | | |X| | |X|O|O| |O| | | | |X| | |X|O|O| |O| | | | |X|X| |X|O|O| |O| | | |O|X|X| |X|O|O| |O|X| | |O|X|X| |X|O|O| It's a stalemate! |O|X|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X|O| | | | | | | |X|O| |X|O| | | | | | |O|X|O| |X|O| | | | | | |O|X|O| |X|O|X| | | | | |O|X|O| |X|O|X| | |O| | |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| | |X| | |O|X| | | |O|X| | |X| | |O|X| | |O|O|X| 'X' Won! |X|X| | |O|X| | |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O| |O| | | | | | |X|X| 'O' Won! |O|O|O| | | | | | |X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| |X| | |O| | |O|X| | |O| |X| | |O| | |O|X| | |O|X|X| 'O' Won! |O|O| | |O|X| | |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | |X| | |X| | |O| | | | |O|X| | |X| | |O| | | |X|O|X| | |X| | |O| | | |X|O|X| | |X|O| |O| | | |X|O|X| | |X|O| |O|X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | |O| | |X| |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X| |O| |O| | | | |O|X| |X|X|O| |O| | | | |O|X| |X|X|O| |O| |O| | |O|X| |X|X|O| |O|X|O| It's a stalemate! |O|O|X| |X|X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | | | |X| | |O|X|O| 'X' Won! |O|X| | | |X| | |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O|X| |X| | | | |O| | | |O|X| |X| |O| |X|O| | | |O|X| |X| |O| |X|O|O| | |O|X| |X| |O| |X|O|O| | |O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| 'O' Won! | |O| | | |O|X| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| | | | | |O| | | |X|X|O| | | |X| |O| | | |X|X|O| |O| |X| |O| | | |X|X|O| |O|X|X| |O| | | |X|X|O| |O|X|X| |O|O| | 'X' Won! |X|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| |X| | |O|X| |O| | | |X| |X| |O|O|X| 'X' Won! |O| | | |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | |X| |O| |O| |X| | | | | |X| |O| |O| |X| |X| | | |X| |O| |O| |X| |X| | |O|X| |O|X|O| |X| |X| | |O|X| |O|X|O| |X|O|X| | |O|X| It's a stalemate! |O|X|O| |X|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X| |X| | | |O|O| | | | |X| |X| | | |O|O| | | |X|X| |X| |O| |O|O| | | |X|X| 'X' Won! |X| |O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | |O| | | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| |O| |O|X| | | | |X| |O|X|O| 'O' Won! |O|X| | | |O|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | |X| |X| | |O| | | | | | |X| |X| |O|O| | | | | | |X| |X| |O|O| | | |X| | |X| |X| |O|O| | |O|X| | |X| |X| |O|O| | |O|X|X| |X|O|X| |O|O| | |O|X|X| 'X' Won! |X|O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| |O|X| | | |O|O| | | |X| |O|X| | | |O|O| | |X|X| 'O' Won! |O|X| | |O|O|O| | |X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| |X| |X| |X| | |O|O| |O| |X| 'X' Won! |X|X|X| | |O|O| |O| |X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | |O| |X| | | | |O|X| | |X|O| |X| | | | |O|X| |O|X|O| |X| | | | |O|X| |O|X|O| |X|X| | | |O|X| |O|X|O| |X|X| | |O|O|X| 'X' Won! |O|X|O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | |O| |X|O| | | | | | |X| |O| |X|O| | | | | | |X|O|O| |X|O| | 'X' Won! |X| | | |X|O|O| |X|O| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | | |O| |X| | |X|O| | |O| | |O| |X| | |X|O| | |O| | |O| |X| |X|X|O| |O|O| | |O| |X| |X|X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | 'X' Won! | | | | |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| |O|O| | | | | | | |X|X| |O|O| | | | | | | |X|X| |O|O| | |O| | | 'X' Won! |X|X|X| |O|O| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | | | |X| |O| |O| |X|X| | |O| |X| |O| |O| |X|X| | 'X' Won! |O| |X| |O|X|O| |X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | |X| | | |O| |X| | | | | |X| | | |O| |X| |O| | | |X| |X| |O| |X| |O| | | |X| |X|O|O| |X| |O| | | |X| |X|O|O| |X|X|O| 'O' Won! |O| |X| |X|O|O| |X|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |O| | | |X| | |X| |O| | |O| | | |X| | |X|O|O| | |O| | | |X| | |X|O|O| |X|O| | |O|X| | |X|O|O| |X|O| | |O|X|X| |X|O|O| |X|O| | 'O' Won! |O|X|X| |X|O|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | | |O|X| | | |X| |O| | | | |O|X| | | |X| |O| |O| | |O|X| |X| |X| |O| |O| |O|O|X| |X| |X| |O| |O| 'X' Won! |O|O|X| |X|X|X| |O| |O| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O|O| | |X| | | |X| | | |O|O| | |X|O| | |X| | | |O|O| | |X|O|X| |X| | | |O|O| | 'O' Won! |X|O|X| |X| | | |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| | |X| | | | |X| | |O|O| | |X|O| | | |X| | |O|O| | |X|O| | |X|X| | 'O' Won! |O|O| | |X|O| | |X|X|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |O| | | |X| | | |X|O| | |O| | | |X| | | |X|O|O| |O| | | |X| |X| |X|O|O| |O| | | |X| |X| |X|O|O| |O| |O| |X| |X| |X|O|O| |O|X|O| It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O|X| | |O| | | | |X| | |O|X|O| |O| | | | |X| | |O|X|O| |O| | | |X|X| | |O|X|O| |O|O| | |X|X| | 'X' Won! |O|X|O| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | |X|X| | | | | | |O|O| | |X|X| | | | |X| |O|O| | |X|X| | 'O' Won! | | |X| |O|O|O| |X|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| |X| | |O| | | | |X| |O| |X| |O|O| | | | |X| |O| |X| |O|O| | |X| |X| |O|O|X| |O|O| | |X| |X| 'X' Won! |O|O|X| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | | |X| |O| |X| | | | | |O| |X| |O|X|X| | | | | |O| |X| |O|X|X| | |O| | |O| |X| 'X' Won! |O|X|X| | |O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X|X| | | | |O| |O| | | |X|X| | |X| |O| |O| | | |X|X| | |X| |O| |O| |O| |X|X| | |X|X|O| |O| |O| |X|X| | 'O' Won! |X|X|O| |O| |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O|X| | |X| |O| | |X| | |O|X| | |X|O|O| | |X| | |O|X| | |X|O|O| | |X|X| |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | |X| |O| | | | | | |X|O| |X| |O| | | | | | |X|O| |X| |O| | |X| | | |X|O| |X|O|O| | |X| | | |X|O| |X|O|O| | |X|X| 'O' Won! | |X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O| | |X| |O| | |X|O| |X|O| | |X| |O| 'O' Won! |O|X|O| |X|O| | |X| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X|O| | 'X' Won! |X| |O| |X| |O| |X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | |X| | | | |O| |X| | | | |X| | |O| |O| |X| | | | |X|X| |O| |O| |X| | | |O|X|X| |O| |O| |X| |X| |O|X|X| |O| |O| 'O' Won! |X| |X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| |O| |X|O| | | | | | |X|X|O| |X|O| | | | | | |X|X|O| |X|O|O| 'X' Won! |X| | | |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O|X| | | | |X| | | | | |O|X| | | | |X| |O| | | |O|X|X| | | |X| |O| | | |O|X|X| | | |X| |O|O| | |O|X|X| | |X|X| |O|O| | 'O' Won! |O|X|X| |O|X|X| |O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X|X| | |O| | | |O| | | |X|X| | |O| | 'X' Won! | |O| | |X|X|X| | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O|X| |O| | | | |X| | |O|O|X| |O| |X| | |X| | |O|O|X| |O|O|X| | |X| | |O|O|X| 'X' Won! |O|O|X| | |X|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| | |O| | |X|O|X| | | |O| | |O|X| |X|O|X| |O| |O| | |O|X| |X|O|X| |O|X|O| | |O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O| |X|X| | | | | | | |O|O| |X|X|O| | | | | |X|O|O| |X|X|O| | |O| | |X|O|O| |X|X|O| 'X' Won! |X|O| | |X|O|O| |X|X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| | | |X| 'O' Won! |O|O|O| | | |X| | | |X| | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | | | | |X| |X| | |O| | | | | | |X|O|X| | |O| | | |X| | |X|O|X| | |O|O| | |X| | |X|O|X| 'X' Won! |X|O|O| | |X| | |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | |X|O|O| | | | | |X| | | |X|O|O| | | |O| |X| |X| |X|O|O| | | |O| |X| |X| |X|O|O| |O| |O| 'X' Won! |X|X|X| |X|O|O| |O| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | |X|O| | | | |X| | | | | |X|O| | | | |X| | | |O| |X|O| | | | |X| | |X|O| |X|O| | |O| |X| | |X|O| |X|O|X| |O| |X| | |X|O| |X|O|X| |O| |X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| | | | |X| | |O|O| | |X| | | | |X| | |O|O| |X|X| | | |O|X| | |O|O| |X|X| | 'X' Won! | |O|X| | |O|O| |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | |X| | |O| |X| | | |O| | |X| | |O|O|X| | | |O| | |X|X| |O|O|X| | | |O| |O|X|X| |O|O|X| | |X|O| |O|X|X| |O|O|X| 'O' Won! |O|X|O| |O|X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | |X|O| | |X| | | |O| | | |X|O| | |X| | | |O|X| | |X|O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| |X|X|O| | |X|O| |O|O|X| |X|X|O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | |O| | | | |O|X| | | | | |O|X| | | |O|X| | | | | |O|X|O| | |O|X| 'X' Won! |X| | | |O|X|O| | |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | | |O| | | |X|O| | | | | |X|O| | |O|X|O| | | | | |X|O| | |O|X|O| | | |X| |X|O| | |O|X|O| | |O|X| |X|O| | |O|X|O| |X|O|X| |X|O| | 'O' Won! |O|X|O| |X|O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | | |X| |X| | | |O|O| | |O| |X| |X| | | |O|O| | |O| |X| |X|X| | |O|O| | |O| |X| |X|X|O| |O|O| | |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | |O|O|X| 'X' Won! |X| | | | |X| | |O|O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |X| | |O| |O| | | |X| | |X| | |O| |O| | |X|X| | |X| | 'O' Won! |O|O|O| | |X|X| | |X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X|O| | |O| | | |X| | |X|X|O| | |O| | | |X| | 'O' Won! |X|X|O| | |O| | |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | | | | | | |X| | |O|X| | |O| | | | |X| | |O|X|X| |O| | | | |X| | |O|X|X| |O| |O| | |X|X| |O|X|X| |O| |O| 'O' Won! |O|X|X| |O|X|X| |O| |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X|X| | | | | | | |O| |O|X|X| | |O| | | | |O| |O|X|X| |X|O| | | |O|O| |O|X|X| |X|O| | |X|O|O| |O|X|X| |X|O| | It's a stalemate! |X|O|O| |O|X|X| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | |O| | | | |X| | |X| | | |O| | | |O|X| | |X| | | |O| | | |O|X| | |X| | | |O|X| | |O|X| | |X|O| | |O|X| 'X' Won! |X|O|X| | |X|O| | |O|X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | |O|O| | |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | | | |O| |O|O|X| |X|X| | | | |O| |O|O|X| |X|X| | | |O|O| 'X' Won! |O|O|X| |X|X|X| | |O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| |O| | | |X| | | | | |O| |O| | | |X| | | |X| |O| |O| | | |X| | |O|X| |O| |O| |X| |X| | |O|X| |O| |O| |X|O|X| | |O|X| |O|X|O| |X|O|X| | |O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | |X| |X| | | |O| |X|O| | |X| |X| |O| |O| |X|O| | 'X' Won! |X| |X| |O|X|O| |X|O| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | |X| | | |O|X| | |O|O| | |X| | | |O|X| | |O|O| | |X|X| |O|O|X| | |O|O| | |X|X| |O|O|X| |X|O|O| | |X|X| It's a stalemate! |O|O|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | |X| | | | | | |O| |X| | |X| | | | |O| 'X' Won! |O| |X| | |X| | |X| |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| |O| |X| | | | | | | |O| |O| |X| | |X| | | | |O| |O| |X| | |X| | |O| |O| |O|X|X| | |X| | |O| |O| 'O' Won! |O|X|X| |O|X| | |O| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | | | |O|X|O| |O|X| | | | |X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X| | | |O|X| |O|X|O| |O|X|X| 'O' Won! |O|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | |O| | | | |O| | | |X| | |O| | | | |O|X| | |X|O| |O| | | | |O|X| | |X|O| |O| |X| | |O|X| | |X|O| |O|O|X| | |O|X| | |X|O| |O|O|X| |X|O|X| It's a stalemate! |O|X|O| |O|O|X| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |X| | | |X| | |O| | | |O|X| | |X|X| | |O| | | |O|X| |O|X|X| | |O| | | |O|X| |O|X|X| | |O| | |X|O|X| |O|X|X| |O|O| | |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |X| | | |O| | | |X| | | |X|O| | |O|X| | |X| | | |X|O| |O|O|X| | |X| | | |X|O| |O|O|X| |X|X| | | |X|O| |O|O|X| |X|X|O| | |X|O| 'X' Won! |O|O|X| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | |X| |X| | |O| | | |O| | |X| |X| | |O|X| | |O| | |X| |X| | |O|X| |O|O| | |X| |X| |X|O|X| |O|O| | |X| |X| 'O' Won! |X|O|X| |O|O| | |X|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O| | | | | |O| | | | |X|O| | | | | |O|X| | | |X|O| | | | | |O|X|O| | |X|O| | | |X| |O|X|O| 'O' Won! | |X|O| | |O|X| |O|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| |X| | | |O| |X| |O| |O| |X| | | 'X' Won! |O| |X| |O|X|O| |X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | | |O| | |X|X| | | |O| | | |O| |O|X|X| | | |O| | |X|O| |O|X|X| | |O|O| | |X|O| |O|X|X| 'X' Won! |X|O|O| | |X|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| | | |X| | |O|O| | | |X|O| | |X| | |O|O| | |X|X|O| 'O' Won! |O|X| | |O|O| | |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | | |O| | | | |O| |X|X| | | |O|X| | | |O| |X|X|O| | |O|X| | | |O| |X|X|O| | |O|X| |X| |O| |X|X|O| |O|O|X| |X| |O| It's a stalemate! |X|X|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | |O| |X| | | |O| | | | |X|O| |X| | | |O| | | | |X|O| |X| | | |O| |O| | |X|O| |X|X| | |O| |O| 'O' Won! | |X|O| |X|X|O| |O| |O| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | | | | |O|X| |X| |O| | | | | | |O|X| |X|O|O| | | | | | |O|X| |X|O|O| |X| | | |O|O|X| |X|O|O| |X| | | |O|O|X| |X|O|O| |X|X| | 'O' Won! |O|O|X| |X|O|O| |X|X|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| |X|O| | | |X| | |O|O|X| |X|O| | |X|X| | |O|O|X| |X|O| | |X|X| | |O|O|X| |X|O|O| 'X' Won! |X|X|X| |O|O|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X| | |X|O| | | |X| | |O|X| | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| | |O|X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X|O| | |O|X| | |X|O| | |X|O| | |O|X| | |X|O| | |X|O|O| |O|X| | |X|O|X| |X|O|O| |O|X| | It's a stalemate! |X|O|X| |X|O|O| |O|X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O|X| | | |O| | |X| | | |O|X| | |O|O| | |X| | | |O|X| | |O|O| |X|X| | 'O' Won! | |O|X| |O|O|O| |X|X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | |O|O| |X| |X| | | | | | |O|O| |X|O|X| | | | | | |O|O| |X|O|X| |X| | | | |O|O| 'O' Won! |X|O|X| |X|O| | | |O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | | |O|X| | | |X| |O| | | |O|O|X| | | |X| |O| | | |O|O|X| | |X|X| |O| | | |O|O|X| | |X|X| |O|O| | 'X' Won! |O|O|X| | |X|X| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X|O| | | |X| | |X|O| | |X|O| | | |X| | |X|O|O| |X|O| | | |X| | |X|O|O| |X|O|X| | |X|O| |X|O|O| |X|O|X| 'X' Won! |X|X|O| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | |O| | | |O| |X| | | | | |O| | | |O|X|X| | | | | |O| |O| |O|X|X| | | | | |O| |O| |O|X|X| |X| | | 'O' Won! |O|O|O| |O|X|X| |X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| | |X| | | |O|X| | | |O|O| |X| | | |O|X| | | |O|O| |X| | | |O|X|X| | |O|O| |X| |O| |O|X|X| |X|O|O| |X| |O| |O|X|X| 'O' Won! |X|O|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | |X| | |X|O| | |O| | |O| |X| | |X|O| | |O| | |O|X|X| | |X|O| | |O| | |O|X|X| | |X|O| |O|O| | |O|X|X| |X|X|O| |O|O| | 'O' Won! |O|X|X| |X|X|O| |O|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| |O| |O|X| | | | | | |X|X|O| |O|X| | | | |O| |X|X|O| |O|X| | 'X' Won! | |X|O| |X|X|O| |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X|O| |X| |O| | |X| | | |X|O| 'O' Won! |X| |O| | |X|O| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | |X|X|O| | | | | | | | | |X|X|O| |O| | | | | | | |X|X|O| |O| |X| | | | | |X|X|O| |O|O|X| | |X| | |X|X|O| |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|X| |X|X|O| |O|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | |O|X| | | |O| | | |X| | |O|X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| | |O|X| | |X|O| 'O' Won! |O|O|X| | |O|X| | |X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | |X| | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X|X| |O| | | |X| |O| | |X|X| |O|O| | 'X' Won! |X| |O| |X|X|X| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| |X| | | |O| | |O| | |X| |X| | |X|O| | |O| | |X| |X| |O|X|O| | |O| | |X| |X| 'X' Won! |O|X|O| | |O| | |X|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | |X| |O| |O|O|X| |X| | | |X| |O| |O|O|X| |X| |X| |X| |O| 'O' Won! |O|O|X| |X|O|X| |X| |O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | |X| | | | | |O| | |X| | |X|O| | | | |O| | |X|X| |X|O| | |O| |O| | |X|X| |X|O| | |O|X|O| | |X|X| |X|O| | |O|X|O| |O|X|X| |X|O| | It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | | | | |X|O| | |X| | | | | | |O|X|O| | |X| | | | | | |O|X|O| | |X| | | | |X| |O|X|O| | |X| | |O| |X| |O|X|O| |X|X| | |O| |X| |O|X|O| |X|X|O| |O| |X| 'X' Won! |O|X|O| |X|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X|X| | |O|O|X| | |O|O| |X|X| | |O|O|X| 'X' Won! |X|O|O| |X|X| | |O|O|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | |O| | |X| | | |O| | | | |O| | |X| |X| |O| | | | |O| | |X| |X| |O| |O| | |O| | |X| |X| |O|X|O| | |O|O| |X| |X| |O|X|O| 'X' Won! | |O|O| |X|X|X| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |X| | | |O| | |X| | | |O|X| | |X|O| | |X| | | |O|X| | |X|O| |O|X| | | |O|X| | |X|O| |O|X|X| | |O|X| |O|X|O| |O|X|X| | |O|X| It's a stalemate! |O|X|O| |O|X|X| |X|O|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| |O| |X| | | |O| | |X|O| |O| |X| | |X|O| | |X|O| |O| |X| | |X|O| |O|X|O| 'X' Won! |O|X|X| | |X|O| |O|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | |O| | | | |O|X| |O|X| | |O| |X| | |O|X| 'O' Won! |O|X| | |O| |X| |O|O|X| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | |X| | | |X| | |O| |O| | |X| | | |X| | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | |O|X| | |O| |O| |X|X| | |O|X|X| 'O' Won! |O|O|O| |X|X| | |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | |X| | | | | |X| | |O| | |X| | | | |O|X| | |O| | |X| |X| | |O|X| | |O| | |X| |X| | |O|X| | |O|O| |X| |X| | |O|X| |X|O|O| 'O' Won! |X|O|X| | |O|X| |X|O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | | |O|X| | | | | |O|X| | |O|O|X| | | | | |O|X| | |O|O|X| | |X| | 'O' Won! |O|X| | |O|O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| |X| | | |X| | | | | |O|O|X| | | |X| | | | | |O|O|X| | | |X| | |X| | |O|O|X| | | |X| | |X|O| |O|O|X| | |X|X| | |X|O| |O|O|X| |O|X|X| | |X|O| 'X' Won! |O|O|X| |O|X|X| |X|X|O| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | |O| | | |O|X| | |X| |X| |O| | | |O|X| | |X|O|X| |O| | | |O|X| | |X|O|X| |O|X| | |O|X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O|X| | | | | | |X| | | |O|X| | |O| | | |X| | | |O|X| | |O|X| | |X| | | |O|X| | |O|X|O| |X| | | 'X' Won! |O|X|X| |O|X|O| |X| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O|X| | |X| | | | | | | |O|X|O| |X| | | | | | | |O|X|O| |X| |X| | | | | |O|X|O| |X| |X| | | |O| |O|X|O| |X| |X| |X| |O| |O|X|O| |X| |X| |X|O|O| 'X' Won! |O|X|O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| |X| | | | | |O| | | |X| |X| |O| | | |O| |X| |X| |X| |O| | |O|O| |X| |X| |X| |O| 'X' Won! |X|O|O| |X| |X| |X| |O| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | |O| | | | |O| |X| |X| |X|O| | | | |O| |X| |X| |X|O|O| | | |O| |X| |X| 'X' Won! |X|O|O| | | |O| |X|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| | | |O| 'X' Won! | | |O| |X|X|X| | | |O| | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | |O| | | | |O| |X| | | | |O|X| | | |O| |X| | | |O|O|X| | | |O| |X| | | |O|O|X| | |X|O| |X| |O| |O|O|X| | |X|O| |X| |O| |O|O|X| |X|X|O| It's a stalemate! |X|O|O| |O|O|X| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | | | |X| |X| |O| |O| |O| | | |X| |X| |O| |O| |O| | |X|X| |X| |O| |O| |O| | |X|X| |X|O|O| 'X' Won! |O| |O| |X|X|X| |X|O|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| |O| | |O| | | | | | |X| |O| |X|O| | | |O| | |X| |O| |X|O| | 'X' Won! |X|O| | |X| |O| |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O| | | | |O| | | |X|X| |O|O| | | |O| | | |X|X| |O|O| | |X|O| | | |X|X| 'O' Won! |O|O|O| |X|O| | | |X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| | |O| |X| | |X| | |X|O| | |O| |X| | |X| | |X|O|O| |O| |X| | |X|X| |X|O|O| |O| |X| |O|X|X| |X|O|O| |O| |X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X|X| | | | |O| | | | | |X|X|O| | |X|O| | | | | |X|X|O| 'O' Won! | |X|O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| | | | | | | | | | |X|O|O| | | | | | |X| | |X|O|O| | | | | | |X|O| |X|O|O| | |X| | | |X|O| |X|O|O| | |X| | |O|X|O| |X|O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | |O| | | | | | | | |X| |X|O| | | | | | | |O|X| |X|O| | | | | | | |O|X| |X|O|X| | | | | |O|O|X| |X|O|X| | | | | |O|O|X| |X|O|X| | |X| | |O|O|X| |X|O|X| |O|X| | 'X' Won! |O|O|X| |X|O|X| |O|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | | | |X| |O| |O| | | |X| |O| |X| |O| |O| | | |X| |O| |X| |O|X|O| | | |X| |O| |X| |O|X|O| | |O|X| |O|X|X| |O|X|O| | |O|X| 'O' Won! |O|X|X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | |O|X| | |O| | | | | | | |O|X| | |O|X| | | | | 'O' Won! | |O|X| | |O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | | | | | | |X|O| | |O|X| | |O| | | |X|O| | |O|X| | |O|X| | |X|O| | |O|X| |O|O|X| | |X|O| | |O|X| 'X' Won! |O|O|X| | |X|O| |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O|O| | | | | | | |X| | |O|O| | | |X| |O| |X| | |O|O| | | |X| |O| |X| | |O|O| |X| |X| |O|O|X| | |O|O| |X| |X| |O|O|X| |X|O|O| |X| |X| 'O' Won! |O|O|X| |X|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | |X|O| | | |O| | |X| | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| | |O|O| |X|X| | |O|X|O| |X|O|O| 'O' Won! |X|X|O| |O|X|O| |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | |O| | |X|X| | |X| |O| | |O| | |X|X| | |X| |O| | |O|O| 'X' Won! |X|X| | |X| |O| |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | |O|X| | | |X| | | |O| | |O|X| | | |X| | | |O|O| |O|X| | |X|X| | | |O|O| |O|X| | 'O' Won! |X|X|O| | |O|O| |O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | |X| |X| | | | |O|O| | | |X| |X|O| | |X|O|O| | | |X| |X|O| | 'O' Won! |X|O|O| | |O|X| |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | |X|X| | |O| | |O| | | |O|X|X| | |O| | |O| | | |O|X|X| | |O|X| |O|O| | |O|X|X| | |O|X| 'X' Won! |O|O|X| |O|X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| |O| | | |O| |O| |X| |X| |O|X| | |O| |O| |X| |X| |O|X|O| |O| |O| |X| |X| |O|X|O| |O|X|O| It's a stalemate! |X|O|X| |O|X|O| |O|X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| |X|X| | | | | | |O| |O| |X|X| | | | | | |O| |O| |X|X| | | |X| | |O| |O| |X|X| | |O|X| | 'X' Won! |O|X|O| |X|X| | |O|X| | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| |X| | | | | | | |O| |X| |X| |O| | | | | |O| |X| |X| |O| | | | | |O|X|X| |X| |O| | | |O| |O|X|X| |X|X|O| | | |O| |O|X|X| 'O' Won! |X|X|O| | |O|O| |O|X|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | |X| |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X| | |X|O| 'O' Won! |O| | | |X|O|X| | |X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| |O|X| | | | | | | |O|X| |O|X|X| | |O| | | |O|X| |O|X|X| |X|O| | | |O|X| |O|X|X| 'O' Won! |X|O|O| | |O|X| |O|X|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X|X| | | |O| | | | | | |X|X| | | |O| | |O| | 'X' Won! |X|X|X| | | |O| | |O| | | | | | | | | | | | | | | | | | | | | | | | |X| | | |O| | | | | | | |X| | |X|O| | | | | | | |X| | |X|O| | |O| | | | |X| | |X|O| | |O| | |X| |X| | |X|O| |O|O| | |X| |X| | |X|O| |O|O|X| |X| |X| |O|X|O| |O|O|X| |X| |X| 'X' Won! |O|X|O| |O|O|X| |X|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | |O|O| | | | | |X| | | | |O|O| | | |X| |X| | | 'O' Won! |O|O|O| | | |X| |X| | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O| | |O| |X| | | | | | |O|X| |O| |X| | | | | | |O|X| |O|O|X| | | | | 'X' Won! | |O|X| |O|O|X| | | |X| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O|O| | | | | | | | |X| |O|O|X| |O| | | | | |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| |O| |X| |X| |O|O|X| 'X' Won! |O| |O| |X|X|X| |O|O|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | |X|O| |O|X| | | | | | |O|X|O| |O|X| | | | | | |O|X|O| |O|X|X| | | | | |O|X|O| |O|X|X| | | |O| 'X' Won! |O|X|O| |O|X|X| | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | | | |O| | |O|X| | | | | | | |O|X| |O|X| | | | | | |O|O|X| |O|X| | |X| | | |O|O|X| |O|X| | |X|O| | |O|O|X| |O|X| | |X|O|X| |O|O|X| 'O' Won! |O|X|O| |X|O|X| |O|O|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | | |O| |X| | |O|O| |X| |X| |O| |X| | |O|O| 'O' Won! |X| |X| |O| |X| |O|O|O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | |X| | |X|O| | |O| |O| | |X| | |X|O| | |O| |O| | |X|X| 'O' Won! |X|O| | |O|O|O| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | |O| | |X| | | | | |O| | |O| | |X| | | | |X|O| |O|O| | |X| | | | |X|O| |O|O| | |X| | | |X|X|O| |O|O| | |X| |O| |X|X|O| |O|O| | |X|X|O| |X|X|O| 'O' Won! |O|O|O| |X|X|O| |X|X|O| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | | | |X| | |X|O| |O| | | | | |X| | |X|O| |O| | | | |X|X| | |X|O| |O|O| | | |X|X| | |X|O| |O|O|X| | |X|X| 'O' Won! | |X|O| |O|O|X| |O|X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X|O| | | | | | | |X| | |X|O| | | | | | |O|X| | |X|O| | | | | 'X' Won! | |O|X| | |X|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | | |O| | | |X| |O| |X| | | |O|O| | |X| |O| |X|X| | |O|O| | |X| |O| |X|X| | |O|O| | |X|O|O| 'X' Won! |X|X|X| |O|O| | |X|O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | |X| | |X| |O| | | | | | |X| | |X| |O| | | |O| | |X| | |X|X|O| | | |O| | |X| | |X|X|O| | |O|O| | |X| | |X|X|O| |X|O|O| |O|X| | |X|X|O| |X|O|O| 'X' Won! |O|X|X| |X|X|O| |X|O|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| |O| |O| | | | | |X| |X| |O| |O| | | | | |X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| | | | |X|X| |X|O|O| |O| |O| | |X|X| |X|O|O| |O|X|O| 'O' Won! |O|X|X| |X|O|O| |O|X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| |O| | | |O| | | | | |X| |O|X| | |O| |O| | | |X| |O|X| | |O| |O| |X| |X| |O|X| | |O| |O| |X| |X| |O|X|O| |O|X|O| |X| |X| |O|X|O| 'O' Won! |O|X|O| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O|X| |O| | | |X| | | | |O|X| |O| | | |X| | | |X|O|X| |O|O| | |X| | | |X|O|X| |O|O| | |X|X| | |X|O|X| 'O' Won! |O|O|O| |X|X| | |X|O|X| | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O|O| | | | | |X|X| | | |O|O| | | | | |X|X| | | |O|O| | |O| | |X|X| | |X|O|O| | |O| | |X|X| | |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| | | |X| | |X| | | |O|O| | | |X| | |X| | | 'X' Won! |O|O|X| | |X| | |X| | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X|O| | |O| | |X| | | | |X|O| | |O| | |X| | | | |X|O| | |O|O| |X| |X| | |X|O| | |O|O| |X| |X| |O|X|O| | |O|O| 'X' Won! |X|X|X| |O|X|O| | |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | | |O| |X| | | |O|X| | | | |O| |X| | | |O|X| | | | |O| |X|X| | |O|X|O| | | |O| |X|X| | |O|X|O| |X| |O| |X|X| | |O|X|O| |X|O|O| |X|X| | 'X' Won! |O|X|O| |X|O|O| |X|X|X| | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | | | | | |X| | | |O| | |O| | | | |X| | | |O| | |O|X| | |O|X| | | |O| | |O|X| |X|O|X| | | |O| | |O|X| 'O' Won! |X|O|X| | |O|O| | |O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | |X| | | |O| | | |O| | | |X| | | |O| | |X|O| | | |X| | | |O| | |X|O| |O| |X| | | |O| |X|X|O| |O| |X| | | |O| |X|X|O| |O| |X| |O| |O| |X|X|O| |O| |X| |O|X|O| 'O' Won! |X|X|O| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| |X| |X| | | | | |O| |O| |X| |X| | | | | |O| |O| |X| |X| | | |X| |O| |O| |X|O|X| | | |X| |O| |O| |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | |X| | | | |X| | | |O| | |X| | | | |X| | |O|O| | |X| | | | |X|X| |O|O| | |X|O| | | |X|X| |O|O| | 'X' Won! |X|O| | |X|X|X| |O|O| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X| | | | |O|X| |O| | | |X|X| | | |O|X| |O| | | 'O' Won! |X|X|O| | |O|X| |O| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | |O| | | |X| | |O|X| | | |O| 'O' Won! |O| |X| | |O|X| | | |O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | |O| | | | | |O| |X| | |X|O| | | | | |O|O|X| | |X|O| | | | | 'X' Won! |O|O|X| | |X|O| |X| | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | | |O|X| | |X|O| | |O| | | |O|X| | |X|O| | |O| | |X|O|X| | |X|O| 'O' Won! |O|O| | |X|O|X| | |X|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | |O| |X| |O| | | | | | | |O|X|X| |O| | | | |O| | |O|X|X| |O|X| | | |O| | |O|X|X| |O|X| | | |O|O| |O|X|X| |O|X| | |X|O|O| |O|X|X| 'O' Won! |O|X|O| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | |X| | |O| | | |X| |O| | |X| | |O| | | |X| |O| | |X|X| |O| | | |X| |O| |O|X|X| |O|X| | |X| |O| |O|X|X| |O|X| | |X|O|O| |O|X|X| It's a stalemate! |O|X|X| |X|O|O| |O|X|X| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | |O| | | |O| | | | |X| | |O| | | |O| | | |X|X| |O|O| | | |O| | | |X|X| 'X' Won! |O|O| | | |O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | |X| |O| | | | | |X| | | |X| |O| | | | | |X|O| | 'X' Won! |X| |O| |X| | | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | |X| | | |X| | | |O| | | |X| | | |X| |O| |O| | |X|X| | | |X| |O| |O| 'O' Won! | |X|X| | | |X| |O|O|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O|X| | | | | | | |O|X| |O|X| | | | | | |X|O|X| |O|X| | |O| | | |X|O|X| |O|X| | |O| |X| |X|O|X| |O|X| | 'O' Won! |O| |X| |X|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | |O|X| |O| | | | | |X| | |O|X| |O| | | | | |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| | | |X| |X| |O|O|X| |O| |O| 'X' Won! |X|X|X| |O|O|X| |O| |O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| | | | | | |X| | |O|O|X| | | | | | |X| | |O|O|X| |X| | | | |X| | |O|O|X| |X|O| | | |X| | |O|O|X| |X|O|X| |O|X| | |O|O|X| |X|O|X| 'X' Won! |O|X|X| |O|O|X| |X|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | |X| |O| | | | | | |X|O| |X| |O| | | | | |X|X|O| |X| |O| | | | | |X|X|O| |X|O|O| | | | | |X|X|O| |X|O|O| | |X| | 'O' Won! |X|X|O| |X|O|O| |O|X| | | | | | | | | | | | | | | | | | | |X| | | | | | | | |O| | |X| | | | | | | | |O| | |X|X| | | | | |O| |O| | |X|X| | | | | 'X' Won! |O| |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | | | |X| | |O| | |O| | | |X| |X| | |O| | |O| | | |X| |X| | |O| | |O|O| | |X| |X| |X|O| | |O|O| | |X| |X| |X|O|O| |O|O| | 'X' Won! |X|X|X| |X|O|O| |O|O| | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| |X| | | | | | | |O|O|X| |X| | | | | | | |O|O|X| |X| | | |X| | | |O|O|X| |X|O| | |X| | | |O|O|X| |X|O| | |X|X| | 'O' Won! |O|O|X| |X|O| | |X|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | |X| | | |X| | | |O| |O| |X| | | |X| | |X|O| |O| |X| |O| |X| | |X|O| |O| |X| |O| |X| | |X|O| |O|X|X| 'O' Won! |O| |X| |O|X|O| |O|X|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| |O| |X| | | | | |O| |X| |O| |X|X| | | | |O| |X| |O| |X|X| | | |O|O| 'X' Won! |X| |O| |X|X|X| | |O|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| |X| | | |X| | | | | |O| |X| |O| |X| | | | | |O| 'X' Won! |X| |O| |X| | | |X| |O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | |X| | | | | |X| |O|O| | |X| | | | | |X| |O|O| | |X| |X| |O| |X| |O|O| | |X| |X| 'X' Won! |O| |X| |O|O| | |X|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | |O| | | |O| | | |X| | | |O|X| | |O| | | |X| | | |O|X|O| |O| | | |X| |X| |O|X|O| |O| | | |X| |X| |O|X|O| |O|O| | 'X' Won! |X| |X| |O|X|O| |O|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | | |X| | | | |O|X| | | | | |X|O| | 'X' Won! | |O|X| | |X| | |X|O| | | | | | | | | | | | | | | | | | |X| | | | | | | |O| | | |X| | | | | | | |O| | | |X| | | | | |X| |O| | | |X|O| | | | |X| |O| | | |X|O| | | |X|X| |O| |O| |X|O| | | |X|X| |O| |O| |X|O|X| | |X|X| 'O' Won! |O|O|O| |X|O|X| | |X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| |O|X| | | | | | | | |O| |O|X| | | |X| | | | |O| |O|X| | |O|X| | | | |O| 'X' Won! |O|X| | |O|X| | | |X|O| | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | |X| | |X| | | | |O| | | |X| | |X| | | | |O|O| | |X| | |X| | | |X|O|O| | |X| | |X|O| | |X|O|O| 'X' Won! |X|X| | |X|O| | |X|O|O| | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | |O| | | | | |X|O|X| | | |O| 'O' Won! |O| | | |X|O|X| | | |O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | 'O' Won! |O| |X| |O|X| | |O| | | | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | | |X|O| |O| | | | | | | | |X|O| |O|X| | | | | | | |X|O| |O|X| | |O| | | | |X|O| 'X' Won! |O|X| | |O|X| | | |X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | |O|X| | | | | |O| | | |X|O|X| | | | | |O|O| | |X|O|X| | |X| | |O|O| | |X|O|X| 'O' Won! | |X| | |O|O|O| |X|O|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | |X| |X| |O| | | | | | | |X| |X| |O| | | |O| | | |X| |X| |O| | | |O| |X| |X| |X| |O| |O| |O| |X| |X| |X| |O| |O| |O|X|X| 'O' Won! |X| |X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | |X| | |O|X| | |O| | | | |X| | |O|X| | |O| | |X| |X| | |O|X| |O|O| | |X| |X| | |O|X| 'X' Won! |O|O|X| |X| |X| | |O|X| | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | | |O| |X| | | | | | |X| | |O| |X| |O| | | | |X| | |O|X|X| |O| | | | |X| | 'O' Won! |O|X|X| |O| | | |O|X| | | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| | | | | | | |X|X| | |O| | | | | |O| |X|X| | 'X' Won! |O| | | | | |O| |X|X|X| | | | | | | | | | | | | | | | | | | | | | |X| | |O| | | | | | | | |X| | |O| |X| | | | | | |X| | |O| |X| | |O| | | |X| | |O| |X| | |O|X| | |X| | |O|O|X| | |O|X| | |X| | |O|O|X| |X|O|X| | |X| | 'O' Won! |O|O|X| |X|O|X| | |X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | |X|X| |O| | | | | | | | |X|X| |O| | | |O| | | | |X|X| |O| | | |O|X| | | |X|X| |O| |O| |O|X| | | |X|X| |O| |O| |O|X|X| 'O' Won! | |X|X| |O|O|O| |O|X|X| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | |X|X| | | | | |O| | | | |X|X| | | | | |O|O| | 'X' Won! |X|X|X| | | | | |O|O| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | |O| |X| | | | |O| | | |X|O| |X| | | | |O| | | |X|O| |X| |O| | |O| | | |X|O| |X| |O| |X|O| | 'O' Won! | |X|O| |X| |O| |X|O|O| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | |X| | | |X| | | | |O| | |X| | | |X| | | |O|O| | |X| | | |X|X| | |O|O| | |X| | |O|X|X| | |O|O| | |X|X| |O|X|X| | |O|O| 'O' Won! | |X|X| |O|X|X| |O|O|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | |X| | | | | | | |O| | | |X|O| | | | |X| |O| | | |X|O| | | | |X| |O|O| | |X|O| | | |X|X| |O|O| | |X|O| | |O|X|X| |O|O| | |X|O| | |O|X|X| |O|O| | |X|O|X| 'O' Won! |O|X|X| |O|O|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | |O| | |X| | | | |O| | |X|O| | |X| | | | |O| | |X|O| | |X| | | |O|O| | |X|O| | |X|X| | |O|O| |O|X|O| | |X|X| | |O|O| 'X' Won! |O|X|O| |X|X|X| | |O|O| | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | | | | |O|O| | | |X| | | | |X| |O|O| | | |X| | | |O|X| |O|O| | | |X|X| | |O|X| 'O' Won! |O|O|O| | |X|X| | |O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | |O| | | | | | | |X| | | |O| | | | |X| | |X| | | |O| |O| | |X| | |X| | | |O|X|O| | |X| | |X|O| | |O|X|O| | |X| | |X|O|X| |O|X|O| | |X| | |X|O|X| |O|X|O| |O|X| | 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O|X| | | | | | |O| | | |O|X| | | | | | |O| |X| |O|X| | | | | | |O| |X| |O|X| | | |O| | 'X' Won! |O| |X| |O|X| | |X|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | |O| | | |X| |O| | | |X| |O| | | |X| |O| | | |X| |O| |O| |X| |O|X| | |X| |O| |O| |X| |O|X| | |X| |O| |O|O|X| |O|X| | |X|X|O| |O|O|X| It's a stalemate! |O|X|O| |X|X|O| |O|O|X| | | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | |O| | | | | | |X| | | |X|O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |X| | |X|O| |O|X|O| | | |X| | |X|O| |O|X|O| |X| |X| | |X|O| |O|X|O| |X|O|X| 'X' Won! |X|X|O| |O|X|O| |X|O|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X|O| | | | | | | |O| | |X|O| |X| | | | | |O| | |X|O| |X| | | |O| |O| |X|X|O| |X| | | |O| |O| 'O' Won! |X|X|O| |X| | | |O|O|O| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | | |X| | |O| | | | | | |O| |X| |X|O| | | | | | |O| |X| |X|O| | |O| | | |O| |X| |X|O| | |O| |X| |O| |X| |X|O|O| |O| |X| |O| |X| 'X' Won! |X|O|O| |O|X|X| |O| |X| | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | |O| |X| | | |X| | | | | |O| |X| | | |X| | | | | |O| |X|O| | 'X' Won! |X| | | |X| |O| |X|O| | | | | | | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | |X|O| | | | |X| |O| | | |X|O| | | |O|X| |O| | | |X|O|X| | |O|X| |O| | | |X|O|X| |O|O|X| |O| | | |X|O|X| |O|O|X| |O|X| | It's a stalemate! |X|O|X| |O|O|X| |O|X|O| | | | | | | | | | | | | | | | | | | |O| | | | | | |X| | | | |O| | | | | | |X| | |O| |O| | | | | | |X| | |O|X|O| | | | | | |X| | |O|X|O| | | |O| |X|X| | |O|X|O| | | |O| 'O' Won! |X|X|O| |O|X|O| | | |O| | | | | | | | | | | | | | | | | | | | | | |O| | | | |X| | | | | | |O| | | | |X| |O| | | | |O| | | | |X| |O|X| | | |O| | | |O|X| |O|X| | | |O| | 'X' Won! | |O|X| |O|X| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |X| | |O| | | |O| |X| | |X| | |O| | | |O| |X| | |X|X| 'O' Won! |O| | | |O| |X| |O|X|X| | | | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | |O| | |O| | | | | |X| | |O| | |O|X| | | |O|X| | |O| | |O|X| | | |O|X| | |O| | |O|X|X| | |O|X| |O|O| | |O|X|X| 'X' Won! | |O|X| |O|O|X| |O|X|X| | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | | |O| |X| |O| |X| | | | | |O| |X|O|O| |X| | | 'X' Won! |X| |O| |X|O|O| |X| | | | | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | |X| | | | | | | |O|X| | |X| | |O| | | | |O|X| |X|X| | |O| | | | |O|X| |X|X| | |O|O| | | |O|X| |X|X| | |O|O|X| | |O|X| |X|X| | |O|O|X| |O|O|X| 'X' Won! |X|X|X| |O|O|X| |O|O|X| | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | | | | | |O| |X| | | | | | | | |X|O| |X| | | |O| | | | |X|O| |X| | | |O|X| | | |X|O| |X| | | |O|X| | |O|X|O| |X| | | |O|X|X| |O|X|O| |X|O| | |O|X|X| |O|X|O| It's a stalemate! |X|O|X| |O|X|X| |O|X|O| | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | |O|X| |O| |X| | | | | 'X' Won! | |O|X| |O| |X| | | |X| | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | | | |X| | | |O|O| | | | | | |X| | | |O|O| | | |X| | |X| | | |O|O| |O| |X| | |X| | | |O|O| |O| |X| |X|X| | | |O|O| |O| |X| |X|X|O| | |O|O| |O|X|X| |X|X|O| 'O' Won! |O|O|O| |O|X|X| |X|X|O| | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | | |X| |O| | | | | | | | | |X| |O| |X| | | | | | |O|X| |O| |X| | |X| | | |O|X| |O| |X| | |X| | |O|O|X| |O| |X| 'X' Won! | |X|X| |O|O|X| |O| |X| | | | | | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | |O| |O|X|X| | | |X| | | |O| |O|X|X| | |O|X| | | |O| |O|X|X| |X|O|X| | | |O| |O|X|X| |X|O|X| |O| |O| |O|X|X| 'X' Won! |X|O|X| |O|X|O| |O|X|X| | | | | | | | | | | | | | | | | | | | | | | |O| | |X| | | | | | | | |O| | |X| | | | |O| | | |O| | |X| | | | |O| | |X|O| |O|X| | | | |O| | |X|O| |O|X| | | | |O| |X|X|O| 'O' Won! |O|X|O| | | |O| |X|X|O| | | | | | | | | | | | | | | | | |O| | | | | | | | | |X| |O| | | | | | | | | |X| |O| | | | |O| | | | |X| |O| | | |X|O| | | | |X| |O|O| | |X|O| | |X| |X| |O|O| | |X|O| | 'O' Won! |X|O|X| |O|O| | |X|O| | | | | | | | | | | | | | | | | | | | |X| | | | | | |O| | | | |X| | | | | | |O| | | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| | |X|X| | | |X| | |O|O| | |X|X| |O| |X| 'X' Won! | |O|O| |X|X|X| |O| |X| | | | | | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X| | | | |X| |O| | | |O|X| | | | |X| |O| | | |O|X|X| | | |X| |O| | | |O|X|X| | |O|X| 'X' Won! |O| |X| |O|X|X| | |O|X| | | | | | | | | | | | | | |X| | | | | | | | | | |O|X| | | | | | | | | | |O|X|X| | | | | | | | | |O|X|X| | |O| | | | | | |O|X|X| | |O|X| | | | | 'O' Won! |O|X|X| | |O|X| | | |O| | | | | | | | | | | | | | | | | | | |O| | | | | |X| | | | | |O| | | | | |X| | | | | |O| | | |O| |X| | | | |X|O| | | |O| |X| | | | |X|O| | |O|O| |X| | | | |X|O| |X|O|O| 'O' Won! |X| |O| | |X|O| |X|O|O|","title":"1.0.2 Load Dataset"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#11-clean-data","text":"We will first need to organize the data into a parsable format.","title":"1.1 Clean Data"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q1","text":"What is the object data and what does it contain? what are the keys of data? what are the keys of each game? # inspect data below by grabbing the first key in data # what are the three different keys within each game? data [ 'game 0' ] {'board': {1: 'O', 2: 'X', 3: 'X', 4: ' ', 5: 'X', 6: ' ', 7: 'O', 8: 'O', 9: 'O'}, 'winner': 'O', 'starting player': 'O'}","title":"Q1"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q2","text":"Using those keys, iterate through every game in data and append the board, the winner, and the starting player to separate lists. Call these lists: boards, winners, and starters boards = [] winners = [] starters = [] for game in data : boards . append ( data [ game ][ 'board' ]) winners . append ( data [ game ][ 'winner' ]) starters . append ( data [ game ][ 'starting player' ])","title":"Q2"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q3","text":"Make a dataframe out of the list boards and call it df . Make a series out of the list winners . Make a series out of the list starters . Make a new column of df called \"Winner\" and set it equal to the pandas Series of the winners. Make a new column of df called \"Starter\" and set it equal to the pandas Series of the starters. # YOUR CODE HERE df = pd . DataFrame ( boards ) df [ \"Winner\" ] = pd . Series ( winners ) df [ \"Starter\" ] = pd . Series ( starters ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 3 4 5 6 7 8 9 Winner Starter 0 O X X X O O O O O 1 O X O X X X X O O X X 2 O X X O O O O 3 X O X X X O O O O O O 4 X X X O O O X O","title":"Q3"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#12-inferential-analysis","text":"We're going to use Bayes Rule or Bayesian Inference to make a probability of winning based on positions of the board. The formula is: $ P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)}$ Where $\\cap$ is the intersection of $A$ and $B$. The example we will use is the following: what is the probability of 'O' being the winner, given that they've played the center piece. $B$ = 'O' played the center piece $A$ = 'O' won the game So what is probability? We will define it in terms of frequencies. So if we are for instance asking what is the probability of player 'O' being in the center piece, it would be defined as: $ P(B) = \\frac{|O_c|} {|O_c| + |X_c| + |empty|}$ Where the pipes, | | , or cardinality represent the count of the indicated observation or set. In this case $O_c$ (O being in the center) and $X_c$ (X being in the center). Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc_Xc_empty 1000 ( df [ 5 ] == 'O' ) . value_counts () False 550 True 450 Name: 5, dtype: int64 # example of assessing the probability of B, O playing the center piece player = 'O' Oc = ( df [ 5 ] == player ) . value_counts () Oc_Xc_empty = df [ 5 ] . value_counts () . sum () Oc / Oc_Xc_empty False 0.55 True 0.45 Name: 5, dtype: float64 # we can also clean this up and replace the denominator with the whole # observation space (which is just the total number of games, df.shape[0]). # example of assesing probabiliy of A ( df [ 'Winner' ] == 'O' ) . value_counts () / df . shape [ 0 ] False 0.571 True 0.429 Name: Winner, dtype: float64 The $P(B|A) * P(A)$ is the intersection of $B$ and $A$. The intersection is defined as the two events occuring together. Continuing with the example, the probablity of 'O' playing the center piece AND 'O' being the winner is the number of times these observations occured together divided by the whole observation space : # in this view, the total times A and B occured together is 247 player = 'O' df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] 253 # the total observation space is 1000 (1000 games) df . shape [ 0 ] 1000 And so we get: $P(B|A) * P(A) = \\frac{247} {1000} = 0.247 $ In code: df . loc [( df [ 'Winner' ] == player ) & ( df [ 5 ] == player )] . shape [ 0 ] / df . shape [ 0 ] 0.253","title":"1.2 Inferential Analysis"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#121-behavioral-analysis-of-the-winner","text":"","title":"1.2.1 Behavioral Analysis of the Winner"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q4","text":"define the 3 different board piece types and label them middle , side , and corner . Middle should be an int and the other two should be lists. # define the 3 different board piece types middle = 5 side = [ 2 , 4 , 6 , 8 ] corner = [ 1 , 3 , 7 , 9 ]","title":"Q4"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#1211-what-is-the-probability-of-winning-after-playing-the-middle-piece","text":"","title":"1.2.1.1 What is the probability of winning after playing the middle piece?"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q5","text":"# A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'X' # define the intersection of A AND B, A_B A_B = df . loc [( df [ 'Winner' ] == player ) & ( df [ middle ] == player )] . shape [ 0 ] / \\ df . shape [ 0 ] # define prob B B = ( df [ middle ] == player ) . sum () / \\ df . shape [ 0 ] # return A_B over B (The prob B given A) A_B / B 0.5807962529274004","title":"Q5"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q6","text":"# A intersect B: X played middle and X won / tot games # B: X played middle / tot games player = 'O' # define the intersection of A AND B, A_B A_B = df . loc [( df [ 'Winner' ] == player ) & ( df [ middle ] == player )] . shape [ 0 ] / \\ df . shape [ 0 ] # define prob B B = ( df [ middle ] == player ) . sum () / \\ df . shape [ 0 ] # return A_B over B (The prob B given A) A_B / B 0.5622222222222222","title":"Q6"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#1212-what-is-the-probability-of-winning-after-playing-a-side-piece","text":"","title":"1.2.1.2 What is the probability of winning after playing a side piece?"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q7","text":"# A intersect B: O played side and O won / tot games # B: O played side / tot games player = 'O' A_B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.4252136752136752 # A intersect B: X played side and X won / tot games # B: X played side / tot games player = 'X' A_B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ side ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.41458106637649617","title":"Q7"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#1213-what-is-the-probability-of-winning-after-playing-a-corner-piece","text":"","title":"1.2.1.3 What is the probability of winning after playing a corner piece?"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q8","text":"# A intersect B: O played corner and O won / tot games # B: O played corner / tot games player = 'O' A_B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.44235033259423506","title":"Q8"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#q9","text":"# A intersect B: X played corner and X won / tot games # B: X played corner / tot games player = 'X' A_B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values )) & ( df [ 'Winner' ] == player )] . shape [ 0 ] / df . shape [ 0 ] B = df . loc [( df [ corner ] . T . apply ( lambda x : player in x . values ))] . shape [ 0 ] / \\ df . shape [ 0 ] A_B / B 0.4375678610206298 Are these results surprising to you? Why? This resource may be illustrative.","title":"Q9"},{"location":"solutions/SOLN_P1_Statistical_Analysis_of_TicTacToe/#13-improving-the-analysis","text":"In this analysis, we only tracked what moves were made, not the order they were made in. It really limited our assessment! How might we change our recording of the games to track order of moves as well? Do we need to track all the moves or just the first and the winner?","title":"1.3 Improving the Analysis"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/","text":"Data Science Foundations Project Part 2: Heuristical Agents \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com We makin' some wack AI today 2.0 Preparing Environment and Importing Data \u00b6 back to top 2.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 2.0.2 Load Dataset \u00b6 back to top 2.1 AI Heuristics \u00b6 Develop a better AI based on your analyses of game play so far. Q1 \u00b6 We need to decide what our heuristic strategy will be and orient around the tools/methods we have available to deliver that strategy. # we will define some variables to help us define the types of positions middle = 5 side = [ 2 , 4 , 6 , 8 ] corner = [ 1 , 3 , 7 , 9 ] # recall that our board is a dictionary tictactoe = TicTacToe () tictactoe . board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} # and we have a win_patterns object to help us with the algorithm tictactoe . win_patterns [[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 4, 7], [2, 5, 8], [3, 6, 9], [1, 5, 9], [7, 5, 3]] for example, if we want to check if the middle piece is available, and play it if it is. How do we do that? # set some key variables player = 'X' opponent = 'O' avail_moves = [ i for i in tictactoe . board . keys () if tictactoe . board [ i ] == ' ' ] # a variable that will keep track if we've found a move we like or not move_found = False # <- some other moves we might want to make would go here -> # # and now for our middle piece play if move_found == False : # if no other move has been found yet if middle in avail_moves : # if middle is available move_found = True # then change our move_found status move = middle # update our move Note: in the following when I say return a move I mean when we wrap this up in a function we will want the return to be for a move. For now I just mean that the result of your code in Q3 is to take the variable name move and set it equal to the tic-tac-toe board piece the AI will play Our standard approach will be to always return a move by the agent . Whether the agent is heruistical or from some other ML framework we always want to return a move Q2 \u00b6 Write down your algorithm steps in markdown. i.e. play winning move block opponent's winning move play corner play center play edge Q3 \u00b6 Begin to codify your algorithm from Q3. Make sure that no matter what, you return a move # some starting variables for you self = TicTacToe () # this is useful cheat for when we actually put this in as a method player_label = 'X' opponent = 'O' avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] # temp board will allow us to play hypothetical moves and see where they get us # in case you need it temp_board = self . board . copy () # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True 2.2 Wrapping our Agent \u00b6 Now that we've created a conditional tree for our AI to make a decision, we need to integrate this within the gaming framework we've made so far. How should we do this? Let's define this thought pattern or tree as an agent. Recall our play_game function within GameEngine def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break ######################################################################## ##################### WE WANT TO CHANGE THESE LINES #################### ######################################################################## else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 2.2.1 Redefining the Random Agent \u00b6 In particular, we want to change lines 30-37 to take our gaming agent in as a parameter to make decisions. Let's try this. In setup_game we want to have the option to set the AI type/level. In play_game we want to make a call to that AI to make the move. For instance, our random AI will go from: while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break to: def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ########## our fresh off the assembly line tictactoe playing robot ########### ############################################################################## def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : pass self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Let's test that our random ai works now in this format random . seed ( 12 ) game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| <__main__.GameEngine at 0x7eff0f31b040> Let's try it with a user player: random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2) 1 select AI level (1, 2) 1 who will go first? (X, (AI), or O (Player)) X | | | | | | | | | | | | | | | | | | | | | |X| | O, what's your move? 1 |O| | | | | | | | |X| | |O| | | | |X| | | |X| | O, what's your move? 3 |O| |O| | |X| | | |X| | |O| |O| | |X| | | |X|X| O, what's your move? 2 'O' Won! |O|O|O| | |X| | | |X|X| <__main__.GameEngine at 0x7eff0f2e36a0> Q4 \u00b6 Now let's fold in our specialized AI agent. Add your code under the heurstic_ai function. Note that the player_label is passed as an input parameter now class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ################### YOUR BADASS HEURISTIC AGENT GOES HERE #################### ############################################################################## def heuristic_ai ( self , player_label ): # SOME HELPER VARIABLES IF YOU NEED THEM opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () ################## YOUR CODE GOES HERE, RETURN THAT MOVE! ################## # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True ############################################################################ return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Q5 \u00b6 And we'll test that it works! random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2) 1 select AI level (1, 2) 2 who will go first? (X, (AI), or O (Player)) X | | | | | | | | | | | | | | | | | | | | | | |X| O, what's your move? 3 | | |O| | | | | | | |X| | | |O| | | | | |X| |X| O, what's your move? 2 | |O|O| | | | | |X| |X| 'X' Won! | |O|O| | | | | |X|X|X| <__main__.GameEngine at 0x7eff4d3304f0> Q6 \u00b6 Test the autorun feature! game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X|O| | |O| |O| | |X|X| |X|O| | |O| |O| |O|X|X| |X|O| | |O| |O| |O|X|X| |X|O|X| 'O' Won! |O|O|O| |O|X|X| |X|O|X| <__main__.GameEngine at 0x7eff0f2bbdc0>","title":"SOLN P2 Heuristical TicTacToe Agents"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#data-science-foundations-project-part-2-heuristical-agents","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com We makin' some wack AI today","title":"Data Science Foundations  Project Part 2: Heuristical Agents"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#20-preparing-environment-and-importing-data","text":"back to top","title":"2.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#201-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"2.0.1 Import Packages"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#202-load-dataset","text":"back to top","title":"2.0.2 Load Dataset"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#21-ai-heuristics","text":"Develop a better AI based on your analyses of game play so far.","title":"2.1 AI Heuristics"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#q1","text":"We need to decide what our heuristic strategy will be and orient around the tools/methods we have available to deliver that strategy. # we will define some variables to help us define the types of positions middle = 5 side = [ 2 , 4 , 6 , 8 ] corner = [ 1 , 3 , 7 , 9 ] # recall that our board is a dictionary tictactoe = TicTacToe () tictactoe . board {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} # and we have a win_patterns object to help us with the algorithm tictactoe . win_patterns [[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 4, 7], [2, 5, 8], [3, 6, 9], [1, 5, 9], [7, 5, 3]] for example, if we want to check if the middle piece is available, and play it if it is. How do we do that? # set some key variables player = 'X' opponent = 'O' avail_moves = [ i for i in tictactoe . board . keys () if tictactoe . board [ i ] == ' ' ] # a variable that will keep track if we've found a move we like or not move_found = False # <- some other moves we might want to make would go here -> # # and now for our middle piece play if move_found == False : # if no other move has been found yet if middle in avail_moves : # if middle is available move_found = True # then change our move_found status move = middle # update our move Note: in the following when I say return a move I mean when we wrap this up in a function we will want the return to be for a move. For now I just mean that the result of your code in Q3 is to take the variable name move and set it equal to the tic-tac-toe board piece the AI will play Our standard approach will be to always return a move by the agent . Whether the agent is heruistical or from some other ML framework we always want to return a move","title":"Q1"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#q2","text":"Write down your algorithm steps in markdown. i.e. play winning move block opponent's winning move play corner play center play edge","title":"Q2"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#q3","text":"Begin to codify your algorithm from Q3. Make sure that no matter what, you return a move # some starting variables for you self = TicTacToe () # this is useful cheat for when we actually put this in as a method player_label = 'X' opponent = 'O' avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] # temp board will allow us to play hypothetical moves and see where they get us # in case you need it temp_board = self . board . copy () # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True","title":"Q3"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#22-wrapping-our-agent","text":"Now that we've created a conditional tree for our AI to make a decision, we need to integrate this within the gaming framework we've made so far. How should we do this? Let's define this thought pattern or tree as an agent. Recall our play_game function within GameEngine def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break ######################################################################## ##################### WE WANT TO CHANGE THESE LINES #################### ######################################################################## else : while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue print ( 'test' ) else : break self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"2.2 Wrapping our Agent"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#221-redefining-the-random-agent","text":"In particular, we want to change lines 30-37 to take our gaming agent in as a parameter to make decisions. Let's try this. In setup_game we want to have the option to set the AI type/level. In play_game we want to make a call to that AI to make the move. For instance, our random AI will go from: while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break to: def random_ai(self): while True: move = random.randint(1,9) if self.board[move] != ' ': continue else: break return move class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ########## our fresh off the assembly line tictactoe playing robot ########### ############################################################################## def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : pass self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Let's test that our random ai works now in this format random . seed ( 12 ) game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | |X| | | | | | |O|O| | | |X| | | |X| | |O|O| | | |X| | | |X| | |O|O| |O| |X| |X| |X| | |O|O| |O| |X| |X| |X| | |O|O| |O|O|X| |X| |X| |X|O|O| |O|O|X| 'O' Won! |X|O|X| |X|O|O| |O|O|X| <__main__.GameEngine at 0x7eff0f31b040> Let's try it with a user player: random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2) 1 select AI level (1, 2) 1 who will go first? (X, (AI), or O (Player)) X | | | | | | | | | | | | | | | | | | | | | |X| | O, what's your move? 1 |O| | | | | | | | |X| | |O| | | | |X| | | |X| | O, what's your move? 3 |O| |O| | |X| | | |X| | |O| |O| | |X| | | |X|X| O, what's your move? 2 'O' Won! |O|O|O| | |X| | | |X|X| <__main__.GameEngine at 0x7eff0f2e36a0>","title":"2.2.1 Redefining the Random Agent"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#q4","text":"Now let's fold in our specialized AI agent. Add your code under the heurstic_ai function. Note that the player_label is passed as an input parameter now class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup ############################################################################## ################### YOUR BADASS HEURISTIC AGENT GOES HERE #################### ############################################################################## def heuristic_ai ( self , player_label ): # SOME HELPER VARIABLES IF YOU NEED THEM opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () ################## YOUR CODE GOES HERE, RETURN THAT MOVE! ################## # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True ############################################################################ return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'human' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ######################################################################## ################# Allow the user to set the ai level ################### ######################################################################## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########################################################################## ############## and automatically set the ai level otherwise ############## ########################################################################## self . ai_level = 1 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : if self . ai_level == 1 : move = self . random_ai () ###################################################################### ############## we will leave this setting empty for now ############## ###################################################################### elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"Q4"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#q5","text":"And we'll test that it works! random . seed ( 12 ) game = GameEngine ( setup = 'user' ) game . setup_game () game . play_game () How many Players? (type 0, 1, or 2) 1 select AI level (1, 2) 2 who will go first? (X, (AI), or O (Player)) X | | | | | | | | | | | | | | | | | | | | | | |X| O, what's your move? 3 | | |O| | | | | | | |X| | | |O| | | | | |X| |X| O, what's your move? 2 | |O|O| | | | | |X| |X| 'X' Won! | |O|O| | | | | |X|X|X| <__main__.GameEngine at 0x7eff4d3304f0>","title":"Q5"},{"location":"solutions/SOLN_P2_Heuristical_TicTacToe_Agents/#q6","text":"Test the autorun feature! game = GameEngine ( setup = 'auto' ) game . setup_game () game . play_game () | | | | | | | | | | | | | | |O| | | | | | | | | | | |O| | | | | |X| | | |O| |O| | | | | |X| | | |O| |O| | | |X| |X| | | |O| |O| | | |X| |X|O| | |O| |O| | |X|X| |X|O| | |O| |O| |O|X|X| |X|O| | |O| |O| |O|X|X| |X|O|X| 'O' Won! |O|O|O| |O|X|X| |X|O|X| <__main__.GameEngine at 0x7eff0f2bbdc0>","title":"Q6"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/","text":"Data Science Foundations Project Part 3: 1-Step Look Ahead \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we're working on a more advanced AI structure: 1-step lookahead. 3.0 Preparing Environment and Importing Data \u00b6 back to top 3.0.1 Import Packages \u00b6 back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] the_final_move = None # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True the_final_move = move break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## self . ai_level = 2 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self 3.0.2 Load Dataset \u00b6 back to top 3.1 Rethinking gameplay \u00b6 To implement the broader strategies used in game theory and machine learning, we need to rebroadcast our approach to creating our AI agent. In the heurstical agent model, we thought in terms of checking for specific move types, defined by what kind of advantage they give us during game play, i.e. see if a winning move is available, a blocking move, if a corner place is free, etc. Rather than thinking with this look and check mindset that is centered around specific strategies and our own prior knowledge about the game (we know that a center piece is statistically likely to give us a higher chance of winning) we will evaluate every available move to the AI, and rate them quantitatively. switching from ordinal to interval Notice the datatype change when we move from giving simple preferences of moves to actual scores of moves. Catalog this in your mind for future reference when considering datatypes! 3.1.1 One-Step Look Ahead \u00b6 For now, when we rate our boards, we will only look 1-step ahead in gameplay. Hence the name we give this AI strategy, 1-step lookahead The beginning portion of our code will look about the same as the heuristic AI model. Recall: def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() but now, instead of searching progressively through our preferred move-types (winning, middle, etc.) . We are going to give every available move (1, 3, 7, etc.) a score. Our score regimen will look like the following: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move Q1 Rewrite avail_moves \u00b6 define avail_moves as a dictionary of available moves with scores for each move as empty strings. We will update this dictionary with numerical scores in the next step # we're going to steal the parameter names to # prototype our new function self = TicTacToe () player_label = 'X' opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] # instead of a list, we want avail_moves to now be a dictionary that will # contain the move and its score avail_moves = { i : ' ' for i in self . board if self . board [ i ] == ' ' } temp_board = self . board . copy () avail_moves {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} Q2 Score each move in avail_moves \u00b6 Now let's fold this into our new one_step_ai function. Remember: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## avail_moves = { i : ' ' for i in self . board if self . board [ i ] == ' ' } for move in avail_moves . keys (): avail_moves [ move ] = 1 temp_board = self . board . copy () # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score avail_moves [ move ] = 100 temp_board [ move ] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## for move in avail_moves . keys (): temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: avail_moves [ move ] = 10 temp_board [ move ] = ' ' ############################################################################## ################### All remaining moves receive a score of 1 ############################################################################## return avail_moves Q3 Test one_step_ai \u00b6 That's great, but how do we check that our code will work when a winning move is available, or a losing move is just around the corner? let's create a unit test for these! # just defining a new game self = TicTacToe () player_label = 'X' # seeding the board with some X's self . board [ 1 ] = 'X' self . board [ 2 ] = 'X' self . board {1: 'X', 2: 'X', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} Now test the winning move. Your code should return 100 at move 3 and 1 everywhere else one_step_ai ( self , player_label ) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} We can test the losing move by reversing the players player_label = 'O' one_step_ai ( self , player_label ) {3: 10, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} great! Let's keep these shorthand codes in mind when we go to write actual unit tests with the one_step_ai function embedded in the GameEngine module. We're not done yet, recall that our other ai agents returned the actual selected move, not a dictionary of the moves with scores. We need to create a move from this dictionary and return it. Here's what the general procedure will look like: Grab the maximum score (after assigning scores to all of avail_moves) Select all moves that have this maximum score Return a random selection of the moves with the max score and then in code format: avail_moves = one_step_ai ( self , player_label ) # 1. grab the maximum score max_score = max ( avail_moves . values ()) # 2. select all moves that have this maximum score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # 3. return a random selection of the moves with the max score move = random . choice ( valid ) move 3 3.2 Putting it all together \u00b6 Q4 Finish one_step_ai to return a move \u00b6 Let's see if we can rewrite our game engine to take new AI models in as a passable parameter. This way our base module will be much cleaner, and allow us to continue to write new functions for the base engine as long as they pass along the same variables. # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( board , win_patterns , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] temp_board = board . copy () # define avail_moves ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## avail_moves = { i : ' ' for i in board if board [ i ] == ' ' } for move in avail_moves . keys (): avail_moves [ move ] = 1 temp_board = board . copy () # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score avail_moves [ move ] = 100 temp_board [ move ] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## for move in avail_moves . keys (): temp_board [ move ] = opponent for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: avail_moves [ move ] = 10 temp_board [ move ] = ' ' # 1. grab the maximum score max_score = max ( avail_moves . values ()) # 2. select all moves that have this maximum score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # 3. return a random selection of the moves with the max score move = random . choice ( valid ) return move 3.2.1 Allow GameEngine to take an ai agent as a passable parameter \u00b6 Let's rewrite our GameEngine to take an ai agent as a passable parameter under user_ai . The default value will be None Additional user_ai criteria will be that user_ai receives board , win_patterns and player_label and returns move . class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' , user_ai = None ): super () . __init__ () self . setup = setup self . user_ai = user_ai def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self . user_ai == None : level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) else : self . ai_level = 3 if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## if self . user_ai == None : self . ai_level = 2 else : self . ai_level = 3 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) ########## # Our user-defined AI agent ########## elif self . ai_level == 3 : move = self . user_ai ( self . board , self . win_patterns , player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Test the auto and user functions game = GameEngine ( setup = 'user' , user_ai = one_step_ai ) game . setup_game () How many Players? (type 0, 1, or 2) 1 who will go first? (X, (AI), or O (Player)) X game . play_game () | | | | | | | | | | | | | | | | |X| | | | | | | O, what's your move? 3 | | |O| |X| | | | | | | | |X|O| |X| | | | | | | O, what's your move? 6 | |X|O| |X| |O| | | | | | |X|O| |X| |O| | | |X| O, what's your move? 1 |O|X|O| |X| |O| | | |X| |O|X|O| |X| |O| | |X|X| O, what's your move? 5 |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| <__main__.GameEngine at 0x7f4614708850> 3.3 Write Unit Tests for the New Code \u00b6 There are many tests we could write here def test_user_ai (): random . seed ( 42 ) game = GameEngine ( setup = 'auto' , user_ai = one_step_ai ) game . setup_game () outcome = game . play_game () assert outcome . winner == 'X' , 'X should have won!' test_user_ai () | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| | |X|O| |O| | | 'X' Won! |X|O|X| | |X|O| |O| |X|","title":"SOLN P3 1 Step Look Ahead Agents"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#data-science-foundations-project-part-3-1-step-look-ahead","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we're working on a more advanced AI structure: 1-step lookahead.","title":"Data Science Foundations  Project Part 3: 1-Step Look Ahead"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#30-preparing-environment-and-importing-data","text":"back to top","title":"3.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#301-import-packages","text":"back to top import random import pandas as pd import numpy as np import matplotlib.pyplot as plt class TicTacToe : # can preset winner and starting player def __init__ ( self , winner = '' , start_player = '' ): self . winner = winner self . start_player = start_player self . board = { 1 : ' ' , 2 : ' ' , 3 : ' ' , 4 : ' ' , 5 : ' ' , 6 : ' ' , 7 : ' ' , 8 : ' ' , 9 : ' ' ,} self . win_patterns = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ], [ 1 , 5 , 9 ], [ 7 , 5 , 3 ]] # the other functions are now passed self def visualize_board ( self ): print ( \"| {} | {} | {} | \\n | {} | {} | {} | \\n | {} | {} | {} | \\n \" . format ( * self . board . values ()) ) def check_winning ( self ): for pattern in self . win_patterns : values = [ self . board [ i ] for i in pattern ] if values == [ 'X' , 'X' , 'X' ]: self . winner = 'X' # we update the winner status return \"'X' Won!\" elif values == [ 'O' , 'O' , 'O' ]: self . winner = 'O' return \"'O' Won!\" return '' def check_stalemate ( self ): if ( ' ' not in self . board . values ()) and ( self . check_winning () == '' ): self . winner = 'Stalemate' return \"It's a stalemate!\" class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' ): super () . __init__ () self . setup = setup def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] the_final_move = None # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True the_final_move = move break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## self . ai_level = 2 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self","title":"3.0.1 Import Packages"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#302-load-dataset","text":"back to top","title":"3.0.2 Load Dataset"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#31-rethinking-gameplay","text":"To implement the broader strategies used in game theory and machine learning, we need to rebroadcast our approach to creating our AI agent. In the heurstical agent model, we thought in terms of checking for specific move types, defined by what kind of advantage they give us during game play, i.e. see if a winning move is available, a blocking move, if a corner place is free, etc. Rather than thinking with this look and check mindset that is centered around specific strategies and our own prior knowledge about the game (we know that a center piece is statistically likely to give us a higher chance of winning) we will evaluate every available move to the AI, and rate them quantitatively. switching from ordinal to interval Notice the datatype change when we move from giving simple preferences of moves to actual scores of moves. Catalog this in your mind for future reference when considering datatypes!","title":"3.1 Rethinking gameplay"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#311-one-step-look-ahead","text":"For now, when we rate our boards, we will only look 1-step ahead in gameplay. Hence the name we give this AI strategy, 1-step lookahead The beginning portion of our code will look about the same as the heuristic AI model. Recall: def heuristic_ai(self, player_label): opponent = ['X', 'O'] opponent.remove(player_label) opponent = opponent[0] avail_moves = [i for i in self.board.keys() if self.board[i] == ' '] temp_board = self.board.copy() but now, instead of searching progressively through our preferred move-types (winning, middle, etc.) . We are going to give every available move (1, 3, 7, etc.) a score. Our score regimen will look like the following: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move","title":"3.1.1 One-Step Look Ahead"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#q1-rewrite-avail_moves","text":"define avail_moves as a dictionary of available moves with scores for each move as empty strings. We will update this dictionary with numerical scores in the next step # we're going to steal the parameter names to # prototype our new function self = TicTacToe () player_label = 'X' opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] # instead of a list, we want avail_moves to now be a dictionary that will # contain the move and its score avail_moves = { i : ' ' for i in self . board if self . board [ i ] == ' ' } temp_board = self . board . copy () avail_moves {1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '}","title":"Q1 Rewrite avail_moves"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#q2-score-each-move-in-avail_moves","text":"Now let's fold this into our new one_step_ai function. Remember: 100 pts: winning move 10 pts: blocks an opponents winning move 1 pt: every other move # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## avail_moves = { i : ' ' for i in self . board if self . board [ i ] == ' ' } for move in avail_moves . keys (): avail_moves [ move ] = 1 temp_board = self . board . copy () # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score avail_moves [ move ] = 100 temp_board [ move ] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## for move in avail_moves . keys (): temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: avail_moves [ move ] = 10 temp_board [ move ] = ' ' ############################################################################## ################### All remaining moves receive a score of 1 ############################################################################## return avail_moves","title":"Q2 Score each move in avail_moves"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#q3-test-one_step_ai","text":"That's great, but how do we check that our code will work when a winning move is available, or a losing move is just around the corner? let's create a unit test for these! # just defining a new game self = TicTacToe () player_label = 'X' # seeding the board with some X's self . board [ 1 ] = 'X' self . board [ 2 ] = 'X' self . board {1: 'X', 2: 'X', 3: ' ', 4: ' ', 5: ' ', 6: ' ', 7: ' ', 8: ' ', 9: ' '} Now test the winning move. Your code should return 100 at move 3 and 1 everywhere else one_step_ai ( self , player_label ) {3: 100, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} We can test the losing move by reversing the players player_label = 'O' one_step_ai ( self , player_label ) {3: 10, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} great! Let's keep these shorthand codes in mind when we go to write actual unit tests with the one_step_ai function embedded in the GameEngine module. We're not done yet, recall that our other ai agents returned the actual selected move, not a dictionary of the moves with scores. We need to create a move from this dictionary and return it. Here's what the general procedure will look like: Grab the maximum score (after assigning scores to all of avail_moves) Select all moves that have this maximum score Return a random selection of the moves with the max score and then in code format: avail_moves = one_step_ai ( self , player_label ) # 1. grab the maximum score max_score = max ( avail_moves . values ()) # 2. select all moves that have this maximum score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # 3. return a random selection of the moves with the max score move = random . choice ( valid ) move 3","title":"Q3 Test one_step_ai"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#32-putting-it-all-together","text":"","title":"3.2 Putting it all together"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#q4-finish-one_step_ai-to-return-a-move","text":"Let's see if we can rewrite our game engine to take new AI models in as a passable parameter. This way our base module will be much cleaner, and allow us to continue to write new functions for the base engine as long as they pass along the same variables. # the beginning portion of our code will look about the same # as the heuristic AI model def one_step_ai ( board , win_patterns , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] temp_board = board . copy () # define avail_moves ############################################################################## ############################# DEFINE avail_moves ############################# ############################################################################## avail_moves = { i : ' ' for i in board if board [ i ] == ' ' } for move in avail_moves . keys (): avail_moves [ move ] = 1 temp_board = board . copy () # first check for a winning move # we're now looping through the keys of our dictionary for move in avail_moves . keys (): temp_board [ move ] = player_label for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: ###################################################################### # if we found a winning move we want to update the move with a score # ###################################################################### # your code to update avail_moves with a score avail_moves [ move ] = 100 temp_board [ move ] = ' ' ############################################################################## ################## Check if the opponent has a winning move ################## ############################################################################## for move in avail_moves . keys (): temp_board [ move ] = opponent for pattern in win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: avail_moves [ move ] = 10 temp_board [ move ] = ' ' # 1. grab the maximum score max_score = max ( avail_moves . values ()) # 2. select all moves that have this maximum score valid = [] for key , value in avail_moves . items (): if value == max_score : valid . append ( key ) # 3. return a random selection of the moves with the max score move = random . choice ( valid ) return move","title":"Q4 Finish one_step_ai to return a move"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#321-allow-gameengine-to-take-an-ai-agent-as-a-passable-parameter","text":"Let's rewrite our GameEngine to take an ai agent as a passable parameter under user_ai . The default value will be None Additional user_ai criteria will be that user_ai receives board , win_patterns and player_label and returns move . class GameEngine ( TicTacToe ): def __init__ ( self , setup = 'auto' , user_ai = None ): super () . __init__ () self . setup = setup self . user_ai = user_ai def heuristic_ai ( self , player_label ): opponent = [ 'X' , 'O' ] opponent . remove ( player_label ) opponent = opponent [ 0 ] avail_moves = [ i for i in self . board . keys () if self . board [ i ] == ' ' ] temp_board = self . board . copy () middle = 5 corner = [ 1 , 3 , 7 , 9 ] side = [ 2 , 4 , 6 , 8 ] # first check for a winning move move_found = False for move in avail_moves : temp_board [ move ] = player_label for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ player_label , player_label , player_label ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check if the opponent has a winning move if move_found == False : for move in avail_moves : temp_board [ move ] = opponent for pattern in self . win_patterns : values = [ temp_board [ i ] for i in pattern ] if values == [ opponent , opponent , opponent ]: move_found = True break if move_found : break else : temp_board [ move ] = ' ' # check corners if move_found == False : move_corner = [ val for val in avail_moves if val in corner ] if len ( move_corner ) > 0 : move = random . choice ( move_corner ) move_found = True # check if middle avail if move_found == False : if middle in avail_moves : move_found = True move = middle # check side if move_found == False : move_side = [ val for val in avail_moves if val in side ] if len ( move_side ) > 0 : move = random . choice ( move_side ) move_found = True return move def random_ai ( self ): while True : move = random . randint ( 1 , 9 ) if self . board [ move ] != ' ' : continue else : break return move def setup_game ( self ): if self . setup == 'user' : players = int ( input ( \"How many Players? (type 0, 1, or 2)\" )) self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'human' }} if players != 2 : ########## # Allow the user to set the ai level ########## ### if they have not provided an ai_agent if self . user_ai == None : level = int ( input ( \"select AI level (1, 2)\" )) if level == 1 : self . ai_level = 1 elif level == 2 : self . ai_level = 2 else : print ( \"Unknown AI level entered, this will cause problems\" ) else : self . ai_level = 3 if players == 1 : first = input ( \"who will go first? (X, (AI), or O (Player))\" ) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'human' }} elif players == 0 : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} elif self . setup == 'auto' : first = random . choice ([ 'X' , 'O' ]) if first == 'O' : self . start_player = 'O' self . player_meta = { 'second' : { 'label' : 'X' , 'type' : 'ai' }, 'first' : { 'label' : 'O' , 'type' : 'ai' }} else : self . start_player = 'X' self . player_meta = { 'first' : { 'label' : 'X' , 'type' : 'ai' }, 'second' : { 'label' : 'O' , 'type' : 'ai' }} ########## # and automatically set the ai level otherwise ########## if self . user_ai == None : self . ai_level = 2 else : self . ai_level = 3 def play_game ( self ): while True : for player in [ 'first' , 'second' ]: self . visualize_board () player_label = self . player_meta [ player ][ 'label' ] player_type = self . player_meta [ player ][ 'type' ] if player_type == 'human' : move = input ( \" {} , what's your move?\" . format ( player_label )) # we're going to allow the user to quit the game from the input line if move in [ 'q' , 'quit' ]: self . winner = 'F' print ( 'quiting the game' ) break move = int ( move ) if self . board [ move ] != ' ' : while True : move = input ( \" {} , that position is already taken! \" \\ \"What's your move?\" . format ( player_label )) move = int ( move ) if self . board [ move ] != ' ' : continue else : break else : ########## # Our level 1 ai agent (random) ########## if self . ai_level == 1 : move = self . random_ai () ########## # Our level 2 ai agent (heuristic) ########## elif self . ai_level == 2 : move = self . heuristic_ai ( player_label ) ########## # Our user-defined AI agent ########## elif self . ai_level == 3 : move = self . user_ai ( self . board , self . win_patterns , player_label ) self . board [ move ] = player_label # the winner varaible will now be check within the board object self . check_winning () self . check_stalemate () if self . winner == '' : continue elif self . winner == 'Stalemate' : print ( self . check_stalemate ()) self . visualize_board () break else : print ( self . check_winning ()) self . visualize_board () break if self . winner != '' : return self Test the auto and user functions game = GameEngine ( setup = 'user' , user_ai = one_step_ai ) game . setup_game () How many Players? (type 0, 1, or 2) 1 who will go first? (X, (AI), or O (Player)) X game . play_game () | | | | | | | | | | | | | | | | |X| | | | | | | O, what's your move? 3 | | |O| |X| | | | | | | | |X|O| |X| | | | | | | O, what's your move? 6 | |X|O| |X| |O| | | | | | |X|O| |X| |O| | | |X| O, what's your move? 1 |O|X|O| |X| |O| | | |X| |O|X|O| |X| |O| | |X|X| O, what's your move? 5 |O|X|O| |X|O|O| | |X|X| 'X' Won! |O|X|O| |X|O|O| |X|X|X| <__main__.GameEngine at 0x7f4614708850>","title":"3.2.1 Allow GameEngine to take an ai agent as a passable parameter"},{"location":"solutions/SOLN_P3_1_Step_Look_Ahead_Agents/#33-write-unit-tests-for-the-new-code","text":"There are many tests we could write here def test_user_ai (): random . seed ( 42 ) game = GameEngine ( setup = 'auto' , user_ai = one_step_ai ) game . setup_game () outcome = game . play_game () assert outcome . winner == 'X' , 'X should have won!' test_user_ai () | | | | | | | | | | | | |X| | | | | | | | | | | |X| | | | | |O| | | | | |X| |X| | | |O| | | | | |X|O|X| | | |O| | | | | |X|O|X| | |X|O| | | | | |X|O|X| | |X|O| |O| | | 'X' Won! |X|O|X| | |X|O| |O| |X|","title":"3.3 Write Unit Tests for the New Code"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/","text":"Data Science Foundations Session 1: Regression and Analysis \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at fitting data to a curve using regression . We will also look at using regression to make predictions for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error. 1.0 Preparing Environment and Importing Data \u00b6 back to top 1.0.1 Import Packages \u00b6 back to top # Import pandas, pyplot, ipywidgets import pandas as pd from matplotlib import pyplot as plt from ipywidgets import interact # Import Scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score # for enrichment topics import seaborn as sns import numpy as np 1.0.2 Load Dataset \u00b6 back to top For our discussion on regression and descriptive statistics today we will use a well known dataset of different wines and their quality ratings df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) df . shape (6497, 13) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 1.1 What is regression? \u00b6 It is the process of finding a relationship between dependent and independent variables to find trends in data. This abstract definition means that you have one variable (the dependent variable) which depends on one or more variables (the independent variables). One of the reasons for which we want to regress data is to understand whether there is a trend between two variables. Housing Prices Example We can imagine this scenario with housing prices. Envision a mixed dataset of continuous and discrete independent variables. Some features could be continuous, floating point values like location ranking and housing condition. Others could be descrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a regression model. 1.2 Linear regression fitting with scikit-learn \u00b6 \ud83c\udfcb\ufe0f Exercise 1: rudimentary EDA \u00b6 What does the data look like? Recall how to visualize data in a pandas dataframe for every column calculate the * skew: `df.skew()` * kurtosis: `df.kurtosis()` * pearsons correlation with the dependent variable: `df.corr()` * number of missing entries `df.isnull()` and organize this into a new dataframe note: pearsons is just one type of correlation, another available to us spearman which differs from pearsons in that it depends on ranked values rather than their direct quantities, you can read more here df . isnull () . sum () type 0 fixed acidity 10 volatile acidity 8 citric acid 3 residual sugar 2 chlorides 2 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 9 sulphates 4 alcohol 0 quality 0 dtype: int64 # Cell for Exercise 1 # part A # using df.<method> define the following four variables with the results from # skew(), kurtosis(), corr() (while selecting for quality), and isnull() # for isnull() you'll notice the return is a dataframe of booleans. we would # like to simply know the number of null values for each column. change the # return of isnull() using the sum() method along the columns skew = df . skew () kurt = df . kurtosis () pear = df . corr ()[ 'quality' ] null = df . isnull () . sum ( axis = 0 ) # part B # on line 13, put these results in a list using square brackets and call # pd.DataFrame on the list to make your new DataFrame! store it under the # variable name dff dff = pd . DataFrame ([ skew , kurt , pear , null ]) # part C # take the transpose of this DataFrame using dff.T. reassign dff to this copy dff = dff . T # part D # set the column names to 'skew', 'kurtosis', 'pearsons _quality', and # 'null count' using dff.columns dff . columns = [ 'skew' , 'kurtosis' , 'pearsons _quality' , 'null count' ] # Now return dff to the output to view your hand work dff # uncomment this line /tmp/ipykernel_1422/4028752270.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. skew = df.skew() /tmp/ipykernel_1422/4028752270.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. kurt = df.kurtosis() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count fixed acidity 1.722805 5.057727 -0.077031 10.0 volatile acidity 1.495512 2.827081 -0.265953 8.0 citric acid 0.473032 2.401582 0.085706 3.0 residual sugar 1.435000 4.358134 -0.036825 2.0 chlorides 5.399849 50.894874 -0.200886 2.0 free sulfur dioxide 1.220066 7.906238 0.055463 0.0 total sulfur dioxide -0.001177 -0.371664 -0.041385 0.0 density 0.503602 6.606067 -0.305858 0.0 pH 0.386966 0.370068 0.019366 9.0 sulphates 1.798467 8.659892 0.038729 4.0 alcohol 0.565718 -0.531687 0.444319 0.0 quality 0.189623 0.232322 1.000000 0.0 type NaN NaN NaN 0.0 I have gone ahead and repeated this exercise with the red vs white wine types: red = df . loc [ df [ 'type' ] == 'red' ] wht = df . loc [ df [ 'type' ] == 'white' ] def get_summary ( df ): skew = df . skew () kurt = df . kurtosis () pear = df . corr ()[ 'quality' ] null = df . isnull () . sum () med = df . median () men = df . mean () dff = pd . DataFrame ([ skew , kurt , pear , null , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'pearsons _quality' , 'null count' , 'median' , 'mean' ] return dff dffr = get_summary ( red ) dffw = get_summary ( wht ) desc = pd . concat ([ dffr , dffw ], keys = [ 'red' , 'white' ]) /tmp/ipykernel_1422/2387423026.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. skew = df.skew() /tmp/ipykernel_1422/2387423026.py:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. kurt = df.kurtosis() /tmp/ipykernel_1422/2387423026.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. med = df.median() /tmp/ipykernel_1422/2387423026.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. men = df.mean() desc .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count median mean red fixed acidity 0.982192 1.132624 0.123834 2.0 7.90000 8.322104 volatile acidity 0.672862 1.226846 -0.390858 1.0 0.52000 0.527738 citric acid 0.317891 -0.788476 0.226917 1.0 0.26000 0.271145 residual sugar 4.540655 28.617595 0.013732 0.0 2.20000 2.538806 chlorides 5.680347 41.715787 -0.128907 0.0 0.07900 0.087467 free sulfur dioxide 1.250567 2.023562 -0.050656 0.0 14.00000 15.874922 total sulfur dioxide 1.515531 3.809824 -0.185100 0.0 38.00000 46.467792 density 0.071288 0.934079 -0.174919 0.0 0.99675 0.996747 pH 0.194803 0.814690 -0.057094 2.0 3.31000 3.310864 sulphates 2.429115 11.712632 0.251685 2.0 0.62000 0.658078 alcohol 0.860829 0.200029 0.476166 0.0 10.20000 10.422983 quality 0.217802 0.296708 1.000000 0.0 6.00000 5.636023 type NaN NaN NaN 0.0 NaN NaN white fixed acidity 0.647981 2.176560 -0.114032 8.0 6.80000 6.855532 volatile acidity 1.578595 5.095526 -0.194976 7.0 0.26000 0.278252 citric acid 1.284217 6.182036 -0.009194 2.0 0.32000 0.334250 residual sugar 1.076601 3.469536 -0.097373 2.0 5.20000 6.393250 chlorides 5.023412 37.560847 -0.210181 2.0 0.04300 0.045778 free sulfur dioxide 1.406745 11.466342 0.008158 0.0 34.00000 35.308085 total sulfur dioxide 0.390710 0.571853 -0.174737 0.0 134.00000 138.360657 density 0.977773 9.793807 -0.307123 0.0 0.99374 0.994027 pH 0.458402 0.532552 0.098858 7.0 3.18000 3.188203 sulphates 0.977361 1.589847 0.053690 2.0 0.47000 0.489835 alcohol 0.487342 -0.698425 0.435575 0.0 10.40000 10.514267 quality 0.155796 0.216526 1.000000 0.0 6.00000 5.877909 type NaN NaN NaN 0.0 NaN NaN def my_fig ( metric = desc . columns ): fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) pd . DataFrame ( desc [ metric ]) . unstack ()[ metric ] . T . plot ( kind = 'barh' , ax = ax ) interact ( my_fig ) interactive(children=(Dropdown(description='metric', options=('skew', 'kurtosis', 'pearsons _quality', 'null c\u2026 <function __main__.my_fig(metric=Index(['skew', 'kurtosis', 'pearsons _quality', 'null count', 'median', 'mean'], dtype='object'))> \ud83d\ude4b Question 1: Discussion Around EDA Plot \u00b6 What do we think of this plot? metric = mean , the cholrides values metric = kurtosis , residual sugar metric = pearsons _quality , magnitudes and directions How to improve the plot, what other plots would we like to see? For instance, what if we were really curious about the high kurtosis for chlorides content? What more would we like to glean about the distribution of chloride content? # we can use df.describe() to take a look at the quantile values and min/max df [ 'chlorides' ] . describe () count 6495.000000 mean 0.056042 std 0.035036 min 0.009000 25% 0.038000 50% 0.047000 75% 0.065000 max 0.611000 Name: chlorides, dtype: float64 # and see how these values appear in a KDE fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) df [ 'chlorides' ] . plot ( kind = 'kde' , ax = ax ) ax . set_xlim ( 0 , .61 ) (0.0, 0.61) # lastly we may want to look at the raw values themselves. We can sort them # too view outliers df [ 'chlorides' ] . sort_values ( ascending = False )[: 50 ] 5156 0.611 5049 0.610 5004 0.467 4979 0.464 5590 0.422 6268 0.415 6270 0.415 5652 0.415 6217 0.414 5949 0.414 5349 0.413 6158 0.403 4981 0.401 5628 0.387 6063 0.369 4915 0.368 5067 0.360 5179 0.358 484 0.346 5189 0.343 4917 0.341 5124 0.337 4940 0.332 1217 0.301 687 0.290 4473 0.271 5079 0.270 6272 0.267 5138 0.263 1865 0.255 5466 0.250 1034 0.244 5674 0.243 5675 0.241 683 0.240 1638 0.239 5045 0.236 6456 0.235 6468 0.230 5465 0.226 5464 0.226 5564 0.222 2186 0.217 5996 0.216 6333 0.214 5206 0.214 6332 0.214 5205 0.213 4497 0.212 1835 0.211 Name: chlorides, dtype: float64 1.2.2 Visualizing the data set - motivating regression analysis \u00b6 In order to demonstrate simple linear regression with this dataset we will look at two particular features: fixed acidity and density . We can create a scatter plot of fixed acidity vs density for the red wine in the dataset using df.plot() and see that there appears to be a general trend between the two features: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) df . loc [ df [ 'type' ] == 'red' ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> Now the question is: How do we quantify this trend? 1.2.3 Estimating the regression coefficients \u00b6 It looks like density increases with fixed acidity following a line, maybe something like y(x)= m \\cdot x + b \\;\\;\\;\\;\\;\\;\\;\\; \\sf{eq. 1} with \\( y=\\sf density \\), \\(x=\\sf fixed \\space acidity\\), and \\(m\\) the slope and \\(b\\) the intercept. To solve the problem, we need to find the values of \\(b\\) and \\(m\\) in equation 1 to best fit the data. This is called linear regression . In linear regression our goal is to minimize the error between computed values of positions \\(y^{\\sf calc}(x_i)\\equiv y^{\\sf calc}_i\\) and known values \\(y^{\\sf exact}(x_i)\\equiv y^{\\sf exact}_i\\), i.e. find \\(b\\) and \\(m\\) which lead to lowest value of \\epsilon (m,b) =SS_{\\sf res}=\\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - m\\cdot x_i - b \\right)^2\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sf{eq. 2} Otherwise known as the residual sum of squares To find out more see e.g. https://en.wikipedia.org/wiki/Simple_linear_regression \ud83d\ude4b Question 2: linear regression loss function \u00b6 Do we always want m and b to be large positive numbers so as to minimize eq. 2? Luckily scikit-learn contains many functions related to regression including linear regression . The function we will use is called LinearRegression() . # Create linear regression object model = linear_model.LinearRegression() # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red['density'].values.reshape(-1, 1) y = red['fixed acidity'].values.reshape(-1, 1) # Create linear regression object model = linear_model . LinearRegression () # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red [ 'density' ] . values . reshape ( - 1 , 1 ) y = red [ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) print(red['density'].values.shape, red['fixed acidity'].values.shape) print(x.shape, y.shape) print ( red [ 'density' ] . values . shape , red [ 'fixed acidity' ] . values . shape ) print ( x . shape , y . shape ) (1599,) (1599,) (1599, 1) (1599, 1) # Fit to the data model.fit(x, y) # Extract the values of interest m = model.coef_[0][0] b = model.intercept_[0] # Print the slope m and intercept b print('Scikit learn - Slope: ', m , 'Intercept: ', b ) What happens when we try to fit the data as is? # Fit to the data # model.fit(x, y) \ud83c\udfcb\ufe0f Exercise 2: drop Null Values (and practice pandas operations) \u00b6 Let's look back at our dataset description dataframe above, what do we notice, what contains null values? There are several strategies for dealing with null values. For now let's take the simplest case, and drop rows in our dataframe that contain null # Cell for Exercise 2 # For this templated exercise you are going to complete everything in one line # of code, but we are going to break it up into steps. So for each part (A, B, # etc.) paste your answer from the previous part to begin (your opertaions will # read from left to right) # step A # select the 'density' and 'fixed acidity' columns of red. make sure the return # is a dataframe df [[ 'density' , 'fixed acidity' ]] # step B # now use the dropna() method on axis 0 (the rows) to drop any null values df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 ) # step B # select column 'density' df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] # step C # select the values df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values # step D # reshape the result with an empty second dimension using .reshape() and store # the result under variable x x = df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values . reshape ( - 1 , 1 ) # repeat the same process with 'fixed acidity' and variable y y = df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) Now that we have our x and y arrays we can fit using ScikitLearn x = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values . reshape ( - 1 , 1 ) y = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) \ud83d\ude4b Question 3: why do we drop null values across both columns? \u00b6 Notice in the above cell how we selected both density and fixed acidity before calling dropna ? Why did we do that? Why didn't we just select density in the x variable case and fixed acidity in the y variable case? # Fit to the data model . fit ( x , y ) # Extract the values of interest m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # Print the slope m and intercept b print ( 'Scikit learn - Slope: ' , m , 'Intercept: ' , b ) Scikit learn - Slope: 616.01314280661 Intercept: -605.6880086750523 \ud83c\udfcb\ufe0f Exercise 3: calculating y_pred \u00b6 Estimate the values of \\(y\\) by using your fitted parameters. Hint: Use your model.coef_ and model.intercept_ parameters to estimate y_pred following equation 1 # define y_pred in terms of m, x, and b y_pred = m * x + b # uncomment the following lines! fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x , y_pred , ls = '' , marker = '*' ) ax . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f6781becca0>] We can also return predictions directly with the model object using the predict() method note: it is great to get in the habit of utilizing model outputs this way, as the API will be similar across all scikit-learn models (and sometimes models in other libraries as well!) # Another way to get this is using the model.predict function y_pred = model . predict ( x ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x , y_pred , ls = '' , marker = '*' ) ax . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f6781b5c790>] 1.3 Error and topics of model fitting (assessing model accuracy) \u00b6 1.3.1 Measuring the quality of fit \u00b6 1.3.1.1 Mean Squared Error \u00b6 The plot in Section 1.2.3 looks good, but numerically what is our error? What is the mean value of $\\epsilon$, i.e. the Mean Squared Error (MSE) ? {\\sf MSE}=\\epsilon_{\\sf ave} = \\frac{\\sum_{i=1}^{N_{\\sf times}}\\left(y^{\\sf exact}_i - m\\cdot t_i - b \\right)^2}{N_{\\sf times}}\\;\\;\\;\\;\\;\\sf eq. 3 # The mean squared error print('Mean squared error: %.2f' % mean_squared_error(y, y_pred)) # The mean squared error print ( 'Mean squared error: %.2f ' % mean_squared_error ( y , y_pred )) Mean squared error: 1.68 1.3.1.2 R-square \u00b6 Another way to measure error is the regression score, \\(R^2\\). \\(R^2\\) is generally defined as the ratio of the total sum of squares \\(SS_{\\sf tot}\\) to the residual sum of squares \\(SS_{\\sf res}\\): SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\; \\sf eq. 4 SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2\\;\\;\\;\\;\\; \\sf eq. 5 R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}} \\;\\;\\;\\;\\;\\; \\sf eq. 6 In eq. 4, \\(\\bar{y}=\\sum_i y^{\\sf exact}_i/N\\) is the average value of y for \\(N\\) points. The best value of \\(R^2\\) is 1 but it can also take a negative value if the error is large. See all the different regression metrics here . \ud83d\ude4b Question 4: lets understand \\(R^2\\) \u00b6 Do we need a large value of \\(SS_{\\sf tot}\\) to minimize \\(R^2\\) - is this something which we have the power to control? # Print the coefficient of determination - 1 is perfect prediction print('Coefficient of determination: %.2f' % r2_score(y, y_pred)) # Print the coefficient of determination - 1 is perfect prediction print ( 'Coefficient of determination: %.2f ' % r2_score ( y , y_pred )) Coefficient of determination: 0.45 1.3.2 Corollaries with classification models \u00b6 For classification tasks, we typically assess accuracy vs MSE or R-square, since we are dealing with categorical rather than numerical predictions. What is accuracy? It is defined as the ratio of True assignments to all assignments. For a binary positive/negative classification task this can be written as the following: Acc = \\frac{T_p + T_n}{F_p + F_n + T_p + T_n} Where \\(T\\) is True, \\(F\\) is false, \\(p\\) is positive, \\(n\\) is negative Just as a quick example, we can perform this type of task on our wine dataset by predicting whether a given data entry is for red or white wine: logdf = df . copy () . dropna ( axis = 0 ) y_train = logdf . pop ( 'type' ) . values . reshape ( - 1 , 1 ) x_train = logdf . dropna ( axis = 0 ) . values # train a logistic regression model on the training set from sklearn.linear_model import LogisticRegression # instantiate model logreg = LogisticRegression () # fit model logreg . fit ( x_train , y_train ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( LogisticRegression() # make class predictions for the testing set y_pred_class = logreg . predict ( x_train ) # calculate accuracy from sklearn import metrics print ( metrics . accuracy_score ( y_train , y_pred_class )) 0.9797307751818041 1.3.3 Beyond a single input feature \u00b6 ( also: quick appreciative beat for folding in domain area expertise into our models and features ) The acidity of the wine (the dependent variable v) could depend on: potassium from the soil (increases alkalinity) unripe grapes (increases acidity) grapes grown in colder climates or reduced sunshine create less sugar (increases acidity) preprocessing such as adding tartaric acid to the grape juice before fermentation (increases acidity) malolactic fermentation (reduces acidity) + others So in our lab today we will look at folding in additional variables in our dataset into the model 1.4 Multivariate regression \u00b6 Let's now turn our attention to wine quality. The value we aim to predict or evaluate is the quality of each wine in our dataset. This is our dependent variable. We will look at how this is related to the 12 other independent variables, also known as input features . We're going to do this with only the red wine data red . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 4898 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 4899 red 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5 4900 red 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5 4901 red 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6 4902 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 1.4.1 Linear regression with all input fields \u00b6 For this example, notice we have a categorical data variable in the 'type' column. We will ignore this for now, and only work with our red wines. In the future we will discuss how to deal with categorical variable such as this in a mathematical representation. # this is a list of all our features or independent variables features = list ( red . columns [ 1 :]) # we're going to remove our target or dependent variable, density from this # list features . remove ( 'density' ) # now we define X and y according to these lists of names X = red . dropna ( axis = 0 )[ features ] . values y = red . dropna ( axis = 0 )[ 'density' ] . values # we will talk about scaling/centering our data at a later time X = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) red . isnull () . sum ( axis = 0 ) # we are getting rid of some nasty nulls! type 0 fixed acidity 2 volatile acidity 1 citric acid 1 residual sugar 0 chlorides 0 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 2 sulphates 2 alcohol 0 quality 0 dtype: int64 # Create linear regression object - note that we are using all the input features model = linear_model.LinearRegression() model.fit(X, y) y_calc = model.predict(X) # Create linear regression object - note that we are using all the input features model = linear_model . LinearRegression () model . fit ( X , y ) y_calc = model . predict ( X ) Let's see what the coefficients look like ... print(\"Fit coefficients: \\n\", model.coef_, \"\\nNumber of coefficients:\", len(model.coef_)) print ( \"Fit coefficients: \\n \" , model . coef_ , \" \\n Number of coefficients:\" , len ( model . coef_ )) Fit coefficients: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] Number of coefficients: 11 We have 11 !!! That's because we are regressing respect to all 11 independent variables !!! So now, y_{\\sf calc}= m_1x_1 +\\, m_2x_2 \\,+ \\,m_3x_3 \\,+\\,... \\,+ \\,b =\\sum_{i=1}^{13}m_i x_i + b\\;\\;\\;\\;\\; \\sf eq. 7 print(\"We have 13 slopes / weights:\\n\\n\", model.coef_) print(\"\\nAnd one intercept: \", model.intercept_) print ( \"We have 11 slopes / weights: \\n\\n \" , model . coef_ ) print ( \" \\n And one intercept: \" , model . intercept_ ) We have 11 slopes / weights: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] And one intercept: 0.9967517451349656 # This size should match the number of columns in X if len(X[0]) == len(model.coef_): print(\"All good! The number of coefficients matches the number of input features.\") else: print(\"Hmm .. something strange is going on.\") # This size should match the number of columns in X if len ( X [ 0 ]) == len ( model . coef_ ): print ( \"All good! The number of coefficients matches the number of input features.\" ) else : print ( \"Hmm .. something strange is going on.\" ) All good! The number of coefficients matches the number of input features. \ud83c\udfcb\ufe0f Exercise 4: evaluate the error \u00b6 Let's evaluate the error by computing the MSE and \\(R^2\\) metrics (see eq. 3 and 6). # The mean squared error # part A # calculate the MSE using mean_squared_error() # mse = # part B # calculate the R square using r2_score() # r2 = print('Mean squared error: {:.2f}'.format(mse) print('Coefficient of determination: {:.2f}'.format(r2) # The mean squared error # part A # calculate the MSE using mean_squared_error() mse = mean_squared_error ( y , model . predict ( X )) # part B # calculate the R square using r2_score() r2 = r2_score ( y , model . predict ( X )) print ( 'Mean squared error: {:.2e} ' . format ( mse )) print ( 'Coefficient of determination: {:.2f} ' . format ( r2 )) Mean squared error: 5.62e-07 Coefficient of determination: 0.84 \ud83c\udfcb\ufe0f Exercise 5: make a plot of y actual vs y predicted \u00b6 We can also look at how well the computed values match the true values graphically by generating a scatterplot. # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style # plt.plot() plt.title(\"Linear regression - computed values on entire data set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style plt . plot ( y , model . predict ( X ), ls = '' , marker = '.' ) plt . title ( \"Linear regression - computed values on entire data set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () \ud83c\udf52 1.4.2 Enrichment : Splitting into train and test sets \u00b6 note: more of this topic is covered in Model Selection and Validation To see whether we can predict, we will carry out our regression only on a part, 80%, of the full data set. This part is called the training data. We will then test the trained model to predict the rest of the data, 20% - the test data. The function which fits won't see the test data until it has to predict it. We will motivate the use of train/test sets more explicitly in Model Selection and Validation We start by splitting out data using scikit-learn's train_test_split() function: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) ``` ```python X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) Now we check the size of y_train and y_test , the sum should be the size of y! If this works then we move on and carry out regression but we only use the training data! if len(y_test)+len(y_train) == len(y): print('All good, ready to to go and regress!\\n') # Carry out linear regression print('Running linear regression algorithm on the training set\\n') model = linear_model.LinearRegression() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_pred_test = model.predict(X_test) if len ( y_test ) + len ( y_train ) == len ( y ): print ( 'All good, ready to to go and regress! \\n ' ) # Carry out linear regression print ( 'Running linear regression algorithm on the training set \\n ' ) model = linear_model . LinearRegression () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_pred_test = model . predict ( X_test ) All good, ready to to go and regress! Running linear regression algorithm on the training set Fit coefficients and intercept: [ 1.62385613e-03 1.10578142e-04 7.75216492e-07 5.87755741e-04 7.65190323e-05 -1.03490059e-04 8.87357873e-05 7.79083342e-04 2.23534769e-04 -9.99858829e-04 5.85256438e-06] 0.9967531628434799 Now we can plot our predicted values to see how accurate we are in predicting. We will generate a scatterplot and computing the MSE and \\(R^2\\) metrics of error. sns.scatterplot(x=y_pred_test, y=y_test, color=\"mediumvioletred\", s=50) plt.title(\"Linear regression - predict test set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred_test)) sns . scatterplot ( x = y_pred_test , y = y_test , color = \"mediumvioletred\" , s = 50 ) plt . title ( \"Linear regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2e ' % mean_squared_error ( y_test , y_pred_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_pred_test )) Mean squared error: 5.45e-07 Coefficient of determination: 0.87 1.4.2.1 Other data considerations \u00b6 Do we need all the independent variables? Topics of interential statistics covered in a couple sessions Can we output integer quality scores? Topics of non-binary classification tasks covered in week 4 \ud83c\udf52 1.4.3 Enrichment : Other regression algorithms \u00b6 There are many other regression algorithms. The two we want to highlight here are Ridge, LASSO, and Elastic Net. They differ by an added term to the loss function. Let's review. Eq. 2 expanded to multivariate form yields: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 for Ridge regression, we add a regularization term known as L2 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}\\beta_{j}^2 for LASSO (Least Absolute Shrinkage and Selection Operator) we add L1 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}|\\beta_{j}| The key difference here is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. Elastic Net is a combination of these two regularization methods. model = linear_model.Ridge() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_calc_test = model.predict(X_test) model = linear_model . Ridge () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_calc_test = model . predict ( X_test ) Fit coefficients and intercept: [ 1.61930554e-03 1.11227142e-04 2.64709094e-06 5.87271456e-04 7.58510569e-05 -1.02851782e-04 8.76686650e-05 7.75641517e-04 2.23315063e-04 -9.98653815e-04 5.26839010e-06] 0.9967531358810221 sns.scatterplot(x=y_calc_test, y=y_test, color=\"lightseagreen\", s=50) plt.title(\"Ridge regression - predict test set\",fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_calc_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_calc_test)) sns . scatterplot ( x = y_calc_test , y = y_test , color = \"lightseagreen\" , s = 50 ) plt . title ( \"Ridge regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2e ' % mean_squared_error ( y_test , y_calc_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_calc_test )) Mean squared error: 5.45e-07 Coefficient of determination: 0.87 \ud83c\udfcb\ufe0f Exercise 6: Tune Hyperparameter for Ridge Regression \u00b6 Use the docstring to peak into the hyperparameters for Ridge Regression. What is the optimal value of lambda? Plot the \\(\\beta\\) values vs \\(\\lambda\\) from the results of your analysis # cell for exercise 3 out_lambdas = [] out_coefs = [] out_scores = [] for i in range ( 10 ): lambdas = [] coefs = [] scores = [] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) for lamb in range ( 1 , int ( 5e3 ), 20 ): model = linear_model . Ridge ( alpha = lamb ) model . fit ( X_train , y_train ) lambdas . append ( lamb ) coefs . append ( model . coef_ ) scores . append ( r2_score ( y_test , model . predict ( X_test ))) # print('MSE: %.4f' % mean_squared_error(y_test, model.predict(X_test))) # print('R2: %.4f' % r2_score(y_test, model.predict(X_test))) out_lambdas . append ( lambdas ) out_coefs . append ( coefs ) out_scores . append ( scores ) coef_means = np . array ( out_coefs ) . mean ( axis = 0 ) coef_stds = np . array ( out_coefs ) . std ( axis = 0 ) results_means = pd . DataFrame ( coef_means , columns = features ) results_stds = pd . DataFrame ( coef_stds , columns = features ) results_means [ 'lambda' ] = [ i for i in lambdas ] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) for feat in features : ax . errorbar ([ i for i in lambdas ], results_means [ feat ], yerr = results_stds [ feat ], label = feat ) # results.plot('lambda', 'scores', ax=ax[1]) ax . legend () <matplotlib.legend.Legend at 0x7f6777ffbe20> results = pd . DataFrame ( coefs , columns = features ) results [ 'lambda' ] = [ i for i in lambdas ] results [ 'scores' ] = scores fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) for feat in features : results . plot ( 'lambda' , feat , ax = ax [ 0 ]) results . plot ( 'lambda' , 'scores' , ax = ax [ 1 ]) <AxesSubplot:xlabel='lambda'> \ud83c\udf52 1.5 Enrichment : Additional Regression Exercises \u00b6 Problem 1) Number and choice of input features \u00b6 Load the red wine dataset and evaluate how the linear regression predictions changes as you change the number and choice of input features . The total number of columns in X is 11 and each column represent a specific input feature. Estimate the MSE print(X_train.shape) print ( X_train . shape ) (1274, 11) If you want to use the first 5 features you could proceed as following: X_train_five = X_train[:,0:5] X_test_five = X_test[:,0:5] X_train_five = X_train [:, 0 : 5 ] X_test_five = X_test [:, 0 : 5 ] Check that the new variables have the shape your expect print(X_train_five.shape) print(X_test_five.shape) print ( X_train_five . shape ) print ( X_test_five . shape ) (1274, 5) (319, 5) Now you can use these to train your linear regression model and repeat for different numbers or sets of input features! Note that you do not need to change the output feature! It's size is independent from the number of input features, yet recall that its length is the same as the number of values per input feature. Questions to think about while you work on this problem - How many input feature variables does one need? Is there a maximum or minimum number? - Could one input feature variable be better than the rest? - What if values are missing for one of the input feature variables - is it still worth using it? - Can you use L1 or L2 to determine these optimum features more quickly? Problem 2) Type of regression algorithm \u00b6 Try using other types of linear regression methods on the wine dataset: the LASSO model and the Elastic net model which are described by the sklearn.linear_model.ElasticNet() sklearn.linear_model.Lasso() scikit-learn functions. For more detail see ElasticNet and Lasso . Questions to think about while you work on this problem - How does the error change with each model? - Which model seems to perform best? - How can you optimize the hyperparameter, \\(\\lambda\\) - Does one model do better than the other at determining which input features are more important? - How about non linear regression / what if the data does not follow a line? from sklearn.linear_model import ElasticNet from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.linear_model import LinearRegression for model in [ ElasticNet , Lasso , Ridge , LinearRegression ]: model = model () model . fit ( X_train , y_train ) print ( str ( model )) print ( 'Mean squared error: %.ef' % mean_squared_error ( y_test , model . predict ( X_test ))) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , model . predict ( X_test ))) print () ElasticNet() Mean squared error: 4e-06f Coefficient of determination: -0.01 Lasso() Mean squared error: 4e-06f Coefficient of determination: -0.01 Ridge() Mean squared error: 6e-07f Coefficient of determination: 0.85 LinearRegression() Mean squared error: 6e-07f Coefficient of determination: 0.85 References \u00b6 Linear Regression To find out more see simple linear regression scikit-learn Scikit-learn Linear regression in scikit-learn Metrics of error The Boston dataset Pearson correlation To find out more see pearson Irreducible error, bias and variance Great Coursera videos here and here","title":"SOLN S1 Regression and Analysis"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#data-science-foundations-session-1-regression-and-analysis","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at fitting data to a curve using regression . We will also look at using regression to make predictions for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error.","title":"Data Science Foundations Session 1: Regression and Analysis"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#10-preparing-environment-and-importing-data","text":"back to top","title":"1.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#101-import-packages","text":"back to top # Import pandas, pyplot, ipywidgets import pandas as pd from matplotlib import pyplot as plt from ipywidgets import interact # Import Scikit-Learn library for the regression models import sklearn from sklearn import linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score # for enrichment topics import seaborn as sns import numpy as np","title":"1.0.1 Import Packages"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#102-load-dataset","text":"back to top For our discussion on regression and descriptive statistics today we will use a well known dataset of different wines and their quality ratings df = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) df . shape (6497, 13) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 white 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 white 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 white 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 white 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6","title":"1.0.2 Load Dataset"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#11-what-is-regression","text":"It is the process of finding a relationship between dependent and independent variables to find trends in data. This abstract definition means that you have one variable (the dependent variable) which depends on one or more variables (the independent variables). One of the reasons for which we want to regress data is to understand whether there is a trend between two variables. Housing Prices Example We can imagine this scenario with housing prices. Envision a mixed dataset of continuous and discrete independent variables. Some features could be continuous, floating point values like location ranking and housing condition. Others could be descrete like the number of rooms or bathrooms. We could take these features and use them to predict a house value. This would be a regression model.","title":"1.1 What is regression?"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#12-linear-regression-fitting-with-scikit-learn","text":"","title":"1.2  Linear regression fitting with scikit-learn"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#exercise-1-rudimentary-eda","text":"What does the data look like? Recall how to visualize data in a pandas dataframe for every column calculate the * skew: `df.skew()` * kurtosis: `df.kurtosis()` * pearsons correlation with the dependent variable: `df.corr()` * number of missing entries `df.isnull()` and organize this into a new dataframe note: pearsons is just one type of correlation, another available to us spearman which differs from pearsons in that it depends on ranked values rather than their direct quantities, you can read more here df . isnull () . sum () type 0 fixed acidity 10 volatile acidity 8 citric acid 3 residual sugar 2 chlorides 2 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 9 sulphates 4 alcohol 0 quality 0 dtype: int64 # Cell for Exercise 1 # part A # using df.<method> define the following four variables with the results from # skew(), kurtosis(), corr() (while selecting for quality), and isnull() # for isnull() you'll notice the return is a dataframe of booleans. we would # like to simply know the number of null values for each column. change the # return of isnull() using the sum() method along the columns skew = df . skew () kurt = df . kurtosis () pear = df . corr ()[ 'quality' ] null = df . isnull () . sum ( axis = 0 ) # part B # on line 13, put these results in a list using square brackets and call # pd.DataFrame on the list to make your new DataFrame! store it under the # variable name dff dff = pd . DataFrame ([ skew , kurt , pear , null ]) # part C # take the transpose of this DataFrame using dff.T. reassign dff to this copy dff = dff . T # part D # set the column names to 'skew', 'kurtosis', 'pearsons _quality', and # 'null count' using dff.columns dff . columns = [ 'skew' , 'kurtosis' , 'pearsons _quality' , 'null count' ] # Now return dff to the output to view your hand work dff # uncomment this line /tmp/ipykernel_1422/4028752270.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. skew = df.skew() /tmp/ipykernel_1422/4028752270.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. kurt = df.kurtosis() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count fixed acidity 1.722805 5.057727 -0.077031 10.0 volatile acidity 1.495512 2.827081 -0.265953 8.0 citric acid 0.473032 2.401582 0.085706 3.0 residual sugar 1.435000 4.358134 -0.036825 2.0 chlorides 5.399849 50.894874 -0.200886 2.0 free sulfur dioxide 1.220066 7.906238 0.055463 0.0 total sulfur dioxide -0.001177 -0.371664 -0.041385 0.0 density 0.503602 6.606067 -0.305858 0.0 pH 0.386966 0.370068 0.019366 9.0 sulphates 1.798467 8.659892 0.038729 4.0 alcohol 0.565718 -0.531687 0.444319 0.0 quality 0.189623 0.232322 1.000000 0.0 type NaN NaN NaN 0.0 I have gone ahead and repeated this exercise with the red vs white wine types: red = df . loc [ df [ 'type' ] == 'red' ] wht = df . loc [ df [ 'type' ] == 'white' ] def get_summary ( df ): skew = df . skew () kurt = df . kurtosis () pear = df . corr ()[ 'quality' ] null = df . isnull () . sum () med = df . median () men = df . mean () dff = pd . DataFrame ([ skew , kurt , pear , null , med , men ]) dff = dff . T dff . columns = [ 'skew' , 'kurtosis' , 'pearsons _quality' , 'null count' , 'median' , 'mean' ] return dff dffr = get_summary ( red ) dffw = get_summary ( wht ) desc = pd . concat ([ dffr , dffw ], keys = [ 'red' , 'white' ]) /tmp/ipykernel_1422/2387423026.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. skew = df.skew() /tmp/ipykernel_1422/2387423026.py:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. kurt = df.kurtosis() /tmp/ipykernel_1422/2387423026.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. med = df.median() /tmp/ipykernel_1422/2387423026.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. men = df.mean() desc .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } skew kurtosis pearsons _quality null count median mean red fixed acidity 0.982192 1.132624 0.123834 2.0 7.90000 8.322104 volatile acidity 0.672862 1.226846 -0.390858 1.0 0.52000 0.527738 citric acid 0.317891 -0.788476 0.226917 1.0 0.26000 0.271145 residual sugar 4.540655 28.617595 0.013732 0.0 2.20000 2.538806 chlorides 5.680347 41.715787 -0.128907 0.0 0.07900 0.087467 free sulfur dioxide 1.250567 2.023562 -0.050656 0.0 14.00000 15.874922 total sulfur dioxide 1.515531 3.809824 -0.185100 0.0 38.00000 46.467792 density 0.071288 0.934079 -0.174919 0.0 0.99675 0.996747 pH 0.194803 0.814690 -0.057094 2.0 3.31000 3.310864 sulphates 2.429115 11.712632 0.251685 2.0 0.62000 0.658078 alcohol 0.860829 0.200029 0.476166 0.0 10.20000 10.422983 quality 0.217802 0.296708 1.000000 0.0 6.00000 5.636023 type NaN NaN NaN 0.0 NaN NaN white fixed acidity 0.647981 2.176560 -0.114032 8.0 6.80000 6.855532 volatile acidity 1.578595 5.095526 -0.194976 7.0 0.26000 0.278252 citric acid 1.284217 6.182036 -0.009194 2.0 0.32000 0.334250 residual sugar 1.076601 3.469536 -0.097373 2.0 5.20000 6.393250 chlorides 5.023412 37.560847 -0.210181 2.0 0.04300 0.045778 free sulfur dioxide 1.406745 11.466342 0.008158 0.0 34.00000 35.308085 total sulfur dioxide 0.390710 0.571853 -0.174737 0.0 134.00000 138.360657 density 0.977773 9.793807 -0.307123 0.0 0.99374 0.994027 pH 0.458402 0.532552 0.098858 7.0 3.18000 3.188203 sulphates 0.977361 1.589847 0.053690 2.0 0.47000 0.489835 alcohol 0.487342 -0.698425 0.435575 0.0 10.40000 10.514267 quality 0.155796 0.216526 1.000000 0.0 6.00000 5.877909 type NaN NaN NaN 0.0 NaN NaN def my_fig ( metric = desc . columns ): fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) pd . DataFrame ( desc [ metric ]) . unstack ()[ metric ] . T . plot ( kind = 'barh' , ax = ax ) interact ( my_fig ) interactive(children=(Dropdown(description='metric', options=('skew', 'kurtosis', 'pearsons _quality', 'null c\u2026 <function __main__.my_fig(metric=Index(['skew', 'kurtosis', 'pearsons _quality', 'null count', 'median', 'mean'], dtype='object'))>","title":"\ud83c\udfcb\ufe0f Exercise 1: rudimentary EDA"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#question-1-discussion-around-eda-plot","text":"What do we think of this plot? metric = mean , the cholrides values metric = kurtosis , residual sugar metric = pearsons _quality , magnitudes and directions How to improve the plot, what other plots would we like to see? For instance, what if we were really curious about the high kurtosis for chlorides content? What more would we like to glean about the distribution of chloride content? # we can use df.describe() to take a look at the quantile values and min/max df [ 'chlorides' ] . describe () count 6495.000000 mean 0.056042 std 0.035036 min 0.009000 25% 0.038000 50% 0.047000 75% 0.065000 max 0.611000 Name: chlorides, dtype: float64 # and see how these values appear in a KDE fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) df [ 'chlorides' ] . plot ( kind = 'kde' , ax = ax ) ax . set_xlim ( 0 , .61 ) (0.0, 0.61) # lastly we may want to look at the raw values themselves. We can sort them # too view outliers df [ 'chlorides' ] . sort_values ( ascending = False )[: 50 ] 5156 0.611 5049 0.610 5004 0.467 4979 0.464 5590 0.422 6268 0.415 6270 0.415 5652 0.415 6217 0.414 5949 0.414 5349 0.413 6158 0.403 4981 0.401 5628 0.387 6063 0.369 4915 0.368 5067 0.360 5179 0.358 484 0.346 5189 0.343 4917 0.341 5124 0.337 4940 0.332 1217 0.301 687 0.290 4473 0.271 5079 0.270 6272 0.267 5138 0.263 1865 0.255 5466 0.250 1034 0.244 5674 0.243 5675 0.241 683 0.240 1638 0.239 5045 0.236 6456 0.235 6468 0.230 5465 0.226 5464 0.226 5564 0.222 2186 0.217 5996 0.216 6333 0.214 5206 0.214 6332 0.214 5205 0.213 4497 0.212 1835 0.211 Name: chlorides, dtype: float64","title":"\ud83d\ude4b Question 1: Discussion Around EDA Plot"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#122-visualizing-the-data-set-motivating-regression-analysis","text":"In order to demonstrate simple linear regression with this dataset we will look at two particular features: fixed acidity and density . We can create a scatter plot of fixed acidity vs density for the red wine in the dataset using df.plot() and see that there appears to be a general trend between the two features: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) df . loc [ df [ 'type' ] == 'red' ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> Now the question is: How do we quantify this trend?","title":"1.2.2 Visualizing the data set - motivating regression analysis"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#123-estimating-the-regression-coefficients","text":"It looks like density increases with fixed acidity following a line, maybe something like y(x)= m \\cdot x + b \\;\\;\\;\\;\\;\\;\\;\\; \\sf{eq. 1} with \\( y=\\sf density \\), \\(x=\\sf fixed \\space acidity\\), and \\(m\\) the slope and \\(b\\) the intercept. To solve the problem, we need to find the values of \\(b\\) and \\(m\\) in equation 1 to best fit the data. This is called linear regression . In linear regression our goal is to minimize the error between computed values of positions \\(y^{\\sf calc}(x_i)\\equiv y^{\\sf calc}_i\\) and known values \\(y^{\\sf exact}(x_i)\\equiv y^{\\sf exact}_i\\), i.e. find \\(b\\) and \\(m\\) which lead to lowest value of \\epsilon (m,b) =SS_{\\sf res}=\\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y^{\\sf exact}_i - m\\cdot x_i - b \\right)^2\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sf{eq. 2} Otherwise known as the residual sum of squares To find out more see e.g. https://en.wikipedia.org/wiki/Simple_linear_regression","title":"1.2.3 Estimating the regression coefficients"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#question-2-linear-regression-loss-function","text":"Do we always want m and b to be large positive numbers so as to minimize eq. 2? Luckily scikit-learn contains many functions related to regression including linear regression . The function we will use is called LinearRegression() . # Create linear regression object model = linear_model.LinearRegression() # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red['density'].values.reshape(-1, 1) y = red['fixed acidity'].values.reshape(-1, 1) # Create linear regression object model = linear_model . LinearRegression () # Use model to fit to the data, the x values are densities and the y values are fixed acidity # Note that we need to reshape the vectors to be of the shape x - (n_samples, n_features) and y (n_samples, n_targets) x = red [ 'density' ] . values . reshape ( - 1 , 1 ) y = red [ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) print(red['density'].values.shape, red['fixed acidity'].values.shape) print(x.shape, y.shape) print ( red [ 'density' ] . values . shape , red [ 'fixed acidity' ] . values . shape ) print ( x . shape , y . shape ) (1599,) (1599,) (1599, 1) (1599, 1) # Fit to the data model.fit(x, y) # Extract the values of interest m = model.coef_[0][0] b = model.intercept_[0] # Print the slope m and intercept b print('Scikit learn - Slope: ', m , 'Intercept: ', b ) What happens when we try to fit the data as is? # Fit to the data # model.fit(x, y)","title":"\ud83d\ude4b Question 2: linear regression loss function"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#exercise-2-drop-null-values-and-practice-pandas-operations","text":"Let's look back at our dataset description dataframe above, what do we notice, what contains null values? There are several strategies for dealing with null values. For now let's take the simplest case, and drop rows in our dataframe that contain null # Cell for Exercise 2 # For this templated exercise you are going to complete everything in one line # of code, but we are going to break it up into steps. So for each part (A, B, # etc.) paste your answer from the previous part to begin (your opertaions will # read from left to right) # step A # select the 'density' and 'fixed acidity' columns of red. make sure the return # is a dataframe df [[ 'density' , 'fixed acidity' ]] # step B # now use the dropna() method on axis 0 (the rows) to drop any null values df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 ) # step B # select column 'density' df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] # step C # select the values df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values # step D # reshape the result with an empty second dimension using .reshape() and store # the result under variable x x = df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values . reshape ( - 1 , 1 ) # repeat the same process with 'fixed acidity' and variable y y = df [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'fixed acidity' ] . values . reshape ( - 1 , 1 ) Now that we have our x and y arrays we can fit using ScikitLearn x = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'density' ] . values . reshape ( - 1 , 1 ) y = red [[ 'density' , 'fixed acidity' ]] . dropna ( axis = 0 )[ 'fixed acidity' ] . values . reshape ( - 1 , 1 )","title":"\ud83c\udfcb\ufe0f Exercise 2: drop Null Values (and practice pandas operations)"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#question-3-why-do-we-drop-null-values-across-both-columns","text":"Notice in the above cell how we selected both density and fixed acidity before calling dropna ? Why did we do that? Why didn't we just select density in the x variable case and fixed acidity in the y variable case? # Fit to the data model . fit ( x , y ) # Extract the values of interest m = model . coef_ [ 0 ][ 0 ] b = model . intercept_ [ 0 ] # Print the slope m and intercept b print ( 'Scikit learn - Slope: ' , m , 'Intercept: ' , b ) Scikit learn - Slope: 616.01314280661 Intercept: -605.6880086750523","title":"\ud83d\ude4b Question 3: why do we drop null values across both columns?"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#exercise-3-calculating-y_pred","text":"Estimate the values of \\(y\\) by using your fitted parameters. Hint: Use your model.coef_ and model.intercept_ parameters to estimate y_pred following equation 1 # define y_pred in terms of m, x, and b y_pred = m * x + b # uncomment the following lines! fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x , y_pred , ls = '' , marker = '*' ) ax . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f6781becca0>] We can also return predictions directly with the model object using the predict() method note: it is great to get in the habit of utilizing model outputs this way, as the API will be similar across all scikit-learn models (and sometimes models in other libraries as well!) # Another way to get this is using the model.predict function y_pred = model . predict ( x ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . plot ( x , y_pred , ls = '' , marker = '*' ) ax . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f6781b5c790>]","title":"\ud83c\udfcb\ufe0f Exercise 3: calculating y_pred"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#13-error-and-topics-of-model-fitting-assessing-model-accuracy","text":"","title":"1.3 Error and topics of model fitting (assessing model accuracy)"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#131-measuring-the-quality-of-fit","text":"","title":"1.3.1 Measuring the quality of fit"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#1311-mean-squared-error","text":"The plot in Section 1.2.3 looks good, but numerically what is our error? What is the mean value of $\\epsilon$, i.e. the Mean Squared Error (MSE) ? {\\sf MSE}=\\epsilon_{\\sf ave} = \\frac{\\sum_{i=1}^{N_{\\sf times}}\\left(y^{\\sf exact}_i - m\\cdot t_i - b \\right)^2}{N_{\\sf times}}\\;\\;\\;\\;\\;\\sf eq. 3 # The mean squared error print('Mean squared error: %.2f' % mean_squared_error(y, y_pred)) # The mean squared error print ( 'Mean squared error: %.2f ' % mean_squared_error ( y , y_pred )) Mean squared error: 1.68","title":"1.3.1.1 Mean Squared Error"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#1312-r-square","text":"Another way to measure error is the regression score, \\(R^2\\). \\(R^2\\) is generally defined as the ratio of the total sum of squares \\(SS_{\\sf tot}\\) to the residual sum of squares \\(SS_{\\sf res}\\): SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\; \\sf eq. 4 SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2\\;\\;\\;\\;\\; \\sf eq. 5 R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}} \\;\\;\\;\\;\\;\\; \\sf eq. 6 In eq. 4, \\(\\bar{y}=\\sum_i y^{\\sf exact}_i/N\\) is the average value of y for \\(N\\) points. The best value of \\(R^2\\) is 1 but it can also take a negative value if the error is large. See all the different regression metrics here .","title":"1.3.1.2 R-square"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#question-4-lets-understand-r2","text":"Do we need a large value of \\(SS_{\\sf tot}\\) to minimize \\(R^2\\) - is this something which we have the power to control? # Print the coefficient of determination - 1 is perfect prediction print('Coefficient of determination: %.2f' % r2_score(y, y_pred)) # Print the coefficient of determination - 1 is perfect prediction print ( 'Coefficient of determination: %.2f ' % r2_score ( y , y_pred )) Coefficient of determination: 0.45","title":"\ud83d\ude4b Question 4: lets understand \\(R^2\\)"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#132-corollaries-with-classification-models","text":"For classification tasks, we typically assess accuracy vs MSE or R-square, since we are dealing with categorical rather than numerical predictions. What is accuracy? It is defined as the ratio of True assignments to all assignments. For a binary positive/negative classification task this can be written as the following: Acc = \\frac{T_p + T_n}{F_p + F_n + T_p + T_n} Where \\(T\\) is True, \\(F\\) is false, \\(p\\) is positive, \\(n\\) is negative Just as a quick example, we can perform this type of task on our wine dataset by predicting whether a given data entry is for red or white wine: logdf = df . copy () . dropna ( axis = 0 ) y_train = logdf . pop ( 'type' ) . values . reshape ( - 1 , 1 ) x_train = logdf . dropna ( axis = 0 ) . values # train a logistic regression model on the training set from sklearn.linear_model import LogisticRegression # instantiate model logreg = LogisticRegression () # fit model logreg . fit ( x_train , y_train ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( LogisticRegression() # make class predictions for the testing set y_pred_class = logreg . predict ( x_train ) # calculate accuracy from sklearn import metrics print ( metrics . accuracy_score ( y_train , y_pred_class )) 0.9797307751818041","title":"1.3.2 Corollaries with classification models"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#133-beyond-a-single-input-feature","text":"( also: quick appreciative beat for folding in domain area expertise into our models and features ) The acidity of the wine (the dependent variable v) could depend on: potassium from the soil (increases alkalinity) unripe grapes (increases acidity) grapes grown in colder climates or reduced sunshine create less sugar (increases acidity) preprocessing such as adding tartaric acid to the grape juice before fermentation (increases acidity) malolactic fermentation (reduces acidity) + others So in our lab today we will look at folding in additional variables in our dataset into the model","title":"1.3.3 Beyond a single input feature"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#14-multivariate-regression","text":"Let's now turn our attention to wine quality. The value we aim to predict or evaluate is the quality of each wine in our dataset. This is our dependent variable. We will look at how this is related to the 12 other independent variables, also known as input features . We're going to do this with only the red wine data red . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 4898 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 4899 red 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5 4900 red 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5 4901 red 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6 4902 red 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5","title":"1.4 Multivariate regression"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#141-linear-regression-with-all-input-fields","text":"For this example, notice we have a categorical data variable in the 'type' column. We will ignore this for now, and only work with our red wines. In the future we will discuss how to deal with categorical variable such as this in a mathematical representation. # this is a list of all our features or independent variables features = list ( red . columns [ 1 :]) # we're going to remove our target or dependent variable, density from this # list features . remove ( 'density' ) # now we define X and y according to these lists of names X = red . dropna ( axis = 0 )[ features ] . values y = red . dropna ( axis = 0 )[ 'density' ] . values # we will talk about scaling/centering our data at a later time X = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) red . isnull () . sum ( axis = 0 ) # we are getting rid of some nasty nulls! type 0 fixed acidity 2 volatile acidity 1 citric acid 1 residual sugar 0 chlorides 0 free sulfur dioxide 0 total sulfur dioxide 0 density 0 pH 2 sulphates 2 alcohol 0 quality 0 dtype: int64 # Create linear regression object - note that we are using all the input features model = linear_model.LinearRegression() model.fit(X, y) y_calc = model.predict(X) # Create linear regression object - note that we are using all the input features model = linear_model . LinearRegression () model . fit ( X , y ) y_calc = model . predict ( X ) Let's see what the coefficients look like ... print(\"Fit coefficients: \\n\", model.coef_, \"\\nNumber of coefficients:\", len(model.coef_)) print ( \"Fit coefficients: \\n \" , model . coef_ , \" \\n Number of coefficients:\" , len ( model . coef_ )) Fit coefficients: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] Number of coefficients: 11 We have 11 !!! That's because we are regressing respect to all 11 independent variables !!! So now, y_{\\sf calc}= m_1x_1 +\\, m_2x_2 \\,+ \\,m_3x_3 \\,+\\,... \\,+ \\,b =\\sum_{i=1}^{13}m_i x_i + b\\;\\;\\;\\;\\; \\sf eq. 7 print(\"We have 13 slopes / weights:\\n\\n\", model.coef_) print(\"\\nAnd one intercept: \", model.intercept_) print ( \"We have 11 slopes / weights: \\n\\n \" , model . coef_ ) print ( \" \\n And one intercept: \" , model . intercept_ ) We have 11 slopes / weights: [ 1.64059336e-03 1.23999138e-04 1.16115898e-05 5.83002013e-04 8.35961822e-05 -9.17472420e-05 8.61246026e-05 7.80966358e-04 2.24558885e-04 -9.80600257e-04 -1.75587885e-05] And one intercept: 0.9967517451349656 # This size should match the number of columns in X if len(X[0]) == len(model.coef_): print(\"All good! The number of coefficients matches the number of input features.\") else: print(\"Hmm .. something strange is going on.\") # This size should match the number of columns in X if len ( X [ 0 ]) == len ( model . coef_ ): print ( \"All good! The number of coefficients matches the number of input features.\" ) else : print ( \"Hmm .. something strange is going on.\" ) All good! The number of coefficients matches the number of input features.","title":"1.4.1 Linear regression with all input fields"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#exercise-4-evaluate-the-error","text":"Let's evaluate the error by computing the MSE and \\(R^2\\) metrics (see eq. 3 and 6). # The mean squared error # part A # calculate the MSE using mean_squared_error() # mse = # part B # calculate the R square using r2_score() # r2 = print('Mean squared error: {:.2f}'.format(mse) print('Coefficient of determination: {:.2f}'.format(r2) # The mean squared error # part A # calculate the MSE using mean_squared_error() mse = mean_squared_error ( y , model . predict ( X )) # part B # calculate the R square using r2_score() r2 = r2_score ( y , model . predict ( X )) print ( 'Mean squared error: {:.2e} ' . format ( mse )) print ( 'Coefficient of determination: {:.2f} ' . format ( r2 )) Mean squared error: 5.62e-07 Coefficient of determination: 0.84","title":"\ud83c\udfcb\ufe0f Exercise 4: evaluate the error"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#exercise-5-make-a-plot-of-y-actual-vs-y-predicted","text":"We can also look at how well the computed values match the true values graphically by generating a scatterplot. # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style # plt.plot() plt.title(\"Linear regression - computed values on entire data set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() # generate a plot of y predicted vs y actual using plt.plot() # remember you must set ls to an empty string and marker to some marker style plt . plot ( y , model . predict ( X ), ls = '' , marker = '.' ) plt . title ( \"Linear regression - computed values on entire data set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show ()","title":"\ud83c\udfcb\ufe0f Exercise 5: make a plot of y actual vs y predicted"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#142-enrichment-splitting-into-train-and-test-sets","text":"note: more of this topic is covered in Model Selection and Validation To see whether we can predict, we will carry out our regression only on a part, 80%, of the full data set. This part is called the training data. We will then test the trained model to predict the rest of the data, 20% - the test data. The function which fits won't see the test data until it has to predict it. We will motivate the use of train/test sets more explicitly in Model Selection and Validation We start by splitting out data using scikit-learn's train_test_split() function: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) ``` ```python X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) Now we check the size of y_train and y_test , the sum should be the size of y! If this works then we move on and carry out regression but we only use the training data! if len(y_test)+len(y_train) == len(y): print('All good, ready to to go and regress!\\n') # Carry out linear regression print('Running linear regression algorithm on the training set\\n') model = linear_model.LinearRegression() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_pred_test = model.predict(X_test) if len ( y_test ) + len ( y_train ) == len ( y ): print ( 'All good, ready to to go and regress! \\n ' ) # Carry out linear regression print ( 'Running linear regression algorithm on the training set \\n ' ) model = linear_model . LinearRegression () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_pred_test = model . predict ( X_test ) All good, ready to to go and regress! Running linear regression algorithm on the training set Fit coefficients and intercept: [ 1.62385613e-03 1.10578142e-04 7.75216492e-07 5.87755741e-04 7.65190323e-05 -1.03490059e-04 8.87357873e-05 7.79083342e-04 2.23534769e-04 -9.99858829e-04 5.85256438e-06] 0.9967531628434799 Now we can plot our predicted values to see how accurate we are in predicting. We will generate a scatterplot and computing the MSE and \\(R^2\\) metrics of error. sns.scatterplot(x=y_pred_test, y=y_test, color=\"mediumvioletred\", s=50) plt.title(\"Linear regression - predict test set\", fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred_test)) sns . scatterplot ( x = y_pred_test , y = y_test , color = \"mediumvioletred\" , s = 50 ) plt . title ( \"Linear regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2e ' % mean_squared_error ( y_test , y_pred_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_pred_test )) Mean squared error: 5.45e-07 Coefficient of determination: 0.87","title":"\ud83c\udf52 1.4.2 Enrichment: Splitting into train and test sets"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#1421-other-data-considerations","text":"Do we need all the independent variables? Topics of interential statistics covered in a couple sessions Can we output integer quality scores? Topics of non-binary classification tasks covered in week 4","title":"1.4.2.1 Other data considerations"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#143-enrichment-other-regression-algorithms","text":"There are many other regression algorithms. The two we want to highlight here are Ridge, LASSO, and Elastic Net. They differ by an added term to the loss function. Let's review. Eq. 2 expanded to multivariate form yields: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 for Ridge regression, we add a regularization term known as L2 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}\\beta_{j}^2 for LASSO (Least Absolute Shrinkage and Selection Operator) we add L1 regularization: \\sum_{i=1}^{N}(y_i - \\sum_{j=1}^{P}x_{ij}\\beta_{j})^2 + \\lambda \\sum_{j=1}^{P}|\\beta_{j}| The key difference here is that LASSO will allow coefficients to shrink to 0 while Ridge regression will not. Elastic Net is a combination of these two regularization methods. model = linear_model.Ridge() model.fit(X_train, y_train) print('Fit coefficients and intercept:\\n\\n', model.coef_, '\\n\\n', model.intercept_ ) # Predict on the test set y_calc_test = model.predict(X_test) model = linear_model . Ridge () model . fit ( X_train , y_train ) print ( 'Fit coefficients and intercept: \\n\\n ' , model . coef_ , ' \\n\\n ' , model . intercept_ ) # Predict on the test set y_calc_test = model . predict ( X_test ) Fit coefficients and intercept: [ 1.61930554e-03 1.11227142e-04 2.64709094e-06 5.87271456e-04 7.58510569e-05 -1.02851782e-04 8.76686650e-05 7.75641517e-04 2.23315063e-04 -9.98653815e-04 5.26839010e-06] 0.9967531358810221 sns.scatterplot(x=y_calc_test, y=y_test, color=\"lightseagreen\", s=50) plt.title(\"Ridge regression - predict test set\",fontsize=16) plt.xlabel(\"y$^{\\sf calc}$\") plt.ylabel(\"y$^{\\sf true}$\") plt.show() print('Mean squared error: %.2f' % mean_squared_error(y_test, y_calc_test)) print('Coefficient of determination: %.2f' % r2_score(y_test, y_calc_test)) sns . scatterplot ( x = y_calc_test , y = y_test , color = \"lightseagreen\" , s = 50 ) plt . title ( \"Ridge regression - predict test set\" , fontsize = 16 ) plt . xlabel ( \"y$^{\\sf calc}$\" ) plt . ylabel ( \"y$^{\\sf true}$\" ) plt . show () print ( 'Mean squared error: %.2e ' % mean_squared_error ( y_test , y_calc_test )) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , y_calc_test )) Mean squared error: 5.45e-07 Coefficient of determination: 0.87","title":"\ud83c\udf52 1.4.3 Enrichment: Other regression algorithms"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#exercise-6-tune-hyperparameter-for-ridge-regression","text":"Use the docstring to peak into the hyperparameters for Ridge Regression. What is the optimal value of lambda? Plot the \\(\\beta\\) values vs \\(\\lambda\\) from the results of your analysis # cell for exercise 3 out_lambdas = [] out_coefs = [] out_scores = [] for i in range ( 10 ): lambdas = [] coefs = [] scores = [] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) for lamb in range ( 1 , int ( 5e3 ), 20 ): model = linear_model . Ridge ( alpha = lamb ) model . fit ( X_train , y_train ) lambdas . append ( lamb ) coefs . append ( model . coef_ ) scores . append ( r2_score ( y_test , model . predict ( X_test ))) # print('MSE: %.4f' % mean_squared_error(y_test, model.predict(X_test))) # print('R2: %.4f' % r2_score(y_test, model.predict(X_test))) out_lambdas . append ( lambdas ) out_coefs . append ( coefs ) out_scores . append ( scores ) coef_means = np . array ( out_coefs ) . mean ( axis = 0 ) coef_stds = np . array ( out_coefs ) . std ( axis = 0 ) results_means = pd . DataFrame ( coef_means , columns = features ) results_stds = pd . DataFrame ( coef_stds , columns = features ) results_means [ 'lambda' ] = [ i for i in lambdas ] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) for feat in features : ax . errorbar ([ i for i in lambdas ], results_means [ feat ], yerr = results_stds [ feat ], label = feat ) # results.plot('lambda', 'scores', ax=ax[1]) ax . legend () <matplotlib.legend.Legend at 0x7f6777ffbe20> results = pd . DataFrame ( coefs , columns = features ) results [ 'lambda' ] = [ i for i in lambdas ] results [ 'scores' ] = scores fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) for feat in features : results . plot ( 'lambda' , feat , ax = ax [ 0 ]) results . plot ( 'lambda' , 'scores' , ax = ax [ 1 ]) <AxesSubplot:xlabel='lambda'>","title":"\ud83c\udfcb\ufe0f Exercise 6: Tune Hyperparameter for Ridge Regression"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#15-enrichment-additional-regression-exercises","text":"","title":"\ud83c\udf52 1.5 Enrichment: Additional Regression Exercises"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#problem-1-number-and-choice-of-input-features","text":"Load the red wine dataset and evaluate how the linear regression predictions changes as you change the number and choice of input features . The total number of columns in X is 11 and each column represent a specific input feature. Estimate the MSE print(X_train.shape) print ( X_train . shape ) (1274, 11) If you want to use the first 5 features you could proceed as following: X_train_five = X_train[:,0:5] X_test_five = X_test[:,0:5] X_train_five = X_train [:, 0 : 5 ] X_test_five = X_test [:, 0 : 5 ] Check that the new variables have the shape your expect print(X_train_five.shape) print(X_test_five.shape) print ( X_train_five . shape ) print ( X_test_five . shape ) (1274, 5) (319, 5) Now you can use these to train your linear regression model and repeat for different numbers or sets of input features! Note that you do not need to change the output feature! It's size is independent from the number of input features, yet recall that its length is the same as the number of values per input feature. Questions to think about while you work on this problem - How many input feature variables does one need? Is there a maximum or minimum number? - Could one input feature variable be better than the rest? - What if values are missing for one of the input feature variables - is it still worth using it? - Can you use L1 or L2 to determine these optimum features more quickly?","title":"Problem 1) Number and choice of input features"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#problem-2-type-of-regression-algorithm","text":"Try using other types of linear regression methods on the wine dataset: the LASSO model and the Elastic net model which are described by the sklearn.linear_model.ElasticNet() sklearn.linear_model.Lasso() scikit-learn functions. For more detail see ElasticNet and Lasso . Questions to think about while you work on this problem - How does the error change with each model? - Which model seems to perform best? - How can you optimize the hyperparameter, \\(\\lambda\\) - Does one model do better than the other at determining which input features are more important? - How about non linear regression / what if the data does not follow a line? from sklearn.linear_model import ElasticNet from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.linear_model import LinearRegression for model in [ ElasticNet , Lasso , Ridge , LinearRegression ]: model = model () model . fit ( X_train , y_train ) print ( str ( model )) print ( 'Mean squared error: %.ef' % mean_squared_error ( y_test , model . predict ( X_test ))) print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , model . predict ( X_test ))) print () ElasticNet() Mean squared error: 4e-06f Coefficient of determination: -0.01 Lasso() Mean squared error: 4e-06f Coefficient of determination: -0.01 Ridge() Mean squared error: 6e-07f Coefficient of determination: 0.85 LinearRegression() Mean squared error: 6e-07f Coefficient of determination: 0.85","title":"Problem 2) Type of regression algorithm"},{"location":"solutions/SOLN_S1_Regression_and_Analysis/#references","text":"Linear Regression To find out more see simple linear regression scikit-learn Scikit-learn Linear regression in scikit-learn Metrics of error The Boston dataset Pearson correlation To find out more see pearson Irreducible error, bias and variance Great Coursera videos here and here","title":"References"},{"location":"solutions/SOLN_S2_Inferential_Statistics/","text":"Data Science Foundations, Session 2: Inferential Statistics \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at the utility of EDA combined with inferential statistics. 2.0 Preparing Environment and Importing Data \u00b6 back to top 2.0.1 Import Packages \u00b6 back to top # The modules we've seen before import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns # our stats modules import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy 2.0.2 Load Dataset \u00b6 back to top For this session, we will use dummy datasets from sklearn. df = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... ... ... ... ... ... 1663 Tiramisu Chocolate Outer Doughnut Pear Amethyst Fickelgruber 12/2020 38128.802589 0.420111 1664 Tiramisu Chocolate Outer Doughnut Pear Burgundy Zebrabar 12/2020 108.642857 0.248659 1665 Tiramisu Chocolate Outer Doughnut Pear Teal Zebrabar 12/2020 3517.933333 0.378501 1666 Tiramisu Chocolate Outer Doughnut Rock and Rye Amethyst Slugworth 12/2020 10146.898432 0.213149 1667 Tiramisu Chocolate Outer Doughnut Rock and Rye Burgundy Zebrabar 12/2020 1271.904762 0.431813 1668 rows \u00d7 9 columns descriptors = df . columns [: - 2 ] for col in df . columns [: - 2 ]: print ( col ) print ( df [ col ] . unique ()) print () Base Cake ['Butter' 'Cheese' 'Chiffon' 'Pound' 'Sponge' 'Tiramisu'] Truffle Type ['Candy Outer' 'Chocolate Outer' 'Jelly Filled'] Primary Flavor ['Butter Pecan' 'Ginger Lime' 'Margarita' 'Pear' 'Pink Lemonade' 'Raspberry Ginger Ale' 'Sassafras' 'Spice' 'Wild Cherry Cream' 'Cream Soda' 'Horchata' 'Kettle Corn' 'Lemon Bar' 'Orange Pineapple\\tP' 'Plum' 'Orange' 'Butter Toffee' 'Lemon' 'Acai Berry' 'Apricot' 'Birch Beer' 'Cherry Cream Spice' 'Creme de Menthe' 'Fruit Punch' 'Ginger Ale' 'Grand Mariner' 'Orange Brandy' 'Pecan' 'Toasted Coconut' 'Watermelon' 'Wintergreen' 'Vanilla' 'Bavarian Cream' 'Black Licorice' 'Caramel Cream' 'Cheesecake' 'Cherry Cola' 'Coffee' 'Irish Cream' 'Lemon Custard' 'Mango' 'Sour' 'Amaretto' 'Blueberry' 'Butter Milk' 'Chocolate Mint' 'Coconut' 'Dill Pickle' 'Gingersnap' 'Chocolate' 'Doughnut'] Secondary Flavor ['Toffee' 'Banana' 'Rum' 'Tutti Frutti' 'Vanilla' 'Mixed Berry' 'Whipped Cream' 'Apricot' 'Passion Fruit' 'Peppermint' 'Dill Pickle' 'Black Cherry' 'Wild Cherry Cream' 'Papaya' 'Mango' 'Cucumber' 'Egg Nog' 'Pear' 'Rock and Rye' 'Tangerine' 'Apple' 'Black Currant' 'Kiwi' 'Lemon' 'Hazelnut' 'Butter Rum' 'Fuzzy Navel' 'Mojito' 'Ginger Beer'] Color Group ['Taupe' 'Amethyst' 'Burgundy' 'White' 'Black' 'Opal' 'Citrine' 'Rose' 'Slate' 'Teal' 'Tiffany' 'Olive'] Customer ['Slugworth' 'Perk-a-Cola' 'Fickelgruber' 'Zebrabar' \"Dandy's Candies\"] Date ['1/2020' '2/2020' '3/2020' '4/2020' '5/2020' '6/2020' '7/2020' '8/2020' '9/2020' '10/2020' '11/2020' '12/2020'] 2.1 Many Flavors of Statistical Tests \u00b6 https://luminousmen.com/post/descriptive-and-inferential-statistics Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (\u201cinferences\u201d) from that data. With inferential statistics, you take data from samples and make generalizations about a population statshowto Moods Median Test Kruskal-Wallis Test (Another comparison of Medians test) T-Test Analysis of Variance (ANOVA) One Way ANOVA Two Way ANOVA MANOVA Factorial ANOVA When do I use each of these? We will talk about this as we proceed through the examples. This page from minitab has good rules of thumb on the subject. 2.1.1 What is Mood's Median? \u00b6 You can use Chi-Square to test for a goodness of fit (whether a sample of data represents a distribution) or whether two variables are related (using a contingency table, which we will create below!) A special case of Pearon's Chi-Squared Test: We create a table that counts the observations above and below the global median for two different groups. We then perform a chi-squared test of significance on this contingency table Null hypothesis: the Medians are all equal The chi-square test statistic: x^2 = \\sum{\\frac{(O-E)^2}{E}} Where \\(O\\) is the observed frequency and \\(E\\) is the expected frequency. Let's take an example , say we have three shifts with the following production rates: np . random . seed ( 7 ) shift_one = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_two = [ round ( i ) for i in np . random . normal ( 21 , 3 , 10 )] print ( shift_one ) print ( shift_two ) [21, 15, 16, 17, 14, 16, 16, 11, 19, 18] [19, 20, 23, 20, 20, 17, 23, 21, 22, 16] stat , p , m , table = scipy . stats . median_test ( shift_one , shift_two , correction = False ) what is median_test returning? print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 7.20 p-value of the test: 0.007 the grand median: 18.5 Let's evaluate that test statistic ourselves by taking a look at the contingency table: table array([[2, 8], [8, 2]]) This is easier to make sense of if we order the shift times shift_one . sort () shift_one [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] When we look at shift one, we see that 8 values are at or below the grand median. shift_two . sort () shift_two [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] For shift two, only two are at or below the grand median. Since the sample sizes are the same, the expected value for both groups is the same, 5 above and 5 below the grand median. The chi-square is then: X^2 = \\frac{(2-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(2-5)^2}{5} ( 3 - 5 ) ** 2 / 5 + ( 7 - 5 ) ** 2 / 5 + ( 7 - 5 ) ** 2 / 5 + ( 3 - 5 ) ** 2 / 5 3.2 Our p-value, or the probability of observing the null-hypothsis, is under 0.05. We can conclude that these shift performances were drawn under seperate distributions. For comparison, let's do this analysis again with shifts of equal performances np . random . seed ( 3 ) shift_three = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_four = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] stat , p , m , table = scipy . stats . median_test ( shift_three , shift_four , correction = False ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000 the grand median: 15.5 and the shift raw values: shift_three . sort () shift_four . sort () print ( shift_three ) print ( shift_four ) [10, 14, 15, 15, 15, 16, 16, 16, 17, 21] [11, 12, 13, 14, 15, 16, 19, 19, 19, 21] table array([[5, 5], [5, 5]]) 2.1.2 When to Use Mood's? \u00b6 Mood's Median Test is highly flexible but has the following assumptions: Considers only one categorical factor Response variable is continuous (our shift rates) Data does not need to be normally distributed But the distributions are similarly shaped Sample sizes can be unequal and small (less than 20 observations) Other considerations: Not as powerful as Kruskal-Wallis Test but still useful for small sample sizes or when there are outliers \ud83c\udfcb\ufe0f Exercise 1: Use Mood's Median Test \u00b6 Part A Perform moods median test on Base Cake (Categorical Variable) and EBITDA/KG (Continuous Variable) in Truffle data \u00b6 We're also going to get some practice with pandas groupby. # what is returned by this groupby? gp = df . groupby ( 'Base Cake' ) How do we find out? We could iterate through it: # seems to be a tuple of some sort for i in gp : print ( i ) break ('Butter', Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns]) # the first object appears to be the group print ( i [ 0 ]) # the second object appears to be the df belonging to that group print ( i [ 1 ]) Butter Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns] going back to our diagram from our earlier pandas session. It looks like whenever we split in the groupby method, we create separate dataframes as well as their group label: Ok, so we know gp is separate dataframes. How do we turn the 'EBITDA/KG' column of each dataframe into arrays to then pass to median_test ? # complete this for loop for i , j in gp : print ( i ) print ( j [ 'EBITDA/KG' ] . values ) break # turn 'EBITDA/KG' of j into an array using the .values attribute # print this to the screen Butter [ 5.00423594e-01 2.20395451e-01 1.71013869e-01 2.33024872e-01 4.80689371e-01 1.64934546e-01 2.03213256e-01 1.78681400e-01 1.25050726e-01 2.17021951e-01 7.95955185e-02 3.25042287e-01 2.17551215e-01 2.48152299e-01 -1.20503094e-02 1.47190567e-01 3.84488948e-01 2.05438764e-01 1.32190256e-01 3.23019144e-01 -9.73361477e-03 1.98397692e-01 1.67067902e-01 -2.60063690e-02 1.30365325e-01 2.36337749e-01 -9.70556780e-02 1.59051819e-01 -8.76572259e-02 -3.32199843e-02 -5.05704451e-02 -5.56458806e-02 -8.86273564e-02 4.32267857e-02 -1.88615579e-01 4.24939227e-01 9.35136847e-02 -3.43605950e-02 1.63823520e-01 2.78522916e-01 1.29207730e-01 1.79194495e-01 1.37419569e-01 1.31372653e-01 2.53275225e-01 2.26761431e-01 1.10173466e-01 1.99338787e-01 -2.01250197e-01 1.16567591e-01 1.32324984e-01 4.02912418e-01 9.35051765e-02 1.65865814e-01 2.12269112e-01 2.53461571e-01 1.89055713e-01 1.20416365e-01 3.95276612e-02 2.93121770e-01 1.40947082e-01 -1.21555832e-01 1.56455622e-01 -1.29776953e-02 -6.17934014e-02 -8.19904808e-02 -3.14711557e-02 -8.03820228e-02 1.63839981e-01 8.34406336e-02 1.49369698e-01 1.05990633e-01 1.27399979e-01 2.26634255e-01 -2.20801929e-03 -6.92044284e-02 1.74048414e-01 1.30933438e-01 1.27620323e-01 2.78652749e-01 2.14772018e-01 1.40864278e-01 1.23745138e-01 1.66586809e-01 2.91940995e-01 2.49925584e-01 8.65447719e-02 3.80907774e-01 2.70851719e-01 3.32946265e-01 9.00795862e-03 2.00960974e-01 2.72623570e-01 3.35902190e-01 1.27337723e-01 2.36618545e-01 -6.82774785e-02 3.13166906e-01 2.15752651e-01 9.29694447e-02 3.60809152e-02 2.32488112e-01 3.38200308e-02 1.70916188e-01 2.81620452e-01 -1.61981289e-01 -4.14570666e-02 1.13465970e-02 2.28733252e-01 9.87516565e-02 3.52732668e-02 6.32598661e-02 2.10300526e-01 1.98761726e-01 1.38832882e-01 2.95465366e-01 2.68022024e-01 3.22389724e-01 4.04867623e-01 2.38086167e-01 1.12586985e-01 1.94010438e-01 1.96757297e-01 1.65215620e-01 1.22730941e-02 1.14415249e-01 3.26252563e-01 1.89080695e-01 -5.11830382e-02 2.41661008e-01 2.00063672e-01 3.07633312e-01 4.20740234e-01 1.34764192e-01 -4.75993730e-02 1.52973888e-02 1.87709908e-01 7.20193743e-02 3.48745346e-02 2.77659158e-01 2.73466257e-01 1.32419725e-01 2.85933859e-02 3.99622870e-02 -7.46829380e-02 9.03915641e-02 -9.61708181e-02 7.16896946e-02 1.08714611e-01 1.18536709e-01 8.52229628e-02 4.13523715e-01 7.71194281e-01 1.73738798e-01 3.05406909e-01 1.53831064e-01 2.06911408e-01 1.13075512e-01 1.29416734e-01 1.60275533e-01 2.29962628e-01 2.50895646e-01 1.73060658e-01 2.01020670e-01 3.16227457e-01 1.57652647e-01 5.47188384e-02 2.61436808e-01 1.46570523e-01 1.58977569e-01 2.11215119e-01 1.40679855e-01 -8.00696326e-02 1.59842103e-01 2.00211820e-01 9.92221921e-02 -1.91516176e-02 -5.02510162e-02 -9.15402427e-02 4.28019215e-02 1.06537078e-01 -3.24195486e-01 1.79861627e-02 -1.29900711e-01 -1.18627679e-01 -1.26903307e-01 -1.12941251e-01 2.81344485e-01 -5.75519167e-02 1.62155727e-02 2.14084866e-01 2.05315240e-01 1.27598359e-01 1.89025252e-01 3.96820478e-01 1.20290515e-01 3.32130996e-01 1.37858897e-01 9.78393589e-02 3.51731323e-01 1.10782088e-01 2.27390210e-01 3.89559348e-01 1.74184808e-01 3.08568571e-01 1.71747215e-01 2.33275587e-01 2.56728635e-01 3.02423314e-01 2.74374851e-01 3.27629705e-02 5.61005655e-02 1.68330538e-01 1.12578506e-01 1.08314409e-02 1.33944964e-01 -2.12285231e-01 -1.21224032e-01 1.07819533e-01 3.17613330e-02 2.84300351e-01 -1.58586907e-01 1.36753020e-01 1.26197635e-01 7.40448636e-02 2.35065994e-01 -6.15319415e-02 -7.51966701e-02 4.13427726e-01 1.60539980e-01 1.09901498e-01 1.74329568e-01 1.48135527e-01 1.85728609e-01 2.85476612e-01 2.24898461e-01 1.33343564e-01 1.80618963e-01 2.03080820e-02 2.16728570e-01 1.86566493e-01 1.25929822e-01 1.79317565e-01 3.88162321e-01 2.03009067e-01 2.64872648e-01 4.95978731e-01 1.52347749e-01 -7.23596372e-02 1.29552280e-01 6.16496157e-02 1.05956924e-01 -2.71699836e-01 -5.64473565e-03 -2.50275527e-02 1.29269950e-01 -1.87247727e-01 -3.49347255e-01 -1.93280406e-01 7.87217542e-02 2.21951811e-01 7.10999656e-02 3.49382049e-02 1.48398799e-01 5.65517753e-02 1.05690961e-01 2.55476023e-01 1.28401889e-01 1.33289903e-01 1.14201836e-01 1.43169893e-01 5.69591438e-01 1.54755202e-01 1.55028578e-01 1.64827975e-01 4.67083700e-01 3.31029661e-02 1.62382617e-01 1.54156022e-01 6.55873722e-01 -5.31208735e-02 2.37122763e-01 2.71368392e-01 4.69144223e-01 1.62923984e-01 1.22718216e-01 1.68055251e-01 1.35999904e-01 2.04736813e-01 1.27146904e-01 -1.12549423e-01 3.24840692e-03 7.10375441e-02 7.90146006e-03 5.79775663e-02 -1.57867224e-01 1.33194074e-01 1.11364361e-01 1.95665062e-01 5.57144416e-02 -6.22623725e-02 2.59366443e-01 1.96512306e-02 -2.47699823e-02 3.37429602e-01 1.84628626e-01 2.42417229e-01 1.88852778e-01 2.10930109e-01 2.10416004e-01 2.81527817e-01 5.45666352e-01 1.85856370e-01 4.88939364e-01 1.29308220e-01 1.30534366e-01 4.31600221e-01 1.42478827e-01 1.11633119e-01 1.45026679e-01 2.79724659e-01 3.33422150e-01 4.92846588e-01 1.88026032e-01 4.35734950e-01 1.29765005e-01 1.36498013e-01 1.27056277e-01 2.39063615e-01 -1.49002763e-01 2.00230923e-02 1.23378339e-01 6.12350194e-02 -1.57952580e-01 5.93742728e-02 -6.88460761e-03 7.48854198e-02 6.45607765e-02 8.47908994e-03 2.15403273e-01 6.38359483e-02 -6.30232436e-04 4.09513551e-01 3.59478228e-01 1.15102395e-01 1.56907967e-01 1.60361237e-01 3.16259692e-01 4.37763243e-01 1.82457530e-01 3.12791208e-01 1.59771151e-01 -6.63636501e-02 3.37363422e-01 2.58858115e-01 1.81217734e-01 3.73234115e-02 1.44936318e-01 3.16879135e-01 4.73967251e-01 2.43696316e-01 2.73749525e-01 2.46270449e-02 2.27465471e-01 1.71915626e-01 6.96528119e-02 1.51926333e-01 1.91790172e-01 -1.70457889e-01 1.94258861e-02 1.05929285e-01 2.46869777e-01 -6.42981449e-03 1.22480623e-01 1.27650832e-01 1.23734951e-01 2.01582021e-01 7.66321281e-02 1.25943788e-01 -5.22321249e-02 2.95908687e-01 3.44925520e-01 1.07812252e-01 1.15365733e-01 2.13185926e-01 1.29626595e-01 4.15526961e-01 1.23294607e-01 1.45059294e-01 1.81411556e-01 1.06561684e-01 1.20626826e-01 2.19538968e-01 3.16034720e-01 9.72365601e-02 1.83261409e-01 1.47228661e-01 1.57946602e-01 3.83712037e-01 1.36031656e-01 3.75214905e-02 1.97768668e-02 3.06073435e-02 -1.01445936e-01 1.41457346e-01 4.89799924e-02 1.35908206e-01 2.95765484e-02 1.34596792e-01 -2.45031560e-01 9.09800159e-02 -2.80465423e-02 4.60956009e-03 4.76391647e-02 9.71343281e-02 6.71838252e-02 -1.45994631e-02 -5.39188915e-02 2.79919933e-01 2.31919186e-01 1.12801182e-01 1.13704532e-01 4.26356671e-01 1.90428244e-01 1.10496872e-01 3.31699294e-01 1.36443699e-01 1.97119264e-01 -5.57694684e-03 1.11270325e-01 4.61516648e-01 2.68630982e-01 1.00774945e-01 1.41438672e-01 3.97197924e-01 1.92009640e-01 1.34873803e-01 2.20134800e-01 1.11572142e-01 2.04669213e-02 2.21970350e-01 -1.13088611e-01 2.39645009e-01 2.70424952e-01 2.65250470e-01 7.79145265e-02 4.09394578e-03 -2.78502700e-01 -1.88647588e-02 -8.11508107e-02 2.05797599e-01 1.58278762e-01 -1.59274599e-01 4.31328198e-01 1.05097241e-01 1.85069899e-01] After you've completed the previous step, turn this into a list comprehension and pass the result to a variable called margins # complete the code below margins = [ j [ 'EBITDA/KG' ] . values for i , j in gp ] Remember the list unpacking we did for the tic tac toe project? We're going to do the same thing here. Unpack the margins list for median_test and run the cell below! # complete the following line stat , p , m , table = scipy . stats . median_test ( * margins , correction = False ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.2e} \" . format ( p )) print ( \"the grand median: {:.2e} \" . format ( m )) The pearsons chi-square test statistic: 448.81 p-value of the test: 8.85e-95 the grand median: 2.16e-01 Part B View the distributions of the data using matplotlib and seaborn \u00b6 What a fantastic statistical result we found! Can we affirm our result with some visualizations? I hope so! Create a boxplot below using pandas. In your call to df.boxplot() the by parameter should be set to Base Cake and the column parameter should be set to EBITDA/KG # YOUR BOXPLOT HERE fig , ax = plt . subplots ( figsize = ( 10 , 5 )) df . boxplot ( by = 'Base Cake' , column = 'EBITDA/KG' , ax = ax ) <AxesSubplot:title={'center':'EBITDA/KG'}, xlabel='Base Cake'> For comparison, I've shown the boxplot below using seaborn! fig , ax = plt . subplots ( figsize = ( 10 , 7 )) ax = sns . boxplot ( x = 'Base Cake' , y = 'EBITDA/KG' , data = df , color = '#A0cbe8' ) Part C Perform Moods Median on all the other groups \u00b6 # Recall the other descriptors we have descriptors Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') for desc in descriptors : # YOUR CODE FORM MARGINS BELOW gp = df . groupby ( desc ) margins = [ j [ 'EBITDA/KG' ] . values for i , j in gp ] # UNPACK MARGINS INTO MEDIAN_TEST stat , p , m , table = scipy . stats . median_test ( * margins , correction = False ) print ( desc ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:e} \" . format ( p )) print ( \"the grand median: {} \" . format ( m ), end = ' \\n\\n ' ) Base Cake The pearsons chi-square test statistic: 448.81 p-value of the test: 8.851450e-95 the grand median: 0.21604872880760184 Truffle Type The pearsons chi-square test statistic: 22.86 p-value of the test: 1.088396e-05 the grand median: 0.21604872880760184 Primary Flavor The pearsons chi-square test statistic: 638.99 p-value of the test: 3.918933e-103 the grand median: 0.21604872880760184 Secondary Flavor The pearsons chi-square test statistic: 323.13 p-value of the test: 6.083210e-52 the grand median: 0.21604872880760184 Color Group The pearsons chi-square test statistic: 175.18 p-value of the test: 1.011412e-31 the grand median: 0.21604872880760184 Customer The pearsons chi-square test statistic: 5.66 p-value of the test: 2.257760e-01 the grand median: 0.21604872880760184 Date The pearsons chi-square test statistic: 5.27 p-value of the test: 9.175929e-01 the grand median: 0.21604872880760184 Part D Many boxplots \u00b6 And finally, we will confirm these visually. Complete the Boxplot for each group: for desc in descriptors : fig , ax = plt . subplots ( figsize = ( 10 , 5 )) sns . boxplot ( x = desc , y = 'EBITDA/KG' , data = df , color = '#A0cbe8' , ax = ax ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 9 ( ) missing from current font. fig.canvas.print_figure(bytes_io, **kw) 2.1.3 What is a T-test? \u00b6 There are 1-sample and 2-sample T-tests (note: we would use a 1-sample T-test just to determine if the sample mean is equal to a hypothesized population mean) Within 2-sample T-tests we have independent and dependent T-tests (uncorrelated or correlated samples) For independent, two-sample T-tests: Equal variance (or pooled) T-test scipy.stats.ttest_ind(equal_var=True) Unequal variance T-test scipy.stats.ttest_ind(equal_var=False) also called Welch's T-test For dependent T-tests: * Paired (or correlated) T-test * scipy.stats.ttest_rel A full discussion on T-tests is outside the scope of this session, but we can refer to wikipedia for more information, including formulas on how each statistic is computed: * student's T-test 2.1.3.1 Demonstration of T-tests \u00b6 back to top We'll assume our shifts are of equal variance and proceed with the appropriate independent two-sample T-test... print ( shift_one ) print ( shift_two ) [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] To calculate the T-test, we follow a slightly different statistical formula: $T=\\frac{\\mu_1 - \\mu_2}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$ where $\\mu$ are the means of the two groups, $n$ are the sample sizes and $s$ is the pooled standard deviation, also known as the cummulative variance (depending on if you square it or not): $s= \\sqrt{\\frac{(n_1-1)\\sigma_1^2 + (n_2-1)\\sigma_2^2}{n_1 + n_2 - 2}}$ where $\\sigma$ are the standard deviations. What you'll notice here is we are combining the two variances, we can only do this if we assume the variances are somewhat equal, this is known as the equal variances t-test. mean_shift_one = np . mean ( shift_one ) mean_shift_two = np . mean ( shift_two ) print ( mean_shift_one , mean_shift_two ) 16.3 20.1 com_var = (( np . sum ([( i - mean_shift_one ) ** 2 for i in shift_one ]) + np . sum ([( i - mean_shift_two ) ** 2 for i in shift_two ])) / ( len ( shift_one ) + len ( shift_two ) - 2 )) print ( com_var ) 6.5 T = ( np . abs ( mean_shift_one - mean_shift_two ) / ( np . sqrt ( com_var / len ( shift_one ) + com_var / len ( shift_two )))) T 3.3328204733667115 We see that this hand-computed result matches that of the scipy module: scipy . stats . ttest_ind ( shift_two , shift_one , equal_var = True ) Ttest_indResult(statistic=3.3328204733667115, pvalue=0.0037029158660758575) 2.1.4 What are F-statistics and the F-test? \u00b6 The F-statistic is simply a ratio of two variances, or the ratio of mean squares mean squares is the estimate of population variance that accounts for the degrees of freedom to compute that estimate. We will explore this in the context of ANOVA 2.1.4.1 What is Analysis of Variance? \u00b6 ANOVA uses the F-test to determine whether the variability between group means is larger than the variability within the groups. If that statistic is large enough, you can conclude that the means of the groups are not equal. The caveat is that ANOVA tells us whether there is a difference in means but it does not tell us where the difference is. To find where the difference is between the groups, we have to conduct post-hoc tests. There are two main types: * One-way (one factor) and * Two-way (two factor) where factor is an independent variable Ind A Ind B Dep X H 10 X I 12 Y I 11 Y H 20 ANOVA Hypotheses \u00b6 Null hypothesis : group means are equal Alternative hypothesis : at least one group mean is different from the other groups ANOVA Assumptions \u00b6 Residuals (experimental error) are normally distributed (test with Shapiro-Wilk) Homogeneity of variances (variances are equal between groups) (test with Bartlett's) Observations are sampled independently from each other Note: ANOVA assumptions can be checked using test statistics (e.g. Shapiro-Wilk, Bartlett\u2019s, Levene\u2019s test) and the visual approaches such as residual plots (e.g. QQ-plots) and histograms. Steps for ANOVA \u00b6 Check sample sizes: equal observations must be in each group Calculate Sum of Square between groups and within groups (\\(SS_B, SS_E\\)) Calculate Mean Square between groups and within groups (\\(MS_B, MS_E\\)) Calculate F value (\\(MS_B/MS_E\\)) This might be easier to see in a table: Source of Variation degree of freedom (Df) Sum of squares (SS) Mean square (MS) F value Between Groups Df_b = P-1 SS_B MS_B = SS_B / Df_B MS_B / MS_E Within Groups Df_E = P(N-1) SS_E MS_E = SS_E / Df_E total Df_T = PN-1 SS_T Where: SS_B = \\sum_{i}^{P}{(\\bar{y}_i-\\bar{y})^2} SS_E = \\sum_{ik}^{PN}{(\\bar{y}_{ik}-\\bar{y}_i)^2} SS_T = SS_B + SS_E Let's go back to our shift data to take an example: shifts = pd . DataFrame ([ shift_one , shift_two , shift_three , shift_four ]) . T shifts . columns = [ 'A' , 'B' , 'C' , 'D' ] shifts . boxplot () <AxesSubplot:> 2.1.4.2 SNS Boxplot \u00b6 this is another great way to view boxplot data. Notice how sns also shows us the raw data alongside the box and whiskers using a swarmplot . shift_melt = pd . melt ( shifts . reset_index (), id_vars = [ 'index' ], value_vars = [ 'A' , 'B' , 'C' , 'D' ]) shift_melt . columns = [ 'index' , 'shift' , 'rate' ] ax = sns . boxplot ( x = 'shift' , y = 'rate' , data = shift_melt , color = '#A0cbe8' ) ax = sns . swarmplot ( x = \"shift\" , y = \"rate\" , data = shift_melt , color = '#79706e' ) Anyway back to ANOVA... fvalue , pvalue = stats . f_oneway ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( fvalue , pvalue ) 5.599173553719008 0.0029473487978665873 We can get this in the format of the table we saw above: # get ANOVA table import statsmodels.api as sm from statsmodels.formula.api import ols # Ordinary Least Squares (OLS) model model = ols ( 'rate ~ C(shift)' , data = shift_melt ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(shift) 135.5 3.0 5.599174 0.002947 Residual 290.4 36.0 NaN NaN The Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9750654697418213 0.5121709108352661 We can use Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. w , pvalue = stats . bartlett ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( w , pvalue ) 1.3763632854696672 0.711084540821183 2.1.4.3 ANOVA Interpretation \u00b6 The p value form ANOVA analysis is significant ( p < 0.05) and we can conclude there are significant difference between the shifts. But we do not know which shift(s) are different. For this we need to perform a post hoc test. There are a multitude of these that are beyond the scope of this discussion ( Tukey-kramer is one such test) 2.1.5 Putting it all together \u00b6 In summary, there are many statistical tests at our disposal when performing inferential statistical analysis. In times like these, a simple decision tree can be extraordinarily useful! source: scribbr \ud83c\udf52 2.2 Enrichment: Evaluate statistical significance of product margin: a snake in the garden \u00b6 2.2.1 Mood's Median on product descriptors \u00b6 The first issue we run into with moods is... what? We can only perform moods on two groups at a time. How can we get around this? Let's take a look at the category with the fewest descriptors. If we remember, this was the Truffle Types. df . columns Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date', 'KG', 'EBITDA/KG'], dtype='object') df [ 'Truffle Type' ] . unique () array(['Candy Outer', 'Chocolate Outer', 'Jelly Filled'], dtype=object) col = 'Truffle Type' moodsdf = pd . DataFrame () for truff in df [ col ] . unique (): # for each group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) print ( \" {} : N= {} \" . format ( truff , size )) print ( \"Welch's T-Test for Unequal Variances\" ) print ( scipy . stats . ttest_ind ( group , pop , equal_var = False )) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue print () moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ col , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] Candy Outer: N=288 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=-2.7615297773427474, pvalue=0.0059110489226580736) Chocolate Outer: N=1356 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=4.409449025092911, pvalue=1.1932685612874952e-05) Jelly Filled: N=24 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=-8.414252306793504, pvalue=7.929912531660087e-09) \ud83d\ude4b\u200d\u2640\ufe0f Question 1: Moods Results on Truffle Type \u00b6 What do we notice about the resultant table? p-values Most are quite small (really low probability of achieving these table results under a single distribution) group sizes: our Jelly Filled group is relatively small moodsdf . sort_values ( 'p_value' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Truffle Type pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Jelly Filled 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 0 Chocolate Outer 6.627496 0.010042 0.216049 0.262601 0.225562 1356 0.000012 [[699, 135], [657, 177]] 0 Candy Outer 1.515066 0.218368 0.216049 0.230075 0.204264 288 0.005911 [[134, 700], [154, 680]] We can go ahead and repeat this analysis for all of our product categories: df . columns [: 5 ] Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group'], dtype='object') moodsdf = pd . DataFrame () for col in df . columns [: 5 ]: for truff in df [ col ] . unique (): group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] print ( moodsdf . shape ) (101, 10) moodsdf = moodsdf . loc [( moodsdf [ 'welch p' ] < 0.005 ) & ( moodsdf [ 'p_value' ] < 0.005 )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( moodsdf . shape ) (51, 10) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Secondary Flavor Papaya 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 1 Primary Flavor Orange Pineapple\\tP 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 2 Primary Flavor Cherry Cream Spice 10.156401 0.001438 0.216049 0.018702 0.009701 12 0.000001 [[0, 834], [12, 822]] 3 Secondary Flavor Cucumber 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 4 Truffle Type Jelly Filled 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 5 Primary Flavor Orange 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 6 Primary Flavor Toasted Coconut 15.261253 0.000094 0.216049 0.037002 0.028392 24 0.0 [[2, 832], [22, 812]] 7 Secondary Flavor Apricot 15.261253 0.000094 0.216049 0.060312 0.037422 24 0.0 [[2, 832], [22, 812]] 8 Primary Flavor Kettle Corn 29.062065 0.0 0.216049 0.055452 0.045891 60 0.0 [[9, 825], [51, 783]] 9 Primary Flavor Acai Berry 18.643248 0.000016 0.216049 0.036505 0.049466 24 0.0 [[1, 833], [23, 811]] 10 Primary Flavor Pink Lemonade 10.156401 0.001438 0.216049 0.039862 0.056349 12 0.000011 [[0, 834], [12, 822]] 11 Secondary Flavor Black Cherry 58.900366 0.0 0.216049 0.055975 0.062898 96 0.0 [[11, 823], [85, 749]] 12 Primary Flavor Watermelon 15.261253 0.000094 0.216049 0.04405 0.067896 24 0.0 [[2, 832], [22, 812]] 13 Primary Flavor Plum 34.851608 0.0 0.216049 0.084963 0.079993 72 0.0 [[11, 823], [61, 773]] 14 Secondary Flavor Dill Pickle 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 15 Primary Flavor Horchata 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 16 Primary Flavor Lemon Custard 12.217457 0.000473 0.216049 0.079389 0.087969 24 0.000006 [[3, 831], [21, 813]] 17 Primary Flavor Fruit Punch 10.156401 0.001438 0.216049 0.078935 0.090326 12 0.000076 [[0, 834], [12, 822]] 18 Base Cake Chiffon 117.046226 0.0 0.216049 0.127851 0.125775 288 0.0 [[60, 774], [228, 606]] 19 Base Cake Butter 134.36727 0.0 0.216049 0.142082 0.139756 456 0.0 [[122, 712], [334, 500]] 20 Secondary Flavor Banana 10.805348 0.001012 0.216049 0.163442 0.15537 60 0.0 [[17, 817], [43, 791]] 21 Primary Flavor Cream Soda 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 22 Secondary Flavor Peppermint 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 23 Primary Flavor Grand Mariner 10.581767 0.001142 0.216049 0.197463 0.165529 72 0.000829 [[22, 812], [50, 784]] 24 Color Group Amethyst 20.488275 0.000006 0.216049 0.195681 0.167321 300 0.0 [[114, 720], [186, 648]] 25 Color Group Burgundy 10.999677 0.000911 0.216049 0.193048 0.171465 120 0.000406 [[42, 792], [78, 756]] 26 Color Group White 35.76526 0.0 0.216049 0.19 0.177264 432 0.0 [[162, 672], [270, 564]] 27 Color Group Opal 11.587164 0.000664 0.216049 0.317878 0.259304 324 0.0 [[190, 644], [134, 700]] 28 Secondary Flavor Apple 27.283292 0.0 0.216049 0.326167 0.293876 36 0.001176 [[34, 800], [2, 832]] 29 Secondary Flavor Tangerine 32.626389 0.0 0.216049 0.342314 0.319273 48 0.000113 [[44, 790], [4, 830]] 30 Secondary Flavor Black Currant 34.778391 0.0 0.216049 0.357916 0.332449 36 0.0 [[36, 798], [0, 834]] 31 Secondary Flavor Pear 16.614303 0.000046 0.216049 0.373034 0.33831 60 0.000031 [[46, 788], [14, 820]] 32 Primary Flavor Vanilla 34.778391 0.0 0.216049 0.378053 0.341626 36 0.000001 [[36, 798], [0, 834]] 33 Color Group Citrine 10.156401 0.001438 0.216049 0.390728 0.342512 12 0.001925 [[12, 822], [0, 834]] 34 Color Group Teal 13.539679 0.000234 0.216049 0.323955 0.3446 96 0.00121 [[66, 768], [30, 804]] 35 Base Cake Tiramisu 52.360619 0.0 0.216049 0.388267 0.362102 144 0.0 [[114, 720], [30, 804]] 36 Primary Flavor Doughnut 74.935256 0.0 0.216049 0.439721 0.379361 108 0.0 [[98, 736], [10, 824]] 37 Secondary Flavor Ginger Beer 22.363443 0.000002 0.216049 0.444895 0.382283 24 0.000481 [[24, 810], [0, 834]] 38 Color Group Rose 18.643248 0.000016 0.216049 0.42301 0.407061 24 0.000062 [[23, 811], [1, 833]] 39 Base Cake Cheese 66.804744 0.0 0.216049 0.450934 0.435638 84 0.0 [[79, 755], [5, 829]] 40 Primary Flavor Butter Toffee 60.181468 0.0 0.216049 0.50366 0.456343 60 0.0 [[60, 774], [0, 834]] 41 Color Group Slate 10.156401 0.001438 0.216049 0.540214 0.483138 12 0.000017 [[12, 822], [0, 834]] 42 Primary Flavor Gingersnap 22.363443 0.000002 0.216049 0.643218 0.623627 24 0.0 [[24, 810], [0, 834]] 43 Primary Flavor Dill Pickle 22.363443 0.000002 0.216049 0.642239 0.655779 24 0.0 [[24, 810], [0, 834]] 44 Color Group Olive 44.967537 0.0 0.216049 0.637627 0.670186 60 0.0 [[56, 778], [4, 830]] 45 Primary Flavor Butter Milk 10.156401 0.001438 0.216049 0.699284 0.688601 12 0.0 [[12, 822], [0, 834]] 46 Base Cake Sponge 127.156266 0.0 0.216049 0.698996 0.699355 120 0.0 [[120, 714], [0, 834]] 47 Primary Flavor Chocolate Mint 10.156401 0.001438 0.216049 0.685546 0.699666 12 0.0 [[12, 822], [0, 834]] 48 Primary Flavor Coconut 10.156401 0.001438 0.216049 0.732777 0.717641 12 0.0 [[12, 822], [0, 834]] 49 Primary Flavor Blueberry 22.363443 0.000002 0.216049 0.759643 0.72536 24 0.0 [[24, 810], [0, 834]] 50 Primary Flavor Amaretto 10.156401 0.001438 0.216049 0.782156 0.764845 12 0.0 [[12, 822], [0, 834]] \ud83c\udf52\ud83c\udf52 2.2.2 Enrichment : Broad Analysis of Categories: ANOVA \u00b6 Recall our \"melted\" shift data. It will be useful to think of getting our Truffle data in this format: shift_melt . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index shift rate 0 0 A 11 1 1 A 14 2 2 A 15 3 3 A 16 4 4 A 16 df . columns = df . columns . str . replace ( ' ' , '_' ) df . columns = df . columns . str . replace ( '/' , '_' ) # get ANOVA table # Ordinary Least Squares (OLS) model model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Recall the Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9576056599617004 1.2598073820281984e-21 And the Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. gb = df . groupby ( 'Truffle_Type' )[ 'EBITDA_KG' ] gb <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fb6940a7370> w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( w , pvalue ) 109.93252546442552 1.344173733366234e-24 Wow it looks like our data is not drawn from a normal distribution! Let's check this for other categories... We can wrap these in a for loop: for col in df . columns [: 5 ]: print ( col ) model = ols ( 'EBITDA_KG ~ C( {} )' . format ( col ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) w , pvalue = stats . shapiro ( model . resid ) print ( \"Shapiro: \" , w , pvalue ) gb = df . groupby ( col )[ 'EBITDA_KG' ] w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( \"Bartlett: \" , w , pvalue ) print () Base_Cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Base_Cake) 39.918103 5.0 314.869955 1.889884e-237 Residual 42.140500 1662.0 NaN NaN Shapiro: 0.9634131193161011 4.1681337029688696e-20 Bartlett: 69.83288886114286 1.1102218566048873e-13 Truffle_Type .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Shapiro: 0.9576056599617004 1.2598073820281984e-21 Bartlett: 109.93252546442552 1.344173733366234e-24 Primary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Primary_Flavor) 50.270639 50.0 51.143649 1.153434e-292 Residual 31.787964 1617.0 NaN NaN Shapiro: 0.948470413684845 9.90281706784179e-24 Bartlett: 210.15130419114982 1.5872504991225816e-21 Secondary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Secondary_Flavor) 15.088382 28.0 13.188089 1.929302e-54 Residual 66.970220 1639.0 NaN NaN Shapiro: 0.9548103213310242 2.649492974953278e-22 Bartlett: 420.6274502894812 1.2373007035089195e-71 Color_Group .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Color_Group) 16.079685 11.0 36.689347 6.544980e-71 Residual 65.978918 1656.0 NaN NaN Shapiro: 0.969061017036438 1.8926407335144587e-18 Bartlett: 136.55525281340468 8.164787784033709e-24 \ud83c\udf52\ud83c\udf52 2.2.3 Enrichment : Visual Analysis of Residuals: QQ-Plots \u00b6 This can be distressing and is often why we want visual methods to see what is going on with our data! model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () #create instance of influence influence = model . get_influence () #obtain standardized residuals standardized_residuals = influence . resid_studentized_internal # res.anova_std_residuals are standardized residuals obtained from ANOVA (check above) sm . qqplot ( standardized_residuals , line = '45' ) plt . xlabel ( \"Theoretical Quantiles\" ) plt . ylabel ( \"Standardized Residuals\" ) plt . show () # histogram plt . hist ( model . resid , bins = 'auto' , histtype = 'bar' , ec = 'k' ) plt . xlabel ( \"Residuals\" ) plt . ylabel ( 'Frequency' ) plt . show () We see that a lot of our data is swayed by extremely high and low values, so what can we conclude? You need the right test statistic for the right job, in this case, we are littered with unequal variance in our groupings so we use the moods median and welch (unequal variance t-test) to make conclusions about our data References \u00b6 Renesh Bedre ANOVA Minitab ANOVA Analytics Vidhya ANOVA Renesh Bedre Hypothesis Testing Real Statistics Turkey-kramer Mutual Information","title":"SOLN S2 Inferential Statistics"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#data-science-foundations-session-2-inferential-statistics","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will look at the utility of EDA combined with inferential statistics.","title":"Data Science Foundations, Session 2: Inferential Statistics"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#20-preparing-environment-and-importing-data","text":"back to top","title":"2.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#201-import-packages","text":"back to top # The modules we've seen before import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns # our stats modules import random import scipy.stats as stats import statsmodels.api as sm from statsmodels.formula.api import ols import scipy","title":"2.0.1 Import Packages"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#202-load-dataset","text":"back to top For this session, we will use dummy datasets from sklearn. df = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... ... ... ... ... ... 1663 Tiramisu Chocolate Outer Doughnut Pear Amethyst Fickelgruber 12/2020 38128.802589 0.420111 1664 Tiramisu Chocolate Outer Doughnut Pear Burgundy Zebrabar 12/2020 108.642857 0.248659 1665 Tiramisu Chocolate Outer Doughnut Pear Teal Zebrabar 12/2020 3517.933333 0.378501 1666 Tiramisu Chocolate Outer Doughnut Rock and Rye Amethyst Slugworth 12/2020 10146.898432 0.213149 1667 Tiramisu Chocolate Outer Doughnut Rock and Rye Burgundy Zebrabar 12/2020 1271.904762 0.431813 1668 rows \u00d7 9 columns descriptors = df . columns [: - 2 ] for col in df . columns [: - 2 ]: print ( col ) print ( df [ col ] . unique ()) print () Base Cake ['Butter' 'Cheese' 'Chiffon' 'Pound' 'Sponge' 'Tiramisu'] Truffle Type ['Candy Outer' 'Chocolate Outer' 'Jelly Filled'] Primary Flavor ['Butter Pecan' 'Ginger Lime' 'Margarita' 'Pear' 'Pink Lemonade' 'Raspberry Ginger Ale' 'Sassafras' 'Spice' 'Wild Cherry Cream' 'Cream Soda' 'Horchata' 'Kettle Corn' 'Lemon Bar' 'Orange Pineapple\\tP' 'Plum' 'Orange' 'Butter Toffee' 'Lemon' 'Acai Berry' 'Apricot' 'Birch Beer' 'Cherry Cream Spice' 'Creme de Menthe' 'Fruit Punch' 'Ginger Ale' 'Grand Mariner' 'Orange Brandy' 'Pecan' 'Toasted Coconut' 'Watermelon' 'Wintergreen' 'Vanilla' 'Bavarian Cream' 'Black Licorice' 'Caramel Cream' 'Cheesecake' 'Cherry Cola' 'Coffee' 'Irish Cream' 'Lemon Custard' 'Mango' 'Sour' 'Amaretto' 'Blueberry' 'Butter Milk' 'Chocolate Mint' 'Coconut' 'Dill Pickle' 'Gingersnap' 'Chocolate' 'Doughnut'] Secondary Flavor ['Toffee' 'Banana' 'Rum' 'Tutti Frutti' 'Vanilla' 'Mixed Berry' 'Whipped Cream' 'Apricot' 'Passion Fruit' 'Peppermint' 'Dill Pickle' 'Black Cherry' 'Wild Cherry Cream' 'Papaya' 'Mango' 'Cucumber' 'Egg Nog' 'Pear' 'Rock and Rye' 'Tangerine' 'Apple' 'Black Currant' 'Kiwi' 'Lemon' 'Hazelnut' 'Butter Rum' 'Fuzzy Navel' 'Mojito' 'Ginger Beer'] Color Group ['Taupe' 'Amethyst' 'Burgundy' 'White' 'Black' 'Opal' 'Citrine' 'Rose' 'Slate' 'Teal' 'Tiffany' 'Olive'] Customer ['Slugworth' 'Perk-a-Cola' 'Fickelgruber' 'Zebrabar' \"Dandy's Candies\"] Date ['1/2020' '2/2020' '3/2020' '4/2020' '5/2020' '6/2020' '7/2020' '8/2020' '9/2020' '10/2020' '11/2020' '12/2020']","title":"2.0.2 Load Dataset"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#21-many-flavors-of-statistical-tests","text":"https://luminousmen.com/post/descriptive-and-inferential-statistics Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (\u201cinferences\u201d) from that data. With inferential statistics, you take data from samples and make generalizations about a population statshowto Moods Median Test Kruskal-Wallis Test (Another comparison of Medians test) T-Test Analysis of Variance (ANOVA) One Way ANOVA Two Way ANOVA MANOVA Factorial ANOVA When do I use each of these? We will talk about this as we proceed through the examples. This page from minitab has good rules of thumb on the subject.","title":"2.1 Many Flavors of Statistical Tests"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#211-what-is-moods-median","text":"You can use Chi-Square to test for a goodness of fit (whether a sample of data represents a distribution) or whether two variables are related (using a contingency table, which we will create below!) A special case of Pearon's Chi-Squared Test: We create a table that counts the observations above and below the global median for two different groups. We then perform a chi-squared test of significance on this contingency table Null hypothesis: the Medians are all equal The chi-square test statistic: x^2 = \\sum{\\frac{(O-E)^2}{E}} Where \\(O\\) is the observed frequency and \\(E\\) is the expected frequency. Let's take an example , say we have three shifts with the following production rates: np . random . seed ( 7 ) shift_one = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_two = [ round ( i ) for i in np . random . normal ( 21 , 3 , 10 )] print ( shift_one ) print ( shift_two ) [21, 15, 16, 17, 14, 16, 16, 11, 19, 18] [19, 20, 23, 20, 20, 17, 23, 21, 22, 16] stat , p , m , table = scipy . stats . median_test ( shift_one , shift_two , correction = False ) what is median_test returning? print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 7.20 p-value of the test: 0.007 the grand median: 18.5 Let's evaluate that test statistic ourselves by taking a look at the contingency table: table array([[2, 8], [8, 2]]) This is easier to make sense of if we order the shift times shift_one . sort () shift_one [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] When we look at shift one, we see that 8 values are at or below the grand median. shift_two . sort () shift_two [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] For shift two, only two are at or below the grand median. Since the sample sizes are the same, the expected value for both groups is the same, 5 above and 5 below the grand median. The chi-square is then: X^2 = \\frac{(2-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(8-5)^2}{5} + \\frac{(2-5)^2}{5} ( 3 - 5 ) ** 2 / 5 + ( 7 - 5 ) ** 2 / 5 + ( 7 - 5 ) ** 2 / 5 + ( 3 - 5 ) ** 2 / 5 3.2 Our p-value, or the probability of observing the null-hypothsis, is under 0.05. We can conclude that these shift performances were drawn under seperate distributions. For comparison, let's do this analysis again with shifts of equal performances np . random . seed ( 3 ) shift_three = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] shift_four = [ round ( i ) for i in np . random . normal ( 16 , 3 , 10 )] stat , p , m , table = scipy . stats . median_test ( shift_three , shift_four , correction = False ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.3f} \" . format ( p )) print ( \"the grand median: {} \" . format ( m )) The pearsons chi-square test statistic: 0.00 p-value of the test: 1.000 the grand median: 15.5 and the shift raw values: shift_three . sort () shift_four . sort () print ( shift_three ) print ( shift_four ) [10, 14, 15, 15, 15, 16, 16, 16, 17, 21] [11, 12, 13, 14, 15, 16, 19, 19, 19, 21] table array([[5, 5], [5, 5]])","title":"2.1.1 What is Mood's Median?"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#212-when-to-use-moods","text":"Mood's Median Test is highly flexible but has the following assumptions: Considers only one categorical factor Response variable is continuous (our shift rates) Data does not need to be normally distributed But the distributions are similarly shaped Sample sizes can be unequal and small (less than 20 observations) Other considerations: Not as powerful as Kruskal-Wallis Test but still useful for small sample sizes or when there are outliers","title":"2.1.2 When to Use Mood's?"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#exercise-1-use-moods-median-test","text":"","title":"\ud83c\udfcb\ufe0f Exercise 1: Use Mood's Median Test"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#part-a-perform-moods-median-test-on-base-cake-categorical-variable-and-ebitdakg-continuous-variable-in-truffle-data","text":"We're also going to get some practice with pandas groupby. # what is returned by this groupby? gp = df . groupby ( 'Base Cake' ) How do we find out? We could iterate through it: # seems to be a tuple of some sort for i in gp : print ( i ) break ('Butter', Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns]) # the first object appears to be the group print ( i [ 0 ]) # the second object appears to be the df belonging to that group print ( i [ 1 ]) Butter Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group \\ 0 Butter Candy Outer Butter Pecan Toffee Taupe 1 Butter Candy Outer Ginger Lime Banana Amethyst 2 Butter Candy Outer Ginger Lime Banana Burgundy 3 Butter Candy Outer Ginger Lime Banana White 4 Butter Candy Outer Ginger Lime Rum Amethyst ... ... ... ... ... ... 1562 Butter Chocolate Outer Plum Black Cherry Opal 1563 Butter Chocolate Outer Plum Black Cherry White 1564 Butter Chocolate Outer Plum Mango Black 1565 Butter Jelly Filled Orange Cucumber Amethyst 1566 Butter Jelly Filled Orange Cucumber Burgundy Customer Date KG EBITDA/KG 0 Slugworth 1/2020 53770.342593 0.500424 1 Slugworth 1/2020 466477.578125 0.220395 2 Perk-a-Cola 1/2020 80801.728070 0.171014 3 Fickelgruber 1/2020 18046.111111 0.233025 4 Fickelgruber 1/2020 19147.454268 0.480689 ... ... ... ... ... 1562 Fickelgruber 12/2020 9772.200521 0.158279 1563 Perk-a-Cola 12/2020 10861.245675 -0.159275 1564 Slugworth 12/2020 3578.592163 0.431328 1565 Slugworth 12/2020 21438.187500 0.105097 1566 Dandy's Candies 12/2020 15617.489115 0.185070 [456 rows x 9 columns] going back to our diagram from our earlier pandas session. It looks like whenever we split in the groupby method, we create separate dataframes as well as their group label: Ok, so we know gp is separate dataframes. How do we turn the 'EBITDA/KG' column of each dataframe into arrays to then pass to median_test ? # complete this for loop for i , j in gp : print ( i ) print ( j [ 'EBITDA/KG' ] . values ) break # turn 'EBITDA/KG' of j into an array using the .values attribute # print this to the screen Butter [ 5.00423594e-01 2.20395451e-01 1.71013869e-01 2.33024872e-01 4.80689371e-01 1.64934546e-01 2.03213256e-01 1.78681400e-01 1.25050726e-01 2.17021951e-01 7.95955185e-02 3.25042287e-01 2.17551215e-01 2.48152299e-01 -1.20503094e-02 1.47190567e-01 3.84488948e-01 2.05438764e-01 1.32190256e-01 3.23019144e-01 -9.73361477e-03 1.98397692e-01 1.67067902e-01 -2.60063690e-02 1.30365325e-01 2.36337749e-01 -9.70556780e-02 1.59051819e-01 -8.76572259e-02 -3.32199843e-02 -5.05704451e-02 -5.56458806e-02 -8.86273564e-02 4.32267857e-02 -1.88615579e-01 4.24939227e-01 9.35136847e-02 -3.43605950e-02 1.63823520e-01 2.78522916e-01 1.29207730e-01 1.79194495e-01 1.37419569e-01 1.31372653e-01 2.53275225e-01 2.26761431e-01 1.10173466e-01 1.99338787e-01 -2.01250197e-01 1.16567591e-01 1.32324984e-01 4.02912418e-01 9.35051765e-02 1.65865814e-01 2.12269112e-01 2.53461571e-01 1.89055713e-01 1.20416365e-01 3.95276612e-02 2.93121770e-01 1.40947082e-01 -1.21555832e-01 1.56455622e-01 -1.29776953e-02 -6.17934014e-02 -8.19904808e-02 -3.14711557e-02 -8.03820228e-02 1.63839981e-01 8.34406336e-02 1.49369698e-01 1.05990633e-01 1.27399979e-01 2.26634255e-01 -2.20801929e-03 -6.92044284e-02 1.74048414e-01 1.30933438e-01 1.27620323e-01 2.78652749e-01 2.14772018e-01 1.40864278e-01 1.23745138e-01 1.66586809e-01 2.91940995e-01 2.49925584e-01 8.65447719e-02 3.80907774e-01 2.70851719e-01 3.32946265e-01 9.00795862e-03 2.00960974e-01 2.72623570e-01 3.35902190e-01 1.27337723e-01 2.36618545e-01 -6.82774785e-02 3.13166906e-01 2.15752651e-01 9.29694447e-02 3.60809152e-02 2.32488112e-01 3.38200308e-02 1.70916188e-01 2.81620452e-01 -1.61981289e-01 -4.14570666e-02 1.13465970e-02 2.28733252e-01 9.87516565e-02 3.52732668e-02 6.32598661e-02 2.10300526e-01 1.98761726e-01 1.38832882e-01 2.95465366e-01 2.68022024e-01 3.22389724e-01 4.04867623e-01 2.38086167e-01 1.12586985e-01 1.94010438e-01 1.96757297e-01 1.65215620e-01 1.22730941e-02 1.14415249e-01 3.26252563e-01 1.89080695e-01 -5.11830382e-02 2.41661008e-01 2.00063672e-01 3.07633312e-01 4.20740234e-01 1.34764192e-01 -4.75993730e-02 1.52973888e-02 1.87709908e-01 7.20193743e-02 3.48745346e-02 2.77659158e-01 2.73466257e-01 1.32419725e-01 2.85933859e-02 3.99622870e-02 -7.46829380e-02 9.03915641e-02 -9.61708181e-02 7.16896946e-02 1.08714611e-01 1.18536709e-01 8.52229628e-02 4.13523715e-01 7.71194281e-01 1.73738798e-01 3.05406909e-01 1.53831064e-01 2.06911408e-01 1.13075512e-01 1.29416734e-01 1.60275533e-01 2.29962628e-01 2.50895646e-01 1.73060658e-01 2.01020670e-01 3.16227457e-01 1.57652647e-01 5.47188384e-02 2.61436808e-01 1.46570523e-01 1.58977569e-01 2.11215119e-01 1.40679855e-01 -8.00696326e-02 1.59842103e-01 2.00211820e-01 9.92221921e-02 -1.91516176e-02 -5.02510162e-02 -9.15402427e-02 4.28019215e-02 1.06537078e-01 -3.24195486e-01 1.79861627e-02 -1.29900711e-01 -1.18627679e-01 -1.26903307e-01 -1.12941251e-01 2.81344485e-01 -5.75519167e-02 1.62155727e-02 2.14084866e-01 2.05315240e-01 1.27598359e-01 1.89025252e-01 3.96820478e-01 1.20290515e-01 3.32130996e-01 1.37858897e-01 9.78393589e-02 3.51731323e-01 1.10782088e-01 2.27390210e-01 3.89559348e-01 1.74184808e-01 3.08568571e-01 1.71747215e-01 2.33275587e-01 2.56728635e-01 3.02423314e-01 2.74374851e-01 3.27629705e-02 5.61005655e-02 1.68330538e-01 1.12578506e-01 1.08314409e-02 1.33944964e-01 -2.12285231e-01 -1.21224032e-01 1.07819533e-01 3.17613330e-02 2.84300351e-01 -1.58586907e-01 1.36753020e-01 1.26197635e-01 7.40448636e-02 2.35065994e-01 -6.15319415e-02 -7.51966701e-02 4.13427726e-01 1.60539980e-01 1.09901498e-01 1.74329568e-01 1.48135527e-01 1.85728609e-01 2.85476612e-01 2.24898461e-01 1.33343564e-01 1.80618963e-01 2.03080820e-02 2.16728570e-01 1.86566493e-01 1.25929822e-01 1.79317565e-01 3.88162321e-01 2.03009067e-01 2.64872648e-01 4.95978731e-01 1.52347749e-01 -7.23596372e-02 1.29552280e-01 6.16496157e-02 1.05956924e-01 -2.71699836e-01 -5.64473565e-03 -2.50275527e-02 1.29269950e-01 -1.87247727e-01 -3.49347255e-01 -1.93280406e-01 7.87217542e-02 2.21951811e-01 7.10999656e-02 3.49382049e-02 1.48398799e-01 5.65517753e-02 1.05690961e-01 2.55476023e-01 1.28401889e-01 1.33289903e-01 1.14201836e-01 1.43169893e-01 5.69591438e-01 1.54755202e-01 1.55028578e-01 1.64827975e-01 4.67083700e-01 3.31029661e-02 1.62382617e-01 1.54156022e-01 6.55873722e-01 -5.31208735e-02 2.37122763e-01 2.71368392e-01 4.69144223e-01 1.62923984e-01 1.22718216e-01 1.68055251e-01 1.35999904e-01 2.04736813e-01 1.27146904e-01 -1.12549423e-01 3.24840692e-03 7.10375441e-02 7.90146006e-03 5.79775663e-02 -1.57867224e-01 1.33194074e-01 1.11364361e-01 1.95665062e-01 5.57144416e-02 -6.22623725e-02 2.59366443e-01 1.96512306e-02 -2.47699823e-02 3.37429602e-01 1.84628626e-01 2.42417229e-01 1.88852778e-01 2.10930109e-01 2.10416004e-01 2.81527817e-01 5.45666352e-01 1.85856370e-01 4.88939364e-01 1.29308220e-01 1.30534366e-01 4.31600221e-01 1.42478827e-01 1.11633119e-01 1.45026679e-01 2.79724659e-01 3.33422150e-01 4.92846588e-01 1.88026032e-01 4.35734950e-01 1.29765005e-01 1.36498013e-01 1.27056277e-01 2.39063615e-01 -1.49002763e-01 2.00230923e-02 1.23378339e-01 6.12350194e-02 -1.57952580e-01 5.93742728e-02 -6.88460761e-03 7.48854198e-02 6.45607765e-02 8.47908994e-03 2.15403273e-01 6.38359483e-02 -6.30232436e-04 4.09513551e-01 3.59478228e-01 1.15102395e-01 1.56907967e-01 1.60361237e-01 3.16259692e-01 4.37763243e-01 1.82457530e-01 3.12791208e-01 1.59771151e-01 -6.63636501e-02 3.37363422e-01 2.58858115e-01 1.81217734e-01 3.73234115e-02 1.44936318e-01 3.16879135e-01 4.73967251e-01 2.43696316e-01 2.73749525e-01 2.46270449e-02 2.27465471e-01 1.71915626e-01 6.96528119e-02 1.51926333e-01 1.91790172e-01 -1.70457889e-01 1.94258861e-02 1.05929285e-01 2.46869777e-01 -6.42981449e-03 1.22480623e-01 1.27650832e-01 1.23734951e-01 2.01582021e-01 7.66321281e-02 1.25943788e-01 -5.22321249e-02 2.95908687e-01 3.44925520e-01 1.07812252e-01 1.15365733e-01 2.13185926e-01 1.29626595e-01 4.15526961e-01 1.23294607e-01 1.45059294e-01 1.81411556e-01 1.06561684e-01 1.20626826e-01 2.19538968e-01 3.16034720e-01 9.72365601e-02 1.83261409e-01 1.47228661e-01 1.57946602e-01 3.83712037e-01 1.36031656e-01 3.75214905e-02 1.97768668e-02 3.06073435e-02 -1.01445936e-01 1.41457346e-01 4.89799924e-02 1.35908206e-01 2.95765484e-02 1.34596792e-01 -2.45031560e-01 9.09800159e-02 -2.80465423e-02 4.60956009e-03 4.76391647e-02 9.71343281e-02 6.71838252e-02 -1.45994631e-02 -5.39188915e-02 2.79919933e-01 2.31919186e-01 1.12801182e-01 1.13704532e-01 4.26356671e-01 1.90428244e-01 1.10496872e-01 3.31699294e-01 1.36443699e-01 1.97119264e-01 -5.57694684e-03 1.11270325e-01 4.61516648e-01 2.68630982e-01 1.00774945e-01 1.41438672e-01 3.97197924e-01 1.92009640e-01 1.34873803e-01 2.20134800e-01 1.11572142e-01 2.04669213e-02 2.21970350e-01 -1.13088611e-01 2.39645009e-01 2.70424952e-01 2.65250470e-01 7.79145265e-02 4.09394578e-03 -2.78502700e-01 -1.88647588e-02 -8.11508107e-02 2.05797599e-01 1.58278762e-01 -1.59274599e-01 4.31328198e-01 1.05097241e-01 1.85069899e-01] After you've completed the previous step, turn this into a list comprehension and pass the result to a variable called margins # complete the code below margins = [ j [ 'EBITDA/KG' ] . values for i , j in gp ] Remember the list unpacking we did for the tic tac toe project? We're going to do the same thing here. Unpack the margins list for median_test and run the cell below! # complete the following line stat , p , m , table = scipy . stats . median_test ( * margins , correction = False ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:.2e} \" . format ( p )) print ( \"the grand median: {:.2e} \" . format ( m )) The pearsons chi-square test statistic: 448.81 p-value of the test: 8.85e-95 the grand median: 2.16e-01","title":"Part A Perform moods median test on Base Cake (Categorical Variable) and EBITDA/KG (Continuous Variable) in Truffle data"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#part-b-view-the-distributions-of-the-data-using-matplotlib-and-seaborn","text":"What a fantastic statistical result we found! Can we affirm our result with some visualizations? I hope so! Create a boxplot below using pandas. In your call to df.boxplot() the by parameter should be set to Base Cake and the column parameter should be set to EBITDA/KG # YOUR BOXPLOT HERE fig , ax = plt . subplots ( figsize = ( 10 , 5 )) df . boxplot ( by = 'Base Cake' , column = 'EBITDA/KG' , ax = ax ) <AxesSubplot:title={'center':'EBITDA/KG'}, xlabel='Base Cake'> For comparison, I've shown the boxplot below using seaborn! fig , ax = plt . subplots ( figsize = ( 10 , 7 )) ax = sns . boxplot ( x = 'Base Cake' , y = 'EBITDA/KG' , data = df , color = '#A0cbe8' )","title":"Part B View the distributions of the data using matplotlib and seaborn"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#part-c-perform-moods-median-on-all-the-other-groups","text":"# Recall the other descriptors we have descriptors Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') for desc in descriptors : # YOUR CODE FORM MARGINS BELOW gp = df . groupby ( desc ) margins = [ j [ 'EBITDA/KG' ] . values for i , j in gp ] # UNPACK MARGINS INTO MEDIAN_TEST stat , p , m , table = scipy . stats . median_test ( * margins , correction = False ) print ( desc ) print ( \"The pearsons chi-square test statistic: {:.2f} \" . format ( stat )) print ( \"p-value of the test: {:e} \" . format ( p )) print ( \"the grand median: {} \" . format ( m ), end = ' \\n\\n ' ) Base Cake The pearsons chi-square test statistic: 448.81 p-value of the test: 8.851450e-95 the grand median: 0.21604872880760184 Truffle Type The pearsons chi-square test statistic: 22.86 p-value of the test: 1.088396e-05 the grand median: 0.21604872880760184 Primary Flavor The pearsons chi-square test statistic: 638.99 p-value of the test: 3.918933e-103 the grand median: 0.21604872880760184 Secondary Flavor The pearsons chi-square test statistic: 323.13 p-value of the test: 6.083210e-52 the grand median: 0.21604872880760184 Color Group The pearsons chi-square test statistic: 175.18 p-value of the test: 1.011412e-31 the grand median: 0.21604872880760184 Customer The pearsons chi-square test statistic: 5.66 p-value of the test: 2.257760e-01 the grand median: 0.21604872880760184 Date The pearsons chi-square test statistic: 5.27 p-value of the test: 9.175929e-01 the grand median: 0.21604872880760184","title":"Part C Perform Moods Median on all the other groups"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#part-d-many-boxplots","text":"And finally, we will confirm these visually. Complete the Boxplot for each group: for desc in descriptors : fig , ax = plt . subplots ( figsize = ( 10 , 5 )) sns . boxplot ( x = desc , y = 'EBITDA/KG' , data = df , color = '#A0cbe8' , ax = ax ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 9 ( ) missing from current font. fig.canvas.print_figure(bytes_io, **kw)","title":"Part D Many boxplots"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#213-what-is-a-t-test","text":"There are 1-sample and 2-sample T-tests (note: we would use a 1-sample T-test just to determine if the sample mean is equal to a hypothesized population mean) Within 2-sample T-tests we have independent and dependent T-tests (uncorrelated or correlated samples) For independent, two-sample T-tests: Equal variance (or pooled) T-test scipy.stats.ttest_ind(equal_var=True) Unequal variance T-test scipy.stats.ttest_ind(equal_var=False) also called Welch's T-test For dependent T-tests: * Paired (or correlated) T-test * scipy.stats.ttest_rel A full discussion on T-tests is outside the scope of this session, but we can refer to wikipedia for more information, including formulas on how each statistic is computed: * student's T-test","title":"2.1.3 What is a T-test?"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#2131-demonstration-of-t-tests","text":"back to top We'll assume our shifts are of equal variance and proceed with the appropriate independent two-sample T-test... print ( shift_one ) print ( shift_two ) [11, 14, 15, 16, 16, 16, 17, 18, 19, 21] [16, 17, 19, 20, 20, 20, 21, 22, 23, 23] To calculate the T-test, we follow a slightly different statistical formula: $T=\\frac{\\mu_1 - \\mu_2}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$ where $\\mu$ are the means of the two groups, $n$ are the sample sizes and $s$ is the pooled standard deviation, also known as the cummulative variance (depending on if you square it or not): $s= \\sqrt{\\frac{(n_1-1)\\sigma_1^2 + (n_2-1)\\sigma_2^2}{n_1 + n_2 - 2}}$ where $\\sigma$ are the standard deviations. What you'll notice here is we are combining the two variances, we can only do this if we assume the variances are somewhat equal, this is known as the equal variances t-test. mean_shift_one = np . mean ( shift_one ) mean_shift_two = np . mean ( shift_two ) print ( mean_shift_one , mean_shift_two ) 16.3 20.1 com_var = (( np . sum ([( i - mean_shift_one ) ** 2 for i in shift_one ]) + np . sum ([( i - mean_shift_two ) ** 2 for i in shift_two ])) / ( len ( shift_one ) + len ( shift_two ) - 2 )) print ( com_var ) 6.5 T = ( np . abs ( mean_shift_one - mean_shift_two ) / ( np . sqrt ( com_var / len ( shift_one ) + com_var / len ( shift_two )))) T 3.3328204733667115 We see that this hand-computed result matches that of the scipy module: scipy . stats . ttest_ind ( shift_two , shift_one , equal_var = True ) Ttest_indResult(statistic=3.3328204733667115, pvalue=0.0037029158660758575)","title":"2.1.3.1 Demonstration of T-tests"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#214-what-are-f-statistics-and-the-f-test","text":"The F-statistic is simply a ratio of two variances, or the ratio of mean squares mean squares is the estimate of population variance that accounts for the degrees of freedom to compute that estimate. We will explore this in the context of ANOVA","title":"2.1.4 What are F-statistics and the F-test?"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#2141-what-is-analysis-of-variance","text":"ANOVA uses the F-test to determine whether the variability between group means is larger than the variability within the groups. If that statistic is large enough, you can conclude that the means of the groups are not equal. The caveat is that ANOVA tells us whether there is a difference in means but it does not tell us where the difference is. To find where the difference is between the groups, we have to conduct post-hoc tests. There are two main types: * One-way (one factor) and * Two-way (two factor) where factor is an independent variable Ind A Ind B Dep X H 10 X I 12 Y I 11 Y H 20","title":"2.1.4.1 What is Analysis of Variance?"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#anova-hypotheses","text":"Null hypothesis : group means are equal Alternative hypothesis : at least one group mean is different from the other groups","title":"ANOVA Hypotheses"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#anova-assumptions","text":"Residuals (experimental error) are normally distributed (test with Shapiro-Wilk) Homogeneity of variances (variances are equal between groups) (test with Bartlett's) Observations are sampled independently from each other Note: ANOVA assumptions can be checked using test statistics (e.g. Shapiro-Wilk, Bartlett\u2019s, Levene\u2019s test) and the visual approaches such as residual plots (e.g. QQ-plots) and histograms.","title":"ANOVA Assumptions"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#steps-for-anova","text":"Check sample sizes: equal observations must be in each group Calculate Sum of Square between groups and within groups (\\(SS_B, SS_E\\)) Calculate Mean Square between groups and within groups (\\(MS_B, MS_E\\)) Calculate F value (\\(MS_B/MS_E\\)) This might be easier to see in a table: Source of Variation degree of freedom (Df) Sum of squares (SS) Mean square (MS) F value Between Groups Df_b = P-1 SS_B MS_B = SS_B / Df_B MS_B / MS_E Within Groups Df_E = P(N-1) SS_E MS_E = SS_E / Df_E total Df_T = PN-1 SS_T Where: SS_B = \\sum_{i}^{P}{(\\bar{y}_i-\\bar{y})^2} SS_E = \\sum_{ik}^{PN}{(\\bar{y}_{ik}-\\bar{y}_i)^2} SS_T = SS_B + SS_E Let's go back to our shift data to take an example: shifts = pd . DataFrame ([ shift_one , shift_two , shift_three , shift_four ]) . T shifts . columns = [ 'A' , 'B' , 'C' , 'D' ] shifts . boxplot () <AxesSubplot:>","title":"Steps for ANOVA"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#2142-sns-boxplot","text":"this is another great way to view boxplot data. Notice how sns also shows us the raw data alongside the box and whiskers using a swarmplot . shift_melt = pd . melt ( shifts . reset_index (), id_vars = [ 'index' ], value_vars = [ 'A' , 'B' , 'C' , 'D' ]) shift_melt . columns = [ 'index' , 'shift' , 'rate' ] ax = sns . boxplot ( x = 'shift' , y = 'rate' , data = shift_melt , color = '#A0cbe8' ) ax = sns . swarmplot ( x = \"shift\" , y = \"rate\" , data = shift_melt , color = '#79706e' ) Anyway back to ANOVA... fvalue , pvalue = stats . f_oneway ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( fvalue , pvalue ) 5.599173553719008 0.0029473487978665873 We can get this in the format of the table we saw above: # get ANOVA table import statsmodels.api as sm from statsmodels.formula.api import ols # Ordinary Least Squares (OLS) model model = ols ( 'rate ~ C(shift)' , data = shift_melt ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(shift) 135.5 3.0 5.599174 0.002947 Residual 290.4 36.0 NaN NaN The Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9750654697418213 0.5121709108352661 We can use Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. w , pvalue = stats . bartlett ( shifts [ 'A' ], shifts [ 'B' ], shifts [ 'C' ], shifts [ 'D' ]) print ( w , pvalue ) 1.3763632854696672 0.711084540821183","title":"2.1.4.2 SNS Boxplot"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#2143-anova-interpretation","text":"The p value form ANOVA analysis is significant ( p < 0.05) and we can conclude there are significant difference between the shifts. But we do not know which shift(s) are different. For this we need to perform a post hoc test. There are a multitude of these that are beyond the scope of this discussion ( Tukey-kramer is one such test)","title":"2.1.4.3 ANOVA Interpretation"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#215-putting-it-all-together","text":"In summary, there are many statistical tests at our disposal when performing inferential statistical analysis. In times like these, a simple decision tree can be extraordinarily useful! source: scribbr","title":"2.1.5 Putting it all together"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#22-enrichment-evaluate-statistical-significance-of-product-margin-a-snake-in-the-garden","text":"","title":"\ud83c\udf52 2.2 Enrichment: Evaluate statistical significance of product margin: a snake in the garden"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#221-moods-median-on-product-descriptors","text":"The first issue we run into with moods is... what? We can only perform moods on two groups at a time. How can we get around this? Let's take a look at the category with the fewest descriptors. If we remember, this was the Truffle Types. df . columns Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date', 'KG', 'EBITDA/KG'], dtype='object') df [ 'Truffle Type' ] . unique () array(['Candy Outer', 'Chocolate Outer', 'Jelly Filled'], dtype=object) col = 'Truffle Type' moodsdf = pd . DataFrame () for truff in df [ col ] . unique (): # for each group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) print ( \" {} : N= {} \" . format ( truff , size )) print ( \"Welch's T-Test for Unequal Variances\" ) print ( scipy . stats . ttest_ind ( group , pop , equal_var = False )) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue print () moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ col , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] Candy Outer: N=288 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=-2.7615297773427474, pvalue=0.0059110489226580736) Chocolate Outer: N=1356 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=4.409449025092911, pvalue=1.1932685612874952e-05) Jelly Filled: N=24 Welch's T-Test for Unequal Variances Ttest_indResult(statistic=-8.414252306793504, pvalue=7.929912531660087e-09)","title":"2.2.1 Mood's Median on product descriptors"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#question-1-moods-results-on-truffle-type","text":"What do we notice about the resultant table? p-values Most are quite small (really low probability of achieving these table results under a single distribution) group sizes: our Jelly Filled group is relatively small moodsdf . sort_values ( 'p_value' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Truffle Type pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Jelly Filled 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 0 Chocolate Outer 6.627496 0.010042 0.216049 0.262601 0.225562 1356 0.000012 [[699, 135], [657, 177]] 0 Candy Outer 1.515066 0.218368 0.216049 0.230075 0.204264 288 0.005911 [[134, 700], [154, 680]] We can go ahead and repeat this analysis for all of our product categories: df . columns [: 5 ] Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group'], dtype='object') moodsdf = pd . DataFrame () for col in df . columns [: 5 ]: for truff in df [ col ] . unique (): group = df . loc [ df [ col ] == truff ][ 'EBITDA/KG' ] pop = df . loc [ ~ ( df [ col ] == truff )][ 'EBITDA/KG' ] stat , p , m , table = scipy . stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) welchp = scipy . stats . ttest_ind ( group , pop , equal_var = False ) . pvalue moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , welchp , table ]) . T ]) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'welch p' , 'table' ] print ( moodsdf . shape ) (101, 10) moodsdf = moodsdf . loc [( moodsdf [ 'welch p' ] < 0.005 ) & ( moodsdf [ 'p_value' ] < 0.005 )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( moodsdf . shape ) (51, 10) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size welch p table 0 Secondary Flavor Papaya 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 1 Primary Flavor Orange Pineapple\\tP 18.643248 0.000016 0.216049 0.016747 0.002458 24 0.0 [[1, 833], [23, 811]] 2 Primary Flavor Cherry Cream Spice 10.156401 0.001438 0.216049 0.018702 0.009701 12 0.000001 [[0, 834], [12, 822]] 3 Secondary Flavor Cucumber 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 4 Truffle Type Jelly Filled 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 5 Primary Flavor Orange 18.643248 0.000016 0.216049 0.051382 0.017933 24 0.0 [[1, 833], [23, 811]] 6 Primary Flavor Toasted Coconut 15.261253 0.000094 0.216049 0.037002 0.028392 24 0.0 [[2, 832], [22, 812]] 7 Secondary Flavor Apricot 15.261253 0.000094 0.216049 0.060312 0.037422 24 0.0 [[2, 832], [22, 812]] 8 Primary Flavor Kettle Corn 29.062065 0.0 0.216049 0.055452 0.045891 60 0.0 [[9, 825], [51, 783]] 9 Primary Flavor Acai Berry 18.643248 0.000016 0.216049 0.036505 0.049466 24 0.0 [[1, 833], [23, 811]] 10 Primary Flavor Pink Lemonade 10.156401 0.001438 0.216049 0.039862 0.056349 12 0.000011 [[0, 834], [12, 822]] 11 Secondary Flavor Black Cherry 58.900366 0.0 0.216049 0.055975 0.062898 96 0.0 [[11, 823], [85, 749]] 12 Primary Flavor Watermelon 15.261253 0.000094 0.216049 0.04405 0.067896 24 0.0 [[2, 832], [22, 812]] 13 Primary Flavor Plum 34.851608 0.0 0.216049 0.084963 0.079993 72 0.0 [[11, 823], [61, 773]] 14 Secondary Flavor Dill Pickle 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 15 Primary Flavor Horchata 10.156401 0.001438 0.216049 0.037042 0.082494 12 0.000007 [[0, 834], [12, 822]] 16 Primary Flavor Lemon Custard 12.217457 0.000473 0.216049 0.079389 0.087969 24 0.000006 [[3, 831], [21, 813]] 17 Primary Flavor Fruit Punch 10.156401 0.001438 0.216049 0.078935 0.090326 12 0.000076 [[0, 834], [12, 822]] 18 Base Cake Chiffon 117.046226 0.0 0.216049 0.127851 0.125775 288 0.0 [[60, 774], [228, 606]] 19 Base Cake Butter 134.36727 0.0 0.216049 0.142082 0.139756 456 0.0 [[122, 712], [334, 500]] 20 Secondary Flavor Banana 10.805348 0.001012 0.216049 0.163442 0.15537 60 0.0 [[17, 817], [43, 791]] 21 Primary Flavor Cream Soda 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 22 Secondary Flavor Peppermint 9.511861 0.002041 0.216049 0.150265 0.163455 24 0.000002 [[4, 830], [20, 814]] 23 Primary Flavor Grand Mariner 10.581767 0.001142 0.216049 0.197463 0.165529 72 0.000829 [[22, 812], [50, 784]] 24 Color Group Amethyst 20.488275 0.000006 0.216049 0.195681 0.167321 300 0.0 [[114, 720], [186, 648]] 25 Color Group Burgundy 10.999677 0.000911 0.216049 0.193048 0.171465 120 0.000406 [[42, 792], [78, 756]] 26 Color Group White 35.76526 0.0 0.216049 0.19 0.177264 432 0.0 [[162, 672], [270, 564]] 27 Color Group Opal 11.587164 0.000664 0.216049 0.317878 0.259304 324 0.0 [[190, 644], [134, 700]] 28 Secondary Flavor Apple 27.283292 0.0 0.216049 0.326167 0.293876 36 0.001176 [[34, 800], [2, 832]] 29 Secondary Flavor Tangerine 32.626389 0.0 0.216049 0.342314 0.319273 48 0.000113 [[44, 790], [4, 830]] 30 Secondary Flavor Black Currant 34.778391 0.0 0.216049 0.357916 0.332449 36 0.0 [[36, 798], [0, 834]] 31 Secondary Flavor Pear 16.614303 0.000046 0.216049 0.373034 0.33831 60 0.000031 [[46, 788], [14, 820]] 32 Primary Flavor Vanilla 34.778391 0.0 0.216049 0.378053 0.341626 36 0.000001 [[36, 798], [0, 834]] 33 Color Group Citrine 10.156401 0.001438 0.216049 0.390728 0.342512 12 0.001925 [[12, 822], [0, 834]] 34 Color Group Teal 13.539679 0.000234 0.216049 0.323955 0.3446 96 0.00121 [[66, 768], [30, 804]] 35 Base Cake Tiramisu 52.360619 0.0 0.216049 0.388267 0.362102 144 0.0 [[114, 720], [30, 804]] 36 Primary Flavor Doughnut 74.935256 0.0 0.216049 0.439721 0.379361 108 0.0 [[98, 736], [10, 824]] 37 Secondary Flavor Ginger Beer 22.363443 0.000002 0.216049 0.444895 0.382283 24 0.000481 [[24, 810], [0, 834]] 38 Color Group Rose 18.643248 0.000016 0.216049 0.42301 0.407061 24 0.000062 [[23, 811], [1, 833]] 39 Base Cake Cheese 66.804744 0.0 0.216049 0.450934 0.435638 84 0.0 [[79, 755], [5, 829]] 40 Primary Flavor Butter Toffee 60.181468 0.0 0.216049 0.50366 0.456343 60 0.0 [[60, 774], [0, 834]] 41 Color Group Slate 10.156401 0.001438 0.216049 0.540214 0.483138 12 0.000017 [[12, 822], [0, 834]] 42 Primary Flavor Gingersnap 22.363443 0.000002 0.216049 0.643218 0.623627 24 0.0 [[24, 810], [0, 834]] 43 Primary Flavor Dill Pickle 22.363443 0.000002 0.216049 0.642239 0.655779 24 0.0 [[24, 810], [0, 834]] 44 Color Group Olive 44.967537 0.0 0.216049 0.637627 0.670186 60 0.0 [[56, 778], [4, 830]] 45 Primary Flavor Butter Milk 10.156401 0.001438 0.216049 0.699284 0.688601 12 0.0 [[12, 822], [0, 834]] 46 Base Cake Sponge 127.156266 0.0 0.216049 0.698996 0.699355 120 0.0 [[120, 714], [0, 834]] 47 Primary Flavor Chocolate Mint 10.156401 0.001438 0.216049 0.685546 0.699666 12 0.0 [[12, 822], [0, 834]] 48 Primary Flavor Coconut 10.156401 0.001438 0.216049 0.732777 0.717641 12 0.0 [[12, 822], [0, 834]] 49 Primary Flavor Blueberry 22.363443 0.000002 0.216049 0.759643 0.72536 24 0.0 [[24, 810], [0, 834]] 50 Primary Flavor Amaretto 10.156401 0.001438 0.216049 0.782156 0.764845 12 0.0 [[12, 822], [0, 834]]","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: Moods Results on Truffle Type"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#222-enrichment-broad-analysis-of-categories-anova","text":"Recall our \"melted\" shift data. It will be useful to think of getting our Truffle data in this format: shift_melt . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index shift rate 0 0 A 11 1 1 A 14 2 2 A 15 3 3 A 16 4 4 A 16 df . columns = df . columns . str . replace ( ' ' , '_' ) df . columns = df . columns . str . replace ( '/' , '_' ) # get ANOVA table # Ordinary Least Squares (OLS) model model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) anova_table # output (ANOVA F and p value) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Recall the Shapiro-Wilk test can be used to check the normal distribution of residuals . Null hypothesis: data is drawn from normal distribution. w , pvalue = stats . shapiro ( model . resid ) print ( w , pvalue ) 0.9576056599617004 1.2598073820281984e-21 And the Bartlett\u2019s test to check the Homogeneity of variances . Null hypothesis: samples from populations have equal variances. gb = df . groupby ( 'Truffle_Type' )[ 'EBITDA_KG' ] gb <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fb6940a7370> w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( w , pvalue ) 109.93252546442552 1.344173733366234e-24 Wow it looks like our data is not drawn from a normal distribution! Let's check this for other categories... We can wrap these in a for loop: for col in df . columns [: 5 ]: print ( col ) model = ols ( 'EBITDA_KG ~ C( {} )' . format ( col ), data = df ) . fit () anova_table = sm . stats . anova_lm ( model , typ = 2 ) display ( anova_table ) w , pvalue = stats . shapiro ( model . resid ) print ( \"Shapiro: \" , w , pvalue ) gb = df . groupby ( col )[ 'EBITDA_KG' ] w , pvalue = stats . bartlett ( * [ gb . get_group ( x ) for x in gb . groups ]) print ( \"Bartlett: \" , w , pvalue ) print () Base_Cake .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Base_Cake) 39.918103 5.0 314.869955 1.889884e-237 Residual 42.140500 1662.0 NaN NaN Shapiro: 0.9634131193161011 4.1681337029688696e-20 Bartlett: 69.83288886114286 1.1102218566048873e-13 Truffle_Type .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Truffle_Type) 1.250464 2.0 12.882509 0.000003 Residual 80.808138 1665.0 NaN NaN Shapiro: 0.9576056599617004 1.2598073820281984e-21 Bartlett: 109.93252546442552 1.344173733366234e-24 Primary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Primary_Flavor) 50.270639 50.0 51.143649 1.153434e-292 Residual 31.787964 1617.0 NaN NaN Shapiro: 0.948470413684845 9.90281706784179e-24 Bartlett: 210.15130419114982 1.5872504991225816e-21 Secondary_Flavor .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Secondary_Flavor) 15.088382 28.0 13.188089 1.929302e-54 Residual 66.970220 1639.0 NaN NaN Shapiro: 0.9548103213310242 2.649492974953278e-22 Bartlett: 420.6274502894812 1.2373007035089195e-71 Color_Group .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(>F) C(Color_Group) 16.079685 11.0 36.689347 6.544980e-71 Residual 65.978918 1656.0 NaN NaN Shapiro: 0.969061017036438 1.8926407335144587e-18 Bartlett: 136.55525281340468 8.164787784033709e-24","title":"\ud83c\udf52\ud83c\udf52 2.2.2 Enrichment: Broad Analysis of Categories: ANOVA"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#223-enrichment-visual-analysis-of-residuals-qq-plots","text":"This can be distressing and is often why we want visual methods to see what is going on with our data! model = ols ( 'EBITDA_KG ~ C(Truffle_Type)' , data = df ) . fit () #create instance of influence influence = model . get_influence () #obtain standardized residuals standardized_residuals = influence . resid_studentized_internal # res.anova_std_residuals are standardized residuals obtained from ANOVA (check above) sm . qqplot ( standardized_residuals , line = '45' ) plt . xlabel ( \"Theoretical Quantiles\" ) plt . ylabel ( \"Standardized Residuals\" ) plt . show () # histogram plt . hist ( model . resid , bins = 'auto' , histtype = 'bar' , ec = 'k' ) plt . xlabel ( \"Residuals\" ) plt . ylabel ( 'Frequency' ) plt . show () We see that a lot of our data is swayed by extremely high and low values, so what can we conclude? You need the right test statistic for the right job, in this case, we are littered with unequal variance in our groupings so we use the moods median and welch (unequal variance t-test) to make conclusions about our data","title":"\ud83c\udf52\ud83c\udf52 2.2.3 Enrichment: Visual Analysis of Residuals: QQ-Plots"},{"location":"solutions/SOLN_S2_Inferential_Statistics/#references","text":"Renesh Bedre ANOVA Minitab ANOVA Analytics Vidhya ANOVA Renesh Bedre Hypothesis Testing Real Statistics Turkey-kramer Mutual Information","title":"References"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/","text":"Data Science Foundations, Session 3: Model Selection and Validation \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com At the end of session 1, we saw the basic recipe for creating a supervised machine learning model: Environment setup and importing data Rudimentary exploratory data analysis Feature engineering (these were created for us) Choosing and training a model: choose model choose hyperparameters fit using (training) data predict using (validation) data In session 1, I chose our model and hyperparameters preemptively. How did I do that? In the real world, you won't necessarily have the best intution about how to make these choices. In today's session, we will algorithmize the way we approach choosing and training a model Note: I will import libraries at the beginning of this notebook, as is good practice, but will reimport them as they are used to remind ourselves where each method came from! 3.0 Preparing Environment and Importing Data \u00b6 back to top 3.0.1 Import Packages \u00b6 back to top # Pandas library for the pandas dataframes import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import seaborn as sns import random import scipy.stats as stats from patsy import dmatrices from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import LabelEncoder , StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn import metrics from sklearn.metrics import r2_score , mean_squared_error from sklearn.datasets import load_iris 3.0.2 Load Dataset \u00b6 back to top In session 1 we cursorily discussed why we may need strategies for validating our model. Here we'll discuss it more in depth. I'm going to take a simple example. In the following, I have a dataset that contains some data about flowers. It's a very famous dataset used in the ML world iris = load_iris () X = iris . data y = iris . target print ( X . shape ) print ( y . shape ) (150, 4) (150,) let's go ahead and load our wine dataset as well... wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) wine . dropna ( inplace = True ) wine [ 'quality_label' ] = wine [ 'quality' ] . apply ( lambda x : 'low' if x <= 5 else 'med' if x <= 7 else 'high' ) class_tp = { 'red' : 0 , 'white' : 1 } y_tp = wine [ 'type' ] . map ( class_tp ) wine [ 'type_encoding' ] = y_tp class_ql = { 'low' : 0 , 'med' : 1 , 'high' : 2 } y_ql = wine [ 'quality_label' ] . map ( class_ql ) wine [ 'quality_encoding' ] = y_ql wine . drop ([ 'type' , 'quality_label' , 'quality' ], axis = 1 , inplace = True ) wine . columns = wine . columns . str . replace ( ' ' , '_' ) wine . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide density pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1 3.1 Model Validation \u00b6 back to top doing it the wrong way While we're here, I'm going to introduce a VERY SIMPLE supervised learning method called K-Nearest Neighbors. 3.1.0 K-Nearest Neighbors \u00b6 back to top K-Nearest Neighbors is perhaps the simplest algorithm of them all. It is essentially a lookup table: We select the hyperparameter K, and when assigning a new value a data label, assign it according to, the majority label in the vicinity of the new datapoint. The vicinity being determined by K, the number of nearest neighbors we are going to assess. knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X , y ) KNeighborsRegressor(n_neighbors=1) knn . score ( X , y ) 1.0 Wow! we achieved a model with a perfect score! But is this really how we would expect the model to perform on data it had never seen before? Probably not. How do we actually check the performance of our model? 3.1.1 Holdout Sets \u00b6 back to top The way we account for unseen data, in practice, is to leave a portion of the dataset out for testing. This way, we can estimate how our model will perform on entirely new data it may come across in application. X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.6 , random_state = 42 ) knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X_train , y_train ) print ( knn . score ( X_test , y_test )) 0.9753593429158111 We see that we get a more reasonable value for our performance! 3.1.2 Data Leakage and Cross-Validation \u00b6 back to top An even more rigorous method to leaving out a single test set, is to perform cross validation. Imagine a situation where we are trying to estimate the best value of K in our KNN algorithm. If we continually train our model with new values of K on our training set, and test with our testing set, \"knowledge\" of our test set values with leak into our model, as we choose the best value for K based on how it performs on our test set (even though we did not train on this test set). We call this phenomenon data leakage . CV or Cross Validation overcomes this by only evaluating our parameters with our training set. [image src](https://scikit-learn.org/stable/modules/cross_validation.html) In this scheme, we don't evaluate our model on the test set until the very end. Rather, we estimate our hyperparameter performances by slicing the training set into cross folds [image src](https://scikit-learn.org/stable/modules/cross_validation.html) from sklearn.model_selection import cross_val_score scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) scores array([0.91666667, 0.81725888, 0.85714286, 1. , 0.91 ]) print ( \" %0.2f accuracy with a standard deviation of %0.3f \" % ( scores . mean (), scores . std ())) 0.90 accuracy with a standard deviation of 0.062 More information on the cross_val_score method in sklearn can be found here An additional topic on cross validation is the extreme leave-one-out validation, you can read more about that here Taking these building blocks, if we wanted to for example optimize the number of nearest neighbors without incurring data leakage. We might take the following approach: for k in range ( 1 , 10 ): knn = KNeighborsRegressor ( n_neighbors = k ) scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) print ( \"k= %0.0f , %0.3f accuracy with a standard deviation of %0.3f \" % ( k , scores . mean (), scores . std ())) k=1, 0.900 accuracy with a standard deviation of 0.062 k=2, 0.925 accuracy with a standard deviation of 0.071 k=3, 0.916 accuracy with a standard deviation of 0.077 k=4, 0.932 accuracy with a standard deviation of 0.046 k=5, 0.928 accuracy with a standard deviation of 0.045 k=6, 0.924 accuracy with a standard deviation of 0.047 k=7, 0.929 accuracy with a standard deviation of 0.041 k=8, 0.924 accuracy with a standard deviation of 0.047 k=9, 0.923 accuracy with a standard deviation of 0.042 k = 4 # select best hyperparameters knn = KNeighborsRegressor ( n_neighbors = k ) knn . fit ( X_train , y_train ) # final train print ( knn . score ( X_test , y_test )) 0.9845995893223819 3.1.3 Bias-Variance Tradeoff \u00b6 back to top This next concept will be most easily understood if we go ahead an make up some data ourselves, I'm going to do that now. # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) Text(0, 0.5, 'Y') Let's fit to just a portion of this data random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] plt . plot ( X_train , y_train , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f9c34de5820>] Now let's take two extreme scenarios: fitting to these datapoints a linear line and a high order polynomial. Keeping in mind the larger dataset (the population) as well as the (irreducible) error we introduced in our data generating function will really illustrate our point! # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) # plot linear model ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) # plot polynomial model ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has high bias because we are forcing the functional form without much consideration to the underlying data \u2014 we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has low variance with respect to the underlying data. On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has low bias because we don't introduce many constraints on the final form of the model. it is high variance because depending on the underlying training data, the final outcome of the model can change quite drastically! In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further: what happens when we retrain these models on different samples of the data population and let's use this to better understand what we mean by bias and variance what happens when we tie this back in with the irreducible error we introduced to the data generator? and let's use this to better understand irreducible error # we're going to perform this task again, but this time for 5 rounds, training # on different samples of the population data random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) for samples in range ( 5 ): X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) As we can see, depending on what data we train our model on, the high bias model changes relatively slightly, while the high variance model changes a whole awful lot! The high variance model is prone to something we call overfitting . It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the population : # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance models are particuarly prone to the phenomenon of over fitting and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this. x = np . arange ( 20 , 100 ) y = [ func ( t , err = 0 ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f9c3424b970>] random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') This time, our high variance model really gets it ! And this is because the data we trained on actually is a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the bias variance tradeoff \ud83c\udfcb\ufe0f Exercise 1: Quantitatively Define Performance \u00b6 Up until now, we've explored this idea of bias variance tradeoff from a qualitative standpoint. As an exercise, continue with this idea, this time calculating the mean squared error (MSE) and R-square between the model and UNSEEN (non-training data) population data. error metrics r2_score mean_squared_error Do this for a 9 th order polynomial and repeat for population data with low, med, and high degrees of error and small and large training fractions (for 6 total comparisons). Complete the chart below. error training fraction MSE R2 0.1 .2 5.20e-03 0.97 0.1 .8 3.24e-03 0.98 0.4 .2 8.32e-02 0.65 0.4 .8 5.18e-02 0.80 0.8 .2 3.33e-01 0.08 0.8 .8 2.07e-01 0.52 # Code Cell for Exercise 1 from sklearn.preprocessing import PolynomialFeatures random . seed ( 42 ) # function to generate data def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) ################################################################################ ########## CHANGE ERR TO CHANGE THE AMOUNT OF NOISE IN YOUR DATA ############### ################################################################################ err = .8 # change the error (.1 - 0.9) y_actual = [ func ( t , err ) for t in x ] ################################################################################ ### SAMPLE THE DATA FOR TRAINING ################################################################################ train_size = 0.8 # change the training size x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = train_size , random_state = 42 ) # solving our training data with a 9-degree polynomial coefs = np . polyfit ( x_train , y_train , 9 ) # generate y data with 9-degree polynomial model and X_seq y_pred = np . polyval ( coefs , x_test ) ################################################################################ ### CALCULATE MSE AND R2 ################################################################################ mse = mean_squared_error ( y_test , y_pred ) r2 = r2_score ( y_test , y_pred ) # look at results print ( \"irreducible error: {} \" . format ( err )) print ( \"training fraction: {} \" . format ( train_size )) print ( \"mean square error: {:.2e} \" . format ( mse )) print ( \"r2: {:.2f} \" . format ( r2 )) irreducible error: 0.8 training fraction: 0.8 mean square error: 2.07e-01 r2: 0.52 3.1.4 Learning Curves \u00b6 back to top To move from qualitative to quantitative understanding of bias-variance tradeoff we need to introduce some metric for model performance. A good one to use here is R-square, a measure of the degree to which predictions match actual values. We can import a tool from sklearn to calculate this for us. from sklearn.metrics import r2_score 3.1.4.1 Considering Model Complexity \u00b6 back to top In a learning curve, we will typically plot the training and testing scores together, to give a sense of when we have either too much bias or too much variance in our model. I'm going to go ahead and recreate the original data distribution we introduced in 1.1.3 # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f9c34c91d30>] Now let's itteratively introduce more complexity into our model random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score = r2_score ( np . polyval ( coefs , X_train ), y_train ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score , ls = '' , marker = '.' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f9c34c57f40> As we see from both plots, the score on the training data increases with added model complexity. Giving us the expected perfect fit when the order is the same as the number of data points! This is part I of our learning curve. Part II consists of plotting the training data score with the testing data score. Something else I'm going to do, is define the training portion of the data as a fraction of the overall dataset . This is a typical practice in machine learning. To keep the comparisons the same as up until now, I will keep this training fraction low at .2 random . seed ( 42 ) # defining my training fraction training_frac = .2 # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) train_indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in train_indices ] test_indices = [ i for i in range ( len ( x )) if i not in train_indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) ax [ 0 ] . plot ( X_test , y_test , ls = '' , marker = '.' , color = 'grey' , alpha = 0.5 ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score1 )) ax [ 1 ] . plot ( complexity , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend ([ 'Train $R^2$' , 'Test $R^2$' ]) ax [ 0 ] . legend () <matplotlib.legend.Legend at 0x7f9c2c7c2580> As we can see, The 2 nd order polynomial achieves the greatest best test set data \\(R^2\\), while the highest order polynomial achieves the best training set data \\(R^2\\). This learning curve is explanative of what we see generally, namely a divergence after some degree of complexity between training and test set performances. In this case, we would resolve to choose the 2 nd order polynomial as the best model for our data. img src 3.1.4.2 Considering Training Set Size \u00b6 back to top The last piece of the puzzle we require, to fully cover learning curves, is the effect of training data size on the model. Let's explore. random . seed ( 42 ) # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .9 , 50 ): # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax . plot ( training_frac , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax . plot ( training_frac , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax . set_title ( \"9th-order Polynomial Score with Increasing Training Set Size\" ) ax . legend ([ 'Train' , 'Test' ]) ax . set_xlabel ( 'Training Fraction' ) ax . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$') What we see here is a trend that happens generally, as our amount of training data increases, our models handle more complexity. This is because the more data we have, the more we are able to dicipher noise from signal. Our model will ignore noise as it does not generalize well across the other datapoints. Instead, it will will fit to relationships that appear across the entire training dataset. img src \ud83c\udfcb\ufe0f Exercise 2: Visualization \u00b6 Starting with the code below, make a side-by-side plot of a 3 rd degree polynomial and a 9 th degree polynomial. On the x axis slowly increase the training set size, on the y axis plot the scores for the training and test sets. # Code Cell for Exercise 2 random . seed ( 42 ) # create the figure and axes fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .95 , 50 ): # create test and training data x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = training_frac , random_state = 42 ) # solving our training data with a n-degree polynomial coefs1 = np . polyfit ( x_train , y_train , 9 ) coefs2 = np . polyfit ( x_train , y_train , 3 ) # recording the scores for the training and test sets score1_train = r2_score ( np . polyval ( coefs1 , x_train ), y_train ) score1_test = r2_score ( np . polyval ( coefs1 , x_test ), y_test ) score2_train = r2_score ( np . polyval ( coefs2 , x_train ), y_train ) score2_test = r2_score ( np . polyval ( coefs2 , x_test ), y_test ) ax [ 0 ] . plot ( training_frac , score1_train , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax [ 0 ] . plot ( training_frac , score1_test , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax [ 1 ] . plot ( training_frac , score2_train , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax [ 1 ] . plot ( training_frac , score2_test , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax [ 0 ] . set_title ( \"9th-order Polynomial Score\" ) ax [ 1 ] . set_title ( \"3rd-order Polynomial Score\" ) ax [ 0 ] . legend ([ 'Train' , 'Test' ]) ax [ 0 ] . set_xlabel ( 'Training Fraction' ) ax [ 1 ] . set_xlabel ( 'Training Fraction' ) ax [ 0 ] . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$') \ud83d\ude4b\u200d\u2640\ufe0f Question 1: In what regions of the plots are we overfitting? \u00b6 Where in these plots is overfitting occuring? Why is it different for each polynomial? 3.2 Model Validation in Practice \u00b6 back to top We will now turn our attention to practical implementation. In practice, there are a wide number of variables (called hyperparameters) to consider when choosing a model. Scikit learn has a useful method called Grid Search that will iterate through every possible combination of a range of hyperparameter settings you provide as input. Before we get started with grid search, we'll need to switch over from our numpy polynomial fit method to one in sklearn. Here, the caveat is our actual model will solve for the coefficients infront of the polynomials. We will engineer the polynomial features ourselves. This is an example of feature engineering which we will revisit in depth in the next session. from sklearn.preprocessing import PolynomialFeatures , StandardScaler from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) 3.2.1 Grid Search \u00b6 back to top from sklearn.model_selection import GridSearchCV param_grid = { 'polynomialfeatures__degree' : np . arange ( 10 ), 'linearregression__fit_intercept' : [ True , False ]} grid = GridSearchCV ( PolynomialRegression (), param_grid , cv = 7 ) # create test and training data random . seed ( 42 ) X_train = random . sample ( list ( x ), int ( int ( len ( x )) * .8 )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] grid . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) GridSearchCV(cv=7, estimator=Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()), ('linearregression', LinearRegression())]), param_grid={'linearregression__fit_intercept': [True, False], 'polynomialfeatures__degree': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}) grid . best_params_ {'linearregression__fit_intercept': True, 'polynomialfeatures__degree': 3} to grab the best model from the CV/search outcome. we use grid.best_estimator model = grid . best_estimator_ # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 5 )) ax . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax . plot ( X_train , y_train , ls = '' , marker = '.' ) ax . set_ylim ( min ( y ), max ( y )) ax . set_title ( \"Best Grid Search CV Model\" ) Text(0.5, 1.0, 'Best Grid Search CV Model') \ud83c\udfcb\ufe0f Exercise 3: Grid Search \u00b6 There are two objectives for this exercise: Complete the object param_grid by reading the parameters below in StandardScaler . We would like to include in our grid search whether or not to scale our input features so that they are centered at 0 ( with_mean ) and have unit variance ( with_std ) After you have completed the grid search, create a side-by-side plot of actual versus predicted values for training data (left plot) and testing data (right plot) Include in your plot title the R2 for testing and R2 for training StandardScaler () . get_params () . keys () dict_keys(['copy', 'with_mean', 'with_std']) df = wine . copy () y = df . pop ( 'density' ) X = df display ( X . head ()) display ( y . head ()) print ( X . shape ) print ( y . shape ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) def NormalizedRegression ( ** kwargs ): return make_pipeline ( StandardScaler (), LinearRegression ( ** kwargs )) ###################################################################################################### ####################### YOUR EXTRA PARAMETERS GO IN THE DICTIONARY BELOW ############################# ###################################################################################################### param_grid = { 'linearregression__fit_intercept' : [ True , False ], 'standardscaler__with_mean' : [ True , False ], 'standardscaler__with_std' : [ True , False ]} grid = GridSearchCV ( NormalizedRegression (), param_grid , cv = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 0 1.0010 1 0.9940 2 0.9951 3 0.9956 4 0.9956 Name: density, dtype: float64 (6463, 12) (6463,) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) {'linearregression__fit_intercept': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True} model = grid . best_estimator_ ###################################################################################################### ##################################### ADJUST THE PLOT LOGIC BELOW #################################### ###################################################################################################### fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.962') References \u00b6 back to top Model Validation \u00b6 cross_val_score leave-one-out","title":"SOLN S3 Model Selection and Validation"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#data-science-foundations-session-3-model-selection-and-validation","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com At the end of session 1, we saw the basic recipe for creating a supervised machine learning model: Environment setup and importing data Rudimentary exploratory data analysis Feature engineering (these were created for us) Choosing and training a model: choose model choose hyperparameters fit using (training) data predict using (validation) data In session 1, I chose our model and hyperparameters preemptively. How did I do that? In the real world, you won't necessarily have the best intution about how to make these choices. In today's session, we will algorithmize the way we approach choosing and training a model Note: I will import libraries at the beginning of this notebook, as is good practice, but will reimport them as they are used to remind ourselves where each method came from!","title":"Data Science Foundations, Session 3: Model Selection and Validation"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#30-preparing-environment-and-importing-data","text":"back to top","title":"3.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#301-import-packages","text":"back to top # Pandas library for the pandas dataframes import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import seaborn as sns import random import scipy.stats as stats from patsy import dmatrices from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import LabelEncoder , StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression , LinearRegression from sklearn import metrics from sklearn.metrics import r2_score , mean_squared_error from sklearn.datasets import load_iris","title":"3.0.1 Import Packages"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#302-load-dataset","text":"back to top In session 1 we cursorily discussed why we may need strategies for validating our model. Here we'll discuss it more in depth. I'm going to take a simple example. In the following, I have a dataset that contains some data about flowers. It's a very famous dataset used in the ML world iris = load_iris () X = iris . data y = iris . target print ( X . shape ) print ( y . shape ) (150, 4) (150,) let's go ahead and load our wine dataset as well... wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) wine . dropna ( inplace = True ) wine [ 'quality_label' ] = wine [ 'quality' ] . apply ( lambda x : 'low' if x <= 5 else 'med' if x <= 7 else 'high' ) class_tp = { 'red' : 0 , 'white' : 1 } y_tp = wine [ 'type' ] . map ( class_tp ) wine [ 'type_encoding' ] = y_tp class_ql = { 'low' : 0 , 'med' : 1 , 'high' : 2 } y_ql = wine [ 'quality_label' ] . map ( class_ql ) wine [ 'quality_encoding' ] = y_ql wine . drop ([ 'type' , 'quality_label' , 'quality' ], axis = 1 , inplace = True ) wine . columns = wine . columns . str . replace ( ' ' , '_' ) wine . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide density pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 1 1","title":"3.0.2 Load Dataset"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#31-model-validation","text":"back to top doing it the wrong way While we're here, I'm going to introduce a VERY SIMPLE supervised learning method called K-Nearest Neighbors.","title":"3.1 Model Validation"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#310-k-nearest-neighbors","text":"back to top K-Nearest Neighbors is perhaps the simplest algorithm of them all. It is essentially a lookup table: We select the hyperparameter K, and when assigning a new value a data label, assign it according to, the majority label in the vicinity of the new datapoint. The vicinity being determined by K, the number of nearest neighbors we are going to assess. knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X , y ) KNeighborsRegressor(n_neighbors=1) knn . score ( X , y ) 1.0 Wow! we achieved a model with a perfect score! But is this really how we would expect the model to perform on data it had never seen before? Probably not. How do we actually check the performance of our model?","title":"3.1.0 K-Nearest Neighbors"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#311-holdout-sets","text":"back to top The way we account for unseen data, in practice, is to leave a portion of the dataset out for testing. This way, we can estimate how our model will perform on entirely new data it may come across in application. X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.6 , random_state = 42 ) knn = KNeighborsRegressor ( n_neighbors = 1 ) knn . fit ( X_train , y_train ) print ( knn . score ( X_test , y_test )) 0.9753593429158111 We see that we get a more reasonable value for our performance!","title":"3.1.1 Holdout Sets"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#312-data-leakage-and-cross-validation","text":"back to top An even more rigorous method to leaving out a single test set, is to perform cross validation. Imagine a situation where we are trying to estimate the best value of K in our KNN algorithm. If we continually train our model with new values of K on our training set, and test with our testing set, \"knowledge\" of our test set values with leak into our model, as we choose the best value for K based on how it performs on our test set (even though we did not train on this test set). We call this phenomenon data leakage . CV or Cross Validation overcomes this by only evaluating our parameters with our training set. [image src](https://scikit-learn.org/stable/modules/cross_validation.html) In this scheme, we don't evaluate our model on the test set until the very end. Rather, we estimate our hyperparameter performances by slicing the training set into cross folds [image src](https://scikit-learn.org/stable/modules/cross_validation.html) from sklearn.model_selection import cross_val_score scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) scores array([0.91666667, 0.81725888, 0.85714286, 1. , 0.91 ]) print ( \" %0.2f accuracy with a standard deviation of %0.3f \" % ( scores . mean (), scores . std ())) 0.90 accuracy with a standard deviation of 0.062 More information on the cross_val_score method in sklearn can be found here An additional topic on cross validation is the extreme leave-one-out validation, you can read more about that here Taking these building blocks, if we wanted to for example optimize the number of nearest neighbors without incurring data leakage. We might take the following approach: for k in range ( 1 , 10 ): knn = KNeighborsRegressor ( n_neighbors = k ) scores = cross_val_score ( knn , X_train , y_train , cv = 5 ) print ( \"k= %0.0f , %0.3f accuracy with a standard deviation of %0.3f \" % ( k , scores . mean (), scores . std ())) k=1, 0.900 accuracy with a standard deviation of 0.062 k=2, 0.925 accuracy with a standard deviation of 0.071 k=3, 0.916 accuracy with a standard deviation of 0.077 k=4, 0.932 accuracy with a standard deviation of 0.046 k=5, 0.928 accuracy with a standard deviation of 0.045 k=6, 0.924 accuracy with a standard deviation of 0.047 k=7, 0.929 accuracy with a standard deviation of 0.041 k=8, 0.924 accuracy with a standard deviation of 0.047 k=9, 0.923 accuracy with a standard deviation of 0.042 k = 4 # select best hyperparameters knn = KNeighborsRegressor ( n_neighbors = k ) knn . fit ( X_train , y_train ) # final train print ( knn . score ( X_test , y_test )) 0.9845995893223819","title":"3.1.2 Data Leakage and Cross-Validation"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#313-bias-variance-tradeoff","text":"back to top This next concept will be most easily understood if we go ahead an make up some data ourselves, I'm going to do that now. # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) Text(0, 0.5, 'Y') Let's fit to just a portion of this data random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] plt . plot ( X_train , y_train , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f9c34de5820>] Now let's take two extreme scenarios: fitting to these datapoints a linear line and a high order polynomial. Keeping in mind the larger dataset (the population) as well as the (irreducible) error we introduced in our data generating function will really illustrate our point! # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) # plot linear model ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) # plot polynomial model ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') We've demonstrated two extreme cases. On the left, we limit our regression to only two parameters, a slope and a y-intercept. We say that this model has high bias because we are forcing the functional form without much consideration to the underlying data \u2014 we are saying this data is generated by a linear function, and no matter what data I train on, my final model will still be a straight line that more or less appears the same. Put another way, it has low variance with respect to the underlying data. On the right, we've allowed our model just as many polynomials it needs to perfectly fit the training data! We say this model has low bias because we don't introduce many constraints on the final form of the model. it is high variance because depending on the underlying training data, the final outcome of the model can change quite drastically! In reality, the best model lies somewhere between these two cases. In the next few paragraphs we'll explore this concept further: what happens when we retrain these models on different samples of the data population and let's use this to better understand what we mean by bias and variance what happens when we tie this back in with the irreducible error we introduced to the data generator? and let's use this to better understand irreducible error # we're going to perform this task again, but this time for 5 rounds, training # on different samples of the population data random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) for samples in range ( 5 ): X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 1 ] . set_title ( \"High Variance Model\" ) As we can see, depending on what data we train our model on, the high bias model changes relatively slightly, while the high variance model changes a whole awful lot! The high variance model is prone to something we call overfitting . It fits the training data very well, but at the expense of creating a good, generalizable model that does well on unseen data. Let's take our last models, and plot them along the rest of the unseen data, what we'll call the population : # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') In particular, we see that the high variance model is doing very wacky things, demonstrating behaviors in the model where the underlying population data really gives no indication of such behavior. We say that these high variance models are particuarly prone to the phenomenon of over fitting and this is generally due to the fact that there is irreducible error in the underlying data. Let's demonstrate this. x = np . arange ( 20 , 100 ) y = [ func ( t , err = 0 ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f9c3424b970>] random . seed ( 42 ) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] # we could also do it this way: np.argwhere([i in X_train for i in x]) y_train = [ y [ i ] for i in indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) # solve the slope and intercept of our 1-degree polynomial ;) model = LinearRegression () model . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax [ 0 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 0 ] . set_ylim ( min ( y ), max ( y )) ax [ 0 ] . set_title ( \"High Bias Model\" ) ax [ 1 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), c = 'grey' , ls = '--' ) ax [ 1 ] . plot ( x , y , ls = '' , marker = 'o' , alpha = 0.2 ) ax [ 1 ] . plot ( X_train , y_train , ls = '' , marker = '.' ) ax [ 1 ] . set_ylim ( min ( y ), max ( y )) ax [ 1 ] . set_title ( \"High Variance Model\" ) Text(0.5, 1.0, 'High Variance Model') This time, our high variance model really gets it ! And this is because the data we trained on actually is a good representation of the entire population. But this, in reality, almost never, ever happens. In the real world, we have irreducible error in our data samples, and we must account for this when choosing our model. I'm summary, we call this balance between error in our model functional form, and error from succumbing to irreducible error in our training data, the bias variance tradeoff","title":"3.1.3 Bias-Variance Tradeoff"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#exercise-1-quantitatively-define-performance","text":"Up until now, we've explored this idea of bias variance tradeoff from a qualitative standpoint. As an exercise, continue with this idea, this time calculating the mean squared error (MSE) and R-square between the model and UNSEEN (non-training data) population data. error metrics r2_score mean_squared_error Do this for a 9 th order polynomial and repeat for population data with low, med, and high degrees of error and small and large training fractions (for 6 total comparisons). Complete the chart below. error training fraction MSE R2 0.1 .2 5.20e-03 0.97 0.1 .8 3.24e-03 0.98 0.4 .2 8.32e-02 0.65 0.4 .8 5.18e-02 0.80 0.8 .2 3.33e-01 0.08 0.8 .8 2.07e-01 0.52 # Code Cell for Exercise 1 from sklearn.preprocessing import PolynomialFeatures random . seed ( 42 ) # function to generate data def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) ################################################################################ ########## CHANGE ERR TO CHANGE THE AMOUNT OF NOISE IN YOUR DATA ############### ################################################################################ err = .8 # change the error (.1 - 0.9) y_actual = [ func ( t , err ) for t in x ] ################################################################################ ### SAMPLE THE DATA FOR TRAINING ################################################################################ train_size = 0.8 # change the training size x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = train_size , random_state = 42 ) # solving our training data with a 9-degree polynomial coefs = np . polyfit ( x_train , y_train , 9 ) # generate y data with 9-degree polynomial model and X_seq y_pred = np . polyval ( coefs , x_test ) ################################################################################ ### CALCULATE MSE AND R2 ################################################################################ mse = mean_squared_error ( y_test , y_pred ) r2 = r2_score ( y_test , y_pred ) # look at results print ( \"irreducible error: {} \" . format ( err )) print ( \"training fraction: {} \" . format ( train_size )) print ( \"mean square error: {:.2e} \" . format ( mse )) print ( \"r2: {:.2f} \" . format ( r2 )) irreducible error: 0.8 training fraction: 0.8 mean square error: 2.07e-01 r2: 0.52","title":"\ud83c\udfcb\ufe0f Exercise 1: Quantitatively Define Performance"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#314-learning-curves","text":"back to top To move from qualitative to quantitative understanding of bias-variance tradeoff we need to introduce some metric for model performance. A good one to use here is R-square, a measure of the degree to which predictions match actual values. We can import a tool from sklearn to calculate this for us. from sklearn.metrics import r2_score","title":"3.1.4 Learning Curves"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#3141-considering-model-complexity","text":"back to top In a learning curve, we will typically plot the training and testing scores together, to give a sense of when we have either too much bias or too much variance in our model. I'm going to go ahead and recreate the original data distribution we introduced in 1.1.3 # we can throttle the error rate err = .5 random . seed ( 42 ) # our data has a KNOWN underlying functional form (log(x)) def func ( x , err ): return np . log ( x ) + err * random . randint ( - 1 , 1 ) * random . random () x = np . arange ( 20 , 100 ) y = [ func ( t , err ) for t in x ] plt . plot ( x , y , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7f9c34c91d30>] Now let's itteratively introduce more complexity into our model random . seed ( 42 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) X_train = random . sample ( list ( x ), 10 ) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score = r2_score ( np . polyval ( coefs , X_train ), y_train ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' ) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score , ls = '' , marker = '.' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f9c34c57f40> As we see from both plots, the score on the training data increases with added model complexity. Giving us the expected perfect fit when the order is the same as the number of data points! This is part I of our learning curve. Part II consists of plotting the training data score with the testing data score. Something else I'm going to do, is define the training portion of the data as a fraction of the overall dataset . This is a typical practice in machine learning. To keep the comparisons the same as up until now, I will keep this training fraction low at .2 random . seed ( 42 ) # defining my training fraction training_frac = .2 # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) train_indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in train_indices ] test_indices = [ i for i in range ( len ( x )) if i not in train_indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_train , y_train , ls = '' , marker = '.' , color = 'black' ) ax [ 0 ] . plot ( X_test , y_test , ls = '' , marker = '.' , color = 'grey' , alpha = 0.5 ) for complexity in range ( 1 , 10 ): # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , complexity ) # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax [ 0 ] . plot ( X_seq , np . polyval ( coefs , X_seq ), alpha = 0.5 , ls = '--' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 0 ] . set_ylim ( min ( y_train ), max ( y_train )) ax [ 0 ] . set_title ( \"Predictions with Increasing Model Complexity\" ) ax [ 1 ] . plot ( complexity , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score1 )) ax [ 1 ] . plot ( complexity , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( complexity , score2 )) ax [ 1 ] . set_title ( \"Scores with Increasing Model Complexity\" ) ax [ 1 ] . legend ([ 'Train $R^2$' , 'Test $R^2$' ]) ax [ 0 ] . legend () <matplotlib.legend.Legend at 0x7f9c2c7c2580> As we can see, The 2 nd order polynomial achieves the greatest best test set data \\(R^2\\), while the highest order polynomial achieves the best training set data \\(R^2\\). This learning curve is explanative of what we see generally, namely a divergence after some degree of complexity between training and test set performances. In this case, we would resolve to choose the 2 nd order polynomial as the best model for our data. img src","title":"3.1.4.1 Considering Model Complexity"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#3142-considering-training-set-size","text":"back to top The last piece of the puzzle we require, to fully cover learning curves, is the effect of training data size on the model. Let's explore. random . seed ( 42 ) # initialize the plot and display the data fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .9 , 50 ): # create test and training data X_train = random . sample ( list ( x ), int ( int ( len ( x )) * training_frac )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] # solving our training data with a n-degree polynomial coefs = np . polyfit ( X_train , y_train , 9 ) score1 = r2_score ( np . polyval ( coefs , X_train ), y_train ) score2 = r2_score ( np . polyval ( coefs , X_test ), y_test ) ax . plot ( training_frac , score1 , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax . plot ( training_frac , score2 , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax . set_title ( \"9th-order Polynomial Score with Increasing Training Set Size\" ) ax . legend ([ 'Train' , 'Test' ]) ax . set_xlabel ( 'Training Fraction' ) ax . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$') What we see here is a trend that happens generally, as our amount of training data increases, our models handle more complexity. This is because the more data we have, the more we are able to dicipher noise from signal. Our model will ignore noise as it does not generalize well across the other datapoints. Instead, it will will fit to relationships that appear across the entire training dataset. img src","title":"3.1.4.2 Considering Training Set Size"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#exercise-2-visualization","text":"Starting with the code below, make a side-by-side plot of a 3 rd degree polynomial and a 9 th degree polynomial. On the x axis slowly increase the training set size, on the y axis plot the scores for the training and test sets. # Code Cell for Exercise 2 random . seed ( 42 ) # create the figure and axes fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) for training_frac in np . linspace ( 0.13 , .95 , 50 ): # create test and training data x_train , x_test , y_train , y_test = train_test_split ( x , y_actual , train_size = training_frac , random_state = 42 ) # solving our training data with a n-degree polynomial coefs1 = np . polyfit ( x_train , y_train , 9 ) coefs2 = np . polyfit ( x_train , y_train , 3 ) # recording the scores for the training and test sets score1_train = r2_score ( np . polyval ( coefs1 , x_train ), y_train ) score1_test = r2_score ( np . polyval ( coefs1 , x_test ), y_test ) score2_train = r2_score ( np . polyval ( coefs2 , x_train ), y_train ) score2_test = r2_score ( np . polyval ( coefs2 , x_test ), y_test ) ax [ 0 ] . plot ( training_frac , score1_train , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax [ 0 ] . plot ( training_frac , score1_test , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax [ 1 ] . plot ( training_frac , score2_train , ls = '' , marker = '.' , color = 'blue' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score1 )) ax [ 1 ] . plot ( training_frac , score2_test , ls = '' , marker = 'o' , color = 'red' , label = ' {} -poly, {:.2f} -score' . format ( training_frac , score2 )) ax [ 0 ] . set_title ( \"9th-order Polynomial Score\" ) ax [ 1 ] . set_title ( \"3rd-order Polynomial Score\" ) ax [ 0 ] . legend ([ 'Train' , 'Test' ]) ax [ 0 ] . set_xlabel ( 'Training Fraction' ) ax [ 1 ] . set_xlabel ( 'Training Fraction' ) ax [ 0 ] . set_ylabel ( '$R^2$' ) Text(0, 0.5, '$R^2$')","title":"\ud83c\udfcb\ufe0f Exercise 2: Visualization"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#question-1-in-what-regions-of-the-plots-are-we-overfitting","text":"Where in these plots is overfitting occuring? Why is it different for each polynomial?","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: In what regions of the plots are we overfitting?"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#32-model-validation-in-practice","text":"back to top We will now turn our attention to practical implementation. In practice, there are a wide number of variables (called hyperparameters) to consider when choosing a model. Scikit learn has a useful method called Grid Search that will iterate through every possible combination of a range of hyperparameter settings you provide as input. Before we get started with grid search, we'll need to switch over from our numpy polynomial fit method to one in sklearn. Here, the caveat is our actual model will solve for the coefficients infront of the polynomials. We will engineer the polynomial features ourselves. This is an example of feature engineering which we will revisit in depth in the next session. from sklearn.preprocessing import PolynomialFeatures , StandardScaler from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs ))","title":"3.2 Model Validation in Practice"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#321-grid-search","text":"back to top from sklearn.model_selection import GridSearchCV param_grid = { 'polynomialfeatures__degree' : np . arange ( 10 ), 'linearregression__fit_intercept' : [ True , False ]} grid = GridSearchCV ( PolynomialRegression (), param_grid , cv = 7 ) # create test and training data random . seed ( 42 ) X_train = random . sample ( list ( x ), int ( int ( len ( x )) * .8 )) indices = [ list ( x ) . index ( i ) for i in X_train ] y_train = [ y [ i ] for i in indices ] test_indices = [ i for i in range ( len ( x )) if i not in indices ] X_test = [ x [ i ] for i in test_indices ] y_test = [ y [ i ] for i in test_indices ] grid . fit ( np . array ( X_train ) . reshape ( - 1 , 1 ), y_train ) GridSearchCV(cv=7, estimator=Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()), ('linearregression', LinearRegression())]), param_grid={'linearregression__fit_intercept': [True, False], 'polynomialfeatures__degree': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}) grid . best_params_ {'linearregression__fit_intercept': True, 'polynomialfeatures__degree': 3} to grab the best model from the CV/search outcome. we use grid.best_estimator model = grid . best_estimator_ # create some x data to plot our functions X_seq = np . linspace ( min ( X_train ), max ( X_train ), 300 ) . reshape ( - 1 , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 5 )) ax . plot ( X_seq , model . predict ( X_seq ), c = 'grey' , ls = '--' ) ax . plot ( x , y , ls = '' , marker = '*' , alpha = 0.6 ) ax . plot ( X_train , y_train , ls = '' , marker = '.' ) ax . set_ylim ( min ( y ), max ( y )) ax . set_title ( \"Best Grid Search CV Model\" ) Text(0.5, 1.0, 'Best Grid Search CV Model')","title":"3.2.1 Grid Search"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#exercise-3-grid-search","text":"There are two objectives for this exercise: Complete the object param_grid by reading the parameters below in StandardScaler . We would like to include in our grid search whether or not to scale our input features so that they are centered at 0 ( with_mean ) and have unit variance ( with_std ) After you have completed the grid search, create a side-by-side plot of actual versus predicted values for training data (left plot) and testing data (right plot) Include in your plot title the R2 for testing and R2 for training StandardScaler () . get_params () . keys () dict_keys(['copy', 'with_mean', 'with_std']) df = wine . copy () y = df . pop ( 'density' ) X = df display ( X . head ()) display ( y . head ()) print ( X . shape ) print ( y . shape ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) def NormalizedRegression ( ** kwargs ): return make_pipeline ( StandardScaler (), LinearRegression ( ** kwargs )) ###################################################################################################### ####################### YOUR EXTRA PARAMETERS GO IN THE DICTIONARY BELOW ############################# ###################################################################################################### param_grid = { 'linearregression__fit_intercept' : [ True , False ], 'standardscaler__with_mean' : [ True , False ], 'standardscaler__with_std' : [ True , False ]} grid = GridSearchCV ( NormalizedRegression (), param_grid , cv = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide pH sulphates alcohol type_encoding quality_encoding 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 3.00 0.45 8.8 1 1 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 3.30 0.49 9.5 1 1 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 3.26 0.44 10.1 1 1 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 3.19 0.40 9.9 1 1 0 1.0010 1 0.9940 2 0.9951 3 0.9956 4 0.9956 Name: density, dtype: float64 (6463, 12) (6463,) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) {'linearregression__fit_intercept': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True} model = grid . best_estimator_ ###################################################################################################### ##################################### ADJUST THE PLOT LOGIC BELOW #################################### ###################################################################################################### fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.962')","title":"\ud83c\udfcb\ufe0f Exercise 3: Grid Search"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#references","text":"back to top","title":"References"},{"location":"solutions/SOLN_S3_Model_Selection_and_Validation/#model-validation","text":"cross_val_score leave-one-out","title":"Model Validation"},{"location":"solutions/SOLN_S4_Feature_Engineering/","text":"Data Science Foundations, Session 4: Feature Engineering \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we talked about model pipelines and conveniently began with a suitable set of input data. In the real world, this is hardly ever the case! What is constant is this: at the end of the day, our models need numbers. Not only this, but a suitable set of numbers. What does that mean? The answer to that question is the subject of our session today. 4.0 Preparing Environment and Importing Data \u00b6 back to top 4.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats as stats from scipy.stats import gamma from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor import seaborn as sns ; sns . set () from sklearn.datasets import load_iris from sklearn.metrics import mean_squared_error , r2_score 4.0.2 Load Dataset \u00b6 back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) orders = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_orders.csv' ) time_cols = [ i for i in orders . columns if '/' in i ] margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 dfcat = margin . columns [: - 2 ] dfcat Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') orders . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer 1/2020 2/2020 3/2020 4/2020 5/2020 6/2020 7/2020 8/2020 9/2020 10/2020 11/2020 12/2020 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 4.1 Categorical Features \u00b6 back to top At the end of the day, our algorithms operate on numerical values. How do you get from a series of string values to numerical values? margin [ 'Customer' ] . unique () array(['Slugworth', 'Perk-a-Cola', 'Fickelgruber', 'Zebrabar', \"Dandy's Candies\"], dtype=object) A naive way to do it would be to assign a number to every entry 'Slugworth' = 1 'Perk-a-Cola' = 2 'Dandy's Candies' = 3 but we would inadvertently end up with some weird mathematical relationships between these variables, e.g. Dandy's Candies - Perk-a-Cola = Slugworth (3 - 2 = 1). A work around for this is to think multi-dimensionally we express our categorical values as vectors in a hyperspace where they cannot be expressed in terms of one another, i.e. they are orthogonal 'Slugworth' = [1,0,0] 'Perk-a-Cola' = [0,1,0] 'Dandy's Candies' = [0,0,1] such a scheme, in machine learning vernacular, is termed one-hot encoding. 4.1.1 One-Hot Encoding \u00b6 back to top sklearn has a couple useful libraries for one-hot encoding. let's start with the OneHotEncoder class in its preprocessing library from sklearn.preprocessing import OneHotEncoder # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ 'Customer' ] . values . reshape ( - 1 , 1 ) # fit our encoder to this data enc . fit ( X_cat ) OneHotEncoder() After fitting our encoder, we can then use this object to create our training array. # as a reference here's our original data display ( X_cat [: 10 ]) print ( X_cat . shape , end = ' \\n\\n ' ) onehotlabels = enc . transform ( X_cat ) . toarray () print ( onehotlabels . shape , end = ' \\n\\n ' ) # And here is our new data onehotlabels [: 10 ] array([['Slugworth'], ['Slugworth'], ['Perk-a-Cola'], ['Fickelgruber'], ['Fickelgruber'], ['Fickelgruber'], ['Slugworth'], ['Zebrabar'], ['Slugworth'], ['Zebrabar']], dtype=object) (1668, 1) (1668, 5) array([[0., 0., 0., 1., 0.], [0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]]) We have our customer information one-hot encoded, we need to do this for all our variables and concatenate them with our regular numerical variables in our original dataframe. # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ dfcat ] . values # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [ \"KG\" ] print ( X_num . shape ) X = np . concatenate (( onehotlabels , X_num . values . reshape ( - 1 , 1 )), axis = 1 ) X . shape (1668,) (1668, 119) And now we grab our EBITDA (margin) data for prediction y = margin [ \"EBITDA/KG\" ] \ud83c\udfcb\ufe0f Exercise 1: Create a simple linear model \u00b6 Using the X and y sets, use train_test_split and LinearRegression to make a baseline model based on what we've learned so far. Assess your model performance visually by plottying y_test vs y_test_pred # Cell for Exercise 1 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 ) model = LinearRegression () model . fit ( X_train , y_train ) y_test_pred = model . predict ( X_test ) fig , ax = plt . subplots ( figsize = ( 5 , 5 )) ax . plot ( y_test , y_test_pred , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7fa4b01f90d0>] \ud83d\ude4b Question 1: \u00b6 How can we assess the relative feature importance of the features in our model? We could be tempted to inspect the coefficients ( linear.coef_ ) of our model to evaluate the relative feature importance, but in order to do this our features need to be scaled (so that the relative coefficient sizes are meaningful). What other issues might there be (think categorical vs continuous variables). 4.2 Derived Features \u00b6 back to top Can we recall an example of where we've seen this previously? That's right earlier on in our session on model selection and validation we derived some polynomial features to create our polynomial model using the linear regression class in sklearn. We actually see this a lot in engineering, where we will describe log relationships or some other transformation of the original variable. Actually let me see if I can find an example in my handy BSL... concentration profiles in continous stirred tank vs plug flow reactors. Notice the y-axis is log scale. Thanks Bird, Stewart, Lightfoot! Can we think of other examples where we would like to derive features from our input data? 4.2.1 Creating Polynomials \u00b6 back to top Let's revisit our example from the previous session, right before we introduced Grid Search in sklearn # from Model Selection and Validation, 1.2.1 from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) in the above, we use sklearn's convenient tool, make_pipeline to join together the preprocessing tool PolynomialFeatures and the basic model LinearRegression . Let's take a look at what PolynomialFeatures does to some simple data x = np . arange ( 1 , 11 ) y = x ** 3 print ( x ) print ( y ) [ 1 2 3 4 5 6 7 8 9 10] [ 1 8 27 64 125 216 343 512 729 1000] features = PolynomialFeatures ( degree = 3 ) X2 = features . fit_transform ( x . reshape ( - 1 , 1 )) we see our new feature set contains our original features, plus new features up to the nth-degree polynomial we set when creating the features object from PolynomialFeatures print ( X2 ) [[ 1. 1. 1. 1.] [ 1. 2. 4. 8.] [ 1. 3. 9. 27.] [ 1. 4. 16. 64.] [ 1. 5. 25. 125.] [ 1. 6. 36. 216.] [ 1. 7. 49. 343.] [ 1. 8. 64. 512.] [ 1. 9. 81. 729.] [ 1. 10. 100. 1000.]] model = LinearRegression () . fit ( X2 , y ) yhat = model . predict ( X2 ) plt . scatter ( x , y ) plt . plot ( x , yhat ); 4.2.2 Dealing with Time Series \u00b6 back to top Often, we will be dealing with time series data, whether its data generated by machinery, reactors, or sales and customers. In the following we discuss some simple practices for dealing with time series data. \ud83c\udf52 4.2.2.1 Enrichment : Fast Fourier Transform \u00b6 back to top Sometimes we'll want to create a more sophisticated transformation of our input data. As engineers, this can often have to do with some empirical knowledge we understand about our process. When working with equipment and machinery, we will often want to convert a signal from the time to frequency domain. Let's cover how we can do that with numpy! [img src]( https://www.nti-audio.com/en/support/know-how/fast-fourier-transform-fft#:~:text=The%20%22Fast%20Fourier%20Transform%22%20(,frequency%20information%20about%20the%20signal .) What I've drawn here in the following is called a square-wave signal t = np . linspace ( 0 , 1 , 501 ) # FFT should be given an integer number of cycles so we leave out last sample t = t [: - 1 ] f = 5 # linear frequency in Hz w = f * 2 * np . pi # radial frequency h = 4 # height of square wave amp = 4 * h / np . pi s = amp * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) # here is the call to numpy FFT F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ], d = t [ 1 ]) # reorder frequency spectrum and frequency bins with 0 Hz at the center F = np . fft . fftshift ( F ) freq = np . fft . fftshift ( freq ) # scale frequency spectrum to correct amplitude F = F / t . size # amplitudes amps = [ max ( np . sin ( w * t )), max ( np . sin ( w * t * 3 ) / 3 ), max ( np . sin ( w * t * 5 ) / 5 )] fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) ax [ 0 ] . set_xlim ( 0 , 1 ) ax [ 0 ] . set_xlabel ( 'Time (s)' ) # tells us about the amplitude of the component at the # corresponding frequency. Multiplied by two because the # signal power is split between (-) and (+) frequency branches # of FFT, but we're only visualizing the (+) branch magnitude = 2 * np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , 30 ) ax [ 1 ] . set_xlabel ( 'Frequency (Hz)' ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain') 4.2.2.2 Rolling Windows \u00b6 back to top to see an example of this dataset in action visit this link One powerful technique for dealing with time series data, is to create a rolling window of features based on the historical data. The proper window size can usually be determined by trial and error, or constraints around access to the data itself. In the above gif, we have a window size of 7. What that means is for whatever time step units we are in (that could be minutes, days, months, etc.) we will have 7 of them included in a single instance or observation. This instance or observation is then interpreted by our model and used to assess the target value, typically the quantity in the very next time step after the window (the green bar in the gif). Let's take an example with the orders data tidy_orders = orders . melt ( id_vars = orders . columns [: 6 ], var_name = 'Date' , value_name = 'KG' ) display ( tidy_orders . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 In the next exercise, we are going to attempt to predict an order amount, based on the previous order history. We will scrub all categorical labels and only use historical amounts to inform our models. In effect the data that the model will see will look like the following: fig , ax = plt . subplots ( 3 , 2 , figsize = ( 10 , 20 )) indices = np . argwhere ( ax ) color_dict = { 0 : 'tab:blue' , 1 : 'tab:green' , 2 : 'tab:orange' , 3 : 'tab:red' , 4 : 'tab:pink' , 5 : 'tab:brown' } for index , customer in enumerate ( tidy_orders . Customer . unique ()): orders . loc [ orders . Customer == customer ] . iloc [:, 6 :] . reset_index () . T . plot ( c = color_dict [ index ], legend = False , ax = ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]]) ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]] . set_title ( customer ) What we may notice is that there is very little noise or drift in our order history, but there is certainly some periodicity. The question is can we use a linear model to predict the next order amount based on these history orders? \ud83c\udfcb\ufe0f Exercise 2: Optimize Rolling Window Size for Customer Forecasts \u00b6 For this exercise, you will use the process_data function below to help you optimize the window size for predicting the order quantity in any given month. You will train a LinearRegression model. create a model using a window size of 3 and predict the order quantity for the month immediately following the window create a model for window sizes 1-11 and report the \\(R^2\\) for each model def process_data ( Xy , time_cols = 12 , window = 3 , remove_null = False ): \"\"\" This function splits your time series data into the proper windows Parameters ---------- Xy: array The input data. If there are non-time series columns, assumes they are on the left and time columns are on the right. time_cols: int The number of time columns, default 12 window: int The time window size, default 3 Returns ------- X_: array The independent variables, includes time and non-time series columns with the new window y_: array The dependent variable, selected from the time columns at the end of the window labels: The time series labels, can be used in subsequent plot \"\"\" # separate the non-time series columns X_cat = Xy [:,: - time_cols ] # select the columns to apply the sweeping window X = Xy [:, - time_cols :] X_ = [] y = [] for i in range ( X . shape [ 1 ] - window ): # after attaching the current window to the non-time series # columns, add it to a growing list X_ . append ( np . concatenate (( X_cat , X [:, i : i + window ]), axis = 1 )) # add the next time delta after the window to the list of y # values y . append ( X [:, i + window ]) # X_ is 3D: [number of replicates from sweeping window, # length of input data, # size of new feature with categories and time] # we want to reshape X_ so that the replicates due to the sweeping window is # a part of the same dimension as the instances of the input data X_ = np . array ( X_ ) . reshape ( X . shape [ 0 ] * np . array ( X_ ) . shape [ 0 ], window + X_cat . shape [ 1 ]) y = np . array ( y ) . reshape ( X . shape [ 0 ] * np . array ( y ) . shape [ 0 ],) if remove_null : # remove training data where the target is 0 (may be unfair advantage) X_ = X_ [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] y = y [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] # create labels that show the previous month values used to train the model labels = [] for row in X_ : labels . append ( \"X: {} \" . format ( np . array2string ( row [ - window :] . astype ( float ) . round ()))) return X_ , y , labels # Code Cell for Exercise 2 kg_month_data = orders . values [:, 6 :] # use kg_month_data and the function process_data to create your X, y arrays # then use train_test_split to create train and test portions # USE y_test and y_pred for your actual and true test data # change only window parameter in process_data() print ( \"window R2\" ) for window in range ( 1 , 12 ): ###################### ### YOUR CODE HERE ### ###################### X , y , labels = process_data ( kg_month_data , time_cols = 12 , window = 3 ) features = PolynomialFeatures ( degree = 4 ) X2 = features . fit_transform ( X ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.6 ) model = LinearRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( \" {} , \\t {:.2f} \" . format ( window , r2_score ( y_test , y_pred ))) window R2 1, 0.97 2, 0.88 3, 0.80 4, 0.96 5, 0.90 6, 0.86 7, 0.61 8, 0.89 9, 0.76 10, 0.74 11, 0.87 #### RUN AFTER EXERCISE 2.2.2.2.1 #### fig = px . scatter ( x = y_test , y = y_pred , labels = { \"y\" : \"Prediction\" , \"x\" : \"Actual\" }) fig . update_layout ( autosize = False , width = 800 , height = 500 , title = 'R2: {:.3f} ' . format ( r2_score ( y_test , y_pred )) ) require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"239a6e0b-8482-49c8-9619-ebde564b91cd\")) { Plotly.newPlot( \"239a6e0b-8482-49c8-9619-ebde564b91cd\", [{\"hovertemplate\":\"Actual=%{x}<br>Prediction=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4528.380503144654,64.01384083044982,15617.489114658929,4528.380503144654,5698.392857142857,124348.89322916669,21939.111111111117,5712.33671988389,64983.42138364781,1483.5,93302.72435897436,1236.25,773.5849056603774,518.4642857142858,3798.928571428572,13671.296296296296,188.7692307692308,4057.832278481013,101149.25,14626.074074074077,12274.009146341465,141456.76315789475,4793.829296424452,74.78571428571429,10861.245674740485,86.24595469255664,19129.33333333333,39307.04113924051,74.78571428571429,46247.0,188.7692307692308,85.32258064516128,90677.41228070176,9306.611111111111,220.55016181229772,466477.57812500006,1158.125,220.55016181229772,1901.1290322580644,19129.33333333333,2637.8993710691825,1868.867924528302,3798.928571428572,2252.261437908497,5222.5,181.5552699228792,5038.387096774193,120.98615916955016,10146.898432174508,85.32258064516128,286.6666666666667,18613.222222222223,818.1891025641025,2499.201741654572,2010.9477124183009,6984.389273356401,42704.25889967639,11715.487421383648,39307.04113924051,440.859375,428.0,96.59546925566345,102.64285714285714,3569.0359477124184,13671.296296296296,773.5849056603774,2010.9477124183009,42704.25889967639,40608.41346153846,83347.75862068967,10796.31640058055,74.78571428571429,80801.72807017544,2540.0,5222.5,131.55526992287918,299024.0885416667,3882.680981595092,895.9082278481012,1180.5379746835442,15617.489114658929,85.32258064516128,6185.59509202454,16749.13157894737,47142.92857142857,3786.966463414634,936.280487804878,15244.632812500002,3272.80163599182,1271.904761904762,47142.92857142857,356.6666666666667,10861.245674740485,108.64285714285714,21438.1875,16918.132716049382,7892.142857142857,1901.1290322580644,2647.5,5038.387096774193,1677.5078125000002,39307.04113924051,51795.97953216374,916.3717948717948,41871.69230769231,12870.766853932584,1271.904761904762,181.5552699228792,211540.7575757576,3798.928571428572,2054.328561690525,3578.592162554427,2854.854368932039,51795.97953216374,11791.8,149.57142857142858,103.3653846153846,119015.43674698794,2499.201741654572,19286.94968553459,2185.3410740203194,19569.69230769231,12317.397660818711,6965.361445783132,86.24595469255664,3071.4691011235955,13867.125,50574.625,14626.074074074077,1005.4545454545454,16918.132716049382,2543.106741573034,10245.87593728698,8608.709677419354,150.3654970760234,3517.9333333333334,14626.074074074077,3177.0,2499.201741654572,345.6428571428572,10736.622807017544,13799.769319492503,12691.209677419354,11.175616835994196,2010.9477124183009,10736.622807017544,71138.51966292135,8104.40251572327,62.168396770472896,211540.7575757576,16569.261006289307,445.54838709677415,16918.132716049382,42.484177215189874,150.3654970760234,11.175616835994196,299024.0885416667,579.0625,10146.898432174508,181.5552699228792,494.57142857142856,411.1303344867359,2355.267295597484,494.57142857142856,79091.23475609756,10146.898432174508,1005.4545454545454,5038.387096774193,191.11111111111111,9304.668674698794,19569.69230769231,8250.491573033707,466477.57812500006,3048.0,1630.1966292134832,64.01384083044982,3569.0359477124184,1901.1290322580644,3272.80163599182,9772.200520833334,2185.3410740203194,28694.0,314.3203883495146,8104.40251572327,18557.572327044025,2231.547169811321,1432.2712418300653,773.5849056603774,40735.10802469136,314.3203883495146,471.91011235955057,42.484177215189874,9194.277108433736,8173.714285714285,837.8787878787879,1604.1437908496732,22.50980392156863,14.663461538461538,663176.1973875181,2854.854368932039,428.0,43.64516129032258,1526.2857142857142,1432.2712418300653,5456.578050443081,22331.935185185182,150.3654970760234,4057.832278481013,1868.867924528302,1630.1966292134832,4723.49129172714,5038.387096774193,19364.70552147239,117.22222222222224,15110.75,4057.832278481013,83347.75862068967,7892.142857142857,19129.33333333333,1968.8904494382025,207.39062500000003,62.168396770472896,3578.592162554427,1677.5078125000002,2499.201741654572,1254.4444444444443,1236.25,3578.592162554427,1992.4528301886792,14626.074074074077,1236.25,6965.361445783132,36.37096774193548,40735.10802469136,20.098039215686278,1432.2712418300653,538.8571428571429,101149.25,93302.72435897436,181.5552699228792,28694.0,2054.328561690525,5872.384615384615,31412.04644412192,5872.384615384615,854.952380952381,31412.04644412192,1253403.0130624091,63142.24137931035,20.098039215686278,3569.0359477124184,16569.261006289307,19286.94968553459,19286.94968553459,837.8787878787879,3665.809768637532,108.64285714285714,56.07911392405064,13671.296296296296,371.2903225806451,4057.832278481013,47142.92857142857,21438.1875,54.833333333333336,26081.56401384083,2540.0,115.76923076923076,16.423076923076923,8608.709677419354,4463.780120481927,8250.491573033707,15110.75,8173.714285714285,157100.37650602407,104499.0512820513,9076.930817610064,2611.25,428.0,14.663461538461538,2694.0,3569.0359477124184,86.24595469255664,678.7183544303797,494.57142857142856,712.4603174603175,663176.1973875181,10245.87593728698,5071.786163522012,1868.867924528302,26081.56401384083,3403.344867358708,4793.829296424452,64983.42138364781,3272.80163599182,5712.33671988389,9194.277108433736,608.4770114942529,42.484177215189874,466477.57812500006,3695.444059976932,3517.9333333333334,345.6428571428572,579.0625,7892.142857142857,5038.387096774193,45601.61516853933,1992.4528301886792,2647.5,5872.384615384615,6965.361445783132,64.01384083044982,45601.61516853933,23123.5,1992.4528301886792,2540.0,9060.337370242214,14.663461538461538,3882.680981595092,36.37096774193548,193984.2734375,2231.547169811321,108.64285714285714,329.7142857142857,117.49826989619376,773.5849056603774,36.37096774193548,8608.709677419354,371.2903225806451,45481.42307692308,10245.87593728698,63142.24137931035,678.7183544303797,11555.9375,4528.380503144654,2499.201741654572,10736.622807017544,5222.5,1901.1290322580644,93302.72435897436,1702.1929824561405,114208.8534107402,343.1394601542417,10245.87593728698,8173.714285714285,140637.14285714287,132.94270833333334,579.0625,663176.1973875181,1180.5379746835442,1236.25,26081.56401384083,854.952380952381,2010.9477124183009,30698.85714285714,11.175616835994196,90677.41228070176,38128.80258899677,663176.1973875181,71138.51966292135,1236.25,31412.04644412192,50574.625,83347.75862068967,12274.009146341465,90677.41228070176,808.2857142857143,11.175616835994196,11555.9375,86.24595469255664,41871.69230769231,19129.33333333333,329.7142857142857,1236.25,2637.8993710691825,579.7777777777778,30698.85714285714,329.7142857142857,2647.5,466477.57812500006,102.64285714285714,12691.209677419354,12317.397660818711,220.55016181229772,19147.454268292684,9304.668674698794,4057.832278481013,47142.92857142857,1630.1966292134832,4463.780120481927,2002.1844660194176,5071.786163522012,7035.866666666667,13504.20634920635,21601.383647798742,10245.87593728698,10861.245674740485,176.36477987421384,12691.209677419354,1432.2712418300653,608.4770114942529,10736.622807017544,3695.444059976932,157100.37650602407,1702.1929824561405,51795.97953216374,2386.449438202247,117.22222222222224,120.98615916955016,777.0363321799308,12274.009146341465,2611.25,2242.446601941748,168764.57142857145,627.2222222222222,40608.41346153846,2002.1844660194176,157.46855345911948,2647.5,119015.43674698794,579.0625,329.7142857142857,13121.345911949686,71138.51966292135,207.39062500000003,30698.85714285714,9060.337370242214,1529.7752808988764,3071.4691011235955,46247.0,538.8571428571429,16347.42857142857,23123.5,132.94270833333334,6984.389273356401,12691.209677419354,2499.201741654572,30221.5,15229.451612903224,191.11111111111111,428.0,3578.592162554427,11715.487421383648,19129.33333333333,30221.5,6432.321799307958,41871.69230769231,42.484177215189874,16918.132716049382,3695.444059976932,13504.20634920635,15617.489114658929,117.49826989619376,16569.261006289307,96.59546925566345,678.7183544303797,10146.898432174508,678.7183544303797,131.55526992287918,47142.92857142857,79091.23475609756,736.1797752808989,1180.5379746835442,29354.53846153846,45481.42307692308,21438.1875,6965.361445783132,31412.04644412192,10146.898432174508,11791.8,15617.489114658929,1677.5078125000002,5456.578050443081,14626.074074074077,94285.85714285714,248.6394601542416,5456.578050443081,26081.56401384083,63142.24137931035,12317.397660818711,12317.397660818711,608.4770114942529,11.175616835994196,10861.245674740485,2566.6935483870966,11555.9375,678.7183544303797,1529.7752808988764,1432.2712418300653,518.4642857142858,8104.40251572327,63142.24137931035,3798.928571428572,4057.832278481013,1526.2857142857142,2854.854368932039,3403.344867358708,9306.611111111111,538.8571428571429,13671.296296296296,117.22222222222224,343.1394601542417],\"xaxis\":\"x\",\"y\":[4785.592155979585,378.9394302913829,15365.600809369838,4785.592155979585,4070.255357333951,134757.3771732384,14409.557558767568,5813.845282617682,61516.49587090381,1510.6755468797405,98738.27282520698,1684.2601507265988,1088.107600326137,647.864369876902,4263.755978756964,14168.403568365671,487.3481032940997,4287.792726026477,51779.415544128475,19934.734830917627,12141.409803201397,87747.52966460225,5818.94510670397,377.43130038813075,12797.394868451114,389.0359797814606,19263.507069512012,40163.79013647013,377.43130038813075,23840.068464074022,487.3481032940997,400.48488833402286,126112.56377290396,11223.080944977459,522.1062342859491,415175.9779430929,894.6750043489748,519.4101103091041,2138.6141864173633,19263.507069512012,2790.085070463646,2196.4290704108694,4263.755978756964,2244.514746383961,2963.0052419712265,602.4723068239653,5217.814575086902,410.08771833436947,16913.133647514336,390.2429375597753,489.6064298303322,16311.253798810561,1109.562627058514,2715.348806915594,2281.9990642021894,6353.789558047462,37073.75373050823,11896.354947646087,38882.42895370776,797.0601491084628,703.1493477716122,388.482661364839,481.8437277075316,3747.013377347842,16794.12337436424,1571.4700357476117,2257.4161561731,37073.75373050823,43146.57407027896,76460.52042984031,5813.845282617682,433.93372568151483,50253.31197444373,2781.8507793892813,2963.0052419712265,450.36262704088375,323624.6828685297,2286.347850033103,959.8167805278175,1463.9293549917084,25867.060321463283,388.50455558540244,3461.34813854915,10658.874464205835,48286.68591679114,4070.54807420052,1208.1895902424544,13863.416557439898,3913.805102706469,1545.439458867743,48286.68591679114,664.1495092797293,12797.394868451114,415.88888264864494,24308.715376885695,17460.789585310762,13878.585909613186,2138.6141864173633,3258.406190332729,5925.265679815453,1797.2371240684145,47713.131047358365,51804.0528093027,1099.5606427530397,40683.104312073505,8508.45844428347,1571.3535403980425,505.4911026847705,213132.54291644722,4263.755978756964,2570.353838719455,3926.506181336219,3317.1418553360736,54758.90192605476,20585.396531692135,377.43130038813075,406.91804655949966,120990.40817472423,4395.853637740449,19387.390319584494,2412.686086825897,19699.926384269966,12552.025861937103,7368.391717428835,390.09029678781303,2262.905415567944,11572.521113313724,56931.5110267007,15545.52362930884,1239.906233381196,17460.789585310762,1926.1552505875645,10673.158532726991,9907.699090223898,454.81639486308137,6355.618165841122,19934.734830917627,2858.35606741023,2834.2664976391197,665.469888483012,11592.823783640883,16177.143075924467,12543.725418313918,317.63569157648635,2257.4161561731,15201.471014315619,45645.23591676211,8323.62833082682,376.8168895165831,213132.54291644722,16698.569941711827,663.3571920970296,20710.099320853376,346.28222029939593,513.9332229141203,323.60536008687086,323624.6828685297,1301.212252741579,16913.133647514336,505.4911026847705,632.0783029798903,778.1760841513764,2635.5625663138067,632.0783029798903,76574.72476531091,11492.974658748008,1239.906233381196,5163.94089137517,494.71513791291954,11527.578799029196,19176.79696136619,8979.131225405608,415175.9779430929,2754.6914615362707,2567.0739167219367,378.9394302913829,3747.013377347842,2138.6141864173633,3913.805102706469,10871.491666793676,2516.669574686992,18752.149151877966,636.9170394143573,8323.62833082682,17785.632844249434,2407.321312452001,1686.4843440391876,1051.2986748894868,49435.50780135217,610.4363857799107,960.0492155948752,346.28222029939593,7141.329503589847,8187.405610699661,1148.288091129662,1686.4843440391876,324.6947768318741,320.7835266802878,639820.6545150073,3317.1418553360736,703.1493477716122,340.3871184326995,1531.8394329987157,1686.4843440391876,9236.305456033022,16619.83993793382,463.3944227377964,4287.792726026477,3364.1619784229933,2019.1544585081797,2715.348806915594,5163.94089137517,10185.63650295469,436.06660273854754,26293.480165383102,4218.370391131691,76460.52042984031,7915.880111460361,25978.487349635714,2375.22637673234,489.7605752448112,376.8168895165831,3756.2286447454576,1797.2371240684145,3060.8574440447824,1384.0395692783663,1535.8631900519042,6162.533552625073,2276.6020512892946,14800.536096877271,1535.8631900519042,7368.391717428835,340.3871184326995,40283.97684677175,325.06940680600604,1695.6732046475156,1028.5053850528614,51779.415544128475,92018.35013115287,505.4911026847705,18752.149151877966,3667.712553203018,5968.182975715722,34939.27854355379,6424.274585977019,992.354894936303,30596.638581578518,639820.6545150073,65413.624508456036,324.82371754886964,3747.013377347842,16698.569941711827,19387.390319584494,19387.390319584494,1148.288091129662,6305.285970941779,492.1628050669814,346.28222029939593,14168.403568365671,663.3571920970296,4218.370391131691,45766.272960899,37175.69967364701,369.63985077669076,22891.92825541111,2754.6914615362707,405.6544524204279,319.54816919005003,8698.931074864986,5689.033000226852,11752.173931839643,26293.480165383102,8187.405610699661,117110.65135820676,90877.76737128492,8855.335527589015,3229.016296504339,703.1493477716122,319.72742324233695,1604.2554168141014,3769.9108375996702,396.30197122916206,959.8167805278175,632.0783029798903,999.9729800621147,731503.1707028296,17075.133945943726,5082.686543693957,2107.5040932279385,22891.92825541111,4219.676962291559,5818.94510670397,61516.49587090381,3617.0754679045876,5813.845282617682,7141.329503589847,932.7370475860574,346.28222029939593,415175.9779430929,4555.635369302835,3885.8140646224183,769.1955699368395,953.6647702716481,8337.819513819635,5163.94089137517,48246.709882894014,2276.6020512892946,2886.664844581823,5968.182975715722,7368.391717428835,378.9394302913829,63573.70285359673,40074.17801737161,2276.6020512892946,3138.4978117622,8151.559019833033,319.72742324233695,2286.347850033103,340.3871184326995,172828.89934304,2407.321312452001,410.08048522816836,747.818087951304,407.0672088952674,1088.107600326137,341.517050774033,8698.931074864986,667.3272676429804,39725.42364104348,10673.158532726991,65413.624508456036,959.8167805278175,13195.11592262944,4785.592155979585,2834.2664976391197,15201.471014315619,2963.0052419712265,2425.884027039221,98738.27282520698,1997.7384398419902,115873.73047883328,489.0303345318461,10673.158532726991,9457.064447629586,137428.5757499219,449.0579108608325,1301.212252741579,1085751.3630379443,1502.4134405828336,1684.2601507265988,22891.92825541111,992.354894936303,2244.514746383961,31550.15438840883,316.0906893952995,90462.3574812228,37318.37237669782,639820.6545150073,45645.23591676211,1684.2601507265988,30596.638581578518,56931.5110267007,76460.52042984031,12141.409803201397,95635.30862544454,839.3500692330289,316.62245060704845,13195.11592262944,389.0359797814606,41802.402147140514,19263.507069512012,747.818087951304,1535.8631900519042,2790.085070463646,803.8781088470796,29908.894508749334,648.872490598366,2858.35606741023,415175.9779430929,409.7821718132848,12543.725418313918,17394.662595168105,522.1062342859491,12141.409803201397,9740.51689224938,5199.4234913728005,45766.272960899,2567.0739167219367,4831.715916465941,2417.5880406486763,5082.686543693957,3697.7340063828487,13472.111607949182,20652.753180263422,11602.10423834572,12797.394868451114,471.44073265789405,12543.725418313918,1695.6732046475156,932.7370475860574,10980.322568203586,4555.635369302835,117110.65135820676,2094.8448847085992,54758.90192605476,1826.31018819446,421.9954832228255,410.08771833436947,978.2266656454766,12141.409803201397,4796.262256309382,2236.064155834039,135924.7927322855,1041.1204286449113,40221.84276082843,2273.3851256392354,461.10966099955675,2858.35606741023,143848.60557702882,953.6647702716481,648.872490598366,12664.975331868296,45645.23591676211,489.7605752448112,29908.894508749334,8151.559019833033,2427.747778574022,2262.905415567944,23840.068464074022,866.7970691016989,8187.405610699661,26195.686125920434,449.0579108608325,6353.789558047462,12679.428015781728,2715.348806915594,15684.810589735318,12543.725418313918,504.4494927620649,703.1493477716122,4250.960414621229,11896.354947646087,25978.487349635714,15684.810589735318,5875.699548389586,41802.402147140514,347.0090494475603,16909.279272683176,4555.635369302835,15368.26716660901,16108.716384819742,407.0672088952674,16698.569941711827,388.482661364839,993.553834752781,10573.002781125038,971.4284514069652,450.36262704088375,45766.272960899,76574.72476531091,774.5157686911542,1443.732437768414,19176.79696136619,39725.42364104348,24308.715376885695,8706.164457317907,32091.295039989156,11492.974658748008,13508.076892121393,16108.716384819742,1797.2371240684145,5826.8479598607,15545.52362930884,45766.272960899,438.4351253031718,5826.8479598607,22891.92825541111,65413.624508456036,12552.025861937103,12552.025861937103,932.7370475860574,316.0906893952995,12797.394868451114,2860.1717851882872,11807.964454280216,1123.9090100996427,1913.5805743985902,1816.3382332975395,647.864369876902,8323.62833082682,65413.624508456036,5403.795319141717,4218.370391131691,1531.8394329987157,3111.527273406017,4219.676962291559,11223.080944977459,866.7970691016989,13722.736128528055,436.06660273854754,489.0303345318461],\"yaxis\":\"y\",\"type\":\"scatter\"}], {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Actual\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prediction\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"autosize\":false,\"width\":800,\"height\":500,\"title\":{\"text\":\"R2: 0.869\"}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('239a6e0b-8482-49c8-9619-ebde564b91cd'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; }); 4.2.3 Image Preprocessing \u00b6 back to top Image preprocessing is beyond the scope of this session. We cover this topic in General Applications of Neural Networks . For now, know that there is a wealth of considerations for how to handle images, and they all fit within the realm of feature engineering. 4.3 Transformed Features \u00b6 back to top Transformed features, are features that we would like to augment based on their relationship within their own distribution or to other (allegedly) independent data within our training set. e.g. we're not deriving new features based on some empirical knowledge of the data, rather we are changing them due to statistical properties that we can assess based on the data itself. 4.3.1 Skewness \u00b6 back to top Skewed data can lead to imbalances in our model prediction. Why? Skewed values in the distribution will bias the mean. When assigning weights to this input feature, therefore, the model will give preferential treatment to these values. To demonstrate, I'm going to use scipy to create some skewed data. from scipy.stats import skewnorm a = 10 x = np . linspace ( skewnorm . ppf ( 0.01 , a ), skewnorm . ppf ( 0.99 , a ), 100 ) plt . plot ( x , skewnorm . pdf ( x , a ), 'r-' , lw = 5 , alpha = 0.6 , label = 'skewnorm pdf' ) [<matplotlib.lines.Line2D at 0x7fa48d14a130>] We can now generate a random population based on this distribution r = skewnorm . rvs ( a , size = 1000 ) plt . hist ( r ) (array([113., 267., 225., 172., 116., 62., 26., 13., 2., 4.]), array([-0.19733964, 0.15303313, 0.50340589, 0.85377866, 1.20415142, 1.55452419, 1.90489696, 2.25526972, 2.60564249, 2.95601526, 3.30638802]), <BarContainer object of 10 artists>) Unskewed data will return something close to 0 from calling df.skew() . When dealing with actual data, we can use df.skew() to determine whether we should transform our data. x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) x [ 'Skewed Data' ] . skew () 0.9141902067398219 There are a handful of ways to deal with skewed data: log transform square root transform Box-Cox transform Let's try the first two print ( 'square root transformed skew: {:.4f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) print ( 'log transformed skew: {:.4f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . legend () square root transformed skew: 0.0561 log transformed skew: -1.6916 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in sqrt /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log <matplotlib.legend.Legend at 0x7fa49157d340> We see we didn't get much traction with the log transform, and the log transform will not be able to handle 0 values, and so we will sometimes have to code exceptions for those. Box-Cox is often a good route to go, but it has the added restriction that the data has to all be above 0. Let's create a new distribution with this added restriction a = 6 r = skewnorm . rvs ( a , size = 1000 ) r = [ i for i in r if i > 0 ] plt . hist ( r ) (array([220., 277., 182., 127., 66., 39., 17., 5., 4., 2.]), array([2.17150536e-03, 3.88613862e-01, 7.75056219e-01, 1.16149858e+00, 1.54794093e+00, 1.93438329e+00, 2.32082565e+00, 2.70726800e+00, 3.09371036e+00, 3.48015272e+00, 3.86659507e+00]), <BarContainer object of 10 artists>) from scipy import stats x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ], alpha = 0.5 , label = 'box-cox: {:.2f} ' . format ( pd . DataFrame ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ])[ 0 ] . skew ())) ax . legend () <matplotlib.legend.Legend at 0x7fa492058f40> \ud83c\udfcb\ufe0f Exercise 3: Transform data from a gamma distribution \u00b6 Repeat section 2.3.1, this time synthesizing a gamma distribution and transforming it. Which transformation best reduces the skew? Do this for a dataset that does not contain values at or below 0. # code cell for exercise 3 from scipy.stats import gamma a = 6 r = gamma . rvs ( a , size = 1000 ) r = [ i for i in r if i > 0 ] x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ], alpha = 0.5 , label = 'box-cox: {:.2f} ' . format ( pd . DataFrame ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ])[ 0 ] . skew ())) ax . legend () <matplotlib.legend.Legend at 0x7fa49111dd30> 4.3.2 Colinearity \u00b6 back to top Colinearity can also affect the performance of your machine learning model. In particular, if features are colinear, it can be easy for your model to overfit to your training dataset. This is often mitigated by regularization. If you're curious you can read more about it on this discussion from StackExchange . We will still explore it explicitly here by calculating the Variance Inflation Factor (VIF) on some hypothetical data. VIF = \\frac{1}{1-R^2} Usually we are concerned about data with a VIF above 10 4.3.2.1 Detecting Colinearity \u00b6 back to top from statsmodels.stats.outliers_influence import variance_inflation_factor Step 1: Make some data # we can throttle the error rate random . seed ( 42 ) # x2 will be sqrt of x1 plus some error def func ( x , err ): return x ** .5 + ( err * random . randint ( - 1 , 1 ) * random . random () * x ) x0 = range ( 100 ) x1 = [ func ( i , .05 ) for i in x0 ] # HIGH degree of colinearity with x0 x2 = [ func ( i , 1 ) for i in x0 ] # MED degree of colinearity with x0 x3 = [ random . randint ( 0 , 100 ) for i in x0 ] # NO degree of colinearity with x0 # take a look fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) ax . plot ( x0 , x1 , label = 'x1' ) ax . plot ( x0 , x2 , label = 'x2' ) ax . plot ( x0 , x3 , label = 'x3' ) ax . legend () <matplotlib.legend.Legend at 0x7fa49121ca90> To calculate the colinearities I'm going to aggregate these x's into a dataframe: colin = pd . DataFrame ([ x0 , x1 , x2 , x3 ]) . T colin . columns = [ 'x0' , 'x1' , 'x2' , 'x3' ] colin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 0 0.0 0.000000 0.000000 29.0 1 1.0 1.013751 0.721523 28.0 2 2.0 1.400260 1.414214 3.0 3 3.0 1.630546 -0.438007 84.0 4 4.0 2.017388 4.304847 24.0 Step 2: Calculate VIF factors # calculate VIF factors vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( colin . values , i ) for i in range ( colin . shape [ 1 ])] vif [ \"features\" ] = colin . columns Step 3: Inspect VIF factors # inspect VIF factors display ( vif ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 12.555415 x0 1 15.823872 x1 2 1.030609 x2 3 3.559468 x3 In this case, we may remove either x0 or x1 from the dataset. 4.3.2.2 Fixing Colinearity \u00b6 back to top It is good to aknowledge where colinearity exists as this will influence the interpretability of your model. In most cases, however, it won't have a heavy influence on the performance of your model. A simple method of dealing with colinearity, is to remove the highest VIF features from your model, iteratively, assessing the performance and determining whether to keep the variable or not. Another method is to create some linear combination of the correlated variables. This is encapsulated in the section on dimensionality reduction. 4.3.3 Normalization \u00b6 back to top Many learning algorithms require zero mean and unit variance to behave optimally. Sklearn preprocessing library contains a very usefull class, StandardScaler for handling this automatically for us. from sklearn.preprocessing import StandardScaler scaler = StandardScaler () normed = scaler . fit_transform ( colin ) colin [[ 'x0' , 'x1' , 'x2' , 'x3' ]] . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'> pd . DataFrame ( normed , columns = [[ 'x0' , 'x1' , 'x2' , 'x3' ]]) . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'> \ud83c\udfcb\ufe0f Exercise 4: Normalization affect on VIF \u00b6 In the above, we saw how to scale and center variables. How does this affect VIF? Calculate the VIF for the scaled-centered data # Code Cell for Exercise 4 vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( normed , i ) for i in range ( normed . shape [ 1 ])] vif [ \"features\" ] = colin . columns display ( vif ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 3.286048 x0 1 3.296881 x1 2 1.015805 x2 3 1.035537 x3 4.3.4 Dimensionality Reduction \u00b6 back to top Dimensionality reduction is an awesome way to do feature engineering. It is very commonly used. Because it is also an unsupervised machine learning technique, we will visit this topic in that section. 4.4 Missing Data \u00b6 back to top We will often have missing data in our datasets. How do we deal with this? Let's start by making some data with missing data. We'll use a numpy nan datatype to do this from numpy import nan X = np . array ([[ nan , 0 , 3 ], [ 3 , 7 , 9 ], [ 3 , 5 , 2 ], [ 4 , nan , 6 ], [ 8 , 8 , 1 ]]) y = np . array ([ 14 , 16 , - 1 , 8 , - 5 ]) 4.4.1 Imputation \u00b6 back to top A very common strategy is to impute or fill in the missing data, based on basic statistical descriptions of the feature column (mode, mean, and median) from sklearn.impute import SimpleImputer # strategy = 'mean' will replace nan's with mean value # of the column # others are median and most_frequent (mode) imp = SimpleImputer ( strategy = 'mean' ) X2 = imp . fit_transform ( X ) X2 array([[4.5, 0. , 3. ], [3. , 7. , 9. ], [3. , 5. , 2. ], [4. , 5. , 6. ], [8. , 8. , 1. ]]) 4.4.2 Other Strategies \u00b6 back to top Depending on the severity of missing data, you will sometimes opt to remove the whole column, or perhaps apply some simple learning to fill in the missing data. This is a great article on more advanced strategies for handling missing data. References \u00b6 back to top * Box Cox * Multicolinearity * Missing Data","title":"SOLN S4 Feature Engineering"},{"location":"solutions/SOLN_S4_Feature_Engineering/#data-science-foundations-session-4-feature-engineering","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we talked about model pipelines and conveniently began with a suitable set of input data. In the real world, this is hardly ever the case! What is constant is this: at the end of the day, our models need numbers. Not only this, but a suitable set of numbers. What does that mean? The answer to that question is the subject of our session today.","title":"Data Science Foundations, Session 4: Feature Engineering"},{"location":"solutions/SOLN_S4_Feature_Engineering/#40-preparing-environment-and-importing-data","text":"back to top","title":"4.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_S4_Feature_Engineering/#401-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats as stats from scipy.stats import gamma from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor import seaborn as sns ; sns . set () from sklearn.datasets import load_iris from sklearn.metrics import mean_squared_error , r2_score","title":"4.0.1 Import Packages"},{"location":"solutions/SOLN_S4_Feature_Engineering/#402-load-dataset","text":"back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) orders = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_orders.csv' ) time_cols = [ i for i in orders . columns if '/' in i ] margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 dfcat = margin . columns [: - 2 ] dfcat Index(['Base Cake', 'Truffle Type', 'Primary Flavor', 'Secondary Flavor', 'Color Group', 'Customer', 'Date'], dtype='object') orders . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer 1/2020 2/2020 3/2020 4/2020 5/2020 6/2020 7/2020 8/2020 9/2020 10/2020 11/2020 12/2020 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 53770.342593 40735.108025 40735.108025 40735.108025 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 466477.578125 299024.088542 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 80801.728070 51795.979532 51795.979532 51795.979532 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 18046.111111 13671.296296 13671.296296 13671.296296 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146 19147.454268 12274.009146 12274.009146 12274.009146 12274.009146 12274.009146","title":"4.0.2 Load Dataset"},{"location":"solutions/SOLN_S4_Feature_Engineering/#41-categorical-features","text":"back to top At the end of the day, our algorithms operate on numerical values. How do you get from a series of string values to numerical values? margin [ 'Customer' ] . unique () array(['Slugworth', 'Perk-a-Cola', 'Fickelgruber', 'Zebrabar', \"Dandy's Candies\"], dtype=object) A naive way to do it would be to assign a number to every entry 'Slugworth' = 1 'Perk-a-Cola' = 2 'Dandy's Candies' = 3 but we would inadvertently end up with some weird mathematical relationships between these variables, e.g. Dandy's Candies - Perk-a-Cola = Slugworth (3 - 2 = 1). A work around for this is to think multi-dimensionally we express our categorical values as vectors in a hyperspace where they cannot be expressed in terms of one another, i.e. they are orthogonal 'Slugworth' = [1,0,0] 'Perk-a-Cola' = [0,1,0] 'Dandy's Candies' = [0,0,1] such a scheme, in machine learning vernacular, is termed one-hot encoding.","title":"4.1 Categorical Features"},{"location":"solutions/SOLN_S4_Feature_Engineering/#411-one-hot-encoding","text":"back to top sklearn has a couple useful libraries for one-hot encoding. let's start with the OneHotEncoder class in its preprocessing library from sklearn.preprocessing import OneHotEncoder # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ 'Customer' ] . values . reshape ( - 1 , 1 ) # fit our encoder to this data enc . fit ( X_cat ) OneHotEncoder() After fitting our encoder, we can then use this object to create our training array. # as a reference here's our original data display ( X_cat [: 10 ]) print ( X_cat . shape , end = ' \\n\\n ' ) onehotlabels = enc . transform ( X_cat ) . toarray () print ( onehotlabels . shape , end = ' \\n\\n ' ) # And here is our new data onehotlabels [: 10 ] array([['Slugworth'], ['Slugworth'], ['Perk-a-Cola'], ['Fickelgruber'], ['Fickelgruber'], ['Fickelgruber'], ['Slugworth'], ['Zebrabar'], ['Slugworth'], ['Zebrabar']], dtype=object) (1668, 1) (1668, 5) array([[0., 0., 0., 1., 0.], [0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]]) We have our customer information one-hot encoded, we need to do this for all our variables and concatenate them with our regular numerical variables in our original dataframe. # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ dfcat ] . values # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [ \"KG\" ] print ( X_num . shape ) X = np . concatenate (( onehotlabels , X_num . values . reshape ( - 1 , 1 )), axis = 1 ) X . shape (1668,) (1668, 119) And now we grab our EBITDA (margin) data for prediction y = margin [ \"EBITDA/KG\" ]","title":"4.1.1 One-Hot Encoding"},{"location":"solutions/SOLN_S4_Feature_Engineering/#exercise-1-create-a-simple-linear-model","text":"Using the X and y sets, use train_test_split and LinearRegression to make a baseline model based on what we've learned so far. Assess your model performance visually by plottying y_test vs y_test_pred # Cell for Exercise 1 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 ) model = LinearRegression () model . fit ( X_train , y_train ) y_test_pred = model . predict ( X_test ) fig , ax = plt . subplots ( figsize = ( 5 , 5 )) ax . plot ( y_test , y_test_pred , ls = '' , marker = '.' ) [<matplotlib.lines.Line2D at 0x7fa4b01f90d0>]","title":"\ud83c\udfcb\ufe0f Exercise 1: Create a simple linear model"},{"location":"solutions/SOLN_S4_Feature_Engineering/#question-1","text":"How can we assess the relative feature importance of the features in our model? We could be tempted to inspect the coefficients ( linear.coef_ ) of our model to evaluate the relative feature importance, but in order to do this our features need to be scaled (so that the relative coefficient sizes are meaningful). What other issues might there be (think categorical vs continuous variables).","title":"\ud83d\ude4b Question 1:"},{"location":"solutions/SOLN_S4_Feature_Engineering/#42-derived-features","text":"back to top Can we recall an example of where we've seen this previously? That's right earlier on in our session on model selection and validation we derived some polynomial features to create our polynomial model using the linear regression class in sklearn. We actually see this a lot in engineering, where we will describe log relationships or some other transformation of the original variable. Actually let me see if I can find an example in my handy BSL... concentration profiles in continous stirred tank vs plug flow reactors. Notice the y-axis is log scale. Thanks Bird, Stewart, Lightfoot! Can we think of other examples where we would like to derive features from our input data?","title":"4.2 Derived Features"},{"location":"solutions/SOLN_S4_Feature_Engineering/#421-creating-polynomials","text":"back to top Let's revisit our example from the previous session, right before we introduced Grid Search in sklearn # from Model Selection and Validation, 1.2.1 from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) in the above, we use sklearn's convenient tool, make_pipeline to join together the preprocessing tool PolynomialFeatures and the basic model LinearRegression . Let's take a look at what PolynomialFeatures does to some simple data x = np . arange ( 1 , 11 ) y = x ** 3 print ( x ) print ( y ) [ 1 2 3 4 5 6 7 8 9 10] [ 1 8 27 64 125 216 343 512 729 1000] features = PolynomialFeatures ( degree = 3 ) X2 = features . fit_transform ( x . reshape ( - 1 , 1 )) we see our new feature set contains our original features, plus new features up to the nth-degree polynomial we set when creating the features object from PolynomialFeatures print ( X2 ) [[ 1. 1. 1. 1.] [ 1. 2. 4. 8.] [ 1. 3. 9. 27.] [ 1. 4. 16. 64.] [ 1. 5. 25. 125.] [ 1. 6. 36. 216.] [ 1. 7. 49. 343.] [ 1. 8. 64. 512.] [ 1. 9. 81. 729.] [ 1. 10. 100. 1000.]] model = LinearRegression () . fit ( X2 , y ) yhat = model . predict ( X2 ) plt . scatter ( x , y ) plt . plot ( x , yhat );","title":"4.2.1 Creating Polynomials"},{"location":"solutions/SOLN_S4_Feature_Engineering/#422-dealing-with-time-series","text":"back to top Often, we will be dealing with time series data, whether its data generated by machinery, reactors, or sales and customers. In the following we discuss some simple practices for dealing with time series data.","title":"4.2.2 Dealing with Time Series"},{"location":"solutions/SOLN_S4_Feature_Engineering/#4221-enrichment-fast-fourier-transform","text":"back to top Sometimes we'll want to create a more sophisticated transformation of our input data. As engineers, this can often have to do with some empirical knowledge we understand about our process. When working with equipment and machinery, we will often want to convert a signal from the time to frequency domain. Let's cover how we can do that with numpy! [img src]( https://www.nti-audio.com/en/support/know-how/fast-fourier-transform-fft#:~:text=The%20%22Fast%20Fourier%20Transform%22%20(,frequency%20information%20about%20the%20signal .) What I've drawn here in the following is called a square-wave signal t = np . linspace ( 0 , 1 , 501 ) # FFT should be given an integer number of cycles so we leave out last sample t = t [: - 1 ] f = 5 # linear frequency in Hz w = f * 2 * np . pi # radial frequency h = 4 # height of square wave amp = 4 * h / np . pi s = amp * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) # here is the call to numpy FFT F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ], d = t [ 1 ]) # reorder frequency spectrum and frequency bins with 0 Hz at the center F = np . fft . fftshift ( F ) freq = np . fft . fftshift ( freq ) # scale frequency spectrum to correct amplitude F = F / t . size # amplitudes amps = [ max ( np . sin ( w * t )), max ( np . sin ( w * t * 3 ) / 3 ), max ( np . sin ( w * t * 5 ) / 5 )] fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , amp * np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) ax [ 0 ] . set_xlim ( 0 , 1 ) ax [ 0 ] . set_xlabel ( 'Time (s)' ) # tells us about the amplitude of the component at the # corresponding frequency. Multiplied by two because the # signal power is split between (-) and (+) frequency branches # of FFT, but we're only visualizing the (+) branch magnitude = 2 * np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , 30 ) ax [ 1 ] . set_xlabel ( 'Frequency (Hz)' ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain')","title":"\ud83c\udf52 4.2.2.1 Enrichment: Fast Fourier Transform"},{"location":"solutions/SOLN_S4_Feature_Engineering/#4222-rolling-windows","text":"back to top to see an example of this dataset in action visit this link One powerful technique for dealing with time series data, is to create a rolling window of features based on the historical data. The proper window size can usually be determined by trial and error, or constraints around access to the data itself. In the above gif, we have a window size of 7. What that means is for whatever time step units we are in (that could be minutes, days, months, etc.) we will have 7 of them included in a single instance or observation. This instance or observation is then interpreted by our model and used to assess the target value, typically the quantity in the very next time step after the window (the green bar in the gif). Let's take an example with the orders data tidy_orders = orders . melt ( id_vars = orders . columns [: 6 ], var_name = 'Date' , value_name = 'KG' ) display ( tidy_orders . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 In the next exercise, we are going to attempt to predict an order amount, based on the previous order history. We will scrub all categorical labels and only use historical amounts to inform our models. In effect the data that the model will see will look like the following: fig , ax = plt . subplots ( 3 , 2 , figsize = ( 10 , 20 )) indices = np . argwhere ( ax ) color_dict = { 0 : 'tab:blue' , 1 : 'tab:green' , 2 : 'tab:orange' , 3 : 'tab:red' , 4 : 'tab:pink' , 5 : 'tab:brown' } for index , customer in enumerate ( tidy_orders . Customer . unique ()): orders . loc [ orders . Customer == customer ] . iloc [:, 6 :] . reset_index () . T . plot ( c = color_dict [ index ], legend = False , ax = ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]]) ax [ indices [ index ][ 0 ], indices [ index ][ 1 ]] . set_title ( customer ) What we may notice is that there is very little noise or drift in our order history, but there is certainly some periodicity. The question is can we use a linear model to predict the next order amount based on these history orders?","title":"4.2.2.2 Rolling Windows"},{"location":"solutions/SOLN_S4_Feature_Engineering/#exercise-2-optimize-rolling-window-size-for-customer-forecasts","text":"For this exercise, you will use the process_data function below to help you optimize the window size for predicting the order quantity in any given month. You will train a LinearRegression model. create a model using a window size of 3 and predict the order quantity for the month immediately following the window create a model for window sizes 1-11 and report the \\(R^2\\) for each model def process_data ( Xy , time_cols = 12 , window = 3 , remove_null = False ): \"\"\" This function splits your time series data into the proper windows Parameters ---------- Xy: array The input data. If there are non-time series columns, assumes they are on the left and time columns are on the right. time_cols: int The number of time columns, default 12 window: int The time window size, default 3 Returns ------- X_: array The independent variables, includes time and non-time series columns with the new window y_: array The dependent variable, selected from the time columns at the end of the window labels: The time series labels, can be used in subsequent plot \"\"\" # separate the non-time series columns X_cat = Xy [:,: - time_cols ] # select the columns to apply the sweeping window X = Xy [:, - time_cols :] X_ = [] y = [] for i in range ( X . shape [ 1 ] - window ): # after attaching the current window to the non-time series # columns, add it to a growing list X_ . append ( np . concatenate (( X_cat , X [:, i : i + window ]), axis = 1 )) # add the next time delta after the window to the list of y # values y . append ( X [:, i + window ]) # X_ is 3D: [number of replicates from sweeping window, # length of input data, # size of new feature with categories and time] # we want to reshape X_ so that the replicates due to the sweeping window is # a part of the same dimension as the instances of the input data X_ = np . array ( X_ ) . reshape ( X . shape [ 0 ] * np . array ( X_ ) . shape [ 0 ], window + X_cat . shape [ 1 ]) y = np . array ( y ) . reshape ( X . shape [ 0 ] * np . array ( y ) . shape [ 0 ],) if remove_null : # remove training data where the target is 0 (may be unfair advantage) X_ = X_ [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] y = y [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] # create labels that show the previous month values used to train the model labels = [] for row in X_ : labels . append ( \"X: {} \" . format ( np . array2string ( row [ - window :] . astype ( float ) . round ()))) return X_ , y , labels # Code Cell for Exercise 2 kg_month_data = orders . values [:, 6 :] # use kg_month_data and the function process_data to create your X, y arrays # then use train_test_split to create train and test portions # USE y_test and y_pred for your actual and true test data # change only window parameter in process_data() print ( \"window R2\" ) for window in range ( 1 , 12 ): ###################### ### YOUR CODE HERE ### ###################### X , y , labels = process_data ( kg_month_data , time_cols = 12 , window = 3 ) features = PolynomialFeatures ( degree = 4 ) X2 = features . fit_transform ( X ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.6 ) model = LinearRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( \" {} , \\t {:.2f} \" . format ( window , r2_score ( y_test , y_pred ))) window R2 1, 0.97 2, 0.88 3, 0.80 4, 0.96 5, 0.90 6, 0.86 7, 0.61 8, 0.89 9, 0.76 10, 0.74 11, 0.87 #### RUN AFTER EXERCISE 2.2.2.2.1 #### fig = px . scatter ( x = y_test , y = y_pred , labels = { \"y\" : \"Prediction\" , \"x\" : \"Actual\" }) fig . update_layout ( autosize = False , width = 800 , height = 500 , title = 'R2: {:.3f} ' . format ( r2_score ( y_test , y_pred )) ) require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"239a6e0b-8482-49c8-9619-ebde564b91cd\")) { Plotly.newPlot( \"239a6e0b-8482-49c8-9619-ebde564b91cd\", [{\"hovertemplate\":\"Actual=%{x}<br>Prediction=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4528.380503144654,64.01384083044982,15617.489114658929,4528.380503144654,5698.392857142857,124348.89322916669,21939.111111111117,5712.33671988389,64983.42138364781,1483.5,93302.72435897436,1236.25,773.5849056603774,518.4642857142858,3798.928571428572,13671.296296296296,188.7692307692308,4057.832278481013,101149.25,14626.074074074077,12274.009146341465,141456.76315789475,4793.829296424452,74.78571428571429,10861.245674740485,86.24595469255664,19129.33333333333,39307.04113924051,74.78571428571429,46247.0,188.7692307692308,85.32258064516128,90677.41228070176,9306.611111111111,220.55016181229772,466477.57812500006,1158.125,220.55016181229772,1901.1290322580644,19129.33333333333,2637.8993710691825,1868.867924528302,3798.928571428572,2252.261437908497,5222.5,181.5552699228792,5038.387096774193,120.98615916955016,10146.898432174508,85.32258064516128,286.6666666666667,18613.222222222223,818.1891025641025,2499.201741654572,2010.9477124183009,6984.389273356401,42704.25889967639,11715.487421383648,39307.04113924051,440.859375,428.0,96.59546925566345,102.64285714285714,3569.0359477124184,13671.296296296296,773.5849056603774,2010.9477124183009,42704.25889967639,40608.41346153846,83347.75862068967,10796.31640058055,74.78571428571429,80801.72807017544,2540.0,5222.5,131.55526992287918,299024.0885416667,3882.680981595092,895.9082278481012,1180.5379746835442,15617.489114658929,85.32258064516128,6185.59509202454,16749.13157894737,47142.92857142857,3786.966463414634,936.280487804878,15244.632812500002,3272.80163599182,1271.904761904762,47142.92857142857,356.6666666666667,10861.245674740485,108.64285714285714,21438.1875,16918.132716049382,7892.142857142857,1901.1290322580644,2647.5,5038.387096774193,1677.5078125000002,39307.04113924051,51795.97953216374,916.3717948717948,41871.69230769231,12870.766853932584,1271.904761904762,181.5552699228792,211540.7575757576,3798.928571428572,2054.328561690525,3578.592162554427,2854.854368932039,51795.97953216374,11791.8,149.57142857142858,103.3653846153846,119015.43674698794,2499.201741654572,19286.94968553459,2185.3410740203194,19569.69230769231,12317.397660818711,6965.361445783132,86.24595469255664,3071.4691011235955,13867.125,50574.625,14626.074074074077,1005.4545454545454,16918.132716049382,2543.106741573034,10245.87593728698,8608.709677419354,150.3654970760234,3517.9333333333334,14626.074074074077,3177.0,2499.201741654572,345.6428571428572,10736.622807017544,13799.769319492503,12691.209677419354,11.175616835994196,2010.9477124183009,10736.622807017544,71138.51966292135,8104.40251572327,62.168396770472896,211540.7575757576,16569.261006289307,445.54838709677415,16918.132716049382,42.484177215189874,150.3654970760234,11.175616835994196,299024.0885416667,579.0625,10146.898432174508,181.5552699228792,494.57142857142856,411.1303344867359,2355.267295597484,494.57142857142856,79091.23475609756,10146.898432174508,1005.4545454545454,5038.387096774193,191.11111111111111,9304.668674698794,19569.69230769231,8250.491573033707,466477.57812500006,3048.0,1630.1966292134832,64.01384083044982,3569.0359477124184,1901.1290322580644,3272.80163599182,9772.200520833334,2185.3410740203194,28694.0,314.3203883495146,8104.40251572327,18557.572327044025,2231.547169811321,1432.2712418300653,773.5849056603774,40735.10802469136,314.3203883495146,471.91011235955057,42.484177215189874,9194.277108433736,8173.714285714285,837.8787878787879,1604.1437908496732,22.50980392156863,14.663461538461538,663176.1973875181,2854.854368932039,428.0,43.64516129032258,1526.2857142857142,1432.2712418300653,5456.578050443081,22331.935185185182,150.3654970760234,4057.832278481013,1868.867924528302,1630.1966292134832,4723.49129172714,5038.387096774193,19364.70552147239,117.22222222222224,15110.75,4057.832278481013,83347.75862068967,7892.142857142857,19129.33333333333,1968.8904494382025,207.39062500000003,62.168396770472896,3578.592162554427,1677.5078125000002,2499.201741654572,1254.4444444444443,1236.25,3578.592162554427,1992.4528301886792,14626.074074074077,1236.25,6965.361445783132,36.37096774193548,40735.10802469136,20.098039215686278,1432.2712418300653,538.8571428571429,101149.25,93302.72435897436,181.5552699228792,28694.0,2054.328561690525,5872.384615384615,31412.04644412192,5872.384615384615,854.952380952381,31412.04644412192,1253403.0130624091,63142.24137931035,20.098039215686278,3569.0359477124184,16569.261006289307,19286.94968553459,19286.94968553459,837.8787878787879,3665.809768637532,108.64285714285714,56.07911392405064,13671.296296296296,371.2903225806451,4057.832278481013,47142.92857142857,21438.1875,54.833333333333336,26081.56401384083,2540.0,115.76923076923076,16.423076923076923,8608.709677419354,4463.780120481927,8250.491573033707,15110.75,8173.714285714285,157100.37650602407,104499.0512820513,9076.930817610064,2611.25,428.0,14.663461538461538,2694.0,3569.0359477124184,86.24595469255664,678.7183544303797,494.57142857142856,712.4603174603175,663176.1973875181,10245.87593728698,5071.786163522012,1868.867924528302,26081.56401384083,3403.344867358708,4793.829296424452,64983.42138364781,3272.80163599182,5712.33671988389,9194.277108433736,608.4770114942529,42.484177215189874,466477.57812500006,3695.444059976932,3517.9333333333334,345.6428571428572,579.0625,7892.142857142857,5038.387096774193,45601.61516853933,1992.4528301886792,2647.5,5872.384615384615,6965.361445783132,64.01384083044982,45601.61516853933,23123.5,1992.4528301886792,2540.0,9060.337370242214,14.663461538461538,3882.680981595092,36.37096774193548,193984.2734375,2231.547169811321,108.64285714285714,329.7142857142857,117.49826989619376,773.5849056603774,36.37096774193548,8608.709677419354,371.2903225806451,45481.42307692308,10245.87593728698,63142.24137931035,678.7183544303797,11555.9375,4528.380503144654,2499.201741654572,10736.622807017544,5222.5,1901.1290322580644,93302.72435897436,1702.1929824561405,114208.8534107402,343.1394601542417,10245.87593728698,8173.714285714285,140637.14285714287,132.94270833333334,579.0625,663176.1973875181,1180.5379746835442,1236.25,26081.56401384083,854.952380952381,2010.9477124183009,30698.85714285714,11.175616835994196,90677.41228070176,38128.80258899677,663176.1973875181,71138.51966292135,1236.25,31412.04644412192,50574.625,83347.75862068967,12274.009146341465,90677.41228070176,808.2857142857143,11.175616835994196,11555.9375,86.24595469255664,41871.69230769231,19129.33333333333,329.7142857142857,1236.25,2637.8993710691825,579.7777777777778,30698.85714285714,329.7142857142857,2647.5,466477.57812500006,102.64285714285714,12691.209677419354,12317.397660818711,220.55016181229772,19147.454268292684,9304.668674698794,4057.832278481013,47142.92857142857,1630.1966292134832,4463.780120481927,2002.1844660194176,5071.786163522012,7035.866666666667,13504.20634920635,21601.383647798742,10245.87593728698,10861.245674740485,176.36477987421384,12691.209677419354,1432.2712418300653,608.4770114942529,10736.622807017544,3695.444059976932,157100.37650602407,1702.1929824561405,51795.97953216374,2386.449438202247,117.22222222222224,120.98615916955016,777.0363321799308,12274.009146341465,2611.25,2242.446601941748,168764.57142857145,627.2222222222222,40608.41346153846,2002.1844660194176,157.46855345911948,2647.5,119015.43674698794,579.0625,329.7142857142857,13121.345911949686,71138.51966292135,207.39062500000003,30698.85714285714,9060.337370242214,1529.7752808988764,3071.4691011235955,46247.0,538.8571428571429,16347.42857142857,23123.5,132.94270833333334,6984.389273356401,12691.209677419354,2499.201741654572,30221.5,15229.451612903224,191.11111111111111,428.0,3578.592162554427,11715.487421383648,19129.33333333333,30221.5,6432.321799307958,41871.69230769231,42.484177215189874,16918.132716049382,3695.444059976932,13504.20634920635,15617.489114658929,117.49826989619376,16569.261006289307,96.59546925566345,678.7183544303797,10146.898432174508,678.7183544303797,131.55526992287918,47142.92857142857,79091.23475609756,736.1797752808989,1180.5379746835442,29354.53846153846,45481.42307692308,21438.1875,6965.361445783132,31412.04644412192,10146.898432174508,11791.8,15617.489114658929,1677.5078125000002,5456.578050443081,14626.074074074077,94285.85714285714,248.6394601542416,5456.578050443081,26081.56401384083,63142.24137931035,12317.397660818711,12317.397660818711,608.4770114942529,11.175616835994196,10861.245674740485,2566.6935483870966,11555.9375,678.7183544303797,1529.7752808988764,1432.2712418300653,518.4642857142858,8104.40251572327,63142.24137931035,3798.928571428572,4057.832278481013,1526.2857142857142,2854.854368932039,3403.344867358708,9306.611111111111,538.8571428571429,13671.296296296296,117.22222222222224,343.1394601542417],\"xaxis\":\"x\",\"y\":[4785.592155979585,378.9394302913829,15365.600809369838,4785.592155979585,4070.255357333951,134757.3771732384,14409.557558767568,5813.845282617682,61516.49587090381,1510.6755468797405,98738.27282520698,1684.2601507265988,1088.107600326137,647.864369876902,4263.755978756964,14168.403568365671,487.3481032940997,4287.792726026477,51779.415544128475,19934.734830917627,12141.409803201397,87747.52966460225,5818.94510670397,377.43130038813075,12797.394868451114,389.0359797814606,19263.507069512012,40163.79013647013,377.43130038813075,23840.068464074022,487.3481032940997,400.48488833402286,126112.56377290396,11223.080944977459,522.1062342859491,415175.9779430929,894.6750043489748,519.4101103091041,2138.6141864173633,19263.507069512012,2790.085070463646,2196.4290704108694,4263.755978756964,2244.514746383961,2963.0052419712265,602.4723068239653,5217.814575086902,410.08771833436947,16913.133647514336,390.2429375597753,489.6064298303322,16311.253798810561,1109.562627058514,2715.348806915594,2281.9990642021894,6353.789558047462,37073.75373050823,11896.354947646087,38882.42895370776,797.0601491084628,703.1493477716122,388.482661364839,481.8437277075316,3747.013377347842,16794.12337436424,1571.4700357476117,2257.4161561731,37073.75373050823,43146.57407027896,76460.52042984031,5813.845282617682,433.93372568151483,50253.31197444373,2781.8507793892813,2963.0052419712265,450.36262704088375,323624.6828685297,2286.347850033103,959.8167805278175,1463.9293549917084,25867.060321463283,388.50455558540244,3461.34813854915,10658.874464205835,48286.68591679114,4070.54807420052,1208.1895902424544,13863.416557439898,3913.805102706469,1545.439458867743,48286.68591679114,664.1495092797293,12797.394868451114,415.88888264864494,24308.715376885695,17460.789585310762,13878.585909613186,2138.6141864173633,3258.406190332729,5925.265679815453,1797.2371240684145,47713.131047358365,51804.0528093027,1099.5606427530397,40683.104312073505,8508.45844428347,1571.3535403980425,505.4911026847705,213132.54291644722,4263.755978756964,2570.353838719455,3926.506181336219,3317.1418553360736,54758.90192605476,20585.396531692135,377.43130038813075,406.91804655949966,120990.40817472423,4395.853637740449,19387.390319584494,2412.686086825897,19699.926384269966,12552.025861937103,7368.391717428835,390.09029678781303,2262.905415567944,11572.521113313724,56931.5110267007,15545.52362930884,1239.906233381196,17460.789585310762,1926.1552505875645,10673.158532726991,9907.699090223898,454.81639486308137,6355.618165841122,19934.734830917627,2858.35606741023,2834.2664976391197,665.469888483012,11592.823783640883,16177.143075924467,12543.725418313918,317.63569157648635,2257.4161561731,15201.471014315619,45645.23591676211,8323.62833082682,376.8168895165831,213132.54291644722,16698.569941711827,663.3571920970296,20710.099320853376,346.28222029939593,513.9332229141203,323.60536008687086,323624.6828685297,1301.212252741579,16913.133647514336,505.4911026847705,632.0783029798903,778.1760841513764,2635.5625663138067,632.0783029798903,76574.72476531091,11492.974658748008,1239.906233381196,5163.94089137517,494.71513791291954,11527.578799029196,19176.79696136619,8979.131225405608,415175.9779430929,2754.6914615362707,2567.0739167219367,378.9394302913829,3747.013377347842,2138.6141864173633,3913.805102706469,10871.491666793676,2516.669574686992,18752.149151877966,636.9170394143573,8323.62833082682,17785.632844249434,2407.321312452001,1686.4843440391876,1051.2986748894868,49435.50780135217,610.4363857799107,960.0492155948752,346.28222029939593,7141.329503589847,8187.405610699661,1148.288091129662,1686.4843440391876,324.6947768318741,320.7835266802878,639820.6545150073,3317.1418553360736,703.1493477716122,340.3871184326995,1531.8394329987157,1686.4843440391876,9236.305456033022,16619.83993793382,463.3944227377964,4287.792726026477,3364.1619784229933,2019.1544585081797,2715.348806915594,5163.94089137517,10185.63650295469,436.06660273854754,26293.480165383102,4218.370391131691,76460.52042984031,7915.880111460361,25978.487349635714,2375.22637673234,489.7605752448112,376.8168895165831,3756.2286447454576,1797.2371240684145,3060.8574440447824,1384.0395692783663,1535.8631900519042,6162.533552625073,2276.6020512892946,14800.536096877271,1535.8631900519042,7368.391717428835,340.3871184326995,40283.97684677175,325.06940680600604,1695.6732046475156,1028.5053850528614,51779.415544128475,92018.35013115287,505.4911026847705,18752.149151877966,3667.712553203018,5968.182975715722,34939.27854355379,6424.274585977019,992.354894936303,30596.638581578518,639820.6545150073,65413.624508456036,324.82371754886964,3747.013377347842,16698.569941711827,19387.390319584494,19387.390319584494,1148.288091129662,6305.285970941779,492.1628050669814,346.28222029939593,14168.403568365671,663.3571920970296,4218.370391131691,45766.272960899,37175.69967364701,369.63985077669076,22891.92825541111,2754.6914615362707,405.6544524204279,319.54816919005003,8698.931074864986,5689.033000226852,11752.173931839643,26293.480165383102,8187.405610699661,117110.65135820676,90877.76737128492,8855.335527589015,3229.016296504339,703.1493477716122,319.72742324233695,1604.2554168141014,3769.9108375996702,396.30197122916206,959.8167805278175,632.0783029798903,999.9729800621147,731503.1707028296,17075.133945943726,5082.686543693957,2107.5040932279385,22891.92825541111,4219.676962291559,5818.94510670397,61516.49587090381,3617.0754679045876,5813.845282617682,7141.329503589847,932.7370475860574,346.28222029939593,415175.9779430929,4555.635369302835,3885.8140646224183,769.1955699368395,953.6647702716481,8337.819513819635,5163.94089137517,48246.709882894014,2276.6020512892946,2886.664844581823,5968.182975715722,7368.391717428835,378.9394302913829,63573.70285359673,40074.17801737161,2276.6020512892946,3138.4978117622,8151.559019833033,319.72742324233695,2286.347850033103,340.3871184326995,172828.89934304,2407.321312452001,410.08048522816836,747.818087951304,407.0672088952674,1088.107600326137,341.517050774033,8698.931074864986,667.3272676429804,39725.42364104348,10673.158532726991,65413.624508456036,959.8167805278175,13195.11592262944,4785.592155979585,2834.2664976391197,15201.471014315619,2963.0052419712265,2425.884027039221,98738.27282520698,1997.7384398419902,115873.73047883328,489.0303345318461,10673.158532726991,9457.064447629586,137428.5757499219,449.0579108608325,1301.212252741579,1085751.3630379443,1502.4134405828336,1684.2601507265988,22891.92825541111,992.354894936303,2244.514746383961,31550.15438840883,316.0906893952995,90462.3574812228,37318.37237669782,639820.6545150073,45645.23591676211,1684.2601507265988,30596.638581578518,56931.5110267007,76460.52042984031,12141.409803201397,95635.30862544454,839.3500692330289,316.62245060704845,13195.11592262944,389.0359797814606,41802.402147140514,19263.507069512012,747.818087951304,1535.8631900519042,2790.085070463646,803.8781088470796,29908.894508749334,648.872490598366,2858.35606741023,415175.9779430929,409.7821718132848,12543.725418313918,17394.662595168105,522.1062342859491,12141.409803201397,9740.51689224938,5199.4234913728005,45766.272960899,2567.0739167219367,4831.715916465941,2417.5880406486763,5082.686543693957,3697.7340063828487,13472.111607949182,20652.753180263422,11602.10423834572,12797.394868451114,471.44073265789405,12543.725418313918,1695.6732046475156,932.7370475860574,10980.322568203586,4555.635369302835,117110.65135820676,2094.8448847085992,54758.90192605476,1826.31018819446,421.9954832228255,410.08771833436947,978.2266656454766,12141.409803201397,4796.262256309382,2236.064155834039,135924.7927322855,1041.1204286449113,40221.84276082843,2273.3851256392354,461.10966099955675,2858.35606741023,143848.60557702882,953.6647702716481,648.872490598366,12664.975331868296,45645.23591676211,489.7605752448112,29908.894508749334,8151.559019833033,2427.747778574022,2262.905415567944,23840.068464074022,866.7970691016989,8187.405610699661,26195.686125920434,449.0579108608325,6353.789558047462,12679.428015781728,2715.348806915594,15684.810589735318,12543.725418313918,504.4494927620649,703.1493477716122,4250.960414621229,11896.354947646087,25978.487349635714,15684.810589735318,5875.699548389586,41802.402147140514,347.0090494475603,16909.279272683176,4555.635369302835,15368.26716660901,16108.716384819742,407.0672088952674,16698.569941711827,388.482661364839,993.553834752781,10573.002781125038,971.4284514069652,450.36262704088375,45766.272960899,76574.72476531091,774.5157686911542,1443.732437768414,19176.79696136619,39725.42364104348,24308.715376885695,8706.164457317907,32091.295039989156,11492.974658748008,13508.076892121393,16108.716384819742,1797.2371240684145,5826.8479598607,15545.52362930884,45766.272960899,438.4351253031718,5826.8479598607,22891.92825541111,65413.624508456036,12552.025861937103,12552.025861937103,932.7370475860574,316.0906893952995,12797.394868451114,2860.1717851882872,11807.964454280216,1123.9090100996427,1913.5805743985902,1816.3382332975395,647.864369876902,8323.62833082682,65413.624508456036,5403.795319141717,4218.370391131691,1531.8394329987157,3111.527273406017,4219.676962291559,11223.080944977459,866.7970691016989,13722.736128528055,436.06660273854754,489.0303345318461],\"yaxis\":\"y\",\"type\":\"scatter\"}], {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Actual\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prediction\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"autosize\":false,\"width\":800,\"height\":500,\"title\":{\"text\":\"R2: 0.869\"}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('239a6e0b-8482-49c8-9619-ebde564b91cd'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; });","title":"\ud83c\udfcb\ufe0f Exercise 2: Optimize Rolling Window Size for Customer Forecasts"},{"location":"solutions/SOLN_S4_Feature_Engineering/#423-image-preprocessing","text":"back to top Image preprocessing is beyond the scope of this session. We cover this topic in General Applications of Neural Networks . For now, know that there is a wealth of considerations for how to handle images, and they all fit within the realm of feature engineering.","title":"4.2.3 Image Preprocessing"},{"location":"solutions/SOLN_S4_Feature_Engineering/#43-transformed-features","text":"back to top Transformed features, are features that we would like to augment based on their relationship within their own distribution or to other (allegedly) independent data within our training set. e.g. we're not deriving new features based on some empirical knowledge of the data, rather we are changing them due to statistical properties that we can assess based on the data itself.","title":"4.3 Transformed Features"},{"location":"solutions/SOLN_S4_Feature_Engineering/#431-skewness","text":"back to top Skewed data can lead to imbalances in our model prediction. Why? Skewed values in the distribution will bias the mean. When assigning weights to this input feature, therefore, the model will give preferential treatment to these values. To demonstrate, I'm going to use scipy to create some skewed data. from scipy.stats import skewnorm a = 10 x = np . linspace ( skewnorm . ppf ( 0.01 , a ), skewnorm . ppf ( 0.99 , a ), 100 ) plt . plot ( x , skewnorm . pdf ( x , a ), 'r-' , lw = 5 , alpha = 0.6 , label = 'skewnorm pdf' ) [<matplotlib.lines.Line2D at 0x7fa48d14a130>] We can now generate a random population based on this distribution r = skewnorm . rvs ( a , size = 1000 ) plt . hist ( r ) (array([113., 267., 225., 172., 116., 62., 26., 13., 2., 4.]), array([-0.19733964, 0.15303313, 0.50340589, 0.85377866, 1.20415142, 1.55452419, 1.90489696, 2.25526972, 2.60564249, 2.95601526, 3.30638802]), <BarContainer object of 10 artists>) Unskewed data will return something close to 0 from calling df.skew() . When dealing with actual data, we can use df.skew() to determine whether we should transform our data. x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) x [ 'Skewed Data' ] . skew () 0.9141902067398219 There are a handful of ways to deal with skewed data: log transform square root transform Box-Cox transform Let's try the first two print ( 'square root transformed skew: {:.4f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) print ( 'log transformed skew: {:.4f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . legend () square root transformed skew: 0.0561 log transformed skew: -1.6916 /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in sqrt /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log <matplotlib.legend.Legend at 0x7fa49157d340> We see we didn't get much traction with the log transform, and the log transform will not be able to handle 0 values, and so we will sometimes have to code exceptions for those. Box-Cox is often a good route to go, but it has the added restriction that the data has to all be above 0. Let's create a new distribution with this added restriction a = 6 r = skewnorm . rvs ( a , size = 1000 ) r = [ i for i in r if i > 0 ] plt . hist ( r ) (array([220., 277., 182., 127., 66., 39., 17., 5., 4., 2.]), array([2.17150536e-03, 3.88613862e-01, 7.75056219e-01, 1.16149858e+00, 1.54794093e+00, 1.93438329e+00, 2.32082565e+00, 2.70726800e+00, 3.09371036e+00, 3.48015272e+00, 3.86659507e+00]), <BarContainer object of 10 artists>) from scipy import stats x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ], alpha = 0.5 , label = 'box-cox: {:.2f} ' . format ( pd . DataFrame ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ])[ 0 ] . skew ())) ax . legend () <matplotlib.legend.Legend at 0x7fa492058f40>","title":"4.3.1 Skewness"},{"location":"solutions/SOLN_S4_Feature_Engineering/#exercise-3-transform-data-from-a-gamma-distribution","text":"Repeat section 2.3.1, this time synthesizing a gamma distribution and transforming it. Which transformation best reduces the skew? Do this for a dataset that does not contain values at or below 0. # code cell for exercise 3 from scipy.stats import gamma a = 6 r = gamma . rvs ( a , size = 1000 ) r = [ i for i in r if i > 0 ] x = pd . DataFrame ( r , columns = [ 'Skewed Data' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) ax . hist ( x [ 'Skewed Data' ], alpha = 0.5 , label = 'original: {:.2f} ' . format (( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . sqrt ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'sqrt: {:.2f} ' . format ( np . sqrt ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( np . log ( x [ 'Skewed Data' ]), alpha = 0.5 , label = 'log: {:.2f} ' . format ( np . log ( x [ 'Skewed Data' ]) . skew ())) ax . hist ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ], alpha = 0.5 , label = 'box-cox: {:.2f} ' . format ( pd . DataFrame ( stats . boxcox ( x [ 'Skewed Data' ])[ 0 ])[ 0 ] . skew ())) ax . legend () <matplotlib.legend.Legend at 0x7fa49111dd30>","title":"\ud83c\udfcb\ufe0f Exercise 3: Transform data from a gamma distribution"},{"location":"solutions/SOLN_S4_Feature_Engineering/#432-colinearity","text":"back to top Colinearity can also affect the performance of your machine learning model. In particular, if features are colinear, it can be easy for your model to overfit to your training dataset. This is often mitigated by regularization. If you're curious you can read more about it on this discussion from StackExchange . We will still explore it explicitly here by calculating the Variance Inflation Factor (VIF) on some hypothetical data. VIF = \\frac{1}{1-R^2} Usually we are concerned about data with a VIF above 10","title":"4.3.2 Colinearity"},{"location":"solutions/SOLN_S4_Feature_Engineering/#4321-detecting-colinearity","text":"back to top from statsmodels.stats.outliers_influence import variance_inflation_factor Step 1: Make some data # we can throttle the error rate random . seed ( 42 ) # x2 will be sqrt of x1 plus some error def func ( x , err ): return x ** .5 + ( err * random . randint ( - 1 , 1 ) * random . random () * x ) x0 = range ( 100 ) x1 = [ func ( i , .05 ) for i in x0 ] # HIGH degree of colinearity with x0 x2 = [ func ( i , 1 ) for i in x0 ] # MED degree of colinearity with x0 x3 = [ random . randint ( 0 , 100 ) for i in x0 ] # NO degree of colinearity with x0 # take a look fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) ax . plot ( x0 , x1 , label = 'x1' ) ax . plot ( x0 , x2 , label = 'x2' ) ax . plot ( x0 , x3 , label = 'x3' ) ax . legend () <matplotlib.legend.Legend at 0x7fa49121ca90> To calculate the colinearities I'm going to aggregate these x's into a dataframe: colin = pd . DataFrame ([ x0 , x1 , x2 , x3 ]) . T colin . columns = [ 'x0' , 'x1' , 'x2' , 'x3' ] colin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 0 0.0 0.000000 0.000000 29.0 1 1.0 1.013751 0.721523 28.0 2 2.0 1.400260 1.414214 3.0 3 3.0 1.630546 -0.438007 84.0 4 4.0 2.017388 4.304847 24.0 Step 2: Calculate VIF factors # calculate VIF factors vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( colin . values , i ) for i in range ( colin . shape [ 1 ])] vif [ \"features\" ] = colin . columns Step 3: Inspect VIF factors # inspect VIF factors display ( vif ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 12.555415 x0 1 15.823872 x1 2 1.030609 x2 3 3.559468 x3 In this case, we may remove either x0 or x1 from the dataset.","title":"4.3.2.1 Detecting Colinearity"},{"location":"solutions/SOLN_S4_Feature_Engineering/#4322-fixing-colinearity","text":"back to top It is good to aknowledge where colinearity exists as this will influence the interpretability of your model. In most cases, however, it won't have a heavy influence on the performance of your model. A simple method of dealing with colinearity, is to remove the highest VIF features from your model, iteratively, assessing the performance and determining whether to keep the variable or not. Another method is to create some linear combination of the correlated variables. This is encapsulated in the section on dimensionality reduction.","title":"4.3.2.2 Fixing Colinearity"},{"location":"solutions/SOLN_S4_Feature_Engineering/#433-normalization","text":"back to top Many learning algorithms require zero mean and unit variance to behave optimally. Sklearn preprocessing library contains a very usefull class, StandardScaler for handling this automatically for us. from sklearn.preprocessing import StandardScaler scaler = StandardScaler () normed = scaler . fit_transform ( colin ) colin [[ 'x0' , 'x1' , 'x2' , 'x3' ]] . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'> pd . DataFrame ( normed , columns = [[ 'x0' , 'x1' , 'x2' , 'x3' ]]) . plot ( kind = 'kde' ) <AxesSubplot:ylabel='Density'>","title":"4.3.3 Normalization"},{"location":"solutions/SOLN_S4_Feature_Engineering/#exercise-4-normalization-affect-on-vif","text":"In the above, we saw how to scale and center variables. How does this affect VIF? Calculate the VIF for the scaled-centered data # Code Cell for Exercise 4 vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( normed , i ) for i in range ( normed . shape [ 1 ])] vif [ \"features\" ] = colin . columns display ( vif ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } VIF Factor features 0 3.286048 x0 1 3.296881 x1 2 1.015805 x2 3 1.035537 x3","title":"\ud83c\udfcb\ufe0f Exercise 4: Normalization affect on VIF"},{"location":"solutions/SOLN_S4_Feature_Engineering/#434-dimensionality-reduction","text":"back to top Dimensionality reduction is an awesome way to do feature engineering. It is very commonly used. Because it is also an unsupervised machine learning technique, we will visit this topic in that section.","title":"4.3.4 Dimensionality Reduction"},{"location":"solutions/SOLN_S4_Feature_Engineering/#44-missing-data","text":"back to top We will often have missing data in our datasets. How do we deal with this? Let's start by making some data with missing data. We'll use a numpy nan datatype to do this from numpy import nan X = np . array ([[ nan , 0 , 3 ], [ 3 , 7 , 9 ], [ 3 , 5 , 2 ], [ 4 , nan , 6 ], [ 8 , 8 , 1 ]]) y = np . array ([ 14 , 16 , - 1 , 8 , - 5 ])","title":"4.4 Missing Data"},{"location":"solutions/SOLN_S4_Feature_Engineering/#441-imputation","text":"back to top A very common strategy is to impute or fill in the missing data, based on basic statistical descriptions of the feature column (mode, mean, and median) from sklearn.impute import SimpleImputer # strategy = 'mean' will replace nan's with mean value # of the column # others are median and most_frequent (mode) imp = SimpleImputer ( strategy = 'mean' ) X2 = imp . fit_transform ( X ) X2 array([[4.5, 0. , 3. ], [3. , 7. , 9. ], [3. , 5. , 2. ], [4. , 5. , 6. ], [8. , 8. , 1. ]])","title":"4.4.1 Imputation"},{"location":"solutions/SOLN_S4_Feature_Engineering/#442-other-strategies","text":"back to top Depending on the severity of missing data, you will sometimes opt to remove the whole column, or perhaps apply some simple learning to fill in the missing data. This is a great article on more advanced strategies for handling missing data.","title":"4.4.2 Other Strategies"},{"location":"solutions/SOLN_S4_Feature_Engineering/#references","text":"back to top * Box Cox * Multicolinearity * Missing Data","title":"References"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/","text":"Data Science Foundations, Session 5: Unsupervised Learning: Clustering and Dimensionality Reduction \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we began our discussion on feature engineering and ended with a sneak peak into dimensionality reduction. This last topic deserves a whole session on its own because its use case is not limited to feature engineering! It can also be used as a tool for visualization, for noise filtering, and much more. In this session we discuss dimensionality reduction along with other unsupervised learning methods. Up until now, the only learning estimators we've looked at were supervised ones: estimators that predict labels based on training data. Here, however, we are interested in uncovering aspects of the data without reference to any known labels. The usefulness for these learners will become immediately apparent when we revist our wine quality models from Course 1, Session 7! 5.0 Preparing Environment and Importing Data \u00b6 back to top 5.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score , silhouette_score , calinski_harabasz_score from sklearn.mixture import GaussianMixture from sklearn.impute import SimpleImputer from scipy.spatial.distance import cdist import seaborn as sns ; sns . set () import copy 5.0.2 Load and Process Dataset \u00b6 back to top wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y_wine = X . pop ( target ) X = imp . fit_transform ( X ) X_wine = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X_wine [:, 2 :] = scaler . fit_transform ( X_wine [:, 2 :]) wine = pd . DataFrame ( X_wine , columns = cols ) wine [ target ] = y wine . dropna ( inplace = True ) wine [ 'quality_label' ] = wine [ 'quality' ] . apply ( lambda x : 'low' if x <= 5 else 'med' if x <= 7 else 'high' ) wine [ 'type_encoding' ] = wine [ 'type' ] . map ({ 'red' : 0 , 'white' : 1 }) wine [ 'quality_encoding' ] = wine [ 'quality_label' ] . map ({ 'low' : 0 , 'med' : 1 , 'high' : 2 }) wine . columns = wine . columns . str . replace ( ' ' , '_' ) features = list ( wine . columns [ 1 : - 1 ] . values ) features . remove ( 'quality_label' ) features . remove ( 'quality' ) 5.1 Principal Component Analysis \u00b6 back to top Principle Component Analysis or PCA is one of the most wide spread implementations of dimensionality reduction. In PCA, we find the principle components, or linear recombinations of the dimensions of the data, that best explain the variance of the data. There are mathematical arguments abound for describing how we analytically solve for the principle components and how they relate to other concepts in mathematics (like pythagorean theorem). We'll sidestep that conversation for now, and proceed to our pragmatic demonstrations, as we have done in the past. 5.1.1 The Covariance Matrix \u00b6 back to top In the cell below, we have plotted acidity and density from our familiar wine dataset. fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) wine . loc [ wine [ 'red' ] == 1 ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> X = wine . loc [ wine [ 'red' ] == 1 ][[ 'fixed acidity' , 'density' ]] . values X [: 5 ] array([[0.14156636, 0.9978 ], [0.45029132, 0.9968 ], [0.45029132, 0.997 ], [3.07445349, 0.998 ], [0.14156636, 0.9978 ]]) The principal components, are actually the eigenvectors of the covariance matrix of the standardized data. We will perform this by hand to get an understanding. First we standardize the data from sklearn.preprocessing import StandardScaler X_std = StandardScaler () . fit_transform ( X ) # note I've already done this in 5.0.2 Then we compute the covariance matrix. There is a nice demonstration of computing covariance on stats quest . The covariance can be expressed as: cov(X,Y) = \\frac{1}{n^2}\\sum\\sum(x_i - x_j)(y_i - y_j) \\;\\;\\;\\;\\;\\sf eq. 1 Every \\((x_i - x_j)(y_i - y_j)\\) is the area described by the rectangle between points \\(i\\) and \\(j\\), and we if we deem to color positive changes as red and negative ones as blue, we get a picture like the following: When we view all the pairwise interactions in aggregate we get a sense of how the areas of rectangles made by each observation influence the covariance: As a side note, the covariance term is the numerator in the pearsons correlation we covered last week: \\rho_{x,y} = \\frac{cov(X,Y)}{\\sigma_x\\sigma_y} \\;\\;\\;\\;\\;\\sf eq. 2 import numpy as np mean_vec = np . mean ( X_std , axis = 0 ) cov_mat = ( X_std - mean_vec ) . T . dot (( X_std - mean_vec )) / ( X_std . shape [ 0 ] - 1 ) print ( 'Covariance matrix \\n %s ' % cov_mat ) Covariance matrix [[1.00062578 0.66847772] [0.66847772 1.00062578]] As it is a square symmetric matrix, it can be diagonalized by choosing a new orthogonal coordinate system, given by its eigenvectors. For this particular set of wine data, we will see that the corresponding diagonalized matrix will look like: \\begin{bmatrix} 1.67 & 0 \\\\ 0 & 0.33 \\end{bmatrix} Here's the clincher , what the 0's in this square matrix mean is that in this new coordinate system, there is no covariance between features, and the proportion between variances in this new coordinate system can simply be determined by observing the ratio of their eigenvalues (the values located on the diagonal). Armed with this knowledge, we can now proceed to determine the eigenvalues and eigenvectors of the covariance matrix produced from \\(eq. 1\\). Projecting onto the eigenvectors will yield data in a coordinate system that has no covariance, and the explained variance along each coordinate is captured by the eigenvalues. \ud83c\udf2d 5.1.2 Enrichment: Deriving the Eigenvectors and Eigenvalues \u00b6 The principal components are found mathematically by determining the eigenvectors of the covariance matrix and sorting them by their egienvalues, i.e. their explained variance. There is a mathematical proof 1 , 2 for why this works, but we will not cover that here. So how do we calculate eigenvalues and their correpsonding eigenvectors? This question is usually posited in the following format: A \\cdot v = \\lambda \\cdot v \\;\\;\\;\\;\\;\\sf eq. 3 In this equation A is an n-by-n matrix (our covariance matrix in this case), v is a non-zero n-by-1 vector and \u03bb is a scalar (which may be either real or complex). Any value of \u03bb for which this equation has a solution is known as an eigenvalue of the matrix A. In other words v, is an eigenvector of A if there exists a scalar value such that \\(A \\cdot v\\) and \\(\\lambda \\cdot v\\) will yield the same result In high school or college, we might've solved for these eigenvalues and eigenvectors by hand using simple, integer-bound matrices of A. Here's an example, for this matrix: \\begin{bmatrix} -6 & 3 \\\\ 4 & 5\\end{bmatrix} an eigenvector is: \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} with a corresponding eigenvalue of 6. Taking the requisite dot products for each side of eq. 3, \\(A v\\) gives us: \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} and \\(\\lambda v\\): 6 \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24\\end{bmatrix} 5.1.2.1: Find the Eigenvalues \u00b6 The trick that is employed to decompose these equality statements is to multiply the right hand side of eq. 3 by an identity matrix and then subtract this quantity from both sides of the equation. In the case of \\(v\\) being non-zero, this becomes the determinant : | A - \\lambda I | = 0 In the case of our simple example \\begin{vmatrix} \\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\end{vmatrix} = 0 simplifies to \\begin{vmatrix} -6-\\lambda & 3 \\\\ 4 & 5-\\lambda \\end{vmatrix} = 0 writing out the determinant (-6-\\lambda)(5-\\lambda) - 3 x 4 = 0 gives the quadratic equation \\lambda^2 + \\lambda - 42 = 0 and solving for \\(\\lambda\\) \\lambda = -7 \\space or \\space 6 from scipy.optimize import fsolve , leastsq A = np . array ([[ - 6 , 3 ], [ 4 , 5 ]]) I = np . array ([[ 1 , 0 ],[ 0 , 1 ]]) # define the determinant def det ( lamb ): \"\"\" A: the covariance matrix I: the identity matrix \"\"\" return ( A [ 0 , 0 ] - lamb ) * ( A [ 1 , 1 ] - lamb ) - ( A [ 0 , 1 ] * A [ 1 , 0 ]) root = fsolve ( det , [ - 10 , 10 ]) print ( np . isclose ( det ( root ), [ 0.0 , 0.0 ])) print ( root ) [ True True] [-7. 6.] 5.1.2.2: Find the Eigenvectors \u00b6 We find the eigenvector for each corresponding eigenvalue one at a time \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 6 \\begin{bmatrix} x \\\\ y \\end{bmatrix} multiplying out gives the system of equations -6x + 3y = 6x 4x + 5y = 6y bringing to the left hand side -12x + 3y = 0 4x - 1y = 0 solving for either equation yeilds \\(y = 4x\\) so the eigenvector is \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} def eig ( vec ): \"\"\" A: the covariance matrix lamb: the eigen value \"\"\" return [ A [ 0 , 0 ] * vec [ 0 ] + A [ 0 , 1 ] * vec [ 1 ] - lamb * vec [ 0 ], A [ 1 , 0 ] * vec [ 0 ] + A [ 1 , 1 ] * vec [ 1 ] - lamb * vec [ 1 ]] lamb = round ( root [ 0 ]) vector = fsolve ( eig , [ 1 , - 4 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () lamb = round ( root [ 1 ]) vector = fsolve ( eig , [ 1 , 10 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () -7 [ True True] [-3. 1.] 6 [ True True] [1. 4.] # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( A ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.9486833 -0.24253563] [ 0.31622777 -0.9701425 ]] Eigenvalues [-7. 6.] 5.1.3 Projecting onto the Principal Components \u00b6 To complete our principal component analysis, we need to project our data onto the eigenvectors of the covariance matrix. We can oobtain the eigenvectors and corresponding eigenvalues using np or scipy . Here I've completed the task with np : cov_mat = np . cov ( X_std . T ) # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( cov_mat ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.70710678 -0.70710678] [ 0.70710678 -0.70710678]] Eigenvalues [0.33214806 1.6691035 ] And now we are just going to sort our vectors by their eigenvalues to get the proper order of principal components: # Make a list of (eigenvalue, eigenvector) tuples eig_pairs = [( np . abs ( eig_vals [ i ]), eig_vecs [:, i ]) for i in range ( len ( eig_vals ))] # Sort the (eigenvalue, eigenvector) tuples from high to low eig_pairs . sort ( key = lambda x : x [ 0 ], reverse = True ) # Visually confirm that the list is correctly sorted by decreasing eigenvalues print ( 'Eigenvalues in descending order:' ) for i in eig_pairs : print ( i [ 0 ]) Eigenvalues in descending order: 1.669103500110071 0.3321480643454986 eig_pairs [(1.669103500110071, array([-0.70710678, -0.70710678])), (0.3321480643454986, array([-0.70710678, 0.70710678]))] For those of us who are familiar with eigenvectors, we should recognize that they are necessarily orthogonal to one another. This is good to know from the PCA point of view, because we wouldn't want the variance along one vector to be explained by another. Let's plot these vectors along with the standardized data: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 7 )) ax . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . set_aspect ( 'equal' ) ax . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) (-2.138871623907465, 4.356979103463171) We indeed see that these vectors are orthogonal. For further discussion on the topic of PCA and how it relates to concepts like RSS and Pythagorean Theorem I suggest reading the grandparent, spouse, daughter parable Continuing on with our task of projecting the data onto our principal components, in order to project our data onto the PCs I'll need to reshape eig_pairs : matrix_w = np . hstack (( eig_pairs [ 0 ][ 1 ] . reshape ( 2 , 1 ), eig_pairs [ 1 ][ 1 ] . reshape ( 2 , 1 ))) print ( 'Matrix W: \\n ' , matrix_w ) Matrix W: [[-0.70710678 -0.70710678] [-0.70710678 0.70710678]] And now taking the dot product: Y = X_std . dot ( matrix_w ) plt . scatter ( Y [:, 0 ], Y [:, 1 ]) <matplotlib.collections.PathCollection at 0x7f99baa40190> We see that our data is dispersed nicely along these PCs. 5.1.4 Cumulative Explained Variance \u00b6 Often we will need to decide just how many principal components are enough, especially with high dimensional data containing many colinear variables. To assist with this, data scientists will plot the cumulative explained variance. The explained variance is captured by the eigenvalues (this is why we sort by the eigenvalues in the first place). tot = sum ( eig_vals ) var_exp = [( i / tot ) * 100 for i in sorted ( eig_vals , reverse = True )] cum_var_exp = np . cumsum ( var_exp ) with plt . style . context ( 'seaborn-whitegrid' ): plt . figure ( figsize = ( 7 , 4 )) plt . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'individual explained variance' ) plt . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'cumulative explained variance' ) plt . ylabel ( 'Explained variance ratio' ) plt . xlabel ( 'Principal components' ) plt . legend ( loc = 'center right' ) plt . tight_layout () 5.1.5 PCA with Scikit-Learn \u00b6 But we can avoid the fancy footwork and do all this in sklearn! from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) We see that the values we get are the same as for the hand-calculated eigenvalues and vectors print ( pca . components_ ) [[ 0.70710678 0.70710678] [ 0.70710678 -0.70710678]] And the eigenvalues are under pca.explained_variance_ print ( pca . explained_variance_ ) [1.6691035 0.33214806] 5.1.6 PCA as Dimensionality Reduction \u00b6 back to top One obvious use case for PCA, is to drop the dimensions with lowest explained variance. Continuing with our acidity/density example this will look like the following from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . set_aspect ( 'equal' ) ax [ 0 ] . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax [ 0 ] . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) ax [ 0 ] . set_ylabel ( 'Normalized density' ) ax [ 0 ] . set_xlabel ( 'Normalized acidity' ) ax [ 1 ] . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'Individual' ) ax [ 1 ] . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'Cumulative' ) ax [ 1 ] . set_ylabel ( 'Explained variance ratio' ) ax [ 1 ] . set_xlabel ( 'Principal components' ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f99baaf0100> We can capture 80% of the explained variance along just the first principal component. What does this projection look like? # we set our components to 1 pca = PCA ( n_components = 1 ) pca . fit ( X_std ) # we then project the data onto the first PC # and then rebroadcast this transformation # back onto the orginal dimensions to see # what this looks like in terms of acidity/density X_pca = pca . inverse_transform ( pca . transform ( X_std )) # original data plt . scatter ( X_std [:, 0 ], X_std [:, 1 ], alpha = 0.2 ) # projected data plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.8 ) <matplotlib.collections.PathCollection at 0x7f99bd36a9a0> 5.1.7 PCA for visualization \u00b6 back to top For classification tasks, PCA lends itself as a useful method for seeing how the classes separate on the highest variance dimensions of the data without consideration to the classes themselves, i.e. do our classes seperate out according to the other variables in the dataset. With our wine dataset, we see that the wine types fall out nicely along the first two principal components X_std = StandardScaler () . fit_transform ( wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = wine [ 'white' ] . values , edgecolor = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC') \ud83c\udf2d 5.1.8 Enrichment: PCA as Outlier Removal and Noise Filtering \u00b6 back to top In some cases, it can be advantageous to use PCA as a method for outlier removal. There are many caveats to this that we will not discuss here. But for those who are interested, this is a great overview article. As for noise filteration, Vanderplas' DS handbook has a good section on the topic as does Guido/Muller's Intro to ML with Python by the same pusblisher. 5.1.9 PCA for Feature Engineering \u00b6 back to top Finally, PCA is actually a commonly used preprocessing technique for supervised machine learning models. In the next exercise, our goal will be to use PCA to generate a new set of features, to feed into our linear model. \ud83c\udfcb\ufe0f Exercise 1: PCA as Preprocessing for Models \u00b6 Using the wine data, select any number of the first principal components and attemp to predict density for the red wine data. Consier that the maximum number of principal components are: print ( f \"Max principal components: { X . shape [ 1 ] } \" ) Max principal components: 14 # Code Cell for Exercise 1 ################################################################################ ##### CREATE A SKLEARN-PCA OBJECT, FIT AND TRANSFORM TO THE WINE DATA ########## ################################################################################ pca = PCA ( n_components = 12 ) pca . fit ( X_wine ) X_pca = pca . transform ( X_wine ) ################################################################################ ############################## DO NOT CHANGE BELOW ############################ ################################################################################ plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = wine [ 'white' ] . values , edgecolor = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) plt . show () model = LinearRegression () X_train , X_test , y_train , y_test = train_test_split ( X_pca , y_wine , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( r2_score ( y_test , y_pred )) print ( r2_score ( y_train , model . predict ( X_train ))) 0.9634516142421967 0.953295487875815 5.2 K-Means Clustering \u00b6 back to top We now embark on a second class of unsupervised learning techinques: clustering. The K-means algorithm works under two assumptions: * every cluster can be defined by an arithmetic mean or cluster center * each point is closer to one arithmetic center than the other centers Let's turn back to our wine dataset: X_std = StandardScaler () . fit_transform ( X_wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC') It is fairly obvious under these two dimensions that there are two clusters. The K-means algorithm automatically pics this out from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = 2 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); wa-la! Our simple unsupervised algorithm does a pretty good job of differentiating between the red and white wine clusters. 5.2.1 The Algorithm: Expectation-Maximization \u00b6 back to top We won't go too far into the implementation of the underlying algorithm here. In its basic steps, however, it performs the following: Initialize cluster centers (random guess) Then repeat: E-Step: assign points to the nearest center (arithmetic distance) M-step: set the new center point for each cluster according to the mean of it's datapoint members More information on K-means algorithm can be explored here 5.2.2 Limitations \u00b6 back to top A few brief notes on limitations: the global optimum may not be achieved (no guarantee of finding the overall best solution) the number of clusters must be guessed beforehand cluster boundaries are unavoidably linear and the cluster assignments are unavoidably circular can be slow for large datasets cluster assignments are non probabilistic 3 and 5 motivate our next section, Gaussian Mixture Models 5.2.3 Determining K with the Elbow Method \u00b6 The elbow method is a popular technique for determining the value of k . It involves looping through a range of k 's and assessing some goodness of fit metric. Intuitively, we might presume those metrics involve some measure of the distance of datapoints to their cluster centers. We have options: Distortion: distance ( typically Euclidean ) from the cluster centers averaged across the respective clusters. Inertia: the sum of squared distances of samples to their closest cluster center. Silhouette: calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of The Silhouette Coefficient for a sample is (b-a) / max(b-a) best value is 1 worst value is -1 Calinski Harabasz Score or Variance Ratio: the ratio between within-cluster dispersion and between-cluster dispersion should sound familiar to our ANOVA discussion higher is better And there are many other methods of evaluating cluster assignment performance \ud83d\ude4b\u200d\u2640\ufe0f Question 1 \u00b6 What is the primary difference between Distortion, Intertia vs Silhouette, Calinksi? distortions = [] inertias = [] silhouette = [] variance = [] for k in range ( 1 , 10 ): kmeans = KMeans ( n_clusters = k ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) labels = kmeans . labels_ distortions . append ( sum ( np . min ( cdist ( X_pca , kmeans . cluster_centers_ , 'euclidean' ), axis = 1 )) / X . shape [ 0 ]) inertias . append ( kmeans . inertia_ ) if k > 1 : silhouette . append ( silhouette_score ( X_pca , labels , metric = 'euclidean' )) variance . append ( calinski_harabasz_score ( X_pca , labels )) We note in the following plots that inertia and distortion asymptotically improve with higher k (because they are unregularized) whereas silhouette and calinski metrics are penalized for inter-cluster relatability fig , [[ ax1 , ax2 ], [ ax3 , ax4 ]] = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) ax1 . plot ( range ( 1 , 10 ), distortions ) ax2 . plot ( range ( 1 , 10 ), inertias ) ax3 . plot ( range ( 2 , 10 ), silhouette ) ax4 . plot ( range ( 2 , 10 ), variance ) [<matplotlib.lines.Line2D at 0x7f99b02e5430>] kmeans = KMeans ( n_clusters = 3 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); 5.3 Gaussian Mixture Models \u00b6 back to top in the simplest case, GMMs can be used in the same way as K-means from sklearn.mixture import GaussianMixture gmm = GaussianMixture ( n_components = 2 ) . fit ( X_pca ) labels = gmm . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , alpha = 0.2 , edgecolor = 'grey' ); But because GMM is a probablistic model, we can visualize the certainty with which we assign these clusters. probs = gmm . predict_proba ( X_pca ) print ( probs [ 5 : 20 ] . round ( 3 )) [[1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.]] # convert probs to 1 dimension probs . max ( 1 ) array([1. , 0.99999994, 0.99999999, ..., 1. , 1. , 0.99999189]) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = probs . max ( 1 ), s = 40 , cmap = 'Blues' , alpha = 0.5 , edgecolor = 'grey' ); 5.3.1 Generalizing E-M for GMMs \u00b6 back to top The algorithm for GMMs is very similar to K-means, but now the EM steps are probablistic 1 2 Initialize cluster centers (random guess) Then repeat: E-Step: assign points their probability of belonging to every cluster M-step: set the new center point for each cluster according to the probabilities of all datapoints 5.3.2 GMMs as a Data Generator \u00b6 back to top One particularly fascinating application of GMMs is to use them to generate new data that is similar to the data on which they are modeled. Let's take an example. We're going to use sklearn make_circles function to create some arbitrary data that has a complex relationship along two different axes. # some helper functions borrowed from Jake Vanderplas with a few minor tweaks # https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html from matplotlib.patches import Ellipse def draw_ellipse ( position , covariance , ax = None , ** kwargs ): \"\"\"Draw an ellipse with a given position and covariance\"\"\" ax = ax or plt . gca () # Convert covariance to principal axes if covariance . shape == ( 2 , 2 ): U , s , Vt = np . linalg . svd ( covariance ) angle = np . degrees ( np . arctan2 ( U [ 1 , 0 ], U [ 0 , 0 ])) width , height = 2 * np . sqrt ( s ) else : angle = 0 width , height = 2 * np . sqrt ( covariance ) # Draw the Ellipse for nsig in range ( 1 , 4 ): ax . add_patch ( Ellipse ( position , nsig * width , nsig * height , angle , ** kwargs )) def plot_gmm ( gmm , X , label = True , ax = None , data_alpha = 1 ): ax = ax or plt . gca () labels = gmm . fit ( X ) . predict ( X ) if label : ax . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , zorder = 2 , alpha = data_alpha ) else : ax . scatter ( X [:, 0 ], X [:, 1 ], s = 40 , zorder = 2 , alpha = data_alpha ) ax . axis ( 'equal' ) w_factor = 0.2 / gmm . weights_ . max () for pos , covar , w in zip ( gmm . means_ , gmm . covariances_ , gmm . weights_ ): draw_ellipse ( pos , covar , alpha = w * w_factor ) from sklearn.datasets import make_circles as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ]); As a side note, as a clustering model, the GMM is not particularly useful: gmm2 = GaussianMixture ( n_components = 2 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm2 , X ) But if we use the GMM as a density estimator on the underlying data, rather than as a clustering algorithm, we find a completely different use case: gmm16 = GaussianMixture ( n_components = 16 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm16 , X , label = False ) Now, with the distributions drawn, we can assemble entirely new data: Xnew = gmm16 . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); 5.3.2.1 Determining the number of components \u00b6 back to top Let's think back to session 1 on model selection. How might we determine the best number of components? A couple analytic approaches that we have not much yet discussed, are the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The important thing to note from each of these is that they penalize added complexity to the models, and we would like to minimize their estimated prediction error. n_components = np . arange ( 1 , 42 ) models = [ GaussianMixture ( n , covariance_type = 'full' , random_state = 42 ) . fit ( X ) for n in n_components ] plt . plot ( n_components , [ m . bic ( X ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( X ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); plt . ylabel ( 'est. prediction error' ) Text(0, 0.5, 'est. prediction error') Both AIC and BIC capture a local minimum on the order of 6 Gaussians. This is where the model captures the overall circular modality of the data. However it totally misses the circumscribed nature of the two circles. Increasing this number to 30-40 gaussians captures this feature of the data. The AIC score reflects this while the BIC score (although captures a local minima in the area) does not define this as a global optimum. gmmNew = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmmNew , X , label = True , data_alpha = 0 ) Xnew = gmmNew . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); \ud83c\udfcb\ufe0f Exercise 2: Determine Number of Components for Circular Moons \u00b6 Repeat the above, this time using sklearn.datasets.make_moons # Code Cell for Exercise 2 from sklearn.datasets import make_moons as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) n_components = np . arange ( 1 , 42 ) models = [ GaussianMixture ( n , covariance_type = 'full' , random_state = 42 ) . fit ( X ) for n in n_components ] plt . plot ( n_components , [ m . bic ( X ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( X ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); plt . ylabel ( 'est. prediction error' ) Text(0, 0.5, 'est. prediction error') gmm_moon = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm_moon , X ) Xnew = gmm_moon . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); References \u00b6 PCA \u00b6 Intuitive PCA PCA and Eigenvectors/values GMM \u00b6 GMMs Explained Derive GMM Exercise","title":"SOLN S5 Unsupervised Learning"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#data-science-foundations-session-5-unsupervised-learning-clustering-and-dimensionality-reduction","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In the previous session we began our discussion on feature engineering and ended with a sneak peak into dimensionality reduction. This last topic deserves a whole session on its own because its use case is not limited to feature engineering! It can also be used as a tool for visualization, for noise filtering, and much more. In this session we discuss dimensionality reduction along with other unsupervised learning methods. Up until now, the only learning estimators we've looked at were supervised ones: estimators that predict labels based on training data. Here, however, we are interested in uncovering aspects of the data without reference to any known labels. The usefulness for these learners will become immediately apparent when we revist our wine quality models from Course 1, Session 7!","title":"Data Science Foundations, Session 5: Unsupervised Learning: Clustering and Dimensionality Reduction"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#50-preparing-environment-and-importing-data","text":"back to top","title":"5.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#501-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score , silhouette_score , calinski_harabasz_score from sklearn.mixture import GaussianMixture from sklearn.impute import SimpleImputer from scipy.spatial.distance import cdist import seaborn as sns ; sns . set () import copy","title":"5.0.1 Import Packages"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#502-load-and-process-dataset","text":"back to top wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y_wine = X . pop ( target ) X = imp . fit_transform ( X ) X_wine = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X_wine [:, 2 :] = scaler . fit_transform ( X_wine [:, 2 :]) wine = pd . DataFrame ( X_wine , columns = cols ) wine [ target ] = y wine . dropna ( inplace = True ) wine [ 'quality_label' ] = wine [ 'quality' ] . apply ( lambda x : 'low' if x <= 5 else 'med' if x <= 7 else 'high' ) wine [ 'type_encoding' ] = wine [ 'type' ] . map ({ 'red' : 0 , 'white' : 1 }) wine [ 'quality_encoding' ] = wine [ 'quality_label' ] . map ({ 'low' : 0 , 'med' : 1 , 'high' : 2 }) wine . columns = wine . columns . str . replace ( ' ' , '_' ) features = list ( wine . columns [ 1 : - 1 ] . values ) features . remove ( 'quality_label' ) features . remove ( 'quality' )","title":"5.0.2 Load and Process Dataset"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#51-principal-component-analysis","text":"back to top Principle Component Analysis or PCA is one of the most wide spread implementations of dimensionality reduction. In PCA, we find the principle components, or linear recombinations of the dimensions of the data, that best explain the variance of the data. There are mathematical arguments abound for describing how we analytically solve for the principle components and how they relate to other concepts in mathematics (like pythagorean theorem). We'll sidestep that conversation for now, and proceed to our pragmatic demonstrations, as we have done in the past.","title":"5.1 Principal Component Analysis"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#511-the-covariance-matrix","text":"back to top In the cell below, we have plotted acidity and density from our familiar wine dataset. fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 )) wine . loc [ wine [ 'red' ] == 1 ] . plot ( x = 'fixed acidity' , y = 'density' , ax = ax , ls = '' , marker = '.' ) <AxesSubplot:xlabel='fixed acidity'> X = wine . loc [ wine [ 'red' ] == 1 ][[ 'fixed acidity' , 'density' ]] . values X [: 5 ] array([[0.14156636, 0.9978 ], [0.45029132, 0.9968 ], [0.45029132, 0.997 ], [3.07445349, 0.998 ], [0.14156636, 0.9978 ]]) The principal components, are actually the eigenvectors of the covariance matrix of the standardized data. We will perform this by hand to get an understanding. First we standardize the data from sklearn.preprocessing import StandardScaler X_std = StandardScaler () . fit_transform ( X ) # note I've already done this in 5.0.2 Then we compute the covariance matrix. There is a nice demonstration of computing covariance on stats quest . The covariance can be expressed as: cov(X,Y) = \\frac{1}{n^2}\\sum\\sum(x_i - x_j)(y_i - y_j) \\;\\;\\;\\;\\;\\sf eq. 1 Every \\((x_i - x_j)(y_i - y_j)\\) is the area described by the rectangle between points \\(i\\) and \\(j\\), and we if we deem to color positive changes as red and negative ones as blue, we get a picture like the following: When we view all the pairwise interactions in aggregate we get a sense of how the areas of rectangles made by each observation influence the covariance: As a side note, the covariance term is the numerator in the pearsons correlation we covered last week: \\rho_{x,y} = \\frac{cov(X,Y)}{\\sigma_x\\sigma_y} \\;\\;\\;\\;\\;\\sf eq. 2 import numpy as np mean_vec = np . mean ( X_std , axis = 0 ) cov_mat = ( X_std - mean_vec ) . T . dot (( X_std - mean_vec )) / ( X_std . shape [ 0 ] - 1 ) print ( 'Covariance matrix \\n %s ' % cov_mat ) Covariance matrix [[1.00062578 0.66847772] [0.66847772 1.00062578]] As it is a square symmetric matrix, it can be diagonalized by choosing a new orthogonal coordinate system, given by its eigenvectors. For this particular set of wine data, we will see that the corresponding diagonalized matrix will look like: \\begin{bmatrix} 1.67 & 0 \\\\ 0 & 0.33 \\end{bmatrix} Here's the clincher , what the 0's in this square matrix mean is that in this new coordinate system, there is no covariance between features, and the proportion between variances in this new coordinate system can simply be determined by observing the ratio of their eigenvalues (the values located on the diagonal). Armed with this knowledge, we can now proceed to determine the eigenvalues and eigenvectors of the covariance matrix produced from \\(eq. 1\\). Projecting onto the eigenvectors will yield data in a coordinate system that has no covariance, and the explained variance along each coordinate is captured by the eigenvalues.","title":"5.1.1 The Covariance Matrix"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#512-enrichment-deriving-the-eigenvectors-and-eigenvalues","text":"The principal components are found mathematically by determining the eigenvectors of the covariance matrix and sorting them by their egienvalues, i.e. their explained variance. There is a mathematical proof 1 , 2 for why this works, but we will not cover that here. So how do we calculate eigenvalues and their correpsonding eigenvectors? This question is usually posited in the following format: A \\cdot v = \\lambda \\cdot v \\;\\;\\;\\;\\;\\sf eq. 3 In this equation A is an n-by-n matrix (our covariance matrix in this case), v is a non-zero n-by-1 vector and \u03bb is a scalar (which may be either real or complex). Any value of \u03bb for which this equation has a solution is known as an eigenvalue of the matrix A. In other words v, is an eigenvector of A if there exists a scalar value such that \\(A \\cdot v\\) and \\(\\lambda \\cdot v\\) will yield the same result In high school or college, we might've solved for these eigenvalues and eigenvectors by hand using simple, integer-bound matrices of A. Here's an example, for this matrix: \\begin{bmatrix} -6 & 3 \\\\ 4 & 5\\end{bmatrix} an eigenvector is: \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} with a corresponding eigenvalue of 6. Taking the requisite dot products for each side of eq. 3, \\(A v\\) gives us: \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} and \\(\\lambda v\\): 6 \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24\\end{bmatrix}","title":"\ud83c\udf2d 5.1.2 Enrichment: Deriving the Eigenvectors and Eigenvalues"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#5121-find-the-eigenvalues","text":"The trick that is employed to decompose these equality statements is to multiply the right hand side of eq. 3 by an identity matrix and then subtract this quantity from both sides of the equation. In the case of \\(v\\) being non-zero, this becomes the determinant : | A - \\lambda I | = 0 In the case of our simple example \\begin{vmatrix} \\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\end{vmatrix} = 0 simplifies to \\begin{vmatrix} -6-\\lambda & 3 \\\\ 4 & 5-\\lambda \\end{vmatrix} = 0 writing out the determinant (-6-\\lambda)(5-\\lambda) - 3 x 4 = 0 gives the quadratic equation \\lambda^2 + \\lambda - 42 = 0 and solving for \\(\\lambda\\) \\lambda = -7 \\space or \\space 6 from scipy.optimize import fsolve , leastsq A = np . array ([[ - 6 , 3 ], [ 4 , 5 ]]) I = np . array ([[ 1 , 0 ],[ 0 , 1 ]]) # define the determinant def det ( lamb ): \"\"\" A: the covariance matrix I: the identity matrix \"\"\" return ( A [ 0 , 0 ] - lamb ) * ( A [ 1 , 1 ] - lamb ) - ( A [ 0 , 1 ] * A [ 1 , 0 ]) root = fsolve ( det , [ - 10 , 10 ]) print ( np . isclose ( det ( root ), [ 0.0 , 0.0 ])) print ( root ) [ True True] [-7. 6.]","title":"5.1.2.1: Find the Eigenvalues"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#5122-find-the-eigenvectors","text":"We find the eigenvector for each corresponding eigenvalue one at a time \\begin{bmatrix} -6 & 3 & \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 6 \\begin{bmatrix} x \\\\ y \\end{bmatrix} multiplying out gives the system of equations -6x + 3y = 6x 4x + 5y = 6y bringing to the left hand side -12x + 3y = 0 4x - 1y = 0 solving for either equation yeilds \\(y = 4x\\) so the eigenvector is \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} def eig ( vec ): \"\"\" A: the covariance matrix lamb: the eigen value \"\"\" return [ A [ 0 , 0 ] * vec [ 0 ] + A [ 0 , 1 ] * vec [ 1 ] - lamb * vec [ 0 ], A [ 1 , 0 ] * vec [ 0 ] + A [ 1 , 1 ] * vec [ 1 ] - lamb * vec [ 1 ]] lamb = round ( root [ 0 ]) vector = fsolve ( eig , [ 1 , - 4 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () lamb = round ( root [ 1 ]) vector = fsolve ( eig , [ 1 , 10 ]) print ( lamb ) print ( np . isclose ( eig ( vector ), [ 0.0 , 0.0 ])) vector [ np . argmax ( vector )] = int ( round ( max ( vector ) / min ( vector ))) vector [ np . argmin ( vector )] = 1 print ( vector ) print () -7 [ True True] [-3. 1.] 6 [ True True] [1. 4.] # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( A ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.9486833 -0.24253563] [ 0.31622777 -0.9701425 ]] Eigenvalues [-7. 6.]","title":"5.1.2.2: Find the Eigenvectors"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#513-projecting-onto-the-principal-components","text":"To complete our principal component analysis, we need to project our data onto the eigenvectors of the covariance matrix. We can oobtain the eigenvectors and corresponding eigenvalues using np or scipy . Here I've completed the task with np : cov_mat = np . cov ( X_std . T ) # we can solve for the eigenvalues/vectors of our covariance # matrix using numpy! eig_vals , eig_vecs = np . linalg . eig ( cov_mat ) print ( 'Eigenvectors \\n %s ' % eig_vecs ) print ( ' \\n Eigenvalues \\n %s ' % eig_vals ) Eigenvectors [[-0.70710678 -0.70710678] [ 0.70710678 -0.70710678]] Eigenvalues [0.33214806 1.6691035 ] And now we are just going to sort our vectors by their eigenvalues to get the proper order of principal components: # Make a list of (eigenvalue, eigenvector) tuples eig_pairs = [( np . abs ( eig_vals [ i ]), eig_vecs [:, i ]) for i in range ( len ( eig_vals ))] # Sort the (eigenvalue, eigenvector) tuples from high to low eig_pairs . sort ( key = lambda x : x [ 0 ], reverse = True ) # Visually confirm that the list is correctly sorted by decreasing eigenvalues print ( 'Eigenvalues in descending order:' ) for i in eig_pairs : print ( i [ 0 ]) Eigenvalues in descending order: 1.669103500110071 0.3321480643454986 eig_pairs [(1.669103500110071, array([-0.70710678, -0.70710678])), (0.3321480643454986, array([-0.70710678, 0.70710678]))] For those of us who are familiar with eigenvectors, we should recognize that they are necessarily orthogonal to one another. This is good to know from the PCA point of view, because we wouldn't want the variance along one vector to be explained by another. Let's plot these vectors along with the standardized data: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 7 )) ax . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax . set_aspect ( 'equal' ) ax . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) (-2.138871623907465, 4.356979103463171) We indeed see that these vectors are orthogonal. For further discussion on the topic of PCA and how it relates to concepts like RSS and Pythagorean Theorem I suggest reading the grandparent, spouse, daughter parable Continuing on with our task of projecting the data onto our principal components, in order to project our data onto the PCs I'll need to reshape eig_pairs : matrix_w = np . hstack (( eig_pairs [ 0 ][ 1 ] . reshape ( 2 , 1 ), eig_pairs [ 1 ][ 1 ] . reshape ( 2 , 1 ))) print ( 'Matrix W: \\n ' , matrix_w ) Matrix W: [[-0.70710678 -0.70710678] [-0.70710678 0.70710678]] And now taking the dot product: Y = X_std . dot ( matrix_w ) plt . scatter ( Y [:, 0 ], Y [:, 1 ]) <matplotlib.collections.PathCollection at 0x7f99baa40190> We see that our data is dispersed nicely along these PCs.","title":"5.1.3 Projecting onto the Principal Components"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#514-cumulative-explained-variance","text":"Often we will need to decide just how many principal components are enough, especially with high dimensional data containing many colinear variables. To assist with this, data scientists will plot the cumulative explained variance. The explained variance is captured by the eigenvalues (this is why we sort by the eigenvalues in the first place). tot = sum ( eig_vals ) var_exp = [( i / tot ) * 100 for i in sorted ( eig_vals , reverse = True )] cum_var_exp = np . cumsum ( var_exp ) with plt . style . context ( 'seaborn-whitegrid' ): plt . figure ( figsize = ( 7 , 4 )) plt . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'individual explained variance' ) plt . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'cumulative explained variance' ) plt . ylabel ( 'Explained variance ratio' ) plt . xlabel ( 'Principal components' ) plt . legend ( loc = 'center right' ) plt . tight_layout ()","title":"5.1.4 Cumulative Explained Variance"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#515-pca-with-scikit-learn","text":"But we can avoid the fancy footwork and do all this in sklearn! from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) We see that the values we get are the same as for the hand-calculated eigenvalues and vectors print ( pca . components_ ) [[ 0.70710678 0.70710678] [ 0.70710678 -0.70710678]] And the eigenvalues are under pca.explained_variance_ print ( pca . explained_variance_ ) [1.6691035 0.33214806]","title":"5.1.5 PCA with Scikit-Learn"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#516-pca-as-dimensionality-reduction","text":"back to top One obvious use case for PCA, is to drop the dimensions with lowest explained variance. Continuing with our acidity/density example this will look like the following from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X_std ) PCA(n_components=2) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( X_std [:, 0 ], X_std [:, 1 ], ls = '' , marker = '.' , alpha = 0.5 ) for vec , color in zip ( range ( eig_vecs . shape [ 0 ]),[ 'orange' , 'green' ]): ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) *- eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) *- eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . plot ([ np . mean ( X_std [:, 0 ]), ( eig_pairs [ vec ][ 1 ][ 0 ] + np . mean ( X_std [:, 0 ])) * eig_pairs [ vec ][ 0 ]], [ np . mean ( X_std [:, 1 ]), ( eig_pairs [ vec ][ 1 ][ 1 ] + np . mean ( X_std [:, 1 ])) * eig_pairs [ vec ][ 0 ]], color = f 'tab: { color } ' , linewidth = 4 ) ax [ 0 ] . set_aspect ( 'equal' ) ax [ 0 ] . set_ylim ( min ( X_std [:, 1 ]), max ( X_std [:, 1 ])) ax [ 0 ] . set_xlim ( min ( X_std [:, 0 ]), max ( X_std [:, 0 ])) ax [ 0 ] . set_ylabel ( 'Normalized density' ) ax [ 0 ] . set_xlabel ( 'Normalized acidity' ) ax [ 1 ] . bar ( range ( 2 ), var_exp , alpha = 0.5 , align = 'center' , label = 'Individual' ) ax [ 1 ] . step ( range ( 2 ), cum_var_exp , where = 'mid' , label = 'Cumulative' ) ax [ 1 ] . set_ylabel ( 'Explained variance ratio' ) ax [ 1 ] . set_xlabel ( 'Principal components' ) ax [ 1 ] . legend () <matplotlib.legend.Legend at 0x7f99baaf0100> We can capture 80% of the explained variance along just the first principal component. What does this projection look like? # we set our components to 1 pca = PCA ( n_components = 1 ) pca . fit ( X_std ) # we then project the data onto the first PC # and then rebroadcast this transformation # back onto the orginal dimensions to see # what this looks like in terms of acidity/density X_pca = pca . inverse_transform ( pca . transform ( X_std )) # original data plt . scatter ( X_std [:, 0 ], X_std [:, 1 ], alpha = 0.2 ) # projected data plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.8 ) <matplotlib.collections.PathCollection at 0x7f99bd36a9a0>","title":"5.1.6 PCA as Dimensionality Reduction"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#517-pca-for-visualization","text":"back to top For classification tasks, PCA lends itself as a useful method for seeing how the classes separate on the highest variance dimensions of the data without consideration to the classes themselves, i.e. do our classes seperate out according to the other variables in the dataset. With our wine dataset, we see that the wine types fall out nicely along the first two principal components X_std = StandardScaler () . fit_transform ( wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = wine [ 'white' ] . values , edgecolor = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC')","title":"5.1.7 PCA for visualization"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#518-enrichment-pca-as-outlier-removal-and-noise-filtering","text":"back to top In some cases, it can be advantageous to use PCA as a method for outlier removal. There are many caveats to this that we will not discuss here. But for those who are interested, this is a great overview article. As for noise filteration, Vanderplas' DS handbook has a good section on the topic as does Guido/Muller's Intro to ML with Python by the same pusblisher.","title":"\ud83c\udf2d 5.1.8 Enrichment: PCA as Outlier Removal and Noise Filtering"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#519-pca-for-feature-engineering","text":"back to top Finally, PCA is actually a commonly used preprocessing technique for supervised machine learning models. In the next exercise, our goal will be to use PCA to generate a new set of features, to feed into our linear model.","title":"5.1.9 PCA for Feature Engineering"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#exercise-1-pca-as-preprocessing-for-models","text":"Using the wine data, select any number of the first principal components and attemp to predict density for the red wine data. Consier that the maximum number of principal components are: print ( f \"Max principal components: { X . shape [ 1 ] } \" ) Max principal components: 14 # Code Cell for Exercise 1 ################################################################################ ##### CREATE A SKLEARN-PCA OBJECT, FIT AND TRANSFORM TO THE WINE DATA ########## ################################################################################ pca = PCA ( n_components = 12 ) pca . fit ( X_wine ) X_pca = pca . transform ( X_wine ) ################################################################################ ############################## DO NOT CHANGE BELOW ############################ ################################################################################ plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = wine [ 'white' ] . values , edgecolor = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) plt . show () model = LinearRegression () X_train , X_test , y_train , y_test = train_test_split ( X_pca , y_wine , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( r2_score ( y_test , y_pred )) print ( r2_score ( y_train , model . predict ( X_train ))) 0.9634516142421967 0.953295487875815","title":"\ud83c\udfcb\ufe0f Exercise 1: PCA as Preprocessing for Models"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#52-k-means-clustering","text":"back to top We now embark on a second class of unsupervised learning techinques: clustering. The K-means algorithm works under two assumptions: * every cluster can be defined by an arithmetic mean or cluster center * each point is closer to one arithmetic center than the other centers Let's turn back to our wine dataset: X_std = StandardScaler () . fit_transform ( X_wine ) pca = PCA ( n_components = 2 ) pca . fit ( X_std ) X_pca = pca . transform ( X_std ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], alpha = 0.2 , c = 'grey' ) plt . xlabel ( 'First PC' ) plt . ylabel ( 'Second PC' ) Text(0, 0.5, 'Second PC') It is fairly obvious under these two dimensions that there are two clusters. The K-means algorithm automatically pics this out from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = 2 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); wa-la! Our simple unsupervised algorithm does a pretty good job of differentiating between the red and white wine clusters.","title":"5.2 K-Means Clustering"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#521-the-algorithm-expectation-maximization","text":"back to top We won't go too far into the implementation of the underlying algorithm here. In its basic steps, however, it performs the following: Initialize cluster centers (random guess) Then repeat: E-Step: assign points to the nearest center (arithmetic distance) M-step: set the new center point for each cluster according to the mean of it's datapoint members More information on K-means algorithm can be explored here","title":"5.2.1 The Algorithm: Expectation-Maximization"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#522-limitations","text":"back to top A few brief notes on limitations: the global optimum may not be achieved (no guarantee of finding the overall best solution) the number of clusters must be guessed beforehand cluster boundaries are unavoidably linear and the cluster assignments are unavoidably circular can be slow for large datasets cluster assignments are non probabilistic 3 and 5 motivate our next section, Gaussian Mixture Models","title":"5.2.2 Limitations"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#523-determining-k-with-the-elbow-method","text":"The elbow method is a popular technique for determining the value of k . It involves looping through a range of k 's and assessing some goodness of fit metric. Intuitively, we might presume those metrics involve some measure of the distance of datapoints to their cluster centers. We have options: Distortion: distance ( typically Euclidean ) from the cluster centers averaged across the respective clusters. Inertia: the sum of squared distances of samples to their closest cluster center. Silhouette: calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of The Silhouette Coefficient for a sample is (b-a) / max(b-a) best value is 1 worst value is -1 Calinski Harabasz Score or Variance Ratio: the ratio between within-cluster dispersion and between-cluster dispersion should sound familiar to our ANOVA discussion higher is better And there are many other methods of evaluating cluster assignment performance","title":"5.2.3 Determining K with the Elbow Method"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#question-1","text":"What is the primary difference between Distortion, Intertia vs Silhouette, Calinksi? distortions = [] inertias = [] silhouette = [] variance = [] for k in range ( 1 , 10 ): kmeans = KMeans ( n_clusters = k ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) labels = kmeans . labels_ distortions . append ( sum ( np . min ( cdist ( X_pca , kmeans . cluster_centers_ , 'euclidean' ), axis = 1 )) / X . shape [ 0 ]) inertias . append ( kmeans . inertia_ ) if k > 1 : silhouette . append ( silhouette_score ( X_pca , labels , metric = 'euclidean' )) variance . append ( calinski_harabasz_score ( X_pca , labels )) We note in the following plots that inertia and distortion asymptotically improve with higher k (because they are unregularized) whereas silhouette and calinski metrics are penalized for inter-cluster relatability fig , [[ ax1 , ax2 ], [ ax3 , ax4 ]] = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) ax1 . plot ( range ( 1 , 10 ), distortions ) ax2 . plot ( range ( 1 , 10 ), inertias ) ax3 . plot ( range ( 2 , 10 ), silhouette ) ax4 . plot ( range ( 2 , 10 ), variance ) [<matplotlib.lines.Line2D at 0x7f99b02e5430>] kmeans = KMeans ( n_clusters = 3 ) kmeans . fit ( X_pca ) y_kmeans = kmeans . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = y_kmeans , s = 50 , alpha = 0.5 , edgecolor = 'grey' , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 );","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#53-gaussian-mixture-models","text":"back to top in the simplest case, GMMs can be used in the same way as K-means from sklearn.mixture import GaussianMixture gmm = GaussianMixture ( n_components = 2 ) . fit ( X_pca ) labels = gmm . predict ( X_pca ) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , alpha = 0.2 , edgecolor = 'grey' ); But because GMM is a probablistic model, we can visualize the certainty with which we assign these clusters. probs = gmm . predict_proba ( X_pca ) print ( probs [ 5 : 20 ] . round ( 3 )) [[1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.]] # convert probs to 1 dimension probs . max ( 1 ) array([1. , 0.99999994, 0.99999999, ..., 1. , 1. , 0.99999189]) plt . scatter ( X_pca [:, 0 ], X_pca [:, 1 ], c = probs . max ( 1 ), s = 40 , cmap = 'Blues' , alpha = 0.5 , edgecolor = 'grey' );","title":"5.3 Gaussian Mixture Models"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#531-generalizing-e-m-for-gmms","text":"back to top The algorithm for GMMs is very similar to K-means, but now the EM steps are probablistic 1 2 Initialize cluster centers (random guess) Then repeat: E-Step: assign points their probability of belonging to every cluster M-step: set the new center point for each cluster according to the probabilities of all datapoints","title":"5.3.1 Generalizing E-M for GMMs"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#532-gmms-as-a-data-generator","text":"back to top One particularly fascinating application of GMMs is to use them to generate new data that is similar to the data on which they are modeled. Let's take an example. We're going to use sklearn make_circles function to create some arbitrary data that has a complex relationship along two different axes. # some helper functions borrowed from Jake Vanderplas with a few minor tweaks # https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html from matplotlib.patches import Ellipse def draw_ellipse ( position , covariance , ax = None , ** kwargs ): \"\"\"Draw an ellipse with a given position and covariance\"\"\" ax = ax or plt . gca () # Convert covariance to principal axes if covariance . shape == ( 2 , 2 ): U , s , Vt = np . linalg . svd ( covariance ) angle = np . degrees ( np . arctan2 ( U [ 1 , 0 ], U [ 0 , 0 ])) width , height = 2 * np . sqrt ( s ) else : angle = 0 width , height = 2 * np . sqrt ( covariance ) # Draw the Ellipse for nsig in range ( 1 , 4 ): ax . add_patch ( Ellipse ( position , nsig * width , nsig * height , angle , ** kwargs )) def plot_gmm ( gmm , X , label = True , ax = None , data_alpha = 1 ): ax = ax or plt . gca () labels = gmm . fit ( X ) . predict ( X ) if label : ax . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , zorder = 2 , alpha = data_alpha ) else : ax . scatter ( X [:, 0 ], X [:, 1 ], s = 40 , zorder = 2 , alpha = data_alpha ) ax . axis ( 'equal' ) w_factor = 0.2 / gmm . weights_ . max () for pos , covar , w in zip ( gmm . means_ , gmm . covariances_ , gmm . weights_ ): draw_ellipse ( pos , covar , alpha = w * w_factor ) from sklearn.datasets import make_circles as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ]); As a side note, as a clustering model, the GMM is not particularly useful: gmm2 = GaussianMixture ( n_components = 2 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm2 , X ) But if we use the GMM as a density estimator on the underlying data, rather than as a clustering algorithm, we find a completely different use case: gmm16 = GaussianMixture ( n_components = 16 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm16 , X , label = False ) Now, with the distributions drawn, we can assemble entirely new data: Xnew = gmm16 . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]);","title":"5.3.2 GMMs as a Data Generator"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#5321-determining-the-number-of-components","text":"back to top Let's think back to session 1 on model selection. How might we determine the best number of components? A couple analytic approaches that we have not much yet discussed, are the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The important thing to note from each of these is that they penalize added complexity to the models, and we would like to minimize their estimated prediction error. n_components = np . arange ( 1 , 42 ) models = [ GaussianMixture ( n , covariance_type = 'full' , random_state = 42 ) . fit ( X ) for n in n_components ] plt . plot ( n_components , [ m . bic ( X ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( X ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); plt . ylabel ( 'est. prediction error' ) Text(0, 0.5, 'est. prediction error') Both AIC and BIC capture a local minimum on the order of 6 Gaussians. This is where the model captures the overall circular modality of the data. However it totally misses the circumscribed nature of the two circles. Increasing this number to 30-40 gaussians captures this feature of the data. The AIC score reflects this while the BIC score (although captures a local minima in the area) does not define this as a global optimum. gmmNew = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmmNew , X , label = True , data_alpha = 0 ) Xnew = gmmNew . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]);","title":"5.3.2.1 Determining the number of components"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#exercise-2-determine-number-of-components-for-circular-moons","text":"Repeat the above, this time using sklearn.datasets.make_moons # Code Cell for Exercise 2 from sklearn.datasets import make_moons as gen X , y = gen ( 200 , noise = 0.02 , random_state = 42 ) n_components = np . arange ( 1 , 42 ) models = [ GaussianMixture ( n , covariance_type = 'full' , random_state = 42 ) . fit ( X ) for n in n_components ] plt . plot ( n_components , [ m . bic ( X ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( X ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); plt . ylabel ( 'est. prediction error' ) Text(0, 0.5, 'est. prediction error') gmm_moon = GaussianMixture ( n_components = 40 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm_moon , X ) Xnew = gmm_moon . sample ( 400 )[ 0 ] plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]);","title":"\ud83c\udfcb\ufe0f Exercise 2: Determine Number of Components for Circular Moons"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#references","text":"","title":"References"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#pca","text":"Intuitive PCA PCA and Eigenvectors/values","title":"PCA"},{"location":"solutions/SOLN_S5_Unsupervised_Learning/#gmm","text":"GMMs Explained Derive GMM Exercise","title":"GMM"},{"location":"solutions/SOLN_S6_Bagging/","text":"Data Science Foundations Session 6: Bagging Decision Trees and Random Forests \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're going back to the topic of supervised learning models. These models however, belong to a special class of methods called bagging, or bootstrap aggregation. Bagging is an ensemble learning method. In this method, many weak classifiers cast their votes in a general election for the final prediction. The weak learners that random forests are made of, are called decision trees. 6.0 Preparing Environment and Importing Data \u00b6 back to top 6.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score from sklearn.model_selection import train_test_split from sklearn import metrics 6.0.2 Load Dataset \u00b6 back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) print ( margin . shape , end = ' \\n\\n ' ) display ( margin . head ()) (1668, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 We're going to recreate the same operations we employed in Session 4, Feature Engineering: # identify categorical columns cat_cols = margin . columns [: 7 ] # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ cat_cols ] # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [[ 'KG' ]] X_truf = np . concatenate (( onehotlabels , X_num . values ), axis = 1 ) # grab our y data y_truf = margin [ 'EBITDA/KG' ] . values Lastly, to create a classification task, we're going to identify high, med, and low value products: print ( 'bad less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .25 )), end = ' \\n\\n ' ) print ( 'low less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .5 )), end = ' \\n\\n ' ) print ( 'med less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .75 )), end = ' \\n\\n ' ) pd . DataFrame ( margin [ margin . columns [ - 2 ]]) . boxplot ( showfliers = False ) bad less than: 0.12 low less than: 0.22 med less than: 0.35 <AxesSubplot:> margin [ 'profitability' ] = margin [ margin . columns [ - 1 ]] . apply ( lambda x : 'bad' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .25 ) else 'low' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .50 ) else 'med' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .75 ) else 'high' ) margin [ 'profitability' ] . hist () <AxesSubplot:> class_profit = { 'bad' : 0 , 'low' : 1 , 'med' : 2 , 'high' : 3 } y_truf_class = margin [ 'profitability' ] . map ( class_profit ) . values margin [ 'profitability_encoding' ] = y_truf_class margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG profitability profitability_encoding 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 high 3 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 med 2 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 low 1 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 med 2 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 high 3 6.1 Decision Trees \u00b6 back to top In essence, a decision tree is a series of binary questions. Let's begin this discussion by talking about how we make decision trees in sklearn. 6.1.1 Creating a Decision Tree \u00b6 back to top from sklearn import tree X = [[ 0 , 0 ], [ 1 , 1 ]] y = [ 0 , 1 ] clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) After fitting the model we can use the predict method to show the output for a sample clf . predict ([[ 2. , 2. ]]) array([1]) Similar to what we saw with GMMs, we also have access to the probabilities of the outcomes: clf . predict_proba ([[ 2. , 2. ]]) array([[0., 1.]]) Let's now go on to using visual strategies to interpreting trees. 6.1.2 Interpreting a Decision Tree \u00b6 back to top Throughout today, we will discuss many ways to view both a single tree and a random forest of trees. 6.1.2.1 Node & Branch Diagram \u00b6 back to top We can visualize the decision tree: tree . plot_tree ( clf ) [Text(0.5, 0.75, 'X[0] <= 0.5\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'), Text(0.25, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'), Text(0.75, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]')] or, more prettily: import graphviz dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph The gini label, also known as Gini impurity , is a measure of how often a sample passing through the node would be incorrectly labeled if it was randomly assigned a label based on the proportion of all labels passing through the node. So it is a measure of the progress of our tree. Let's take a more complex example from sklearn.datasets import make_classification as gen X , y = gen ( random_state = 42 ) Let's inspect our generated data: print ( X . shape ) print ( y . shape ) y [: 5 ] # a binary classification (100, 20) (100,) array([0, 0, 1, 1, 0]) And now let's train our tree: clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) How do we interpret this graph? dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph Can we confirm the observations in the tree by manually inspecting X and y? y [ X [:, 10 ] < .203 ] array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) We can confirm the gini score of the top left node by hand... scr = [] for j in range ( 1000 ): y_pred = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] y_true = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] scr . append ( mean_squared_error ( y_pred , y_true )) np . mean ( scr ) 0.10817307692307693 Let's take a look at this with our truffle dataset Vary the parameter max_depth what do you notice? Does the term greedy mean anything to you? Do nodes higher in the tree change based on decisions lower in the tree? clf = tree . DecisionTreeClassifier ( max_depth = 1 ) clf . fit ( X_truf , y_truf_class ) DecisionTreeClassifier(max_depth=1) And now lets look at the graph: dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph What is X[4] ??? # It's those tasty sponge cake truffles! enc . get_feature_names_out ()[ 4 ] 'Base Cake_Sponge' This is one great aspect of decision trees, their interpretability . We will perform this analysis again, for now, let's proceed with simpler datasets while exploring the features of decision trees. 6.1.2.1 Decision Boundaries \u00b6 back to top Let's make some random blobs from sklearn.datasets import make_blobs as gen X , y = gen ( random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f1fefd8b130> Let's call up our Classifier again, this time setting the max_depth to two clf = tree . DecisionTreeClassifier ( max_depth = 2 , random_state = 42 ) clf = clf . fit ( X , y ) # Parameters plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) <matplotlib.collections.PathCollection at 0x7f1feef32280> dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph We can see from the output of this graph, that the tree attempts to create the class boundaries as far from the cluster centers as possible. What happens when these clusters overlap? X , y = gen ( random_state = 42 , cluster_std = 3 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f1feeea0e80> Let's go ahead and write our plot into a function def plot_tree ( X , clf ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return plt We see that the boundaries mislabel some points fig = plot_tree ( X , clf ) 6.1.3 Overfitting a Decision Tree \u00b6 back to top Let's increase the max_depth clf = tree . DecisionTreeClassifier ( max_depth = 5 , random_state = 42 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> What we notice is that while the model accurately predicts the training data, we see some spurious labels, noteably the trailing purple bar that extends into the otherwise green region of the data. This is a well known fact about decision trees, that they tend to overfit their training data. In fact, this is a major motivation for why decision trees, a weak classifier, are conveniently packaged into ensembles. We combine the idea of bootstrapping, with decision trees, to come up with an overall better classifier. \ud83c\udfcb\ufe0f Exercise 1: Minimize Overfitting \u00b6 Repeat 6.1.3 with different max_depth settings, also read the docstring and play with any other hyperparameters available to you. What settings do you feel minimize overfitting? The documentation for DecisionTreeClassifier may be helpful # Code Cell for 1 ################################################################################ ##### CHANGE THE HYPERPARAMETERS IN THE CALL TO DECISIONTREECLASSIFIER ######### ################################################################################ clf = tree . DecisionTreeClassifier ( random_state = 42 , max_depth = None , min_samples_split = 3 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = None , max_leaf_nodes = None , min_impurity_decrease = 0.1 , class_weight = None , ccp_alpha = 0.0 ,) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> 6.2 Random Forests and Bagging \u00b6 back to top 6.2.1 What is Bagging? \u00b6 back to top Bagging , or Bootstrap AGGregation is the process of creating subsets of your data and training separate models on them, and using the aggregate votes of the models to make a final prediction. Bootstrapping is a topic in and of itself that we will just touch on here. Without going through the statistical rigor of proof, bootstrapping, or sampling from your observations with replacement, simulates having drawn additional data from the true population. We use this method to create many new datasets that are then used to train separate learners in parallel. This overall approach is called Bagging . A Random Forest is an instance of bagging where the separate learners are decision trees. 6.2.2 Random Forests for Classification \u00b6 back to top from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier tree = DecisionTreeClassifier () bag = BaggingClassifier ( tree , n_estimators = 10 , max_samples = 0.8 , random_state = 1 ) bag . fit ( X , y ) plot_tree ( X , bag ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> In the above, we have bootstrapped by providing each individual tree with 80% of the population data. In practice, Random Forests can achieve even better results by randomizing how the individual classifiers are constructed. In fact there are many unique methods of training individual trees and you can learn more about them here . This randomness is done automatically in sklearn's RandomForestClassifier from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier ( n_estimators = 10 , random_state = 2 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> 6.2.2.1 Interpreting a Random Forest \u00b6 back to top Let's revisit our truffle dataset again, this time with random forests # fit the model clf = RandomForestClassifier ( n_estimators = 100 , min_samples_leaf = 6 ) clf = clf . fit ( X_truf , y_truf_class ) We get a fairly high accuracy when our min_samples_leaf is low and an accuracy that leaves room for improvement when min_samples_leaf is high. This indicates to us the model may be prown to overfitting if we are not careful: accuracy_score ( clf . predict ( X_truf ), y_truf_class ) 0.6133093525179856 We can grab the original feature names with get_feature_names_out() : feats = enc . get_feature_names_out () The feature importances are stored in clf.feature_importances_ . These are calculated from the Mean Decrease in Impurity or MDI also called the Gini Importance . It is the sum of the number of nodes across all trees that include the feature, weighted by the number of samples passing through the node. One downside of estimating feature importance in this way is that it doesn't play well with highly cardinal features (features with many unique values such as mailing addresses, are highly cardinal features) len ( feats ) 118 # grab feature importances imp = clf . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in clf . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ feats , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.098864 0.090298 2 Base Cake_Chiffon 0.058403 0.049159 3 Base Cake_Butter 0.049615 0.046537 4 Base Cake_Pound 0.047093 0.043291 5 Primary Flavor_Butter Toffee 0.03565 0.038741 I'm going to use plotly to create this chart: px . bar ( feat [: 20 ], x = 'feature' , y = 'importance' , error_y = 'std' , title = 'Feature Importance' ) require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"2b50d292-b065-42c3-9980-738d478ac8b2\")) { Plotly.newPlot( \"2b50d292-b065-42c3-9980-738d478ac8b2\", [{\"alignmentgroup\":\"True\",\"error_y\":{\"array\":[0.09029809907413336,0.0491594926001353,0.04653692392065348,0.04329103819043059,0.03874140277043045,0.0349780684180197,0.030094493632483902,0.03311853070594179,0.03369534638488077,0.023175605457143568,0.02003645368097637,0.02649675589259294,0.011595438604605596,0.02247515253748258,0.01375825943123077,0.019922394040156593,0.011655486749852109,0.01266580704339592,0.015074673768077048,0.015049527462185765]},\"hovertemplate\":\"feature=%{x}<br>importance=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"Base Cake_Sponge\",\"Base Cake_Chiffon\",\"Base Cake_Butter\",\"Base Cake_Pound\",\"Primary Flavor_Butter Toffee\",\"Base Cake_Cheese\",\"Primary Flavor_Doughnut\",\"Secondary Flavor_Egg Nog\",\"Color Group_Olive\",\"Truffle Type_Candy Outer\",\"Color Group_White\",\"Secondary Flavor_Black Cherry\",\"Customer_Zebrabar\",\"Base Cake_Tiramisu\",\"Customer_Slugworth\",\"Truffle Type_Chocolate Outer\",\"Customer_Perk-a-Cola\",\"Customer_Dandy's Candies\",\"Color Group_Opal\",\"Secondary Flavor_Apricot\"],\"xaxis\":\"x\",\"y\":[0.0988642579163017,0.0584025133239696,0.049615450872702874,0.04709340666985859,0.03565014121102607,0.02844217305880782,0.027599939763576282,0.02585556386092948,0.02204011487187889,0.020269190892063724,0.019095262999859298,0.018956174795772437,0.016652674359145594,0.016599891454384605,0.0163798681859097,0.014287195738793366,0.013875453357030356,0.013555160346052895,0.013433004919153784,0.012081891079920398],\"yaxis\":\"y\",\"type\":\"bar\"}], {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"feature\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"importance\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Feature Importance\"},\"barmode\":\"relative\"}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('2b50d292-b065-42c3-9980-738d478ac8b2'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; }); \ud83d\ude4b\u200d\u2640\ufe0f Question 1: Feature Importance and Cardinality \u00b6 How does feature importance change in the above plot when we change the minimum leaf size from 6 to 1? \ud83d\ude4b\u200d Question 2: Compare to Moods Median \u00b6 We can then go and look at the different EBITDAs when selecting for each of these features. What do you notice as the primary difference between these results and those from Session 2: Inferential Statistics Exercise 1, Part C when we ran Mood's Median test on this same data? feat . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.098864 0.090298 2 Base Cake_Chiffon 0.058403 0.049159 3 Base Cake_Butter 0.049615 0.046537 4 Base Cake_Pound 0.047093 0.043291 5 Primary Flavor_Butter Toffee 0.03565 0.038741 for feature in feat . iloc [: 10 , 0 ]: group = feature . split ( '_' )[ 0 ] sel = \" \" . join ( feature . split ( '_' )[ 1 :]) pos = margin . loc [( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () neg = margin . loc [ ~ ( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () print ( group + \": \" + sel ) print ( \" \\t with: {:.2f} \" . format ( pos )) print ( \" \\t without: {:.2f} \" . format ( neg )) Base Cake: Sponge with: 0.70 without: 0.20 Base Cake: Chiffon with: 0.13 without: 0.24 Base Cake: Butter with: 0.14 without: 0.26 Base Cake: Pound with: 0.24 without: 0.20 Primary Flavor: Butter Toffee with: 0.46 without: 0.21 Base Cake: Cheese with: 0.44 without: 0.21 Primary Flavor: Doughnut with: 0.38 without: 0.20 Secondary Flavor: Egg Nog with: 0.23 without: 0.21 Color Group: Olive with: 0.67 without: 0.21 Truffle Type: Candy Outer with: 0.20 without: 0.22 6.2.3 Random Forests for Regression \u00b6 back to top from sklearn.ensemble import RandomForestRegressor clf = RandomForestRegressor ( n_estimators = 10 ) Because our labels on our blob data were numerical, we can apply and view the estimator in the same way: clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> I want to revisit a dataset we brought up in Session 2 on feature engineering: t = np . linspace ( 0 , 5 , 200 ) w = 5 h = 4 s = 4 * h / np . pi * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ]) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) # tells us about the amplitude of the component at the # corresponding frequency magnitude = np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , .15 ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain') Let's see if a random forest regression model can capture the wave behavior of the time-series data clf = RandomForestRegressor ( n_estimators = 10 ) clf . fit ( t . reshape ( - 1 , 1 ), s ) RandomForestRegressor(n_estimators=10) t2 = np . linspace ( 0 , 10 , 400 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . plot ( t , s ) ax . plot ( t2 , clf . predict ( t2 . reshape ( - 1 , 1 ))) [<matplotlib.lines.Line2D at 0x7f1fe536df10>] Nice! without specifying any perdiodicity, the random forest does a good job of embedding this periodicity in the final output. \ud83c\udfcb\ufe0f Exercise 2: Practice with Random Forests \u00b6 With the wine dataset: predict: density create a learning curve of train/test score vs model complexity for your random forest model(s) I have provided the cleaned dataset as well as starter code for training the model and making parity plots Do not change the following 3 cells: wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ 'density' ] = y model = RandomForestRegressor ( n_estimators = 65 , criterion = 'squared_error' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , bootstrap = True , oob_score = False , n_jobs = None , random_state = None , verbose = 0 , warm_start = False , ccp_alpha = 0.0 , max_samples = None ,) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax . set_ylabel ( 'Predicted' ) ax . set_xlabel ( 'Actual' ) ax_ . set_xlabel ( 'Actual' ) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.972') Compare these results with our linear model from Lab 3. Recall that we can quickly grab the names of the paramters in our sklearn model: RandomForestRegressor () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Cell for Exercise 2 X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) r2 = [] for n_estimators in range ( 1 , 100 ): model = RandomForestRegressor ( n_estimators = n_estimators ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) r2 . append ([ r2_score ( y_train , model . predict ( X_train )), r2_score ( y_test , model . predict ( X_test ))]) score = np . array ( r2 ) score1 = score [:, 0 ] score2 = score [:, 1 ] fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . plot ( range ( 1 , 100 ), score1 , ls = '' , marker = '.' , color = 'blue' , label = 'Train' ) ax . plot ( range ( 1 , 100 ), score2 , ls = '' , marker = 'o' , color = 'red' , label = 'Test' ) ax . set_title ( \"Scores with Increasing Model Complexity\" ) ax . set_xlabel ( \"Trees in the Forest\" ) ax . set_ylabel ( \"$R^2$\" ) ax . legend () <matplotlib.legend.Legend at 0x7f1fe5063f40>","title":"SOLN S6 Bagging"},{"location":"solutions/SOLN_S6_Bagging/#data-science-foundations-session-6-bagging-decision-trees-and-random-forests","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we're going back to the topic of supervised learning models. These models however, belong to a special class of methods called bagging, or bootstrap aggregation. Bagging is an ensemble learning method. In this method, many weak classifiers cast their votes in a general election for the final prediction. The weak learners that random forests are made of, are called decision trees.","title":"Data Science Foundations  Session 6: Bagging  Decision Trees and Random Forests"},{"location":"solutions/SOLN_S6_Bagging/#60-preparing-environment-and-importing-data","text":"back to top","title":"6.0 Preparing Environment and Importing Data"},{"location":"solutions/SOLN_S6_Bagging/#601-import-packages","text":"back to top import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import plotly.express as px import random import scipy.stats from sklearn.preprocessing import OneHotEncoder , StandardScaler from sklearn.impute import SimpleImputer from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.ensemble import RandomForestClassifier import seaborn as sns ; sns . set () import graphviz from sklearn.metrics import accuracy_score from ipywidgets import interact , interactive , widgets from sklearn.metrics import mean_squared_error , r2_score from sklearn.model_selection import train_test_split from sklearn import metrics","title":"6.0.1 Import Packages"},{"location":"solutions/SOLN_S6_Bagging/#602-load-dataset","text":"back to top margin = pd . read_csv ( 'https://raw.githubusercontent.com/wesleybeckner/' \\ 'ds_for_engineers/main/data/truffle_margin/truffle_margin_customer.csv' ) print ( margin . shape , end = ' \\n\\n ' ) display ( margin . head ()) (1668, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 We're going to recreate the same operations we employed in Session 4, Feature Engineering: # identify categorical columns cat_cols = margin . columns [: 7 ] # create the encoder object enc = OneHotEncoder () # grab the columns we want to convert from strings X_cat = margin [ cat_cols ] # fit our encoder to this data enc . fit ( X_cat ) onehotlabels = enc . transform ( X_cat ) . toarray () X_num = margin [[ 'KG' ]] X_truf = np . concatenate (( onehotlabels , X_num . values ), axis = 1 ) # grab our y data y_truf = margin [ 'EBITDA/KG' ] . values Lastly, to create a classification task, we're going to identify high, med, and low value products: print ( 'bad less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .25 )), end = ' \\n\\n ' ) print ( 'low less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .5 )), end = ' \\n\\n ' ) print ( 'med less than: {:.2f} ' . format ( margin [ margin . columns [ - 1 ]] . quantile ( .75 )), end = ' \\n\\n ' ) pd . DataFrame ( margin [ margin . columns [ - 2 ]]) . boxplot ( showfliers = False ) bad less than: 0.12 low less than: 0.22 med less than: 0.35 <AxesSubplot:> margin [ 'profitability' ] = margin [ margin . columns [ - 1 ]] . apply ( lambda x : 'bad' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .25 ) else 'low' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .50 ) else 'med' if x <= margin [ margin . columns [ - 1 ]] . quantile ( .75 ) else 'high' ) margin [ 'profitability' ] . hist () <AxesSubplot:> class_profit = { 'bad' : 0 , 'low' : 1 , 'med' : 2 , 'high' : 3 } y_truf_class = margin [ 'profitability' ] . map ( class_profit ) . values margin [ 'profitability_encoding' ] = y_truf_class margin . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Base Cake Truffle Type Primary Flavor Secondary Flavor Color Group Customer Date KG EBITDA/KG profitability profitability_encoding 0 Butter Candy Outer Butter Pecan Toffee Taupe Slugworth 1/2020 53770.342593 0.500424 high 3 1 Butter Candy Outer Ginger Lime Banana Amethyst Slugworth 1/2020 466477.578125 0.220395 med 2 2 Butter Candy Outer Ginger Lime Banana Burgundy Perk-a-Cola 1/2020 80801.728070 0.171014 low 1 3 Butter Candy Outer Ginger Lime Banana White Fickelgruber 1/2020 18046.111111 0.233025 med 2 4 Butter Candy Outer Ginger Lime Rum Amethyst Fickelgruber 1/2020 19147.454268 0.480689 high 3","title":"6.0.2 Load Dataset"},{"location":"solutions/SOLN_S6_Bagging/#61-decision-trees","text":"back to top In essence, a decision tree is a series of binary questions. Let's begin this discussion by talking about how we make decision trees in sklearn.","title":"6.1 Decision Trees"},{"location":"solutions/SOLN_S6_Bagging/#611-creating-a-decision-tree","text":"back to top from sklearn import tree X = [[ 0 , 0 ], [ 1 , 1 ]] y = [ 0 , 1 ] clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) After fitting the model we can use the predict method to show the output for a sample clf . predict ([[ 2. , 2. ]]) array([1]) Similar to what we saw with GMMs, we also have access to the probabilities of the outcomes: clf . predict_proba ([[ 2. , 2. ]]) array([[0., 1.]]) Let's now go on to using visual strategies to interpreting trees.","title":"6.1.1 Creating a Decision Tree"},{"location":"solutions/SOLN_S6_Bagging/#612-interpreting-a-decision-tree","text":"back to top Throughout today, we will discuss many ways to view both a single tree and a random forest of trees.","title":"6.1.2 Interpreting a Decision Tree"},{"location":"solutions/SOLN_S6_Bagging/#6121-node-branch-diagram","text":"back to top We can visualize the decision tree: tree . plot_tree ( clf ) [Text(0.5, 0.75, 'X[0] <= 0.5\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'), Text(0.25, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'), Text(0.75, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]')] or, more prettily: import graphviz dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph The gini label, also known as Gini impurity , is a measure of how often a sample passing through the node would be incorrectly labeled if it was randomly assigned a label based on the proportion of all labels passing through the node. So it is a measure of the progress of our tree. Let's take a more complex example from sklearn.datasets import make_classification as gen X , y = gen ( random_state = 42 ) Let's inspect our generated data: print ( X . shape ) print ( y . shape ) y [: 5 ] # a binary classification (100, 20) (100,) array([0, 0, 1, 1, 0]) And now let's train our tree: clf = tree . DecisionTreeClassifier () clf = clf . fit ( X , y ) How do we interpret this graph? dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph Can we confirm the observations in the tree by manually inspecting X and y? y [ X [:, 10 ] < .203 ] array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) We can confirm the gini score of the top left node by hand... scr = [] for j in range ( 1000 ): y_pred = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] y_true = [ 0 if random . random () > ( 3 / 52 ) else 1 for i in range ( 52 )] scr . append ( mean_squared_error ( y_pred , y_true )) np . mean ( scr ) 0.10817307692307693 Let's take a look at this with our truffle dataset Vary the parameter max_depth what do you notice? Does the term greedy mean anything to you? Do nodes higher in the tree change based on decisions lower in the tree? clf = tree . DecisionTreeClassifier ( max_depth = 1 ) clf . fit ( X_truf , y_truf_class ) DecisionTreeClassifier(max_depth=1) And now lets look at the graph: dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph What is X[4] ??? # It's those tasty sponge cake truffles! enc . get_feature_names_out ()[ 4 ] 'Base Cake_Sponge' This is one great aspect of decision trees, their interpretability . We will perform this analysis again, for now, let's proceed with simpler datasets while exploring the features of decision trees.","title":"6.1.2.1 Node &amp; Branch Diagram"},{"location":"solutions/SOLN_S6_Bagging/#6121-decision-boundaries","text":"back to top Let's make some random blobs from sklearn.datasets import make_blobs as gen X , y = gen ( random_state = 42 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f1fefd8b130> Let's call up our Classifier again, this time setting the max_depth to two clf = tree . DecisionTreeClassifier ( max_depth = 2 , random_state = 42 ) clf = clf . fit ( X , y ) # Parameters plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) <matplotlib.collections.PathCollection at 0x7f1feef32280> dot_data = tree . export_graphviz ( clf , out_file = None ) graph = graphviz . Source ( dot_data ) graph We can see from the output of this graph, that the tree attempts to create the class boundaries as far from the cluster centers as possible. What happens when these clusters overlap? X , y = gen ( random_state = 42 , cluster_std = 3 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f1feeea0e80> Let's go ahead and write our plot into a function def plot_tree ( X , clf ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) plt . tight_layout ( h_pad = 0.5 , w_pad = 0.5 , pad = 2.5 ) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return plt We see that the boundaries mislabel some points fig = plot_tree ( X , clf )","title":"6.1.2.1 Decision Boundaries"},{"location":"solutions/SOLN_S6_Bagging/#613-overfitting-a-decision-tree","text":"back to top Let's increase the max_depth clf = tree . DecisionTreeClassifier ( max_depth = 5 , random_state = 42 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> What we notice is that while the model accurately predicts the training data, we see some spurious labels, noteably the trailing purple bar that extends into the otherwise green region of the data. This is a well known fact about decision trees, that they tend to overfit their training data. In fact, this is a major motivation for why decision trees, a weak classifier, are conveniently packaged into ensembles. We combine the idea of bootstrapping, with decision trees, to come up with an overall better classifier.","title":"6.1.3 Overfitting a Decision Tree"},{"location":"solutions/SOLN_S6_Bagging/#exercise-1-minimize-overfitting","text":"Repeat 6.1.3 with different max_depth settings, also read the docstring and play with any other hyperparameters available to you. What settings do you feel minimize overfitting? The documentation for DecisionTreeClassifier may be helpful # Code Cell for 1 ################################################################################ ##### CHANGE THE HYPERPARAMETERS IN THE CALL TO DECISIONTREECLASSIFIER ######### ################################################################################ clf = tree . DecisionTreeClassifier ( random_state = 42 , max_depth = None , min_samples_split = 3 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = None , max_leaf_nodes = None , min_impurity_decrease = 0.1 , class_weight = None , ccp_alpha = 0.0 ,) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'>","title":"\ud83c\udfcb\ufe0f Exercise 1: Minimize Overfitting"},{"location":"solutions/SOLN_S6_Bagging/#62-random-forests-and-bagging","text":"back to top","title":"6.2 Random Forests and Bagging"},{"location":"solutions/SOLN_S6_Bagging/#621-what-is-bagging","text":"back to top Bagging , or Bootstrap AGGregation is the process of creating subsets of your data and training separate models on them, and using the aggregate votes of the models to make a final prediction. Bootstrapping is a topic in and of itself that we will just touch on here. Without going through the statistical rigor of proof, bootstrapping, or sampling from your observations with replacement, simulates having drawn additional data from the true population. We use this method to create many new datasets that are then used to train separate learners in parallel. This overall approach is called Bagging . A Random Forest is an instance of bagging where the separate learners are decision trees.","title":"6.2.1 What is Bagging?"},{"location":"solutions/SOLN_S6_Bagging/#622-random-forests-for-classification","text":"back to top from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier tree = DecisionTreeClassifier () bag = BaggingClassifier ( tree , n_estimators = 10 , max_samples = 0.8 , random_state = 1 ) bag . fit ( X , y ) plot_tree ( X , bag ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> In the above, we have bootstrapped by providing each individual tree with 80% of the population data. In practice, Random Forests can achieve even better results by randomizing how the individual classifiers are constructed. In fact there are many unique methods of training individual trees and you can learn more about them here . This randomness is done automatically in sklearn's RandomForestClassifier from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier ( n_estimators = 10 , random_state = 2 ) clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'>","title":"6.2.2 Random Forests for Classification"},{"location":"solutions/SOLN_S6_Bagging/#6221-interpreting-a-random-forest","text":"back to top Let's revisit our truffle dataset again, this time with random forests # fit the model clf = RandomForestClassifier ( n_estimators = 100 , min_samples_leaf = 6 ) clf = clf . fit ( X_truf , y_truf_class ) We get a fairly high accuracy when our min_samples_leaf is low and an accuracy that leaves room for improvement when min_samples_leaf is high. This indicates to us the model may be prown to overfitting if we are not careful: accuracy_score ( clf . predict ( X_truf ), y_truf_class ) 0.6133093525179856 We can grab the original feature names with get_feature_names_out() : feats = enc . get_feature_names_out () The feature importances are stored in clf.feature_importances_ . These are calculated from the Mean Decrease in Impurity or MDI also called the Gini Importance . It is the sum of the number of nodes across all trees that include the feature, weighted by the number of samples passing through the node. One downside of estimating feature importance in this way is that it doesn't play well with highly cardinal features (features with many unique values such as mailing addresses, are highly cardinal features) len ( feats ) 118 # grab feature importances imp = clf . feature_importances_ # their std std = np . std ([ tree . feature_importances_ for tree in clf . estimators_ ], axis = 0 ) # create new dataframe feat = pd . DataFrame ([ feats , imp , std ]) . T feat . columns = [ 'feature' , 'importance' , 'std' ] feat = feat . sort_values ( 'importance' , ascending = False ) feat = feat . reset_index ( drop = True ) feat . dropna ( inplace = True ) feat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.098864 0.090298 2 Base Cake_Chiffon 0.058403 0.049159 3 Base Cake_Butter 0.049615 0.046537 4 Base Cake_Pound 0.047093 0.043291 5 Primary Flavor_Butter Toffee 0.03565 0.038741 I'm going to use plotly to create this chart: px . bar ( feat [: 20 ], x = 'feature' , y = 'importance' , error_y = 'std' , title = 'Feature Importance' ) require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"2b50d292-b065-42c3-9980-738d478ac8b2\")) { Plotly.newPlot( \"2b50d292-b065-42c3-9980-738d478ac8b2\", [{\"alignmentgroup\":\"True\",\"error_y\":{\"array\":[0.09029809907413336,0.0491594926001353,0.04653692392065348,0.04329103819043059,0.03874140277043045,0.0349780684180197,0.030094493632483902,0.03311853070594179,0.03369534638488077,0.023175605457143568,0.02003645368097637,0.02649675589259294,0.011595438604605596,0.02247515253748258,0.01375825943123077,0.019922394040156593,0.011655486749852109,0.01266580704339592,0.015074673768077048,0.015049527462185765]},\"hovertemplate\":\"feature=%{x}<br>importance=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"Base Cake_Sponge\",\"Base Cake_Chiffon\",\"Base Cake_Butter\",\"Base Cake_Pound\",\"Primary Flavor_Butter Toffee\",\"Base Cake_Cheese\",\"Primary Flavor_Doughnut\",\"Secondary Flavor_Egg Nog\",\"Color Group_Olive\",\"Truffle Type_Candy Outer\",\"Color Group_White\",\"Secondary Flavor_Black Cherry\",\"Customer_Zebrabar\",\"Base Cake_Tiramisu\",\"Customer_Slugworth\",\"Truffle Type_Chocolate Outer\",\"Customer_Perk-a-Cola\",\"Customer_Dandy's Candies\",\"Color Group_Opal\",\"Secondary Flavor_Apricot\"],\"xaxis\":\"x\",\"y\":[0.0988642579163017,0.0584025133239696,0.049615450872702874,0.04709340666985859,0.03565014121102607,0.02844217305880782,0.027599939763576282,0.02585556386092948,0.02204011487187889,0.020269190892063724,0.019095262999859298,0.018956174795772437,0.016652674359145594,0.016599891454384605,0.0163798681859097,0.014287195738793366,0.013875453357030356,0.013555160346052895,0.013433004919153784,0.012081891079920398],\"yaxis\":\"y\",\"type\":\"bar\"}], {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"feature\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"importance\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Feature Importance\"},\"barmode\":\"relative\"}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('2b50d292-b065-42c3-9980-738d478ac8b2'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; });","title":"6.2.2.1 Interpreting a Random Forest"},{"location":"solutions/SOLN_S6_Bagging/#question-1-feature-importance-and-cardinality","text":"How does feature importance change in the above plot when we change the minimum leaf size from 6 to 1?","title":"\ud83d\ude4b\u200d\u2640\ufe0f Question 1: Feature Importance and Cardinality"},{"location":"solutions/SOLN_S6_Bagging/#question-2-compare-to-moods-median","text":"We can then go and look at the different EBITDAs when selecting for each of these features. What do you notice as the primary difference between these results and those from Session 2: Inferential Statistics Exercise 1, Part C when we ran Mood's Median test on this same data? feat . iloc [: 5 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature importance std 0 Base Cake_Sponge 0.098864 0.090298 2 Base Cake_Chiffon 0.058403 0.049159 3 Base Cake_Butter 0.049615 0.046537 4 Base Cake_Pound 0.047093 0.043291 5 Primary Flavor_Butter Toffee 0.03565 0.038741 for feature in feat . iloc [: 10 , 0 ]: group = feature . split ( '_' )[ 0 ] sel = \" \" . join ( feature . split ( '_' )[ 1 :]) pos = margin . loc [( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () neg = margin . loc [ ~ ( margin [ group ] == sel )][ 'EBITDA/KG' ] . median () print ( group + \": \" + sel ) print ( \" \\t with: {:.2f} \" . format ( pos )) print ( \" \\t without: {:.2f} \" . format ( neg )) Base Cake: Sponge with: 0.70 without: 0.20 Base Cake: Chiffon with: 0.13 without: 0.24 Base Cake: Butter with: 0.14 without: 0.26 Base Cake: Pound with: 0.24 without: 0.20 Primary Flavor: Butter Toffee with: 0.46 without: 0.21 Base Cake: Cheese with: 0.44 without: 0.21 Primary Flavor: Doughnut with: 0.38 without: 0.20 Secondary Flavor: Egg Nog with: 0.23 without: 0.21 Color Group: Olive with: 0.67 without: 0.21 Truffle Type: Candy Outer with: 0.20 without: 0.22","title":"\ud83d\ude4b\u200d Question 2: Compare to Moods Median"},{"location":"solutions/SOLN_S6_Bagging/#623-random-forests-for-regression","text":"back to top from sklearn.ensemble import RandomForestRegressor clf = RandomForestRegressor ( n_estimators = 10 ) Because our labels on our blob data were numerical, we can apply and view the estimator in the same way: clf = clf . fit ( X , y ) plot_tree ( X , clf ) <module 'matplotlib.pyplot' from '/home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py'> I want to revisit a dataset we brought up in Session 2 on feature engineering: t = np . linspace ( 0 , 5 , 200 ) w = 5 h = 4 s = 4 * h / np . pi * ( np . sin ( w * t ) + np . sin ( 3 * w * t ) / 3 + np . sin ( 5 * w * t ) / 5 ) F = np . fft . fft ( s ) freq = np . fft . fftfreq ( t . shape [ - 1 ]) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax [ 0 ] . plot ( t , s ) ax [ 0 ] . plot ( t , np . sin ( w * t ), ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 3 ) / 3 , ls = '--' ) ax [ 0 ] . plot ( t , np . sin ( w * t * 5 ) / 5 , ls = '--' ) ax [ 0 ] . set_title ( 'Time Domain' ) # tells us about the amplitude of the component at the # corresponding frequency magnitude = np . sqrt ( F . real ** 2 + F . imag ** 2 ) ax [ 1 ] . plot ( freq , magnitude ) ax [ 1 ] . set_xlim ( 0 , .15 ) ax [ 1 ] . set_title ( 'Frequency Domain' ) Text(0.5, 1.0, 'Frequency Domain') Let's see if a random forest regression model can capture the wave behavior of the time-series data clf = RandomForestRegressor ( n_estimators = 10 ) clf . fit ( t . reshape ( - 1 , 1 ), s ) RandomForestRegressor(n_estimators=10) t2 = np . linspace ( 0 , 10 , 400 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . plot ( t , s ) ax . plot ( t2 , clf . predict ( t2 . reshape ( - 1 , 1 ))) [<matplotlib.lines.Line2D at 0x7f1fe536df10>] Nice! without specifying any perdiodicity, the random forest does a good job of embedding this periodicity in the final output.","title":"6.2.3 Random Forests for Regression"},{"location":"solutions/SOLN_S6_Bagging/#exercise-2-practice-with-random-forests","text":"With the wine dataset: predict: density create a learning curve of train/test score vs model complexity for your random forest model(s) I have provided the cleaned dataset as well as starter code for training the model and making parity plots Do not change the following 3 cells: wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # infer str cols str_cols = list ( wine . select_dtypes ( include = 'object' ) . columns ) #set target col target = 'density' enc = OneHotEncoder () imp = SimpleImputer () enc . fit_transform ( wine [ str_cols ]) X_cat = enc . transform ( wine [ str_cols ]) . toarray () X = wine . copy () [ X . pop ( i ) for i in str_cols ] y = X . pop ( target ) X = imp . fit_transform ( X ) X = np . hstack ([ X_cat , X ]) cols = [ i . split ( \"_\" )[ 1 ] for i in enc . get_feature_names_out ()] cols += list ( wine . columns ) cols . remove ( target ) [ cols . remove ( i ) for i in str_cols ] scaler = StandardScaler () X [:, 2 :] = scaler . fit_transform ( X [:, 2 :]) wine = pd . DataFrame ( X , columns = cols ) wine [ 'density' ] = y model = RandomForestRegressor ( n_estimators = 65 , criterion = 'squared_error' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , bootstrap = True , oob_score = False , n_jobs = None , random_state = None , verbose = 0 , warm_start = False , ccp_alpha = 0.0 , max_samples = None ,) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax . plot ( y_test , model . predict ( X_test ), ls = '' , marker = '.' ) ax_ . plot ( y_train , model . predict ( X_train ), ls = '' , marker = '.' ) ax . set_title ( \"Train, R2: {:.3f} \" . format ( r2_score ( y_train , model . predict ( X_train )))) ax . set_ylabel ( 'Predicted' ) ax . set_xlabel ( 'Actual' ) ax_ . set_xlabel ( 'Actual' ) ax_ . set_title ( \"Test, R2: {:.3f} \" . format ( r2_score ( y_test , model . predict ( X_test )))) Text(0.5, 1.0, 'Test, R2: 0.972') Compare these results with our linear model from Lab 3. Recall that we can quickly grab the names of the paramters in our sklearn model: RandomForestRegressor () . get_params () {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} # Cell for Exercise 2 X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 0.8 , random_state = 42 ) r2 = [] for n_estimators in range ( 1 , 100 ): model = RandomForestRegressor ( n_estimators = n_estimators ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) r2 . append ([ r2_score ( y_train , model . predict ( X_train )), r2_score ( y_test , model . predict ( X_test ))]) score = np . array ( r2 ) score1 = score [:, 0 ] score2 = score [:, 1 ] fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . plot ( range ( 1 , 100 ), score1 , ls = '' , marker = '.' , color = 'blue' , label = 'Train' ) ax . plot ( range ( 1 , 100 ), score2 , ls = '' , marker = 'o' , color = 'red' , label = 'Test' ) ax . set_title ( \"Scores with Increasing Model Complexity\" ) ax . set_xlabel ( \"Trees in the Forest\" ) ax . set_ylabel ( \"$R^2$\" ) ax . legend () <matplotlib.legend.Legend at 0x7f1fe5063f40>","title":"\ud83c\udfcb\ufe0f Exercise 2: Practice with Random Forests"},{"location":"solutions/SOLN_X1_Thinking_Data/","text":"Data Science Foundations Extras 1: Thinking Data \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to take our newfound knowledge from the course, and practice how we can leverage data to build predictive models. We'll start with a feature engineering problem on some dummy data. This will get us thinking creatively about problem solving. We will then pivot over to an Airbnb dataset . After performing some general, exploratory data analysis, we will solve the following business case: Airbnb is interested in using historical list prices from their airbnb hosts, to make pricing suggestions to new hosts. How can we use this existing datset to assist with this price listing suggestion? Prepare Environment and Import Data \u00b6 back to top # basic packages import pandas as pd import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , silhouette_score , calinski_harabasz_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score Warm Up \u00b6 Add aditional feature(s) to X to predict y with a model limited to a linear classification boundary from sklearn.datasets import make_circles X , y = make_circles ( random_state = 42 , noise = .01 ) relabel = dict ( zip ([ 0 , 1 , 2 , 3 ],[ 0 , 1 , 0 , 1 ])) y = np . vectorize ( relabel . get )( y ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f211fc48550> X2 = ( X ** 2 ) . sum ( axis = 1 ) X_ = np . hstack (( X , X2 . reshape ( - 1 , 1 ))) We can separate: px . scatter_3d ( x = X_ [:, 0 ], y = X_ [:, 1 ], z = X_ [:, 2 ], color = y ) and now predict model = LogisticRegression () model . fit ( X_ , y ) y_pred = model . predict ( X_ ) r2_score ( y , y_pred ) 1.0 Build a Baseline \u00b6 Exploratory Data Analysis \u00b6 which columns are numerical, string; which contain nans/nulls; what is the VIF between features airbnb = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/datasets/main/datasets/airbnb/AB_NYC_2019.csv\" ) airbnb . shape (48895, 16) airbnb . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 0 2539 Clean & quiet apt home by the park 2787 John Brooklyn Kensington 40.64749 -73.97237 Private room 149 1 9 2018-10-19 0.21 6 365 1 2595 Skylit Midtown Castle 2845 Jennifer Manhattan Midtown 40.75362 -73.98377 Entire home/apt 225 1 45 2019-05-21 0.38 2 355 2 3647 THE VILLAGE OF HARLEM....NEW YORK ! 4632 Elisabeth Manhattan Harlem 40.80902 -73.94190 Private room 150 3 0 NaN NaN 1 365 3 3831 Cozy Entire Floor of Brownstone 4869 LisaRoxanne Brooklyn Clinton Hill 40.68514 -73.95976 Entire home/apt 89 1 270 2019-07-05 4.64 1 194 4 5022 Entire Apt: Spacious Studio/Loft by central park 7192 Laura Manhattan East Harlem 40.79851 -73.94399 Entire home/apt 80 10 9 2018-11-19 0.10 1 0 airbnb . dtypes id int64 name object host_id int64 host_name object neighbourhood_group object neighbourhood object latitude float64 longitude float64 room_type object price int64 minimum_nights int64 number_of_reviews int64 last_review object reviews_per_month float64 calculated_host_listings_count int64 availability_365 int64 dtype: object airbnb . isnull () . sum ( axis = 0 ) id 0 name 16 host_id 0 host_name 21 neighbourhood_group 0 neighbourhood 0 latitude 0 longitude 0 room_type 0 price 0 minimum_nights 0 number_of_reviews 0 last_review 10052 reviews_per_month 10052 calculated_host_listings_count 0 availability_365 0 dtype: int64 airbnb . nunique () id 48895 name 47905 host_id 37457 host_name 11452 neighbourhood_group 5 neighbourhood 221 latitude 19048 longitude 14718 room_type 3 price 674 minimum_nights 109 number_of_reviews 394 last_review 1764 reviews_per_month 937 calculated_host_listings_count 47 availability_365 366 dtype: int64 plt . figure ( figsize = ( 10 , 6 )) sns . scatterplot ( x = airbnb . longitude , y = airbnb . latitude , hue = airbnb . neighbourhood_group ) plt . ioff () <matplotlib.pyplot._IoffContext at 0x7f211d15bb20> X = airbnb . copy () reviews_per_month has some 'nans' X_num = X . select_dtypes ( exclude = 'object' ) X_num . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 48890 36484665 8232441 40.67853 -73.94995 70 2 0 NaN 2 9 48891 36485057 6570630 40.70184 -73.93317 40 4 0 NaN 2 36 48892 36485431 23492952 40.81475 -73.94867 115 10 0 NaN 1 27 48893 36485609 30985759 40.75751 -73.99112 55 1 0 NaN 6 2 48894 36487245 68119814 40.76404 -73.98933 90 7 0 NaN 1 23 X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 149 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 225 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 150 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 89 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 80 10 9 0.10 1 0 X_num . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 count 4.889500e+04 4.889500e+04 48895.000000 48895.000000 48895.000000 48895.000000 48895.000000 38843.000000 48895.000000 48895.000000 mean 1.901714e+07 6.762001e+07 40.728949 -73.952170 152.720687 7.029962 23.274466 1.373221 7.143982 112.781327 std 1.098311e+07 7.861097e+07 0.054530 0.046157 240.154170 20.510550 44.550582 1.680442 32.952519 131.622289 min 2.539000e+03 2.438000e+03 40.499790 -74.244420 0.000000 1.000000 0.000000 0.010000 1.000000 0.000000 25% 9.471945e+06 7.822033e+06 40.690100 -73.983070 69.000000 1.000000 1.000000 0.190000 1.000000 0.000000 50% 1.967728e+07 3.079382e+07 40.723070 -73.955680 106.000000 3.000000 5.000000 0.720000 1.000000 45.000000 75% 2.915218e+07 1.074344e+08 40.763115 -73.936275 175.000000 5.000000 24.000000 2.020000 2.000000 227.000000 max 3.648724e+07 2.743213e+08 40.913060 -73.712990 10000.000000 1250.000000 629.000000 58.500000 327.000000 365.000000 X . dropna ( inplace = True ) X_num = X . select_dtypes ( exclude = 'object' ) vif = [ variance_inflation_factor ( X_num . values , i ) for i in range ( X_num . shape [ 1 ])] pd . DataFrame ( vif , index = X_num . columns ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 id 2.180074 host_id 2.836905 latitude 0.775769 longitude 425502.981678 price 1.012423 minimum_nights 1.039144 number_of_reviews 2.348200 reviews_per_month 2.314318 calculated_host_listings_count 1.067389 availability_365 1.139558 X_num . drop ( 'longitude' , axis = 1 , inplace = True ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy X_num .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 149 1 9 0.21 6 365 1 2595 2845 40.75362 225 1 45 0.38 2 355 3 3831 4869 40.68514 89 1 270 4.64 1 194 4 5022 7192 40.79851 80 10 9 0.10 1 0 5 5099 7322 40.74767 200 3 74 0.59 1 129 ... ... ... ... ... ... ... ... ... ... 48782 36425863 83554966 40.78099 129 1 1 1.00 1 147 48790 36427429 257683179 40.75104 45 1 1 1.00 6 339 48799 36438336 211644523 40.54179 235 1 1 1.00 1 87 48805 36442252 273841667 40.80787 100 1 2 2.00 1 40 48852 36455809 74162901 40.69805 30 1 1 1.00 1 1 38821 rows \u00d7 9 columns vif = [ variance_inflation_factor ( X_num . values , i ) for i in range ( X_num . shape [ 1 ])] pd . DataFrame ( vif , index = X_num . columns ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 id 8.424770 host_id 2.827543 latitude 7.297302 price 1.538975 minimum_nights 1.157468 number_of_reviews 3.215893 reviews_per_month 3.858006 calculated_host_listings_count 1.106414 availability_365 2.035592 Feature Engineering \u00b6 Say we want to predict pricing, using an ML model. How would you build your features? Based on the number of null values, what would you do with the last_review and reviews_per_month column? X = airbnb . copy () y = X . pop ( 'price' ) X_cat = X . select_dtypes ( include = 'object' ) X_cat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name host_name neighbourhood_group neighbourhood room_type last_review 0 Clean & quiet apt home by the park John Brooklyn Kensington Private room 2018-10-19 1 Skylit Midtown Castle Jennifer Manhattan Midtown Entire home/apt 2019-05-21 2 THE VILLAGE OF HARLEM....NEW YORK ! Elisabeth Manhattan Harlem Private room NaN 3 Cozy Entire Floor of Brownstone LisaRoxanne Brooklyn Clinton Hill Entire home/apt 2019-07-05 4 Entire Apt: Spacious Studio/Loft by central park Laura Manhattan East Harlem Entire home/apt 2018-11-19 based on the number of unique columns, we may want to remove name , host_name , and last_review X_cat . nunique () name 47905 host_name 11452 neighbourhood_group 5 neighbourhood 221 room_type 3 last_review 1764 dtype: int64 X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () And now we deal with the numerical columns X_num = X . select_dtypes ( exclude = 'object' ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 10 9 0.10 1 0 both id and host_id will be highly cardinal without telling us much about the behavior of unseen data. We should remove them. We'll also drop the columns with nans for now X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_num = X_num . dropna ( axis = 1 ) X_enc_df = pd . DataFrame ( X_enc , columns = enc . get_feature_names_out ()) X_feat = pd . concat (( X_enc_df , X_num ), axis = 1 ) X_feat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } neighbourhood_group_Bronx neighbourhood_group_Brooklyn neighbourhood_group_Manhattan neighbourhood_group_Queens neighbourhood_group_Staten Island neighbourhood_Allerton neighbourhood_Arden Heights neighbourhood_Arrochar neighbourhood_Arverne neighbourhood_Astoria ... neighbourhood_Woodside room_type_Entire home/apt room_type_Private room room_type_Shared room latitude longitude minimum_nights number_of_reviews calculated_host_listings_count availability_365 0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 1.0 0.0 40.64749 -73.97237 1 9 6 365 1 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 40.75362 -73.98377 1 45 2 355 2 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 1.0 0.0 40.80902 -73.94190 3 0 1 365 3 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 40.68514 -73.95976 1 270 1 194 4 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 40.79851 -73.94399 10 9 1 0 5 rows \u00d7 235 columns Feature Transformation \u00b6 What features do you think will cause the most problems if untransformed? Scale and Center all but the target variable, price scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (48895, 235) (48895,) Model Baseline \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = LinearRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) r2_score ( y_train , model . predict ( X_train )) 0.11264603204210533 r2_score ( y_test , y_pred ) -1.563294115330747e+17 model = RandomForestRegressor () model . fit ( X_train , y_train ) r2_score ( y_train , model . predict ( X_train )) 0.8597830223730762 r2_score ( y_test , model . predict ( X_test )) 0.10233675407266163 both of these results from the LinearRegression and RandomForest models indicate overfitting Back to Feature Engineering \u00b6 \ud83c\udf1f - keep this feature \ud83d\udca1 - interesting behavior discovered \ud83d\udc4e - don't keep this feature \ud83d\udd2e - try for next time To try: drop nan rows not columns remove outliers (filter by group) PCA of one hot encoded vectors (will help with linear model) transform 'last review date' (str) into 'days since last review' (number) \ud83c\udf1f NaNs - Drop Row-wise \u00b6 X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } latitude longitude minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 40.64749 -73.97237 1 9 0.21 6 365 1 40.75362 -73.98377 1 45 0.38 2 355 3 40.68514 -73.95976 1 270 4.64 1 194 4 40.79851 -73.94399 10 9 0.10 1 0 5 40.74767 -73.97500 3 74 0.59 1 129 X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38821, 233) (38821,) Train/Eval \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestRegressor ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train R2: { r2_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test R2: { r2_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train R2: 0.88 Test R2: 0.23 \ud83d\udca1 Outliers - by Borough \u00b6 X = airbnb . copy () X = X . dropna ( axis = 0 ) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = X [ 'neighbourhood_group' ], y = X [ 'price' ], ax = ax ) # ax.set_ylim(0, 1500) <AxesSubplot:xlabel='neighbourhood_group', ylabel='price'> fig , ax = plt . subplots ( figsize = ( 15 , 10 )) sns . kdeplot ( hue = X [ 'neighbourhood_group' ], x = X [ 'price' ], ax = ax ) <AxesSubplot:xlabel='price', ylabel='Density'> X = X . loc [ X . groupby ( 'neighbourhood_group' ) . apply ( lambda x : x [ 'price' ] < ( x [ 'price' ] . std () * 3 )) . unstack ( level = 0 ) . any ( axis = 1 )] fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 20 , 10 )) sns . boxplot ( x = X [ 'neighbourhood_group' ], y = X [ 'price' ], ax = ax ) sns . kdeplot ( hue = X [ 'neighbourhood_group' ], x = X [ 'price' ], ax = ax_ ) <AxesSubplot:xlabel='price', ylabel='Density'> y = X . pop ( 'price' ) X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38309, 232) (38309,) Train/Eval \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestRegressor ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train R2: { r2_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test R2: { r2_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train R2: 0.93 Test R2: 0.52 fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax . plot ( y_train , model . predict ( X_train ), ls = '' , marker = ',' ) ax_ . plot ( y_test , model . predict ( X_test ), ls = '' , marker = ',' ) [<matplotlib.lines.Line2D at 0x7f211ce29c40>] \ud83c\udf1f Bin Prices, Classifier Model \u00b6 X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) . values Y = y . reshape ( - 1 , 1 ) labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 y = labels X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38821, 233) (38821,) Train/Eval \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 1.00 Test Acc: 0.60 y_pred = model . predict ( X_train ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) precision recall f1-score support 1 1.00 1.00 1.00 7986 2 1.00 1.00 1.00 7594 3 1.00 1.00 1.00 7878 4 1.00 1.00 1.00 7598 accuracy 1.00 31056 macro avg 1.00 1.00 1.00 31056 weighted avg 1.00 1.00 1.00 31056 <AxesSubplot:> y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) precision recall f1-score support 1 0.70 0.80 0.74 1998 2 0.49 0.44 0.46 1846 3 0.50 0.47 0.48 1986 4 0.66 0.67 0.66 1935 accuracy 0.60 7765 macro avg 0.59 0.59 0.59 7765 weighted avg 0.59 0.60 0.59 7765 <AxesSubplot:> \ud83d\udc4e Cluster Prices, Classifier Model \u00b6 X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) . values Y = y . reshape ( - 1 , 1 ) distortions = [] inertias = [] silhouette = [] variance = [] krange = 20 for k in range ( 1 , krange ): kmeans = KMeans ( n_clusters = k ) kmeans . fit ( Y ) y_kmeans = kmeans . predict ( Y ) labels = kmeans . labels_ distortions . append ( sum ( np . min ( cdist ( Y , kmeans . cluster_centers_ , 'euclidean' ), axis = 1 )) / Y . shape [ 0 ]) inertias . append ( kmeans . inertia_ ) if k > 1 : silhouette . append ( silhouette_score ( Y , labels , metric = 'euclidean' )) variance . append ( calinski_harabasz_score ( Y , labels )) fig , [[ ax1 , ax2 ], [ ax3 , ax4 ]] = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) ax1 . plot ( range ( 1 , krange ), distortions ) ax2 . plot ( range ( 1 , krange ), inertias ) ax3 . plot ( range ( 2 , krange ), silhouette ) ax4 . plot ( range ( 2 , krange ), variance ) [<matplotlib.lines.Line2D at 0x7f211ca89100>] kmeans = KMeans ( n_clusters = 5 ) kmeans . fit ( Y ) y_kmeans = kmeans . predict ( Y ) labels = kmeans . labels_ ks = kmeans . cluster_centers_ ks = ks . flatten () ks = np . sort ( ks ) ks array([ 87.29374822, 230.74992332, 647.13125 , 2728.375 , 8749.75 ]) edges = ( np . diff ( ks ) / 2 + ks [: - 1 ]) . astype ( int ) bins = [] for idx , edge in enumerate ( edges ): if idx == 0 : bins . append ( f \"0- { edge } \" ) elif idx < len ( edges ): bins . append ( f \" { edges [ idx - 1 ] } - { edge } \" ) bins . append ( f \" { edge } +\" ) bins ['0-159', '159-438', '438-1687', '1687-5739', '5739+'] pd . DataFrame ( labels ) . value_counts ( sort = False ) 0 9651 1 8 2 961 3 28153 4 48 dtype: int64 y = labels X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38821, 233) (38821,) Train/Eval \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 1.00 Test Acc: 0.81 y_pred = model . predict ( X_train ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) precision recall f1-score support 0 1.00 1.00 1.00 7687 1 1.00 1.00 1.00 7 2 1.00 1.00 1.00 762 3 1.00 1.00 1.00 22561 4 1.00 1.00 1.00 39 accuracy 1.00 31056 macro avg 1.00 1.00 1.00 31056 weighted avg 1.00 1.00 1.00 31056 <AxesSubplot:> y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) precision recall f1-score support 0 0.64 0.60 0.62 1964 1 0.00 0.00 0.00 1 2 0.71 0.14 0.23 199 3 0.86 0.91 0.88 5592 4 0.67 0.22 0.33 9 accuracy 0.81 7765 macro avg 0.58 0.37 0.41 7765 weighted avg 0.80 0.81 0.80 7765 <AxesSubplot:> \ud83c\udf1f PCA, Feature Reduction \u00b6 The results in Bin Price, Classifier Model indicate overfitting. Let's see if we can reduce the cardinality of our One Hot features X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) . values Y = y . reshape ( - 1 , 1 ) bins = 10 quantiles = bins + 1 labels = y . copy () for idx , quant in enumerate ( np . linspace ( 0 , 1 , quantiles )): if idx == 0 : prev_quant = quant continue if idx == 1 : labels [ labels <= np . quantile ( y , quant )] = 1 elif quant < 1 : labels [( labels > np . quantile ( y , prev_quant )) & ( labels <= np . quantile ( y , quant ))] = idx else : labels [( labels > np . quantile ( y , prev_quant ))] = idx prev_quant = quant print ([ np . quantile ( y , quant ) for quant in np . linspace ( 0 , 1 , quantiles )]) y = labels X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () pca = PCA ( n_components = 3 ) X_pca = pca . fit_transform ( X_enc ) print ( pca . explained_variance_ ) scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_pca , X_std_num )) print ( X_std . shape ) print ( y . shape ) [0.0, 49.0, 60.0, 75.0, 90.0, 101.0, 125.0, 150.0, 190.0, 250.0, 10000.0] [0.52595687 0.42901998 0.16673031] (38821, 10) (38821,) Train/Eval \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) y_pred = model . predict ( X_train ) print ( \"Training Confusion Matrix\" ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) plt . show () y_pred = model . predict ( X_test ) print ( \"Testing Confusion Matrix\" ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) Train Acc: 1.00 Test Acc: 0.32 Training Confusion Matrix precision recall f1-score support 1 1.00 1.00 1.00 3148 2 1.00 1.00 1.00 3241 3 1.00 1.00 1.00 3458 4 1.00 1.00 1.00 3075 5 1.00 1.00 1.00 2658 6 1.00 1.00 1.00 3198 7 1.00 1.00 1.00 3426 8 1.00 1.00 1.00 2759 9 1.00 1.00 1.00 3274 10 1.00 1.00 1.00 2819 accuracy 1.00 31056 macro avg 1.00 1.00 1.00 31056 weighted avg 1.00 1.00 1.00 31056 Testing Confusion Matrix precision recall f1-score support 1 0.51 0.56 0.54 840 2 0.36 0.41 0.39 794 3 0.30 0.31 0.30 824 4 0.26 0.23 0.25 782 5 0.17 0.15 0.16 604 6 0.24 0.23 0.24 813 7 0.25 0.26 0.25 842 8 0.25 0.19 0.22 736 9 0.32 0.35 0.33 818 10 0.46 0.48 0.47 712 accuracy 0.32 7765 macro avg 0.31 0.32 0.31 7765 weighted avg 0.32 0.32 0.32 7765 <AxesSubplot:> \ud83d\udd2e Last Review Date, PCA for Outlier Removal, Impute \u00b6 If I wanted to spend more time on this: remove outliers with PCA the issue with our outlier removal previously, is that we are conditioning on y . As we can't know y in a production setting, this makes our model suspetible to underdetecting true, high-y value signal removing outliers based on the input, X is prefered, and we might try this with PCA turn last_review_date into a number (counts of days) this would change a string (to be one hot encoded) column to a number column (avoids curse of dimensionality) impute missing values we're currently omitting about 20% of our data points, it may give us a boost to impute or otherwise estimate these missing values Hyperparameter Optimization \u00b6 Round 1 \u00b6 We'll start with a broad, shallow search (few trees) param_grid = { 'bootstrap' : [ True , False ], 'criterion' : [ 'gini' , 'entropy' ], 'min_samples_split' : [ 2 , 4 , 6 ], 'min_samples_leaf' : [ 1 , 3 , 5 ], 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'class_weight' : [ 'balanced' , 'balanced_subsample' , None ], 'n_estimators' : [ 1 , 5 ]} grid = GridSearchCV ( RandomForestClassifier (), param_grid , cv = 5 , n_jobs =- 1 , verbose = 3 ) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) print ( grid . best_params_ ) {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 5} model = grid . best_estimator_ model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) y_pred = model . predict ( X_train ) print ( \"Training Confusion Matrix\" ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) plt . show () y_pred = model . predict ( X_test ) print ( \"Testing Confusion Matrix\" ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) Train Acc: 0.79 Test Acc: 0.30 Training Confusion Matrix precision recall f1-score support 1 0.80 0.89 0.84 3148 2 0.79 0.83 0.81 3241 3 0.82 0.74 0.78 3458 4 0.77 0.76 0.77 3075 5 0.74 0.79 0.77 2658 6 0.79 0.74 0.76 3198 7 0.81 0.71 0.76 3426 8 0.77 0.83 0.80 2759 9 0.81 0.77 0.79 3274 10 0.80 0.86 0.83 2819 accuracy 0.79 31056 macro avg 0.79 0.79 0.79 31056 weighted avg 0.79 0.79 0.79 31056 Testing Confusion Matrix precision recall f1-score support 1 0.49 0.58 0.53 840 2 0.33 0.37 0.35 794 3 0.28 0.24 0.26 824 4 0.23 0.21 0.22 782 5 0.17 0.21 0.18 604 6 0.24 0.20 0.22 813 7 0.23 0.20 0.21 842 8 0.23 0.22 0.22 736 9 0.29 0.27 0.28 818 10 0.41 0.47 0.44 712 accuracy 0.30 7765 macro avg 0.29 0.30 0.29 7765 weighted avg 0.29 0.30 0.29 7765 <AxesSubplot:> gs_results = pd . DataFrame ( grid . cv_results_ ) params = list ( gs_results . columns [ gs_results . columns . str . contains ( 'param' )] . values ) params . pop ( - 1 ) display ( gs_results . head ()) print ( params ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_fit_time std_fit_time mean_score_time std_score_time param_bootstrap param_class_weight param_criterion param_max_features param_min_samples_leaf param_min_samples_split ... split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score split5_test_score split6_test_score mean_test_score std_test_score rank_test_score 0 0.118039 0.002019 0.004001 0.000212 True balanced gini auto 1 2 ... 0.230787 0.235745 0.239351 0.232139 0.247520 0.238278 0.222272 0.235156 0.007317 610 1 0.518118 0.006517 0.012629 0.000356 True balanced gini auto 1 2 ... 0.262114 0.268425 0.253324 0.281271 0.273670 0.272543 0.259693 0.267291 0.008820 309 2 0.113497 0.004069 0.003702 0.000130 True balanced gini auto 1 4 ... 0.231463 0.231012 0.236196 0.249268 0.243012 0.240532 0.229937 0.237345 0.006712 567 3 0.482682 0.004648 0.012108 0.000515 True balanced gini auto 1 4 ... 0.268875 0.277890 0.272481 0.275186 0.275248 0.274121 0.272543 0.273764 0.002630 269 4 0.104504 0.004581 0.003597 0.000109 True balanced gini auto 1 6 ... 0.232590 0.227406 0.237773 0.246338 0.249549 0.232191 0.238954 0.237829 0.007357 559 5 rows \u00d7 22 columns ['param_bootstrap', 'param_class_weight', 'param_criterion', 'param_max_features', 'param_min_samples_leaf', 'param_min_samples_split', 'param_n_estimators'] target = 'mean_test_score' moodsdf = pd . DataFrame () for col in params : for truff in gs_results [ col ] . unique (): try : group = gs_results . loc [ gs_results [ col ] == truff ][ target ] pop = gs_results . loc [ ~ ( gs_results [ col ] == truff )][ target ] stat , p , m , table = stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , table ]) . T ]) except : print ( col , truff ) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'table' ] moodsdf [ 'p_value' ] = moodsdf [ 'p_value' ] . astype ( float ) print ( moodsdf . shape ) confidence_level = 0.05 moodsdf = moodsdf . loc [( moodsdf [ 'p_value' ] < confidence_level )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( \"Clearing high p-value...\" ) print ( moodsdf . shape ) param_class_weight None (17, 9) Clearing high p-value... (2, 9) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 0 param_n_estimators 1 644.006173 4.494276e-142 0.260964 0.243444 0.243093 324 [[0, 324], [324, 0]] 1 param_n_estimators 5 644.006173 4.494276e-142 0.260964 0.280889 0.281234 324 [[324, 0], [0, 324]] for param in params : sns . boxplot ( x = gs_results [ param ], y = gs_results [ target ]) plt . show () Round 2 \u00b6 Let's take those best parameters and dig a little deaper print ( grid . best_params_ ) {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 5} param_grid = { 'bootstrap' : [ True , False ], 'criterion' : [ 'gini' , 'entropy' ]} grid = GridSearchCV ( RandomForestClassifier ( min_samples_leaf = 5 , min_samples_split = 2 , max_features = 'sqrt' , class_weight = 'balanced' , n_estimators = 100 ), param_grid , cv = 5 , n_jobs =- 1 , verbose = 2 ) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) Fitting 5 folds for each of 4 candidates, totalling 20 fits {'bootstrap': False, 'criterion': 'entropy'} gs_results2 = pd . DataFrame ( grid . cv_results_ ) params = list ( gs_results2 . columns [ gs_results2 . columns . str . contains ( 'param' )] . values ) params . pop ( - 1 ) display ( gs_results2 . head ()) print ( params ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_fit_time std_fit_time mean_score_time std_score_time param_bootstrap param_criterion params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score 0 20.133523 0.372951 0.581618 0.020547 True gini {'bootstrap': True, 'criterion': 'gini'} 0.306825 0.305909 0.309612 0.314281 0.307036 0.308733 0.003035 2 1 52.458941 0.532249 0.603756 0.138301 True entropy {'bootstrap': True, 'criterion': 'entropy'} 0.311494 0.297859 0.314925 0.312188 0.304460 0.308185 0.006211 4 2 31.101482 0.656385 0.763153 0.090014 False gini {'bootstrap': False, 'criterion': 'gini'} 0.306665 0.303494 0.314603 0.313798 0.302689 0.308250 0.005044 3 3 53.006182 7.189164 0.337193 0.088254 False entropy {'bootstrap': False, 'criterion': 'entropy'} 0.307147 0.303333 0.317501 0.313476 0.307841 0.309860 0.005010 1 ['param_bootstrap', 'param_criterion'] for param in params : sns . boxplot ( x = gs_results [ param ], y = gs_results [ target ]) plt . show () Round 3 \u00b6 And now tune model complexity # Cell for Exercise 2 r2 = [] for n_estimators in range ( 10 , 100 , 10 ): model = RandomForestClassifier ( n_estimators = n_estimators , bootstrap = False , criterion = 'entropy' , min_samples_leaf = 5 , min_samples_split = 2 , max_features = 'sqrt' , class_weight = 'balanced' ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) r2 . append ([ r2_score ( y_train , model . predict ( X_train )), r2_score ( y_test , model . predict ( X_test ))]) score = np . array ( r2 ) score1 = score [:, 0 ] score2 = score [:, 1 ] fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . plot ( range ( 10 , 100 , 10 ), score1 , ls = '' , marker = '.' , color = 'blue' , label = 'Train' ) ax . plot ( range ( 10 , 100 , 10 ), score2 , ls = '' , marker = 'o' , color = 'red' , label = 'Test' ) ax . set_title ( \"Scores with Increasing Model Complexity\" ) ax . set_xlabel ( \"Trees in the Forest\" ) ax . set_ylabel ( \"$R^2$\" ) ax . legend () <matplotlib.legend.Legend at 0x7f20e6d99a00> model = grid . best_estimator_ model . n_estimators = 80 model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) y_pred = model . predict ( X_train ) print ( \"Training Confusion Matrix\" ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) plt . show () y_pred = model . predict ( X_test ) print ( \"Testing Confusion Matrix\" ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) Train Acc: 0.90 Test Acc: 0.32 Training Confusion Matrix precision recall f1-score support 1 0.86 0.95 0.90 3148 2 0.88 0.91 0.90 3241 3 0.92 0.86 0.89 3458 4 0.90 0.88 0.89 3075 5 0.87 0.91 0.89 2658 6 0.91 0.86 0.89 3198 7 0.93 0.85 0.89 3426 8 0.90 0.93 0.91 2759 9 0.91 0.89 0.90 3274 10 0.88 0.94 0.91 2819 accuracy 0.90 31056 macro avg 0.90 0.90 0.90 31056 weighted avg 0.90 0.90 0.90 31056 Testing Confusion Matrix precision recall f1-score support 1 0.51 0.60 0.55 840 2 0.34 0.38 0.36 794 3 0.30 0.26 0.28 824 4 0.27 0.24 0.26 782 5 0.17 0.21 0.19 604 6 0.25 0.22 0.23 813 7 0.25 0.21 0.23 842 8 0.25 0.24 0.24 736 9 0.33 0.30 0.31 818 10 0.44 0.54 0.48 712 accuracy 0.32 7765 macro avg 0.31 0.32 0.31 7765 weighted avg 0.31 0.32 0.32 7765 <AxesSubplot:> After all that work we don't get much lift from the random forest with default hyperparameters Conclusion \u00b6 The Final Classification Model \u00b6 Final model had an F1 score ranging from 19-55% depending on class and a total accuracy of 32% This model could be used to suggest a price band for would-be airbnb hosts in NYC; or a price estimator to assess how changes in listing attributes wil affect price. A potential pitfall could be that new airbnb hosts will not have many total reviews or high variance in the reviews per month. We can currate price signal from the available feature inputs: neighbourhood_group neighbourhood longitude / latitude room_type price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 What worked: dropping nans row-wise allowed us to keep the reviews_per_month column, which gave us an \\(R^2\\) boost of 10% converting from a regression problem to a classification problem allowed us to deal with the long, high-price tail converting one hot encoded vectors to the first principal components kept us from overfitting (although this was not important for the random forrest model) More to try: change last_review_date from datetime or str to int use PCA for outlier removal based on the input data X imput missing values for reviews_per_month to capture an additional 10,000 datapoints Additional Strategies \u00b6 Removing outliers based on the target variable, price , could also be a valid strategy. If we were to employ the model, we would have to be transparent that it should be used to predict prices in the sub $700 range, which is most of the Airbnb business in NYC anyway. At the end of the day, our decisions about model creation need to serve the business need.","title":"SOLN X1 Thinking Data"},{"location":"solutions/SOLN_X1_Thinking_Data/#data-science-foundations-extras-1-thinking-data","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Today we are going to take our newfound knowledge from the course, and practice how we can leverage data to build predictive models. We'll start with a feature engineering problem on some dummy data. This will get us thinking creatively about problem solving. We will then pivot over to an Airbnb dataset . After performing some general, exploratory data analysis, we will solve the following business case: Airbnb is interested in using historical list prices from their airbnb hosts, to make pricing suggestions to new hosts. How can we use this existing datset to assist with this price listing suggestion?","title":"Data Science Foundations  Extras 1: Thinking Data"},{"location":"solutions/SOLN_X1_Thinking_Data/#prepare-environment-and-import-data","text":"back to top # basic packages import pandas as pd import numpy as np import random import copy # visualization packages import matplotlib.pyplot as plt import plotly.express as px import seaborn as sns ; sns . set () import graphviz # stats packages import scipy.stats as stats from scipy.spatial.distance import cdist import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.outliers_influence import variance_inflation_factor # sklearn preprocessing from sklearn.preprocessing import OneHotEncoder , StandardScaler , PolynomialFeatures from sklearn.decomposition import PCA from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline # sklearn modeling from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , AdaBoostClassifier , GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans # sklearn evaluation from sklearn.metrics import mean_squared_error , r2_score , accuracy_score , silhouette_score , calinski_harabasz_score , classification_report , confusion_matrix from sklearn.model_selection import GridSearchCV , cross_val_score","title":"Prepare Environment and Import Data"},{"location":"solutions/SOLN_X1_Thinking_Data/#warm-up","text":"Add aditional feature(s) to X to predict y with a model limited to a linear classification boundary from sklearn.datasets import make_circles X , y = make_circles ( random_state = 42 , noise = .01 ) relabel = dict ( zip ([ 0 , 1 , 2 , 3 ],[ 0 , 1 , 0 , 1 ])) y = np . vectorize ( relabel . get )( y ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' ) <matplotlib.collections.PathCollection at 0x7f211fc48550> X2 = ( X ** 2 ) . sum ( axis = 1 ) X_ = np . hstack (( X , X2 . reshape ( - 1 , 1 ))) We can separate: px . scatter_3d ( x = X_ [:, 0 ], y = X_ [:, 1 ], z = X_ [:, 2 ], color = y ) and now predict model = LogisticRegression () model . fit ( X_ , y ) y_pred = model . predict ( X_ ) r2_score ( y , y_pred ) 1.0","title":"Warm Up"},{"location":"solutions/SOLN_X1_Thinking_Data/#build-a-baseline","text":"","title":"Build a Baseline"},{"location":"solutions/SOLN_X1_Thinking_Data/#exploratory-data-analysis","text":"which columns are numerical, string; which contain nans/nulls; what is the VIF between features airbnb = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/datasets/main/datasets/airbnb/AB_NYC_2019.csv\" ) airbnb . shape (48895, 16) airbnb . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 0 2539 Clean & quiet apt home by the park 2787 John Brooklyn Kensington 40.64749 -73.97237 Private room 149 1 9 2018-10-19 0.21 6 365 1 2595 Skylit Midtown Castle 2845 Jennifer Manhattan Midtown 40.75362 -73.98377 Entire home/apt 225 1 45 2019-05-21 0.38 2 355 2 3647 THE VILLAGE OF HARLEM....NEW YORK ! 4632 Elisabeth Manhattan Harlem 40.80902 -73.94190 Private room 150 3 0 NaN NaN 1 365 3 3831 Cozy Entire Floor of Brownstone 4869 LisaRoxanne Brooklyn Clinton Hill 40.68514 -73.95976 Entire home/apt 89 1 270 2019-07-05 4.64 1 194 4 5022 Entire Apt: Spacious Studio/Loft by central park 7192 Laura Manhattan East Harlem 40.79851 -73.94399 Entire home/apt 80 10 9 2018-11-19 0.10 1 0 airbnb . dtypes id int64 name object host_id int64 host_name object neighbourhood_group object neighbourhood object latitude float64 longitude float64 room_type object price int64 minimum_nights int64 number_of_reviews int64 last_review object reviews_per_month float64 calculated_host_listings_count int64 availability_365 int64 dtype: object airbnb . isnull () . sum ( axis = 0 ) id 0 name 16 host_id 0 host_name 21 neighbourhood_group 0 neighbourhood 0 latitude 0 longitude 0 room_type 0 price 0 minimum_nights 0 number_of_reviews 0 last_review 10052 reviews_per_month 10052 calculated_host_listings_count 0 availability_365 0 dtype: int64 airbnb . nunique () id 48895 name 47905 host_id 37457 host_name 11452 neighbourhood_group 5 neighbourhood 221 latitude 19048 longitude 14718 room_type 3 price 674 minimum_nights 109 number_of_reviews 394 last_review 1764 reviews_per_month 937 calculated_host_listings_count 47 availability_365 366 dtype: int64 plt . figure ( figsize = ( 10 , 6 )) sns . scatterplot ( x = airbnb . longitude , y = airbnb . latitude , hue = airbnb . neighbourhood_group ) plt . ioff () <matplotlib.pyplot._IoffContext at 0x7f211d15bb20> X = airbnb . copy () reviews_per_month has some 'nans' X_num = X . select_dtypes ( exclude = 'object' ) X_num . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 48890 36484665 8232441 40.67853 -73.94995 70 2 0 NaN 2 9 48891 36485057 6570630 40.70184 -73.93317 40 4 0 NaN 2 36 48892 36485431 23492952 40.81475 -73.94867 115 10 0 NaN 1 27 48893 36485609 30985759 40.75751 -73.99112 55 1 0 NaN 6 2 48894 36487245 68119814 40.76404 -73.98933 90 7 0 NaN 1 23 X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 149 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 225 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 150 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 89 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 80 10 9 0.10 1 0 X_num . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 count 4.889500e+04 4.889500e+04 48895.000000 48895.000000 48895.000000 48895.000000 48895.000000 38843.000000 48895.000000 48895.000000 mean 1.901714e+07 6.762001e+07 40.728949 -73.952170 152.720687 7.029962 23.274466 1.373221 7.143982 112.781327 std 1.098311e+07 7.861097e+07 0.054530 0.046157 240.154170 20.510550 44.550582 1.680442 32.952519 131.622289 min 2.539000e+03 2.438000e+03 40.499790 -74.244420 0.000000 1.000000 0.000000 0.010000 1.000000 0.000000 25% 9.471945e+06 7.822033e+06 40.690100 -73.983070 69.000000 1.000000 1.000000 0.190000 1.000000 0.000000 50% 1.967728e+07 3.079382e+07 40.723070 -73.955680 106.000000 3.000000 5.000000 0.720000 1.000000 45.000000 75% 2.915218e+07 1.074344e+08 40.763115 -73.936275 175.000000 5.000000 24.000000 2.020000 2.000000 227.000000 max 3.648724e+07 2.743213e+08 40.913060 -73.712990 10000.000000 1250.000000 629.000000 58.500000 327.000000 365.000000 X . dropna ( inplace = True ) X_num = X . select_dtypes ( exclude = 'object' ) vif = [ variance_inflation_factor ( X_num . values , i ) for i in range ( X_num . shape [ 1 ])] pd . DataFrame ( vif , index = X_num . columns ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 id 2.180074 host_id 2.836905 latitude 0.775769 longitude 425502.981678 price 1.012423 minimum_nights 1.039144 number_of_reviews 2.348200 reviews_per_month 2.314318 calculated_host_listings_count 1.067389 availability_365 1.139558 X_num . drop ( 'longitude' , axis = 1 , inplace = True ) /home/wbeckner/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy X_num .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 149 1 9 0.21 6 365 1 2595 2845 40.75362 225 1 45 0.38 2 355 3 3831 4869 40.68514 89 1 270 4.64 1 194 4 5022 7192 40.79851 80 10 9 0.10 1 0 5 5099 7322 40.74767 200 3 74 0.59 1 129 ... ... ... ... ... ... ... ... ... ... 48782 36425863 83554966 40.78099 129 1 1 1.00 1 147 48790 36427429 257683179 40.75104 45 1 1 1.00 6 339 48799 36438336 211644523 40.54179 235 1 1 1.00 1 87 48805 36442252 273841667 40.80787 100 1 2 2.00 1 40 48852 36455809 74162901 40.69805 30 1 1 1.00 1 1 38821 rows \u00d7 9 columns vif = [ variance_inflation_factor ( X_num . values , i ) for i in range ( X_num . shape [ 1 ])] pd . DataFrame ( vif , index = X_num . columns ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 id 8.424770 host_id 2.827543 latitude 7.297302 price 1.538975 minimum_nights 1.157468 number_of_reviews 3.215893 reviews_per_month 3.858006 calculated_host_listings_count 1.106414 availability_365 2.035592","title":"Exploratory Data Analysis"},{"location":"solutions/SOLN_X1_Thinking_Data/#feature-engineering","text":"Say we want to predict pricing, using an ML model. How would you build your features? Based on the number of null values, what would you do with the last_review and reviews_per_month column? X = airbnb . copy () y = X . pop ( 'price' ) X_cat = X . select_dtypes ( include = 'object' ) X_cat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name host_name neighbourhood_group neighbourhood room_type last_review 0 Clean & quiet apt home by the park John Brooklyn Kensington Private room 2018-10-19 1 Skylit Midtown Castle Jennifer Manhattan Midtown Entire home/apt 2019-05-21 2 THE VILLAGE OF HARLEM....NEW YORK ! Elisabeth Manhattan Harlem Private room NaN 3 Cozy Entire Floor of Brownstone LisaRoxanne Brooklyn Clinton Hill Entire home/apt 2019-07-05 4 Entire Apt: Spacious Studio/Loft by central park Laura Manhattan East Harlem Entire home/apt 2018-11-19 based on the number of unique columns, we may want to remove name , host_name , and last_review X_cat . nunique () name 47905 host_name 11452 neighbourhood_group 5 neighbourhood 221 room_type 3 last_review 1764 dtype: int64 X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () And now we deal with the numerical columns X_num = X . select_dtypes ( exclude = 'object' ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id host_id latitude longitude minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 2539 2787 40.64749 -73.97237 1 9 0.21 6 365 1 2595 2845 40.75362 -73.98377 1 45 0.38 2 355 2 3647 4632 40.80902 -73.94190 3 0 NaN 1 365 3 3831 4869 40.68514 -73.95976 1 270 4.64 1 194 4 5022 7192 40.79851 -73.94399 10 9 0.10 1 0 both id and host_id will be highly cardinal without telling us much about the behavior of unseen data. We should remove them. We'll also drop the columns with nans for now X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_num = X_num . dropna ( axis = 1 ) X_enc_df = pd . DataFrame ( X_enc , columns = enc . get_feature_names_out ()) X_feat = pd . concat (( X_enc_df , X_num ), axis = 1 ) X_feat . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } neighbourhood_group_Bronx neighbourhood_group_Brooklyn neighbourhood_group_Manhattan neighbourhood_group_Queens neighbourhood_group_Staten Island neighbourhood_Allerton neighbourhood_Arden Heights neighbourhood_Arrochar neighbourhood_Arverne neighbourhood_Astoria ... neighbourhood_Woodside room_type_Entire home/apt room_type_Private room room_type_Shared room latitude longitude minimum_nights number_of_reviews calculated_host_listings_count availability_365 0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 1.0 0.0 40.64749 -73.97237 1 9 6 365 1 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 40.75362 -73.98377 1 45 2 355 2 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 1.0 0.0 40.80902 -73.94190 3 0 1 365 3 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 40.68514 -73.95976 1 270 1 194 4 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 40.79851 -73.94399 10 9 1 0 5 rows \u00d7 235 columns","title":"Feature Engineering"},{"location":"solutions/SOLN_X1_Thinking_Data/#feature-transformation","text":"What features do you think will cause the most problems if untransformed? Scale and Center all but the target variable, price scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (48895, 235) (48895,)","title":"Feature Transformation"},{"location":"solutions/SOLN_X1_Thinking_Data/#model-baseline","text":"X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = LinearRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) r2_score ( y_train , model . predict ( X_train )) 0.11264603204210533 r2_score ( y_test , y_pred ) -1.563294115330747e+17 model = RandomForestRegressor () model . fit ( X_train , y_train ) r2_score ( y_train , model . predict ( X_train )) 0.8597830223730762 r2_score ( y_test , model . predict ( X_test )) 0.10233675407266163 both of these results from the LinearRegression and RandomForest models indicate overfitting","title":"Model Baseline"},{"location":"solutions/SOLN_X1_Thinking_Data/#back-to-feature-engineering","text":"\ud83c\udf1f - keep this feature \ud83d\udca1 - interesting behavior discovered \ud83d\udc4e - don't keep this feature \ud83d\udd2e - try for next time To try: drop nan rows not columns remove outliers (filter by group) PCA of one hot encoded vectors (will help with linear model) transform 'last review date' (str) into 'days since last review' (number)","title":"Back to Feature Engineering"},{"location":"solutions/SOLN_X1_Thinking_Data/#nans-drop-row-wise","text":"X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_num . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } latitude longitude minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 0 40.64749 -73.97237 1 9 0.21 6 365 1 40.75362 -73.98377 1 45 0.38 2 355 3 40.68514 -73.95976 1 270 4.64 1 194 4 40.79851 -73.94399 10 9 0.10 1 0 5 40.74767 -73.97500 3 74 0.59 1 129 X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38821, 233) (38821,)","title":"\ud83c\udf1f NaNs - Drop Row-wise"},{"location":"solutions/SOLN_X1_Thinking_Data/#traineval","text":"X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestRegressor ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train R2: { r2_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test R2: { r2_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train R2: 0.88 Test R2: 0.23","title":"Train/Eval"},{"location":"solutions/SOLN_X1_Thinking_Data/#outliers-by-borough","text":"X = airbnb . copy () X = X . dropna ( axis = 0 ) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = X [ 'neighbourhood_group' ], y = X [ 'price' ], ax = ax ) # ax.set_ylim(0, 1500) <AxesSubplot:xlabel='neighbourhood_group', ylabel='price'> fig , ax = plt . subplots ( figsize = ( 15 , 10 )) sns . kdeplot ( hue = X [ 'neighbourhood_group' ], x = X [ 'price' ], ax = ax ) <AxesSubplot:xlabel='price', ylabel='Density'> X = X . loc [ X . groupby ( 'neighbourhood_group' ) . apply ( lambda x : x [ 'price' ] < ( x [ 'price' ] . std () * 3 )) . unstack ( level = 0 ) . any ( axis = 1 )] fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 20 , 10 )) sns . boxplot ( x = X [ 'neighbourhood_group' ], y = X [ 'price' ], ax = ax ) sns . kdeplot ( hue = X [ 'neighbourhood_group' ], x = X [ 'price' ], ax = ax_ ) <AxesSubplot:xlabel='price', ylabel='Density'> y = X . pop ( 'price' ) X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38309, 232) (38309,)","title":"\ud83d\udca1 Outliers - by Borough"},{"location":"solutions/SOLN_X1_Thinking_Data/#traineval_1","text":"X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestRegressor ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train R2: { r2_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test R2: { r2_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train R2: 0.93 Test R2: 0.52 fig , ( ax , ax_ ) = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) ax . plot ( y_train , model . predict ( X_train ), ls = '' , marker = ',' ) ax_ . plot ( y_test , model . predict ( X_test ), ls = '' , marker = ',' ) [<matplotlib.lines.Line2D at 0x7f211ce29c40>]","title":"Train/Eval"},{"location":"solutions/SOLN_X1_Thinking_Data/#bin-prices-classifier-model","text":"X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) . values Y = y . reshape ( - 1 , 1 ) labels = y . copy () labels [ labels <= np . quantile ( y , .25 )] = 1 labels [( labels > np . quantile ( y , .25 )) & ( labels <= np . quantile ( y , .5 ))] = 2 labels [( labels > np . quantile ( y , .5 )) & ( labels <= np . quantile ( y , .75 ))] = 3 labels [( labels > np . quantile ( y , .75 ))] = 4 y = labels X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38821, 233) (38821,)","title":"\ud83c\udf1f Bin Prices, Classifier Model"},{"location":"solutions/SOLN_X1_Thinking_Data/#traineval_2","text":"X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 1.00 Test Acc: 0.60 y_pred = model . predict ( X_train ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) precision recall f1-score support 1 1.00 1.00 1.00 7986 2 1.00 1.00 1.00 7594 3 1.00 1.00 1.00 7878 4 1.00 1.00 1.00 7598 accuracy 1.00 31056 macro avg 1.00 1.00 1.00 31056 weighted avg 1.00 1.00 1.00 31056 <AxesSubplot:> y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) precision recall f1-score support 1 0.70 0.80 0.74 1998 2 0.49 0.44 0.46 1846 3 0.50 0.47 0.48 1986 4 0.66 0.67 0.66 1935 accuracy 0.60 7765 macro avg 0.59 0.59 0.59 7765 weighted avg 0.59 0.60 0.59 7765 <AxesSubplot:>","title":"Train/Eval"},{"location":"solutions/SOLN_X1_Thinking_Data/#cluster-prices-classifier-model","text":"X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) . values Y = y . reshape ( - 1 , 1 ) distortions = [] inertias = [] silhouette = [] variance = [] krange = 20 for k in range ( 1 , krange ): kmeans = KMeans ( n_clusters = k ) kmeans . fit ( Y ) y_kmeans = kmeans . predict ( Y ) labels = kmeans . labels_ distortions . append ( sum ( np . min ( cdist ( Y , kmeans . cluster_centers_ , 'euclidean' ), axis = 1 )) / Y . shape [ 0 ]) inertias . append ( kmeans . inertia_ ) if k > 1 : silhouette . append ( silhouette_score ( Y , labels , metric = 'euclidean' )) variance . append ( calinski_harabasz_score ( Y , labels )) fig , [[ ax1 , ax2 ], [ ax3 , ax4 ]] = plt . subplots ( 2 , 2 , figsize = ( 10 , 10 )) ax1 . plot ( range ( 1 , krange ), distortions ) ax2 . plot ( range ( 1 , krange ), inertias ) ax3 . plot ( range ( 2 , krange ), silhouette ) ax4 . plot ( range ( 2 , krange ), variance ) [<matplotlib.lines.Line2D at 0x7f211ca89100>] kmeans = KMeans ( n_clusters = 5 ) kmeans . fit ( Y ) y_kmeans = kmeans . predict ( Y ) labels = kmeans . labels_ ks = kmeans . cluster_centers_ ks = ks . flatten () ks = np . sort ( ks ) ks array([ 87.29374822, 230.74992332, 647.13125 , 2728.375 , 8749.75 ]) edges = ( np . diff ( ks ) / 2 + ks [: - 1 ]) . astype ( int ) bins = [] for idx , edge in enumerate ( edges ): if idx == 0 : bins . append ( f \"0- { edge } \" ) elif idx < len ( edges ): bins . append ( f \" { edges [ idx - 1 ] } - { edge } \" ) bins . append ( f \" { edge } +\" ) bins ['0-159', '159-438', '438-1687', '1687-5739', '5739+'] pd . DataFrame ( labels ) . value_counts ( sort = False ) 0 9651 1 8 2 961 3 28153 4 48 dtype: int64 y = labels X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_enc , X_std_num )) print ( X_std . shape ) print ( y . shape ) (38821, 233) (38821,)","title":"\ud83d\udc4e Cluster Prices, Classifier Model"},{"location":"solutions/SOLN_X1_Thinking_Data/#traineval_3","text":"X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) Train Acc: 1.00 Test Acc: 0.81 y_pred = model . predict ( X_train ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) precision recall f1-score support 0 1.00 1.00 1.00 7687 1 1.00 1.00 1.00 7 2 1.00 1.00 1.00 762 3 1.00 1.00 1.00 22561 4 1.00 1.00 1.00 39 accuracy 1.00 31056 macro avg 1.00 1.00 1.00 31056 weighted avg 1.00 1.00 1.00 31056 <AxesSubplot:> y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) precision recall f1-score support 0 0.64 0.60 0.62 1964 1 0.00 0.00 0.00 1 2 0.71 0.14 0.23 199 3 0.86 0.91 0.88 5592 4 0.67 0.22 0.33 9 accuracy 0.81 7765 macro avg 0.58 0.37 0.41 7765 weighted avg 0.80 0.81 0.80 7765 <AxesSubplot:>","title":"Train/Eval"},{"location":"solutions/SOLN_X1_Thinking_Data/#pca-feature-reduction","text":"The results in Bin Price, Classifier Model indicate overfitting. Let's see if we can reduce the cardinality of our One Hot features X = airbnb . copy () X = X . dropna ( axis = 0 ) y = X . pop ( 'price' ) . values Y = y . reshape ( - 1 , 1 ) bins = 10 quantiles = bins + 1 labels = y . copy () for idx , quant in enumerate ( np . linspace ( 0 , 1 , quantiles )): if idx == 0 : prev_quant = quant continue if idx == 1 : labels [ labels <= np . quantile ( y , quant )] = 1 elif quant < 1 : labels [( labels > np . quantile ( y , prev_quant )) & ( labels <= np . quantile ( y , quant ))] = idx else : labels [( labels > np . quantile ( y , prev_quant ))] = idx prev_quant = quant print ([ np . quantile ( y , quant ) for quant in np . linspace ( 0 , 1 , quantiles )]) y = labels X_num = X . select_dtypes ( exclude = 'object' ) X_num = X_num . drop ([ 'id' , 'host_id' ], axis = 1 ) X_cat = X . select_dtypes ( include = 'object' ) X_cat = X_cat . drop ([ 'name' , 'host_name' , 'last_review' ], axis = 1 ) enc = OneHotEncoder () X_enc = enc . fit_transform ( X_cat ) . toarray () pca = PCA ( n_components = 3 ) X_pca = pca . fit_transform ( X_enc ) print ( pca . explained_variance_ ) scaler = StandardScaler () X_std_num = scaler . fit_transform ( X_num ) X_std = np . hstack (( X_pca , X_std_num )) print ( X_std . shape ) print ( y . shape ) [0.0, 49.0, 60.0, 75.0, 90.0, 101.0, 125.0, 150.0, 190.0, 250.0, 10000.0] [0.52595687 0.42901998 0.16673031] (38821, 10) (38821,)","title":"\ud83c\udf1f PCA, Feature Reduction"},{"location":"solutions/SOLN_X1_Thinking_Data/#traineval_4","text":"X_train , X_test , y_train , y_test = train_test_split ( X_std , y , train_size = 0.8 , random_state = 42 ) model = RandomForestClassifier ( n_jobs =- 1 ) model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) y_pred = model . predict ( X_train ) print ( \"Training Confusion Matrix\" ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) plt . show () y_pred = model . predict ( X_test ) print ( \"Testing Confusion Matrix\" ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) Train Acc: 1.00 Test Acc: 0.32 Training Confusion Matrix precision recall f1-score support 1 1.00 1.00 1.00 3148 2 1.00 1.00 1.00 3241 3 1.00 1.00 1.00 3458 4 1.00 1.00 1.00 3075 5 1.00 1.00 1.00 2658 6 1.00 1.00 1.00 3198 7 1.00 1.00 1.00 3426 8 1.00 1.00 1.00 2759 9 1.00 1.00 1.00 3274 10 1.00 1.00 1.00 2819 accuracy 1.00 31056 macro avg 1.00 1.00 1.00 31056 weighted avg 1.00 1.00 1.00 31056 Testing Confusion Matrix precision recall f1-score support 1 0.51 0.56 0.54 840 2 0.36 0.41 0.39 794 3 0.30 0.31 0.30 824 4 0.26 0.23 0.25 782 5 0.17 0.15 0.16 604 6 0.24 0.23 0.24 813 7 0.25 0.26 0.25 842 8 0.25 0.19 0.22 736 9 0.32 0.35 0.33 818 10 0.46 0.48 0.47 712 accuracy 0.32 7765 macro avg 0.31 0.32 0.31 7765 weighted avg 0.32 0.32 0.32 7765 <AxesSubplot:>","title":"Train/Eval"},{"location":"solutions/SOLN_X1_Thinking_Data/#last-review-date-pca-for-outlier-removal-impute","text":"If I wanted to spend more time on this: remove outliers with PCA the issue with our outlier removal previously, is that we are conditioning on y . As we can't know y in a production setting, this makes our model suspetible to underdetecting true, high-y value signal removing outliers based on the input, X is prefered, and we might try this with PCA turn last_review_date into a number (counts of days) this would change a string (to be one hot encoded) column to a number column (avoids curse of dimensionality) impute missing values we're currently omitting about 20% of our data points, it may give us a boost to impute or otherwise estimate these missing values","title":"\ud83d\udd2e Last Review Date, PCA for Outlier Removal, Impute"},{"location":"solutions/SOLN_X1_Thinking_Data/#hyperparameter-optimization","text":"","title":"Hyperparameter Optimization"},{"location":"solutions/SOLN_X1_Thinking_Data/#round-1","text":"We'll start with a broad, shallow search (few trees) param_grid = { 'bootstrap' : [ True , False ], 'criterion' : [ 'gini' , 'entropy' ], 'min_samples_split' : [ 2 , 4 , 6 ], 'min_samples_leaf' : [ 1 , 3 , 5 ], 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'class_weight' : [ 'balanced' , 'balanced_subsample' , None ], 'n_estimators' : [ 1 , 5 ]} grid = GridSearchCV ( RandomForestClassifier (), param_grid , cv = 5 , n_jobs =- 1 , verbose = 3 ) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) print ( grid . best_params_ ) {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 5} model = grid . best_estimator_ model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) y_pred = model . predict ( X_train ) print ( \"Training Confusion Matrix\" ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) plt . show () y_pred = model . predict ( X_test ) print ( \"Testing Confusion Matrix\" ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) Train Acc: 0.79 Test Acc: 0.30 Training Confusion Matrix precision recall f1-score support 1 0.80 0.89 0.84 3148 2 0.79 0.83 0.81 3241 3 0.82 0.74 0.78 3458 4 0.77 0.76 0.77 3075 5 0.74 0.79 0.77 2658 6 0.79 0.74 0.76 3198 7 0.81 0.71 0.76 3426 8 0.77 0.83 0.80 2759 9 0.81 0.77 0.79 3274 10 0.80 0.86 0.83 2819 accuracy 0.79 31056 macro avg 0.79 0.79 0.79 31056 weighted avg 0.79 0.79 0.79 31056 Testing Confusion Matrix precision recall f1-score support 1 0.49 0.58 0.53 840 2 0.33 0.37 0.35 794 3 0.28 0.24 0.26 824 4 0.23 0.21 0.22 782 5 0.17 0.21 0.18 604 6 0.24 0.20 0.22 813 7 0.23 0.20 0.21 842 8 0.23 0.22 0.22 736 9 0.29 0.27 0.28 818 10 0.41 0.47 0.44 712 accuracy 0.30 7765 macro avg 0.29 0.30 0.29 7765 weighted avg 0.29 0.30 0.29 7765 <AxesSubplot:> gs_results = pd . DataFrame ( grid . cv_results_ ) params = list ( gs_results . columns [ gs_results . columns . str . contains ( 'param' )] . values ) params . pop ( - 1 ) display ( gs_results . head ()) print ( params ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_fit_time std_fit_time mean_score_time std_score_time param_bootstrap param_class_weight param_criterion param_max_features param_min_samples_leaf param_min_samples_split ... split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score split5_test_score split6_test_score mean_test_score std_test_score rank_test_score 0 0.118039 0.002019 0.004001 0.000212 True balanced gini auto 1 2 ... 0.230787 0.235745 0.239351 0.232139 0.247520 0.238278 0.222272 0.235156 0.007317 610 1 0.518118 0.006517 0.012629 0.000356 True balanced gini auto 1 2 ... 0.262114 0.268425 0.253324 0.281271 0.273670 0.272543 0.259693 0.267291 0.008820 309 2 0.113497 0.004069 0.003702 0.000130 True balanced gini auto 1 4 ... 0.231463 0.231012 0.236196 0.249268 0.243012 0.240532 0.229937 0.237345 0.006712 567 3 0.482682 0.004648 0.012108 0.000515 True balanced gini auto 1 4 ... 0.268875 0.277890 0.272481 0.275186 0.275248 0.274121 0.272543 0.273764 0.002630 269 4 0.104504 0.004581 0.003597 0.000109 True balanced gini auto 1 6 ... 0.232590 0.227406 0.237773 0.246338 0.249549 0.232191 0.238954 0.237829 0.007357 559 5 rows \u00d7 22 columns ['param_bootstrap', 'param_class_weight', 'param_criterion', 'param_max_features', 'param_min_samples_leaf', 'param_min_samples_split', 'param_n_estimators'] target = 'mean_test_score' moodsdf = pd . DataFrame () for col in params : for truff in gs_results [ col ] . unique (): try : group = gs_results . loc [ gs_results [ col ] == truff ][ target ] pop = gs_results . loc [ ~ ( gs_results [ col ] == truff )][ target ] stat , p , m , table = stats . median_test ( group , pop ) median = np . median ( group ) mean = np . mean ( group ) size = len ( group ) moodsdf = pd . concat ([ moodsdf , pd . DataFrame ([ col , truff , stat , p , m , mean , median , size , table ]) . T ]) except : print ( col , truff ) moodsdf . columns = [ 'descriptor' , 'group' , 'pearsons_chi_square' , 'p_value' , 'grand_median' , 'group_mean' , 'group_median' , 'size' , 'table' ] moodsdf [ 'p_value' ] = moodsdf [ 'p_value' ] . astype ( float ) print ( moodsdf . shape ) confidence_level = 0.05 moodsdf = moodsdf . loc [( moodsdf [ 'p_value' ] < confidence_level )] . sort_values ( 'group_median' ) moodsdf = moodsdf . sort_values ( 'group_median' ) . reset_index ( drop = True ) print ( \"Clearing high p-value...\" ) print ( moodsdf . shape ) param_class_weight None (17, 9) Clearing high p-value... (2, 9) moodsdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } descriptor group pearsons_chi_square p_value grand_median group_mean group_median size table 0 param_n_estimators 1 644.006173 4.494276e-142 0.260964 0.243444 0.243093 324 [[0, 324], [324, 0]] 1 param_n_estimators 5 644.006173 4.494276e-142 0.260964 0.280889 0.281234 324 [[324, 0], [0, 324]] for param in params : sns . boxplot ( x = gs_results [ param ], y = gs_results [ target ]) plt . show ()","title":"Round 1"},{"location":"solutions/SOLN_X1_Thinking_Data/#round-2","text":"Let's take those best parameters and dig a little deaper print ( grid . best_params_ ) {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 5} param_grid = { 'bootstrap' : [ True , False ], 'criterion' : [ 'gini' , 'entropy' ]} grid = GridSearchCV ( RandomForestClassifier ( min_samples_leaf = 5 , min_samples_split = 2 , max_features = 'sqrt' , class_weight = 'balanced' , n_estimators = 100 ), param_grid , cv = 5 , n_jobs =- 1 , verbose = 2 ) grid . fit ( X_train , y_train ) print ( grid . best_params_ ) Fitting 5 folds for each of 4 candidates, totalling 20 fits {'bootstrap': False, 'criterion': 'entropy'} gs_results2 = pd . DataFrame ( grid . cv_results_ ) params = list ( gs_results2 . columns [ gs_results2 . columns . str . contains ( 'param' )] . values ) params . pop ( - 1 ) display ( gs_results2 . head ()) print ( params ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_fit_time std_fit_time mean_score_time std_score_time param_bootstrap param_criterion params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score 0 20.133523 0.372951 0.581618 0.020547 True gini {'bootstrap': True, 'criterion': 'gini'} 0.306825 0.305909 0.309612 0.314281 0.307036 0.308733 0.003035 2 1 52.458941 0.532249 0.603756 0.138301 True entropy {'bootstrap': True, 'criterion': 'entropy'} 0.311494 0.297859 0.314925 0.312188 0.304460 0.308185 0.006211 4 2 31.101482 0.656385 0.763153 0.090014 False gini {'bootstrap': False, 'criterion': 'gini'} 0.306665 0.303494 0.314603 0.313798 0.302689 0.308250 0.005044 3 3 53.006182 7.189164 0.337193 0.088254 False entropy {'bootstrap': False, 'criterion': 'entropy'} 0.307147 0.303333 0.317501 0.313476 0.307841 0.309860 0.005010 1 ['param_bootstrap', 'param_criterion'] for param in params : sns . boxplot ( x = gs_results [ param ], y = gs_results [ target ]) plt . show ()","title":"Round 2"},{"location":"solutions/SOLN_X1_Thinking_Data/#round-3","text":"And now tune model complexity # Cell for Exercise 2 r2 = [] for n_estimators in range ( 10 , 100 , 10 ): model = RandomForestClassifier ( n_estimators = n_estimators , bootstrap = False , criterion = 'entropy' , min_samples_leaf = 5 , min_samples_split = 2 , max_features = 'sqrt' , class_weight = 'balanced' ) model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) r2 . append ([ r2_score ( y_train , model . predict ( X_train )), r2_score ( y_test , model . predict ( X_test ))]) score = np . array ( r2 ) score1 = score [:, 0 ] score2 = score [:, 1 ] fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . plot ( range ( 10 , 100 , 10 ), score1 , ls = '' , marker = '.' , color = 'blue' , label = 'Train' ) ax . plot ( range ( 10 , 100 , 10 ), score2 , ls = '' , marker = 'o' , color = 'red' , label = 'Test' ) ax . set_title ( \"Scores with Increasing Model Complexity\" ) ax . set_xlabel ( \"Trees in the Forest\" ) ax . set_ylabel ( \"$R^2$\" ) ax . legend () <matplotlib.legend.Legend at 0x7f20e6d99a00> model = grid . best_estimator_ model . n_estimators = 80 model . fit ( X_train , y_train ) print ( f \"Train Acc: { accuracy_score ( y_train , model . predict ( X_train )) : .2f } \" ) print ( f \"Test Acc: { accuracy_score ( y_test , model . predict ( X_test )) : .2f } \" ) y_pred = model . predict ( X_train ) print ( \"Training Confusion Matrix\" ) print ( classification_report ( y_train , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_train , y_pred ), annot = True , ax = ax ) plt . show () y_pred = model . predict ( X_test ) print ( \"Testing Confusion Matrix\" ) print ( classification_report ( y_test , y_pred , zero_division = 0 )) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 7 )) sns . heatmap ( confusion_matrix ( y_test , y_pred ), annot = True , ax = ax ) Train Acc: 0.90 Test Acc: 0.32 Training Confusion Matrix precision recall f1-score support 1 0.86 0.95 0.90 3148 2 0.88 0.91 0.90 3241 3 0.92 0.86 0.89 3458 4 0.90 0.88 0.89 3075 5 0.87 0.91 0.89 2658 6 0.91 0.86 0.89 3198 7 0.93 0.85 0.89 3426 8 0.90 0.93 0.91 2759 9 0.91 0.89 0.90 3274 10 0.88 0.94 0.91 2819 accuracy 0.90 31056 macro avg 0.90 0.90 0.90 31056 weighted avg 0.90 0.90 0.90 31056 Testing Confusion Matrix precision recall f1-score support 1 0.51 0.60 0.55 840 2 0.34 0.38 0.36 794 3 0.30 0.26 0.28 824 4 0.27 0.24 0.26 782 5 0.17 0.21 0.19 604 6 0.25 0.22 0.23 813 7 0.25 0.21 0.23 842 8 0.25 0.24 0.24 736 9 0.33 0.30 0.31 818 10 0.44 0.54 0.48 712 accuracy 0.32 7765 macro avg 0.31 0.32 0.31 7765 weighted avg 0.31 0.32 0.32 7765 <AxesSubplot:> After all that work we don't get much lift from the random forest with default hyperparameters","title":"Round 3"},{"location":"solutions/SOLN_X1_Thinking_Data/#conclusion","text":"","title":"Conclusion"},{"location":"solutions/SOLN_X1_Thinking_Data/#the-final-classification-model","text":"Final model had an F1 score ranging from 19-55% depending on class and a total accuracy of 32% This model could be used to suggest a price band for would-be airbnb hosts in NYC; or a price estimator to assess how changes in listing attributes wil affect price. A potential pitfall could be that new airbnb hosts will not have many total reviews or high variance in the reviews per month. We can currate price signal from the available feature inputs: neighbourhood_group neighbourhood longitude / latitude room_type price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 What worked: dropping nans row-wise allowed us to keep the reviews_per_month column, which gave us an \\(R^2\\) boost of 10% converting from a regression problem to a classification problem allowed us to deal with the long, high-price tail converting one hot encoded vectors to the first principal components kept us from overfitting (although this was not important for the random forrest model) More to try: change last_review_date from datetime or str to int use PCA for outlier removal based on the input data X imput missing values for reviews_per_month to capture an additional 10,000 datapoints","title":"The Final Classification Model"},{"location":"solutions/SOLN_X1_Thinking_Data/#additional-strategies","text":"Removing outliers based on the target variable, price , could also be a valid strategy. If we were to employ the model, we would have to be transparent that it should be used to predict prices in the sub $700 range, which is most of the Airbnb business in NYC anyway. At the end of the day, our decisions about model creation need to serve the business need.","title":"Additional Strategies"}]}